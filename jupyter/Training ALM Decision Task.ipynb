{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.DataSaver import DataSaver\n",
    "from src.DynamicSystemAnalyzer import *\n",
    "from src.PerformanceAnalyzer import *\n",
    "from src.RNN_numpy import RNN_numpy\n",
    "from src.utils import get_project_root, numpify, orthonormalize\n",
    "from src.Trainer import Trainer\n",
    "from src.RNN_torch import RNN_torch\n",
    "from src.Task import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file last updated:  2023-11-01 19:57:33.859506\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"/Users/jiayizhang/Documents/code_base/rnn-coach/\"\n",
    "task_name = \"ALM\"\n",
    "activation = \"relu\"\n",
    "config_dict = json.load(open(os.path.join(dict_path, \"data\", \"configs\", f'train_config_{task_name}_{activation}_more_trials.json'), mode=\"r\"))\n",
    "print(\"config file last updated: \", config_dict[\"last_compiled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "n_steps = config_dict[\"n_steps\"]\n",
    "task_params = config_dict[\"task_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer:\n",
    "lambda_orth = config_dict[\"lambda_orth\"]\n",
    "lambda_r = config_dict[\"lambda_r\"]\n",
    "max_iter = config_dict[\"max_iter\"]\n",
    "tol = config_dict[\"tol\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "same_batch = config_dict[\"same_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN config:\n",
    "N = config_dict[\"N\"]\n",
    "dt = config_dict[\"dt\"]\n",
    "tau = config_dict[\"tau\"]\n",
    "mask = np.array(config_dict[\"mask\"])\n",
    "input_size = config_dict[\"num_inputs\"]\n",
    "output_size = config_dict[\"num_outputs\"]\n",
    "if (activation == \"relu\"):\n",
    "    activation = lambda x: torch.maximum(x, torch.tensor(0))\n",
    "constrained = config_dict[\"constrained\"]\n",
    "# constrained = False\n",
    "sigma_inp = config_dict[\"sigma_inp\"]\n",
    "sigma_rec = config_dict[\"sigma_rec\"]\n",
    "connectivity_density_rec = config_dict[\"connectivity_density_rec\"]\n",
    "spectral_rad = config_dict[\"sr\"]\n",
    "spectral_rad = 2\n",
    "seed = config_dict[\"seed\"]\n",
    "\n",
    "rng = torch.Generator()\n",
    "if not (seed is None):\n",
    "    rng.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for RNN!\n"
     ]
    }
   ],
   "source": [
    "# creating instances:\n",
    "rnn_torch = RNN_torch(N=N, dt=dt, tau=tau, input_size=input_size,   \n",
    "                      output_size=output_size,\n",
    "                      activation=activation, constrained=constrained,\n",
    "                      sigma_inp=sigma_inp, sigma_rec=sigma_rec,\n",
    "                      connectivity_density_rec=connectivity_density_rec,\n",
    "                      spectral_rad=spectral_rad,\n",
    "                      random_generator=rng, device=\"cpu\")\n",
    "task = eval(\"Task\" + task_name)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_torch.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "trainer = Trainer(RNN=rnn_torch, Task=task,\n",
    "                  max_iter=max_iter, tol=tol,\n",
    "                  optimizer=optimizer, criterion=criterion,\n",
    "                  lambda_orth=lambda_orth, lambda_r=lambda_r)\n",
    "datasaver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17660ee50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect start eigenvalue distribution for the RNN\n",
    "w, v = torch.linalg.eig(rnn_torch.W_rec)\n",
    "rad = torch.absolute(w).detach().numpy()\n",
    "plt.scatter(range(len(rad)), rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize inputs and targets\n",
    "\n",
    "inputs, targets = task.get_batch()\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "channel_names = [\"Left\", \"Right\", \"Go\"]\n",
    "\n",
    "for i in range(input_size):  # left, right, go\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(inputs[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Input Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(targets[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Output Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0786)\n",
      "iteration 0, train loss: \u001b[92m2.01936\u001b[0m, validation loss: \u001b[92m0.078583\u001b[0m\n",
      "tensor(0.0843)\n",
      "iteration 1, train loss: \u001b[92m0.098884\u001b[0m, validation loss: 0.084267\n",
      "tensor(0.0871)\n",
      "iteration 2, train loss: \u001b[92m0.092925\u001b[0m, validation loss: 0.08715\n",
      "tensor(0.0888)\n",
      "iteration 3, train loss: 0.093329, validation loss: 0.088806\n",
      "tensor(0.0899)\n",
      "iteration 4, train loss: 0.093908, validation loss: 0.089859\n",
      "tensor(0.0906)\n",
      "iteration 5, train loss: 0.093773, validation loss: 0.090628\n",
      "tensor(0.0912)\n",
      "iteration 6, train loss: 0.094506, validation loss: 0.091202\n",
      "tensor(0.0916)\n",
      "iteration 7, train loss: 0.095218, validation loss: 0.091626\n",
      "tensor(0.0920)\n",
      "iteration 8, train loss: 0.095438, validation loss: 0.09195\n",
      "tensor(0.0922)\n",
      "iteration 9, train loss: 0.094116, validation loss: 0.09221\n",
      "tensor(0.0924)\n",
      "iteration 10, train loss: 0.095616, validation loss: 0.092413\n",
      "tensor(0.0926)\n",
      "iteration 11, train loss: 0.095557, validation loss: 0.092571\n",
      "tensor(0.0927)\n",
      "iteration 12, train loss: 0.095633, validation loss: 0.092695\n",
      "tensor(0.0928)\n",
      "iteration 13, train loss: 0.095605, validation loss: 0.092786\n",
      "tensor(0.0928)\n",
      "iteration 14, train loss: 0.095281, validation loss: 0.09285\n",
      "tensor(0.0929)\n",
      "iteration 15, train loss: 0.095966, validation loss: 0.092893\n",
      "tensor(0.0929)\n",
      "iteration 16, train loss: 0.095197, validation loss: 0.092916\n",
      "tensor(0.0929)\n",
      "iteration 17, train loss: 0.094931, validation loss: 0.092921\n",
      "tensor(0.0929)\n",
      "iteration 18, train loss: 0.094642, validation loss: 0.092912\n",
      "tensor(0.0929)\n",
      "iteration 19, train loss: 0.095283, validation loss: 0.09288\n",
      "tensor(0.0928)\n",
      "iteration 20, train loss: 0.095763, validation loss: 0.092827\n",
      "tensor(0.0928)\n",
      "iteration 21, train loss: 0.095544, validation loss: 0.092757\n",
      "tensor(0.0927)\n",
      "iteration 22, train loss: 0.095106, validation loss: 0.092673\n",
      "tensor(0.0926)\n",
      "iteration 23, train loss: 0.095045, validation loss: 0.092575\n",
      "tensor(0.0925)\n",
      "iteration 24, train loss: 0.09488, validation loss: 0.09246\n",
      "tensor(0.0923)\n",
      "iteration 25, train loss: 0.095479, validation loss: 0.09233\n",
      "tensor(0.0922)\n",
      "iteration 26, train loss: 0.095226, validation loss: 0.092185\n",
      "tensor(0.0920)\n",
      "iteration 27, train loss: 0.094608, validation loss: 0.092027\n",
      "tensor(0.0919)\n",
      "iteration 28, train loss: 0.094818, validation loss: 0.091853\n",
      "tensor(0.0917)\n",
      "iteration 29, train loss: 0.094453, validation loss: 0.091664\n",
      "tensor(0.0915)\n",
      "iteration 30, train loss: 0.094464, validation loss: 0.091461\n",
      "tensor(0.0912)\n",
      "iteration 31, train loss: 0.094442, validation loss: 0.091244\n",
      "tensor(0.0910)\n",
      "iteration 32, train loss: 0.093869, validation loss: 0.091011\n",
      "tensor(0.0908)\n",
      "iteration 33, train loss: 0.094425, validation loss: 0.090759\n",
      "tensor(0.0905)\n",
      "iteration 34, train loss: 0.093834, validation loss: 0.09049\n",
      "tensor(0.0902)\n",
      "iteration 35, train loss: 0.093217, validation loss: 0.090202\n",
      "tensor(0.0899)\n",
      "iteration 36, train loss: 0.09377, validation loss: 0.089886\n",
      "tensor(0.0895)\n",
      "iteration 37, train loss: 0.093479, validation loss: 0.089544\n",
      "tensor(0.0892)\n",
      "iteration 38, train loss: \u001b[92m0.092457\u001b[0m, validation loss: 0.089174\n",
      "tensor(0.0888)\n",
      "iteration 39, train loss: 0.092757, validation loss: 0.088775\n",
      "tensor(0.0883)\n",
      "iteration 40, train loss: 0.093065, validation loss: 0.08835\n",
      "tensor(0.0879)\n",
      "iteration 41, train loss: 0.092515, validation loss: 0.087897\n",
      "tensor(0.0874)\n",
      "iteration 42, train loss: \u001b[92m0.091967\u001b[0m, validation loss: 0.087413\n",
      "tensor(0.0869)\n",
      "iteration 43, train loss: \u001b[92m0.090855\u001b[0m, validation loss: 0.086895\n",
      "tensor(0.0863)\n",
      "iteration 44, train loss: 0.091389, validation loss: 0.086339\n",
      "tensor(0.0857)\n",
      "iteration 45, train loss: \u001b[92m0.090631\u001b[0m, validation loss: 0.085743\n",
      "tensor(0.0851)\n",
      "iteration 46, train loss: \u001b[92m0.090434\u001b[0m, validation loss: 0.085101\n",
      "tensor(0.0844)\n",
      "iteration 47, train loss: \u001b[92m0.090202\u001b[0m, validation loss: 0.084414\n",
      "tensor(0.0837)\n",
      "iteration 48, train loss: \u001b[92m0.08945\u001b[0m, validation loss: 0.083673\n",
      "tensor(0.0829)\n",
      "iteration 49, train loss: \u001b[92m0.089176\u001b[0m, validation loss: 0.082872\n",
      "tensor(0.0820)\n",
      "iteration 50, train loss: \u001b[92m0.087987\u001b[0m, validation loss: 0.082003\n",
      "tensor(0.0811)\n",
      "iteration 51, train loss: 0.088307, validation loss: 0.081058\n",
      "tensor(0.0800)\n",
      "iteration 52, train loss: \u001b[92m0.087158\u001b[0m, validation loss: 0.080031\n",
      "tensor(0.0789)\n",
      "iteration 53, train loss: \u001b[92m0.086279\u001b[0m, validation loss: 0.078911\n",
      "tensor(0.0777)\n",
      "iteration 54, train loss: \u001b[92m0.085671\u001b[0m, validation loss: \u001b[92m0.077689\u001b[0m\n",
      "tensor(0.0763)\n",
      "iteration 55, train loss: \u001b[92m0.084013\u001b[0m, validation loss: \u001b[92m0.076346\u001b[0m\n",
      "tensor(0.0749)\n",
      "iteration 56, train loss: \u001b[92m0.083892\u001b[0m, validation loss: \u001b[92m0.07489\u001b[0m\n",
      "tensor(0.0733)\n",
      "iteration 57, train loss: \u001b[92m0.082703\u001b[0m, validation loss: \u001b[92m0.073318\u001b[0m\n",
      "tensor(0.0716)\n",
      "iteration 58, train loss: \u001b[92m0.081293\u001b[0m, validation loss: \u001b[92m0.071627\u001b[0m\n",
      "tensor(0.0698)\n",
      "iteration 59, train loss: \u001b[92m0.080002\u001b[0m, validation loss: \u001b[92m0.069817\u001b[0m\n",
      "tensor(0.0679)\n",
      "iteration 60, train loss: \u001b[92m0.077486\u001b[0m, validation loss: \u001b[92m0.06789\u001b[0m\n",
      "tensor(0.0659)\n",
      "iteration 61, train loss: \u001b[92m0.076457\u001b[0m, validation loss: \u001b[92m0.065875\u001b[0m\n",
      "tensor(0.0638)\n",
      "iteration 62, train loss: \u001b[92m0.074563\u001b[0m, validation loss: \u001b[92m0.063811\u001b[0m\n",
      "tensor(0.0618)\n",
      "iteration 63, train loss: \u001b[92m0.072884\u001b[0m, validation loss: \u001b[92m0.061762\u001b[0m\n",
      "tensor(0.0598)\n",
      "iteration 64, train loss: \u001b[92m0.07094\u001b[0m, validation loss: \u001b[92m0.059786\u001b[0m\n",
      "tensor(0.0580)\n",
      "iteration 65, train loss: \u001b[92m0.068518\u001b[0m, validation loss: \u001b[92m0.05799\u001b[0m\n",
      "tensor(0.0565)\n",
      "iteration 66, train loss: \u001b[92m0.066527\u001b[0m, validation loss: \u001b[92m0.056519\u001b[0m\n",
      "tensor(0.0556)\n",
      "iteration 67, train loss: \u001b[92m0.064953\u001b[0m, validation loss: \u001b[92m0.055551\u001b[0m\n",
      "tensor(0.0553)\n",
      "iteration 68, train loss: \u001b[92m0.06378\u001b[0m, validation loss: \u001b[92m0.055261\u001b[0m\n",
      "tensor(0.0558)\n",
      "iteration 69, train loss: \u001b[92m0.062767\u001b[0m, validation loss: 0.055762\n",
      "tensor(0.0569)\n",
      "iteration 70, train loss: 0.063057, validation loss: 0.056891\n",
      "tensor(0.0581)\n",
      "iteration 71, train loss: 0.064226, validation loss: 0.058144\n",
      "tensor(0.0589)\n",
      "iteration 72, train loss: 0.064718, validation loss: 0.058944\n",
      "tensor(0.0590)\n",
      "iteration 73, train loss: 0.065521, validation loss: 0.059037\n",
      "tensor(0.0585)\n",
      "iteration 74, train loss: 0.065708, validation loss: 0.058483\n",
      "tensor(0.0576)\n",
      "iteration 75, train loss: 0.064565, validation loss: 0.05757\n",
      "tensor(0.0566)\n",
      "iteration 76, train loss: 0.064512, validation loss: 0.056591\n",
      "tensor(0.0558)\n",
      "iteration 77, train loss: 0.062961, validation loss: 0.055759\n",
      "tensor(0.0551)\n",
      "iteration 78, train loss: 0.062876, validation loss: \u001b[92m0.055145\u001b[0m\n",
      "tensor(0.0548)\n",
      "iteration 79, train loss: \u001b[92m0.061586\u001b[0m, validation loss: \u001b[92m0.05476\u001b[0m\n",
      "tensor(0.0546)\n",
      "iteration 80, train loss: 0.061603, validation loss: \u001b[92m0.054561\u001b[0m\n",
      "tensor(0.0545)\n",
      "iteration 81, train loss: 0.062037, validation loss: \u001b[92m0.054491\u001b[0m\n",
      "tensor(0.0545)\n",
      "iteration 82, train loss: 0.061811, validation loss: 0.054491\n",
      "tensor(0.0545)\n",
      "iteration 83, train loss: \u001b[92m0.061325\u001b[0m, validation loss: 0.054521\n",
      "tensor(0.0546)\n",
      "iteration 84, train loss: 0.062323, validation loss: 0.05455\n",
      "tensor(0.0546)\n",
      "iteration 85, train loss: 0.061895, validation loss: 0.054561\n",
      "tensor(0.0545)\n",
      "iteration 86, train loss: 0.062066, validation loss: 0.054549\n",
      "tensor(0.0545)\n",
      "iteration 87, train loss: 0.061603, validation loss: 0.054512\n",
      "tensor(0.0545)\n",
      "iteration 88, train loss: 0.062238, validation loss: \u001b[92m0.054455\u001b[0m\n",
      "tensor(0.0544)\n",
      "iteration 89, train loss: 0.062136, validation loss: \u001b[92m0.054383\u001b[0m\n",
      "tensor(0.0543)\n",
      "iteration 90, train loss: 0.061459, validation loss: \u001b[92m0.054307\u001b[0m\n",
      "tensor(0.0542)\n",
      "iteration 91, train loss: 0.062185, validation loss: \u001b[92m0.054235\u001b[0m\n",
      "tensor(0.0542)\n",
      "iteration 92, train loss: \u001b[92m0.060765\u001b[0m, validation loss: \u001b[92m0.054178\u001b[0m\n",
      "tensor(0.0541)\n",
      "iteration 93, train loss: 0.060981, validation loss: \u001b[92m0.054144\u001b[0m\n",
      "tensor(0.0541)\n",
      "iteration 94, train loss: 0.060896, validation loss: \u001b[92m0.054134\u001b[0m\n",
      "tensor(0.0541)\n",
      "iteration 95, train loss: 0.060794, validation loss: 0.054147\n",
      "tensor(0.0542)\n",
      "iteration 96, train loss: \u001b[92m0.060758\u001b[0m, validation loss: 0.054176\n",
      "tensor(0.0542)\n",
      "iteration 97, train loss: 0.061418, validation loss: 0.054219\n",
      "tensor(0.0543)\n",
      "iteration 98, train loss: 0.060766, validation loss: 0.054264\n",
      "tensor(0.0543)\n",
      "iteration 99, train loss: \u001b[92m0.060345\u001b[0m, validation loss: 0.054302\n",
      "tensor(0.0543)\n",
      "iteration 100, train loss: 0.060772, validation loss: 0.054328\n",
      "tensor(0.0543)\n",
      "iteration 101, train loss: 0.060354, validation loss: 0.054336\n",
      "tensor(0.0543)\n",
      "iteration 102, train loss: 0.060611, validation loss: 0.05432\n",
      "tensor(0.0543)\n",
      "iteration 103, train loss: 0.060617, validation loss: 0.054277\n",
      "tensor(0.0542)\n",
      "iteration 104, train loss: 0.060946, validation loss: 0.05422\n",
      "tensor(0.0542)\n",
      "iteration 105, train loss: 0.060501, validation loss: 0.054158\n",
      "tensor(0.0541)\n",
      "iteration 106, train loss: 0.060619, validation loss: \u001b[92m0.054093\u001b[0m\n",
      "tensor(0.0540)\n",
      "iteration 107, train loss: 0.060506, validation loss: \u001b[92m0.054033\u001b[0m\n",
      "tensor(0.0540)\n",
      "iteration 108, train loss: \u001b[92m0.059991\u001b[0m, validation loss: \u001b[92m0.05398\u001b[0m\n",
      "tensor(0.0539)\n",
      "iteration 109, train loss: 0.060571, validation loss: \u001b[92m0.053935\u001b[0m\n",
      "tensor(0.0539)\n",
      "iteration 110, train loss: \u001b[92m0.059705\u001b[0m, validation loss: \u001b[92m0.053901\u001b[0m\n",
      "tensor(0.0539)\n",
      "iteration 111, train loss: 0.060255, validation loss: \u001b[92m0.053873\u001b[0m\n",
      "tensor(0.0539)\n",
      "iteration 112, train loss: 0.05973, validation loss: \u001b[92m0.053851\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 113, train loss: 0.060115, validation loss: \u001b[92m0.053833\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 114, train loss: 0.05972, validation loss: \u001b[92m0.053817\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 115, train loss: 0.06013, validation loss: \u001b[92m0.053803\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 116, train loss: 0.059946, validation loss: \u001b[92m0.053791\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 117, train loss: 0.059836, validation loss: \u001b[92m0.053779\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 118, train loss: 0.059985, validation loss: \u001b[92m0.053769\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 119, train loss: 0.059981, validation loss: \u001b[92m0.053759\u001b[0m\n",
      "tensor(0.0538)\n",
      "iteration 120, train loss: 0.060195, validation loss: \u001b[92m0.053751\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 121, train loss: 0.060015, validation loss: \u001b[92m0.053743\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 122, train loss: 0.059882, validation loss: \u001b[92m0.053737\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 123, train loss: \u001b[92m0.059701\u001b[0m, validation loss: \u001b[92m0.053732\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 124, train loss: \u001b[92m0.059475\u001b[0m, validation loss: \u001b[92m0.053729\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 125, train loss: \u001b[92m0.059391\u001b[0m, validation loss: \u001b[92m0.053727\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 126, train loss: 0.059695, validation loss: \u001b[92m0.053725\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 127, train loss: 0.05953, validation loss: \u001b[92m0.053721\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 128, train loss: 0.059565, validation loss: \u001b[92m0.053713\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 129, train loss: 0.059596, validation loss: \u001b[92m0.053704\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 130, train loss: \u001b[92m0.059153\u001b[0m, validation loss: \u001b[92m0.053693\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 131, train loss: 0.059241, validation loss: \u001b[92m0.05368\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 132, train loss: 0.059827, validation loss: \u001b[92m0.053666\u001b[0m\n",
      "tensor(0.0537)\n",
      "iteration 133, train loss: 0.059583, validation loss: \u001b[92m0.053652\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 134, train loss: 0.05966, validation loss: \u001b[92m0.053637\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 135, train loss: 0.059175, validation loss: \u001b[92m0.053619\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 136, train loss: 0.059437, validation loss: \u001b[92m0.053602\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 137, train loss: \u001b[92m0.058904\u001b[0m, validation loss: \u001b[92m0.053585\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 138, train loss: 0.059267, validation loss: \u001b[92m0.053569\u001b[0m\n",
      "tensor(0.0536)\n",
      "iteration 139, train loss: 0.059301, validation loss: \u001b[92m0.053554\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 140, train loss: \u001b[92m0.058784\u001b[0m, validation loss: \u001b[92m0.05354\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 141, train loss: 0.059405, validation loss: \u001b[92m0.053528\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 142, train loss: 0.059009, validation loss: \u001b[92m0.053516\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 143, train loss: \u001b[92m0.058723\u001b[0m, validation loss: \u001b[92m0.053506\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 144, train loss: 0.058858, validation loss: \u001b[92m0.053495\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 145, train loss: 0.059016, validation loss: \u001b[92m0.053485\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 146, train loss: 0.058864, validation loss: \u001b[92m0.053476\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 147, train loss: 0.059111, validation loss: \u001b[92m0.053466\u001b[0m\n",
      "tensor(0.0535)\n",
      "iteration 148, train loss: 0.05875, validation loss: \u001b[92m0.053457\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 149, train loss: 0.058956, validation loss: \u001b[92m0.053449\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 150, train loss: 0.059016, validation loss: \u001b[92m0.05344\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 151, train loss: 0.059493, validation loss: \u001b[92m0.053433\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 152, train loss: \u001b[92m0.058714\u001b[0m, validation loss: \u001b[92m0.053426\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 153, train loss: \u001b[92m0.058213\u001b[0m, validation loss: \u001b[92m0.053419\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 154, train loss: 0.058859, validation loss: \u001b[92m0.053412\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 155, train loss: 0.058632, validation loss: \u001b[92m0.053406\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 156, train loss: 0.058734, validation loss: \u001b[92m0.053399\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 157, train loss: 0.058886, validation loss: \u001b[92m0.053392\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 158, train loss: 0.05888, validation loss: \u001b[92m0.053384\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 159, train loss: 0.058977, validation loss: \u001b[92m0.053376\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 160, train loss: 0.05869, validation loss: \u001b[92m0.053368\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 161, train loss: 0.058698, validation loss: \u001b[92m0.053359\u001b[0m\n",
      "tensor(0.0534)\n",
      "iteration 162, train loss: 0.058773, validation loss: \u001b[92m0.05335\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 163, train loss: 0.058807, validation loss: \u001b[92m0.053341\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 164, train loss: 0.05883, validation loss: \u001b[92m0.053331\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 165, train loss: 0.058524, validation loss: \u001b[92m0.053321\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 166, train loss: 0.058505, validation loss: \u001b[92m0.053311\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 167, train loss: 0.058361, validation loss: \u001b[92m0.053302\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 168, train loss: \u001b[92m0.058084\u001b[0m, validation loss: \u001b[92m0.053293\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 169, train loss: 0.058556, validation loss: \u001b[92m0.053285\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 170, train loss: 0.058085, validation loss: \u001b[92m0.053277\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 171, train loss: 0.058459, validation loss: \u001b[92m0.053269\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 172, train loss: 0.058226, validation loss: \u001b[92m0.053262\u001b[0m\n",
      "tensor(0.0533)\n",
      "iteration 173, train loss: 0.058286, validation loss: \u001b[92m0.053254\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 174, train loss: 0.058544, validation loss: \u001b[92m0.053246\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 175, train loss: 0.058107, validation loss: \u001b[92m0.053239\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 176, train loss: 0.058171, validation loss: \u001b[92m0.053232\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 177, train loss: \u001b[92m0.057943\u001b[0m, validation loss: \u001b[92m0.053224\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 178, train loss: 0.058078, validation loss: \u001b[92m0.053217\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 179, train loss: \u001b[92m0.057934\u001b[0m, validation loss: \u001b[92m0.05321\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 180, train loss: 0.058304, validation loss: \u001b[92m0.053202\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 181, train loss: 0.058459, validation loss: \u001b[92m0.053195\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 182, train loss: 0.058259, validation loss: \u001b[92m0.053188\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 183, train loss: 0.057955, validation loss: \u001b[92m0.05318\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 184, train loss: 0.057997, validation loss: \u001b[92m0.053173\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 185, train loss: \u001b[92m0.057846\u001b[0m, validation loss: \u001b[92m0.053166\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 186, train loss: 0.058241, validation loss: \u001b[92m0.053159\u001b[0m\n",
      "tensor(0.0532)\n",
      "iteration 187, train loss: 0.058116, validation loss: \u001b[92m0.053152\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 188, train loss: \u001b[92m0.057766\u001b[0m, validation loss: \u001b[92m0.053145\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 189, train loss: \u001b[92m0.057641\u001b[0m, validation loss: \u001b[92m0.053138\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 190, train loss: 0.057964, validation loss: \u001b[92m0.053131\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 191, train loss: 0.058215, validation loss: \u001b[92m0.053124\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 192, train loss: 0.058051, validation loss: \u001b[92m0.053118\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 193, train loss: 0.057757, validation loss: \u001b[92m0.053111\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 194, train loss: 0.05776, validation loss: \u001b[92m0.053104\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 195, train loss: 0.057968, validation loss: \u001b[92m0.053098\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 196, train loss: 0.058042, validation loss: \u001b[92m0.053091\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 197, train loss: 0.058108, validation loss: \u001b[92m0.053085\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 198, train loss: 0.057819, validation loss: \u001b[92m0.05308\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 199, train loss: 0.057762, validation loss: \u001b[92m0.053073\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 200, train loss: 0.058096, validation loss: \u001b[92m0.053067\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 201, train loss: 0.057801, validation loss: \u001b[92m0.053061\u001b[0m\n",
      "tensor(0.0531)\n",
      "iteration 202, train loss: 0.058012, validation loss: \u001b[92m0.053055\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 203, train loss: 0.057679, validation loss: \u001b[92m0.053049\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 204, train loss: 0.057748, validation loss: \u001b[92m0.053041\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 205, train loss: 0.057872, validation loss: \u001b[92m0.053035\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 206, train loss: \u001b[92m0.057628\u001b[0m, validation loss: \u001b[92m0.053028\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 207, train loss: 0.057871, validation loss: \u001b[92m0.05302\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 208, train loss: \u001b[92m0.057513\u001b[0m, validation loss: \u001b[92m0.053013\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 209, train loss: \u001b[92m0.057485\u001b[0m, validation loss: \u001b[92m0.053006\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 210, train loss: \u001b[92m0.057176\u001b[0m, validation loss: \u001b[92m0.052998\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 211, train loss: 0.057714, validation loss: \u001b[92m0.052992\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 212, train loss: 0.057533, validation loss: \u001b[92m0.052985\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 213, train loss: 0.05747, validation loss: \u001b[92m0.052978\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 214, train loss: 0.057638, validation loss: \u001b[92m0.052972\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 215, train loss: 0.057507, validation loss: \u001b[92m0.052966\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 216, train loss: 0.057435, validation loss: \u001b[92m0.052961\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 217, train loss: 0.057372, validation loss: \u001b[92m0.052956\u001b[0m\n",
      "tensor(0.0530)\n",
      "iteration 218, train loss: 0.057593, validation loss: \u001b[92m0.052951\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 219, train loss: 0.057615, validation loss: \u001b[92m0.052946\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 220, train loss: 0.057243, validation loss: \u001b[92m0.052942\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 221, train loss: 0.057556, validation loss: \u001b[92m0.052939\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 222, train loss: 0.057509, validation loss: \u001b[92m0.052935\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 223, train loss: 0.057463, validation loss: \u001b[92m0.052931\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 224, train loss: 0.057373, validation loss: \u001b[92m0.052928\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 225, train loss: 0.057492, validation loss: \u001b[92m0.052924\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 226, train loss: \u001b[92m0.057132\u001b[0m, validation loss: \u001b[92m0.05292\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 227, train loss: \u001b[92m0.057052\u001b[0m, validation loss: \u001b[92m0.052916\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 228, train loss: 0.057506, validation loss: \u001b[92m0.052912\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 229, train loss: 0.05747, validation loss: \u001b[92m0.052908\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 230, train loss: 0.057305, validation loss: \u001b[92m0.052905\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 231, train loss: 0.057312, validation loss: \u001b[92m0.0529\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 232, train loss: 0.057106, validation loss: \u001b[92m0.052896\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 233, train loss: 0.057684, validation loss: \u001b[92m0.052891\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 234, train loss: \u001b[92m0.056871\u001b[0m, validation loss: \u001b[92m0.052886\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 235, train loss: 0.056989, validation loss: \u001b[92m0.052881\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 236, train loss: 0.057189, validation loss: \u001b[92m0.052876\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 237, train loss: 0.05718, validation loss: \u001b[92m0.052871\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 238, train loss: 0.057222, validation loss: \u001b[92m0.052866\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 239, train loss: 0.057321, validation loss: \u001b[92m0.052861\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 240, train loss: 0.0573, validation loss: \u001b[92m0.052857\u001b[0m\n",
      "tensor(0.0529)\n",
      "iteration 241, train loss: 0.057427, validation loss: \u001b[92m0.052854\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 242, train loss: 0.057251, validation loss: \u001b[92m0.05285\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 243, train loss: 0.056879, validation loss: \u001b[92m0.052846\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 244, train loss: \u001b[92m0.056843\u001b[0m, validation loss: \u001b[92m0.052842\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 245, train loss: 0.05705, validation loss: \u001b[92m0.052839\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 246, train loss: 0.05699, validation loss: \u001b[92m0.052836\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 247, train loss: 0.056933, validation loss: \u001b[92m0.052833\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 248, train loss: 0.057182, validation loss: \u001b[92m0.05283\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 249, train loss: 0.057097, validation loss: \u001b[92m0.052827\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 250, train loss: 0.056915, validation loss: \u001b[92m0.052824\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 251, train loss: 0.057053, validation loss: \u001b[92m0.052821\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 252, train loss: 0.056945, validation loss: \u001b[92m0.052817\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 253, train loss: 0.056877, validation loss: \u001b[92m0.052814\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 254, train loss: 0.057052, validation loss: \u001b[92m0.052811\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 255, train loss: 0.057022, validation loss: \u001b[92m0.052808\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 256, train loss: 0.056926, validation loss: \u001b[92m0.052806\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 257, train loss: 0.057037, validation loss: \u001b[92m0.052805\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 258, train loss: 0.056844, validation loss: \u001b[92m0.052803\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 259, train loss: \u001b[92m0.056716\u001b[0m, validation loss: \u001b[92m0.052801\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 260, train loss: 0.056785, validation loss: \u001b[92m0.0528\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 261, train loss: \u001b[92m0.056671\u001b[0m, validation loss: \u001b[92m0.052798\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 262, train loss: 0.056763, validation loss: \u001b[92m0.052795\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 263, train loss: 0.056939, validation loss: \u001b[92m0.052792\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 264, train loss: 0.056804, validation loss: \u001b[92m0.052789\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 265, train loss: 0.056981, validation loss: \u001b[92m0.052787\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 266, train loss: 0.057091, validation loss: \u001b[92m0.052784\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 267, train loss: 0.056947, validation loss: \u001b[92m0.052782\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 268, train loss: \u001b[92m0.056619\u001b[0m, validation loss: \u001b[92m0.052779\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 269, train loss: 0.056745, validation loss: \u001b[92m0.052776\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 270, train loss: 0.056813, validation loss: \u001b[92m0.052773\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 271, train loss: 0.056669, validation loss: \u001b[92m0.052769\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 272, train loss: 0.056855, validation loss: \u001b[92m0.052766\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 273, train loss: 0.056765, validation loss: \u001b[92m0.052762\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 274, train loss: 0.05707, validation loss: \u001b[92m0.05276\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 275, train loss: 0.056772, validation loss: \u001b[92m0.052759\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 276, train loss: 0.056699, validation loss: \u001b[92m0.052758\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 277, train loss: 0.056693, validation loss: 0.052759\n",
      "tensor(0.0528)\n",
      "iteration 278, train loss: 0.056836, validation loss: 0.05276\n",
      "tensor(0.0528)\n",
      "iteration 279, train loss: \u001b[92m0.056605\u001b[0m, validation loss: 0.05276\n",
      "tensor(0.0528)\n",
      "iteration 280, train loss: \u001b[92m0.056472\u001b[0m, validation loss: 0.052759\n",
      "tensor(0.0528)\n",
      "iteration 281, train loss: 0.056838, validation loss: \u001b[92m0.052758\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 282, train loss: \u001b[92m0.056419\u001b[0m, validation loss: \u001b[92m0.052757\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 283, train loss: 0.056482, validation loss: \u001b[92m0.052756\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 284, train loss: 0.056642, validation loss: \u001b[92m0.052755\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 285, train loss: \u001b[92m0.056341\u001b[0m, validation loss: \u001b[92m0.052753\u001b[0m\n",
      "tensor(0.0528)\n",
      "iteration 286, train loss: 0.056393, validation loss: \u001b[92m0.052751\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 287, train loss: 0.056436, validation loss: \u001b[92m0.052749\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 288, train loss: 0.056713, validation loss: \u001b[92m0.052748\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 289, train loss: 0.056538, validation loss: \u001b[92m0.052748\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 290, train loss: 0.056474, validation loss: \u001b[92m0.052747\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 291, train loss: \u001b[92m0.056274\u001b[0m, validation loss: \u001b[92m0.052746\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 292, train loss: 0.056554, validation loss: \u001b[92m0.052746\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 293, train loss: 0.056392, validation loss: 0.052747\n",
      "tensor(0.0527)\n",
      "iteration 294, train loss: 0.056434, validation loss: 0.052748\n",
      "tensor(0.0527)\n",
      "iteration 295, train loss: 0.056633, validation loss: 0.052747\n",
      "tensor(0.0527)\n",
      "iteration 296, train loss: 0.056349, validation loss: 0.052746\n",
      "tensor(0.0527)\n",
      "iteration 297, train loss: 0.056522, validation loss: \u001b[92m0.052745\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 298, train loss: 0.056416, validation loss: \u001b[92m0.052744\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 299, train loss: 0.05649, validation loss: \u001b[92m0.052743\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 300, train loss: 0.056349, validation loss: \u001b[92m0.052742\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 301, train loss: 0.056441, validation loss: \u001b[92m0.05274\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 302, train loss: 0.056359, validation loss: \u001b[92m0.052739\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 303, train loss: \u001b[92m0.056242\u001b[0m, validation loss: \u001b[92m0.052739\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 304, train loss: 0.056297, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 305, train loss: 0.056344, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 306, train loss: 0.05654, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 307, train loss: \u001b[92m0.05609\u001b[0m, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 308, train loss: 0.056405, validation loss: 0.052742\n",
      "tensor(0.0527)\n",
      "iteration 309, train loss: 0.05625, validation loss: 0.052744\n",
      "tensor(0.0527)\n",
      "iteration 310, train loss: 0.05642, validation loss: 0.052745\n",
      "tensor(0.0527)\n",
      "iteration 311, train loss: 0.056234, validation loss: 0.052746\n",
      "tensor(0.0527)\n",
      "iteration 312, train loss: 0.056327, validation loss: 0.052747\n",
      "tensor(0.0527)\n",
      "iteration 313, train loss: 0.056222, validation loss: 0.052747\n",
      "tensor(0.0527)\n",
      "iteration 314, train loss: 0.05671, validation loss: 0.052746\n",
      "tensor(0.0527)\n",
      "iteration 315, train loss: 0.056328, validation loss: 0.052743\n",
      "tensor(0.0527)\n",
      "iteration 316, train loss: 0.056095, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 317, train loss: 0.056201, validation loss: \u001b[92m0.052738\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 318, train loss: 0.056492, validation loss: \u001b[92m0.052736\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 319, train loss: 0.056242, validation loss: \u001b[92m0.052735\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 320, train loss: \u001b[92m0.056081\u001b[0m, validation loss: \u001b[92m0.052734\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 321, train loss: 0.056231, validation loss: \u001b[92m0.052734\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 322, train loss: 0.056153, validation loss: 0.052734\n",
      "tensor(0.0527)\n",
      "iteration 323, train loss: 0.056493, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 324, train loss: 0.056319, validation loss: 0.052737\n",
      "tensor(0.0527)\n",
      "iteration 325, train loss: 0.056413, validation loss: 0.052738\n",
      "tensor(0.0527)\n",
      "iteration 326, train loss: 0.056286, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 327, train loss: \u001b[92m0.056081\u001b[0m, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 328, train loss: 0.056357, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 329, train loss: 0.056321, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 330, train loss: \u001b[92m0.055906\u001b[0m, validation loss: 0.052738\n",
      "tensor(0.0527)\n",
      "iteration 331, train loss: 0.056063, validation loss: 0.052737\n",
      "tensor(0.0527)\n",
      "iteration 332, train loss: 0.056117, validation loss: 0.052736\n",
      "tensor(0.0527)\n",
      "iteration 333, train loss: 0.056053, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 334, train loss: 0.05609, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 335, train loss: 0.056018, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 336, train loss: 0.056303, validation loss: 0.052736\n",
      "tensor(0.0527)\n",
      "iteration 337, train loss: 0.05624, validation loss: 0.052736\n",
      "tensor(0.0527)\n",
      "iteration 338, train loss: 0.055996, validation loss: 0.052737\n",
      "tensor(0.0527)\n",
      "iteration 339, train loss: 0.055977, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 340, train loss: 0.056119, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 341, train loss: 0.056203, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 342, train loss: 0.056188, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 343, train loss: 0.056138, validation loss: 0.052738\n",
      "tensor(0.0527)\n",
      "iteration 344, train loss: 0.056018, validation loss: 0.052736\n",
      "tensor(0.0527)\n",
      "iteration 345, train loss: \u001b[92m0.055752\u001b[0m, validation loss: 0.052734\n",
      "tensor(0.0527)\n",
      "iteration 346, train loss: 0.056253, validation loss: \u001b[92m0.052731\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 347, train loss: 0.056248, validation loss: \u001b[92m0.052729\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 348, train loss: 0.056221, validation loss: \u001b[92m0.052728\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 349, train loss: 0.056038, validation loss: 0.052729\n",
      "tensor(0.0527)\n",
      "iteration 350, train loss: 0.055929, validation loss: 0.05273\n",
      "tensor(0.0527)\n",
      "iteration 351, train loss: 0.056019, validation loss: 0.052732\n",
      "tensor(0.0527)\n",
      "iteration 352, train loss: 0.056078, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 353, train loss: 0.056114, validation loss: 0.052737\n",
      "tensor(0.0527)\n",
      "iteration 354, train loss: 0.055945, validation loss: 0.052739\n",
      "tensor(0.0527)\n",
      "iteration 355, train loss: 0.055791, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 356, train loss: 0.056068, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 357, train loss: 0.055996, validation loss: 0.052741\n",
      "tensor(0.0527)\n",
      "iteration 358, train loss: \u001b[92m0.055695\u001b[0m, validation loss: 0.05274\n",
      "tensor(0.0527)\n",
      "iteration 359, train loss: 0.055943, validation loss: 0.052738\n",
      "tensor(0.0527)\n",
      "iteration 360, train loss: 0.055841, validation loss: 0.052735\n",
      "tensor(0.0527)\n",
      "iteration 361, train loss: 0.055927, validation loss: 0.052732\n",
      "tensor(0.0527)\n",
      "iteration 362, train loss: 0.055715, validation loss: 0.052729\n",
      "tensor(0.0527)\n",
      "iteration 363, train loss: 0.056187, validation loss: \u001b[92m0.052728\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 364, train loss: 0.055755, validation loss: \u001b[92m0.052728\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 365, train loss: 0.055832, validation loss: \u001b[92m0.052727\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 366, train loss: 0.055784, validation loss: \u001b[92m0.052726\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 367, train loss: 0.055752, validation loss: \u001b[92m0.052726\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 368, train loss: 0.055869, validation loss: \u001b[92m0.052725\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 369, train loss: 0.055916, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 370, train loss: 0.055724, validation loss: 0.052727\n",
      "tensor(0.0527)\n",
      "iteration 371, train loss: 0.056061, validation loss: 0.052728\n",
      "tensor(0.0527)\n",
      "iteration 372, train loss: 0.055716, validation loss: 0.05273\n",
      "tensor(0.0527)\n",
      "iteration 373, train loss: 0.056051, validation loss: 0.052732\n",
      "tensor(0.0527)\n",
      "iteration 374, train loss: 0.055866, validation loss: 0.052734\n",
      "tensor(0.0527)\n",
      "iteration 375, train loss: \u001b[92m0.055575\u001b[0m, validation loss: 0.052734\n",
      "tensor(0.0527)\n",
      "iteration 376, train loss: 0.055712, validation loss: 0.052734\n",
      "tensor(0.0527)\n",
      "iteration 377, train loss: 0.055791, validation loss: 0.052732\n",
      "tensor(0.0527)\n",
      "iteration 378, train loss: 0.055654, validation loss: 0.05273\n",
      "tensor(0.0527)\n",
      "iteration 379, train loss: 0.055634, validation loss: 0.052728\n",
      "tensor(0.0527)\n",
      "iteration 380, train loss: 0.055684, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 381, train loss: 0.055688, validation loss: \u001b[92m0.052722\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 382, train loss: 0.055854, validation loss: \u001b[92m0.052721\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 383, train loss: 0.055702, validation loss: \u001b[92m0.052719\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 384, train loss: 0.055833, validation loss: \u001b[92m0.052718\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 385, train loss: \u001b[92m0.055555\u001b[0m, validation loss: \u001b[92m0.052718\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 386, train loss: 0.055638, validation loss: 0.052719\n",
      "tensor(0.0527)\n",
      "iteration 387, train loss: 0.055725, validation loss: 0.052721\n",
      "tensor(0.0527)\n",
      "iteration 388, train loss: 0.055631, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 389, train loss: 0.055728, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 390, train loss: 0.055912, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 391, train loss: 0.055793, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 392, train loss: 0.055664, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 393, train loss: 0.055871, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 394, train loss: 0.055769, validation loss: 0.052723\n",
      "tensor(0.0527)\n",
      "iteration 395, train loss: 0.055679, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 396, train loss: \u001b[92m0.055504\u001b[0m, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 397, train loss: 0.055832, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 398, train loss: 0.055663, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 399, train loss: 0.055762, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 400, train loss: 0.055797, validation loss: 0.052723\n",
      "tensor(0.0527)\n",
      "iteration 401, train loss: 0.055599, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 402, train loss: 0.055815, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 403, train loss: 0.055615, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 404, train loss: \u001b[92m0.055438\u001b[0m, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 405, train loss: 0.055679, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 406, train loss: 0.055534, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 407, train loss: 0.055622, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 408, train loss: 0.055572, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 409, train loss: \u001b[92m0.055346\u001b[0m, validation loss: 0.052723\n",
      "tensor(0.0527)\n",
      "iteration 410, train loss: 0.055554, validation loss: 0.052723\n",
      "tensor(0.0527)\n",
      "iteration 411, train loss: 0.055665, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 412, train loss: 0.055524, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 413, train loss: 0.055775, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 414, train loss: 0.055689, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 415, train loss: 0.055646, validation loss: 0.052724\n",
      "tensor(0.0527)\n",
      "iteration 416, train loss: 0.055709, validation loss: 0.052725\n",
      "tensor(0.0527)\n",
      "iteration 417, train loss: 0.055557, validation loss: 0.052727\n",
      "tensor(0.0527)\n",
      "iteration 418, train loss: 0.055449, validation loss: 0.052729\n",
      "tensor(0.0527)\n",
      "iteration 419, train loss: 0.055412, validation loss: 0.052729\n",
      "tensor(0.0527)\n",
      "iteration 420, train loss: 0.055482, validation loss: 0.05273\n",
      "tensor(0.0527)\n",
      "iteration 421, train loss: 0.055364, validation loss: 0.052731\n",
      "tensor(0.0527)\n",
      "iteration 422, train loss: 0.055685, validation loss: 0.052729\n",
      "tensor(0.0527)\n",
      "iteration 423, train loss: 0.055571, validation loss: 0.052726\n",
      "tensor(0.0527)\n",
      "iteration 424, train loss: 0.055752, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 425, train loss: 0.055616, validation loss: 0.052718\n",
      "tensor(0.0527)\n",
      "iteration 426, train loss: 0.055408, validation loss: \u001b[92m0.052714\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 427, train loss: \u001b[92m0.055285\u001b[0m, validation loss: \u001b[92m0.052711\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 428, train loss: 0.055493, validation loss: \u001b[92m0.052709\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 429, train loss: 0.055361, validation loss: \u001b[92m0.052708\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 430, train loss: 0.055626, validation loss: \u001b[92m0.052707\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 431, train loss: 0.05552, validation loss: \u001b[92m0.052707\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 432, train loss: 0.055549, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 433, train loss: 0.055481, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 434, train loss: 0.055724, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 435, train loss: 0.055503, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 436, train loss: 0.055372, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 437, train loss: 0.055513, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 438, train loss: 0.055427, validation loss: 0.052713\n",
      "tensor(0.0527)\n",
      "iteration 439, train loss: 0.055398, validation loss: 0.052713\n",
      "tensor(0.0527)\n",
      "iteration 440, train loss: 0.055436, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 441, train loss: 0.055412, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 442, train loss: 0.055563, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 443, train loss: 0.05545, validation loss: \u001b[92m0.052706\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 444, train loss: 0.05553, validation loss: \u001b[92m0.052703\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 445, train loss: \u001b[92m0.055265\u001b[0m, validation loss: \u001b[92m0.052701\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 446, train loss: 0.055521, validation loss: \u001b[92m0.0527\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 447, train loss: 0.055329, validation loss: \u001b[92m0.052699\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 448, train loss: 0.055541, validation loss: 0.0527\n",
      "tensor(0.0527)\n",
      "iteration 449, train loss: 0.055305, validation loss: 0.0527\n",
      "tensor(0.0527)\n",
      "iteration 450, train loss: 0.055564, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 451, train loss: 0.05552, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 452, train loss: 0.055435, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 453, train loss: \u001b[92m0.055098\u001b[0m, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 454, train loss: 0.055377, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 455, train loss: 0.055533, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 456, train loss: 0.055397, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 457, train loss: 0.055616, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 458, train loss: 0.055471, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 459, train loss: 0.055186, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 460, train loss: 0.055279, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 461, train loss: 0.055333, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 462, train loss: 0.055117, validation loss: 0.052705\n",
      "tensor(0.0527)\n",
      "iteration 463, train loss: 0.055272, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 464, train loss: 0.055455, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 465, train loss: 0.055547, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 466, train loss: 0.055267, validation loss: 0.0527\n",
      "tensor(0.0527)\n",
      "iteration 467, train loss: 0.055369, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 468, train loss: 0.055251, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 469, train loss: 0.055443, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 470, train loss: 0.05539, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 471, train loss: 0.055342, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 472, train loss: 0.055508, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 473, train loss: 0.055264, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 474, train loss: 0.055284, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 475, train loss: 0.055279, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 476, train loss: 0.055364, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 477, train loss: 0.055177, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 478, train loss: 0.055485, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 479, train loss: 0.055303, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 480, train loss: 0.055343, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 481, train loss: 0.055185, validation loss: \u001b[92m0.052699\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 482, train loss: 0.055107, validation loss: \u001b[92m0.052697\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 483, train loss: 0.055269, validation loss: \u001b[92m0.052695\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 484, train loss: 0.055235, validation loss: \u001b[92m0.052693\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 485, train loss: \u001b[92m0.055044\u001b[0m, validation loss: \u001b[92m0.052693\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 486, train loss: 0.05527, validation loss: 0.052694\n",
      "tensor(0.0527)\n",
      "iteration 487, train loss: \u001b[92m0.054998\u001b[0m, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 488, train loss: 0.055189, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 489, train loss: 0.055243, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 490, train loss: 0.055317, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 491, train loss: 0.055243, validation loss: 0.052714\n",
      "tensor(0.0527)\n",
      "iteration 492, train loss: 0.055254, validation loss: 0.052717\n",
      "tensor(0.0527)\n",
      "iteration 493, train loss: 0.055305, validation loss: 0.05272\n",
      "tensor(0.0527)\n",
      "iteration 494, train loss: 0.055176, validation loss: 0.05272\n",
      "tensor(0.0527)\n",
      "iteration 495, train loss: 0.055233, validation loss: 0.052718\n",
      "tensor(0.0527)\n",
      "iteration 496, train loss: 0.055186, validation loss: 0.052714\n",
      "tensor(0.0527)\n",
      "iteration 497, train loss: 0.055199, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 498, train loss: 0.055267, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 499, train loss: 0.0552, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 500, train loss: 0.05519, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 501, train loss: 0.05513, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 502, train loss: 0.055269, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 503, train loss: 0.055088, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 504, train loss: 0.055293, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 505, train loss: 0.055201, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 506, train loss: 0.055295, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 507, train loss: 0.055275, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 508, train loss: 0.055274, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 509, train loss: 0.055013, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 510, train loss: 0.05515, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 511, train loss: 0.05508, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 512, train loss: 0.05506, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 513, train loss: 0.055243, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 514, train loss: 0.055233, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 515, train loss: 0.055224, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 516, train loss: 0.055214, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 517, train loss: 0.055248, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 518, train loss: 0.055286, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 519, train loss: 0.055065, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 520, train loss: 0.055161, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 521, train loss: 0.055187, validation loss: 0.052715\n",
      "tensor(0.0527)\n",
      "iteration 522, train loss: 0.055044, validation loss: 0.052719\n",
      "tensor(0.0527)\n",
      "iteration 523, train loss: 0.055062, validation loss: 0.05272\n",
      "tensor(0.0527)\n",
      "iteration 524, train loss: \u001b[92m0.054977\u001b[0m, validation loss: 0.052718\n",
      "tensor(0.0527)\n",
      "iteration 525, train loss: 0.055228, validation loss: 0.052715\n",
      "tensor(0.0527)\n",
      "iteration 526, train loss: 0.05504, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 527, train loss: 0.055069, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 528, train loss: 0.055017, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 529, train loss: 0.05516, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 530, train loss: 0.055145, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 531, train loss: 0.055172, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 532, train loss: 0.055115, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 533, train loss: 0.055091, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 534, train loss: 0.055175, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 535, train loss: 0.055204, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 536, train loss: \u001b[92m0.054795\u001b[0m, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 537, train loss: 0.054994, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 538, train loss: 0.055094, validation loss: 0.052714\n",
      "tensor(0.0527)\n",
      "iteration 539, train loss: 0.055052, validation loss: 0.052716\n",
      "tensor(0.0527)\n",
      "iteration 540, train loss: 0.05514, validation loss: 0.052717\n",
      "tensor(0.0527)\n",
      "iteration 541, train loss: 0.055117, validation loss: 0.052717\n",
      "tensor(0.0527)\n",
      "iteration 542, train loss: 0.054921, validation loss: 0.052716\n",
      "tensor(0.0527)\n",
      "iteration 543, train loss: 0.05486, validation loss: 0.052714\n",
      "tensor(0.0527)\n",
      "iteration 544, train loss: 0.055228, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 545, train loss: 0.055017, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 546, train loss: 0.055166, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 547, train loss: 0.054864, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 548, train loss: 0.054942, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 549, train loss: 0.055165, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 550, train loss: 0.055152, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 551, train loss: 0.054835, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 552, train loss: 0.054965, validation loss: 0.052711\n",
      "tensor(0.0527)\n",
      "iteration 553, train loss: 0.055065, validation loss: 0.052715\n",
      "tensor(0.0527)\n",
      "iteration 554, train loss: 0.055092, validation loss: 0.052718\n",
      "tensor(0.0527)\n",
      "iteration 555, train loss: 0.055122, validation loss: 0.052721\n",
      "tensor(0.0527)\n",
      "iteration 556, train loss: 0.055031, validation loss: 0.052723\n",
      "tensor(0.0527)\n",
      "iteration 557, train loss: 0.055, validation loss: 0.052722\n",
      "tensor(0.0527)\n",
      "iteration 558, train loss: 0.055098, validation loss: 0.052719\n",
      "tensor(0.0527)\n",
      "iteration 559, train loss: 0.055275, validation loss: 0.052715\n",
      "tensor(0.0527)\n",
      "iteration 560, train loss: 0.055227, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 561, train loss: 0.055046, validation loss: 0.052704\n",
      "tensor(0.0527)\n",
      "iteration 562, train loss: 0.054992, validation loss: 0.0527\n",
      "tensor(0.0527)\n",
      "iteration 563, train loss: 0.05513, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 564, train loss: 0.055094, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 565, train loss: 0.054958, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 566, train loss: 0.055058, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 567, train loss: 0.054995, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 568, train loss: 0.055, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 569, train loss: 0.05489, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 570, train loss: 0.055095, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 571, train loss: 0.055028, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 572, train loss: 0.055094, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 573, train loss: 0.055045, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 574, train loss: 0.054829, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 575, train loss: \u001b[92m0.054773\u001b[0m, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 576, train loss: 0.054967, validation loss: \u001b[92m0.052693\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 577, train loss: 0.054936, validation loss: \u001b[92m0.05269\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 578, train loss: 0.055036, validation loss: \u001b[92m0.052689\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 579, train loss: 0.05484, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 580, train loss: 0.055052, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 581, train loss: 0.054974, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 582, train loss: 0.054827, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 583, train loss: 0.055041, validation loss: 0.052708\n",
      "tensor(0.0527)\n",
      "iteration 584, train loss: 0.054925, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 585, train loss: 0.054987, validation loss: 0.052712\n",
      "tensor(0.0527)\n",
      "iteration 586, train loss: 0.054958, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 587, train loss: 0.055099, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 588, train loss: 0.054998, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 589, train loss: 0.054841, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 590, train loss: 0.054984, validation loss: 0.052692\n",
      "tensor(0.0527)\n",
      "iteration 591, train loss: 0.054926, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 592, train loss: 0.055047, validation loss: \u001b[92m0.052689\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 593, train loss: 0.054997, validation loss: \u001b[92m0.052688\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 594, train loss: 0.055104, validation loss: \u001b[92m0.052688\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 595, train loss: 0.055017, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 596, train loss: 0.054974, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 597, train loss: 0.054974, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 598, train loss: 0.055027, validation loss: 0.0527\n",
      "tensor(0.0527)\n",
      "iteration 599, train loss: 0.054914, validation loss: 0.052703\n",
      "tensor(0.0527)\n",
      "iteration 600, train loss: 0.05485, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 601, train loss: 0.054938, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 602, train loss: 0.054848, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 603, train loss: 0.054939, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 604, train loss: 0.054939, validation loss: 0.052691\n",
      "tensor(0.0527)\n",
      "iteration 605, train loss: 0.055041, validation loss: 0.052688\n",
      "tensor(0.0527)\n",
      "iteration 606, train loss: 0.054809, validation loss: \u001b[92m0.052687\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 607, train loss: 0.054803, validation loss: 0.052688\n",
      "tensor(0.0527)\n",
      "iteration 608, train loss: 0.055012, validation loss: 0.052692\n",
      "tensor(0.0527)\n",
      "iteration 609, train loss: 0.054966, validation loss: 0.052694\n",
      "tensor(0.0527)\n",
      "iteration 610, train loss: 0.054873, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 611, train loss: 0.054786, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 612, train loss: 0.054881, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 613, train loss: 0.054778, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 614, train loss: 0.054941, validation loss: 0.052694\n",
      "tensor(0.0527)\n",
      "iteration 615, train loss: 0.05491, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 616, train loss: 0.054981, validation loss: 0.052691\n",
      "tensor(0.0527)\n",
      "iteration 617, train loss: 0.054816, validation loss: 0.052689\n",
      "tensor(0.0527)\n",
      "iteration 618, train loss: 0.054823, validation loss: 0.052689\n",
      "tensor(0.0527)\n",
      "iteration 619, train loss: 0.05494, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 620, train loss: \u001b[92m0.054749\u001b[0m, validation loss: 0.052692\n",
      "tensor(0.0527)\n",
      "iteration 621, train loss: 0.054962, validation loss: 0.052694\n",
      "tensor(0.0527)\n",
      "iteration 622, train loss: 0.054801, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 623, train loss: 0.055004, validation loss: 0.052699\n",
      "tensor(0.0527)\n",
      "iteration 624, train loss: 0.054904, validation loss: 0.052702\n",
      "tensor(0.0527)\n",
      "iteration 625, train loss: 0.054785, validation loss: 0.052706\n",
      "tensor(0.0527)\n",
      "iteration 626, train loss: 0.054832, validation loss: 0.052709\n",
      "tensor(0.0527)\n",
      "iteration 627, train loss: 0.054885, validation loss: 0.05271\n",
      "tensor(0.0527)\n",
      "iteration 628, train loss: 0.054799, validation loss: 0.052707\n",
      "tensor(0.0527)\n",
      "iteration 629, train loss: 0.054912, validation loss: 0.052701\n",
      "tensor(0.0527)\n",
      "iteration 630, train loss: 0.054978, validation loss: 0.052696\n",
      "tensor(0.0527)\n",
      "iteration 631, train loss: 0.054774, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 632, train loss: 0.054768, validation loss: \u001b[92m0.052687\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 633, train loss: 0.054845, validation loss: \u001b[92m0.052685\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 634, train loss: 0.054819, validation loss: 0.052687\n",
      "tensor(0.0527)\n",
      "iteration 635, train loss: 0.054929, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 636, train loss: 0.054781, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 637, train loss: 0.054864, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 638, train loss: 0.054813, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 639, train loss: \u001b[92m0.054726\u001b[0m, validation loss: 0.052695\n",
      "tensor(0.0527)\n",
      "iteration 640, train loss: 0.054858, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 641, train loss: \u001b[92m0.054677\u001b[0m, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 642, train loss: 0.054958, validation loss: 0.052698\n",
      "tensor(0.0527)\n",
      "iteration 643, train loss: 0.054973, validation loss: 0.052697\n",
      "tensor(0.0527)\n",
      "iteration 644, train loss: 0.054705, validation loss: 0.052694\n",
      "tensor(0.0527)\n",
      "iteration 645, train loss: 0.054829, validation loss: 0.052691\n",
      "tensor(0.0527)\n",
      "iteration 646, train loss: 0.054722, validation loss: 0.052688\n",
      "tensor(0.0527)\n",
      "iteration 647, train loss: 0.054836, validation loss: 0.052686\n",
      "tensor(0.0527)\n",
      "iteration 648, train loss: 0.054972, validation loss: \u001b[92m0.052683\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 649, train loss: 0.054845, validation loss: \u001b[92m0.052683\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 650, train loss: 0.05485, validation loss: \u001b[92m0.052682\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 651, train loss: 0.054754, validation loss: 0.052683\n",
      "tensor(0.0527)\n",
      "iteration 652, train loss: 0.054768, validation loss: 0.052685\n",
      "tensor(0.0527)\n",
      "iteration 653, train loss: 0.054766, validation loss: 0.052687\n",
      "tensor(0.0527)\n",
      "iteration 654, train loss: 0.05477, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 655, train loss: 0.054822, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 656, train loss: 0.054985, validation loss: 0.052693\n",
      "tensor(0.0527)\n",
      "iteration 657, train loss: \u001b[92m0.054669\u001b[0m, validation loss: 0.05269\n",
      "tensor(0.0527)\n",
      "iteration 658, train loss: 0.054959, validation loss: 0.052686\n",
      "tensor(0.0527)\n",
      "iteration 659, train loss: 0.054747, validation loss: 0.052683\n",
      "tensor(0.0527)\n",
      "iteration 660, train loss: 0.054753, validation loss: \u001b[92m0.052681\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 661, train loss: 0.05473, validation loss: \u001b[92m0.052679\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 662, train loss: 0.05486, validation loss: \u001b[92m0.052678\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 663, train loss: 0.054779, validation loss: 0.052678\n",
      "tensor(0.0527)\n",
      "iteration 664, train loss: 0.054931, validation loss: \u001b[92m0.052678\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 665, train loss: 0.05469, validation loss: 0.052678\n",
      "tensor(0.0527)\n",
      "iteration 666, train loss: 0.054765, validation loss: 0.052679\n",
      "tensor(0.0527)\n",
      "iteration 667, train loss: 0.054774, validation loss: 0.05268\n",
      "tensor(0.0527)\n",
      "iteration 668, train loss: 0.054766, validation loss: 0.052683\n",
      "tensor(0.0527)\n",
      "iteration 669, train loss: 0.054731, validation loss: 0.052686\n",
      "tensor(0.0527)\n",
      "iteration 670, train loss: 0.054789, validation loss: 0.052688\n",
      "tensor(0.0527)\n",
      "iteration 671, train loss: 0.054794, validation loss: 0.052685\n",
      "tensor(0.0527)\n",
      "iteration 672, train loss: 0.054836, validation loss: 0.052681\n",
      "tensor(0.0527)\n",
      "iteration 673, train loss: 0.054676, validation loss: \u001b[92m0.052677\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 674, train loss: 0.054821, validation loss: \u001b[92m0.052674\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 675, train loss: 0.05484, validation loss: \u001b[92m0.052672\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 676, train loss: 0.054792, validation loss: 0.052673\n",
      "tensor(0.0527)\n",
      "iteration 677, train loss: 0.054766, validation loss: 0.052673\n",
      "tensor(0.0527)\n",
      "iteration 678, train loss: 0.054714, validation loss: 0.052674\n",
      "tensor(0.0527)\n",
      "iteration 679, train loss: 0.054792, validation loss: 0.052676\n",
      "tensor(0.0527)\n",
      "iteration 680, train loss: 0.054751, validation loss: 0.052679\n",
      "tensor(0.0527)\n",
      "iteration 681, train loss: 0.054805, validation loss: 0.05268\n",
      "tensor(0.0527)\n",
      "iteration 682, train loss: 0.054744, validation loss: 0.05268\n",
      "tensor(0.0527)\n",
      "iteration 683, train loss: 0.054711, validation loss: 0.05268\n",
      "tensor(0.0527)\n",
      "iteration 684, train loss: 0.054857, validation loss: 0.05268\n",
      "tensor(0.0527)\n",
      "iteration 685, train loss: 0.054799, validation loss: 0.052679\n",
      "tensor(0.0527)\n",
      "iteration 686, train loss: 0.054683, validation loss: 0.052679\n",
      "tensor(0.0527)\n",
      "iteration 687, train loss: 0.054786, validation loss: 0.052677\n",
      "tensor(0.0527)\n",
      "iteration 688, train loss: 0.054775, validation loss: 0.052675\n",
      "tensor(0.0527)\n",
      "iteration 689, train loss: 0.054695, validation loss: 0.052674\n",
      "tensor(0.0527)\n",
      "iteration 690, train loss: 0.054766, validation loss: 0.052674\n",
      "tensor(0.0527)\n",
      "iteration 691, train loss: 0.05474, validation loss: 0.052675\n",
      "tensor(0.0527)\n",
      "iteration 692, train loss: 0.054854, validation loss: 0.052675\n",
      "tensor(0.0527)\n",
      "iteration 693, train loss: 0.054735, validation loss: 0.052673\n",
      "tensor(0.0527)\n",
      "iteration 694, train loss: \u001b[92m0.054625\u001b[0m, validation loss: \u001b[92m0.05267\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 695, train loss: 0.054745, validation loss: \u001b[92m0.052669\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 696, train loss: 0.054701, validation loss: \u001b[92m0.052667\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 697, train loss: 0.054626, validation loss: \u001b[92m0.052665\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 698, train loss: 0.054719, validation loss: \u001b[92m0.052662\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 699, train loss: 0.054694, validation loss: \u001b[92m0.052659\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 700, train loss: 0.054749, validation loss: \u001b[92m0.052657\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 701, train loss: 0.054672, validation loss: \u001b[92m0.052655\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 702, train loss: 0.054783, validation loss: \u001b[92m0.052653\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 703, train loss: 0.054673, validation loss: \u001b[92m0.052653\u001b[0m\n",
      "tensor(0.0527)\n",
      "iteration 704, train loss: 0.054729, validation loss: 0.052654\n",
      "tensor(0.0527)\n",
      "iteration 705, train loss: 0.054738, validation loss: 0.052655\n",
      "tensor(0.0527)\n",
      "iteration 706, train loss: 0.054687, validation loss: 0.052658\n",
      "tensor(0.0527)\n",
      "iteration 707, train loss: 0.054642, validation loss: 0.05266\n",
      "tensor(0.0527)\n",
      "iteration 708, train loss: 0.054669, validation loss: 0.05266\n",
      "tensor(0.0527)\n",
      "iteration 709, train loss: 0.054689, validation loss: 0.052658\n",
      "tensor(0.0527)\n",
      "iteration 710, train loss: \u001b[92m0.054595\u001b[0m, validation loss: 0.052657\n",
      "tensor(0.0527)\n",
      "iteration 711, train loss: 0.054705, validation loss: \u001b[92m0.05265\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 712, train loss: 0.05473, validation loss: \u001b[92m0.052644\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 713, train loss: \u001b[92m0.054557\u001b[0m, validation loss: \u001b[92m0.052642\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 714, train loss: 0.054729, validation loss: \u001b[92m0.052639\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 715, train loss: 0.054673, validation loss: \u001b[92m0.052637\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 716, train loss: 0.054763, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 717, train loss: \u001b[92m0.054552\u001b[0m, validation loss: \u001b[92m0.052637\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 718, train loss: 0.054574, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 719, train loss: 0.054706, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 720, train loss: 0.054579, validation loss: \u001b[92m0.052636\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 721, train loss: 0.054567, validation loss: \u001b[92m0.052635\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 722, train loss: 0.054559, validation loss: \u001b[92m0.052633\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 723, train loss: \u001b[92m0.054534\u001b[0m, validation loss: \u001b[92m0.052631\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 724, train loss: 0.054811, validation loss: \u001b[92m0.05263\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 725, train loss: 0.054583, validation loss: 0.05263\n",
      "tensor(0.0526)\n",
      "iteration 726, train loss: 0.054691, validation loss: \u001b[92m0.052629\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 727, train loss: 0.054671, validation loss: \u001b[92m0.052626\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 728, train loss: \u001b[92m0.054532\u001b[0m, validation loss: \u001b[92m0.052623\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 729, train loss: 0.054782, validation loss: \u001b[92m0.052621\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 730, train loss: 0.054662, validation loss: \u001b[92m0.052621\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 731, train loss: 0.0546, validation loss: 0.052623\n",
      "tensor(0.0526)\n",
      "iteration 732, train loss: 0.05474, validation loss: 0.052628\n",
      "tensor(0.0526)\n",
      "iteration 733, train loss: 0.054666, validation loss: 0.052632\n",
      "tensor(0.0526)\n",
      "iteration 734, train loss: 0.05463, validation loss: 0.052635\n",
      "tensor(0.0526)\n",
      "iteration 735, train loss: 0.054667, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 736, train loss: 0.054626, validation loss: 0.052636\n",
      "tensor(0.0526)\n",
      "iteration 737, train loss: \u001b[92m0.054525\u001b[0m, validation loss: 0.052633\n",
      "tensor(0.0526)\n",
      "iteration 738, train loss: 0.054616, validation loss: 0.052632\n",
      "tensor(0.0526)\n",
      "iteration 739, train loss: 0.054629, validation loss: 0.052629\n",
      "tensor(0.0526)\n",
      "iteration 740, train loss: 0.054762, validation loss: 0.052627\n",
      "tensor(0.0526)\n",
      "iteration 741, train loss: 0.054709, validation loss: 0.052628\n",
      "tensor(0.0526)\n",
      "iteration 742, train loss: 0.054668, validation loss: 0.052629\n",
      "tensor(0.0526)\n",
      "iteration 743, train loss: 0.054589, validation loss: 0.052632\n",
      "tensor(0.0526)\n",
      "iteration 744, train loss: 0.054539, validation loss: 0.052638\n",
      "tensor(0.0526)\n",
      "iteration 745, train loss: 0.054616, validation loss: 0.052642\n",
      "tensor(0.0526)\n",
      "iteration 746, train loss: 0.054658, validation loss: 0.052642\n",
      "tensor(0.0526)\n",
      "iteration 747, train loss: 0.054587, validation loss: 0.052639\n",
      "tensor(0.0526)\n",
      "iteration 748, train loss: 0.054783, validation loss: 0.052632\n",
      "tensor(0.0526)\n",
      "iteration 749, train loss: 0.054659, validation loss: 0.052625\n",
      "tensor(0.0526)\n",
      "iteration 750, train loss: 0.054581, validation loss: \u001b[92m0.05262\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 751, train loss: 0.054765, validation loss: \u001b[92m0.052618\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 752, train loss: 0.05474, validation loss: 0.052618\n",
      "tensor(0.0526)\n",
      "iteration 753, train loss: 0.054646, validation loss: 0.052619\n",
      "tensor(0.0526)\n",
      "iteration 754, train loss: 0.054609, validation loss: 0.052623\n",
      "tensor(0.0526)\n",
      "iteration 755, train loss: 0.054618, validation loss: 0.052629\n",
      "tensor(0.0526)\n",
      "iteration 756, train loss: 0.054604, validation loss: 0.052638\n",
      "tensor(0.0526)\n",
      "iteration 757, train loss: 0.054661, validation loss: 0.052644\n",
      "tensor(0.0526)\n",
      "iteration 758, train loss: 0.054568, validation loss: 0.052645\n",
      "tensor(0.0526)\n",
      "iteration 759, train loss: 0.054646, validation loss: 0.052642\n",
      "tensor(0.0526)\n",
      "iteration 760, train loss: 0.054705, validation loss: 0.052635\n",
      "tensor(0.0526)\n",
      "iteration 761, train loss: \u001b[92m0.054505\u001b[0m, validation loss: 0.052627\n",
      "tensor(0.0526)\n",
      "iteration 762, train loss: 0.054736, validation loss: 0.052619\n",
      "tensor(0.0526)\n",
      "iteration 763, train loss: 0.054531, validation loss: \u001b[92m0.052615\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 764, train loss: 0.054699, validation loss: \u001b[92m0.052614\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 765, train loss: 0.054531, validation loss: 0.052616\n",
      "tensor(0.0526)\n",
      "iteration 766, train loss: 0.054632, validation loss: 0.052624\n",
      "tensor(0.0526)\n",
      "iteration 767, train loss: 0.054596, validation loss: 0.052633\n",
      "tensor(0.0526)\n",
      "iteration 768, train loss: 0.05465, validation loss: 0.052636\n",
      "tensor(0.0526)\n",
      "iteration 769, train loss: \u001b[92m0.054504\u001b[0m, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 770, train loss: 0.05463, validation loss: 0.052635\n",
      "tensor(0.0526)\n",
      "iteration 771, train loss: 0.054545, validation loss: 0.052632\n",
      "tensor(0.0526)\n",
      "iteration 772, train loss: 0.054552, validation loss: 0.052626\n",
      "tensor(0.0526)\n",
      "iteration 773, train loss: \u001b[92m0.054499\u001b[0m, validation loss: 0.052621\n",
      "tensor(0.0526)\n",
      "iteration 774, train loss: 0.054694, validation loss: 0.052617\n",
      "tensor(0.0526)\n",
      "iteration 775, train loss: 0.054542, validation loss: 0.052616\n",
      "tensor(0.0526)\n",
      "iteration 776, train loss: 0.054597, validation loss: 0.052618\n",
      "tensor(0.0526)\n",
      "iteration 777, train loss: \u001b[92m0.05448\u001b[0m, validation loss: 0.052622\n",
      "tensor(0.0526)\n",
      "iteration 778, train loss: 0.054542, validation loss: 0.052626\n",
      "tensor(0.0526)\n",
      "iteration 779, train loss: 0.054523, validation loss: 0.052631\n",
      "tensor(0.0526)\n",
      "iteration 780, train loss: 0.054622, validation loss: 0.052635\n",
      "tensor(0.0526)\n",
      "iteration 781, train loss: 0.054675, validation loss: 0.052637\n",
      "tensor(0.0526)\n",
      "iteration 782, train loss: 0.0546, validation loss: 0.052636\n",
      "tensor(0.0526)\n",
      "iteration 783, train loss: \u001b[92m0.054447\u001b[0m, validation loss: 0.052633\n",
      "tensor(0.0526)\n",
      "iteration 784, train loss: 0.054487, validation loss: 0.052628\n",
      "tensor(0.0526)\n",
      "iteration 785, train loss: 0.054583, validation loss: 0.052622\n",
      "tensor(0.0526)\n",
      "iteration 786, train loss: 0.054509, validation loss: 0.052617\n",
      "tensor(0.0526)\n",
      "iteration 787, train loss: 0.054544, validation loss: 0.052614\n",
      "tensor(0.0526)\n",
      "iteration 788, train loss: 0.054569, validation loss: \u001b[92m0.052612\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 789, train loss: 0.054527, validation loss: 0.052614\n",
      "tensor(0.0526)\n",
      "iteration 790, train loss: 0.054485, validation loss: 0.052617\n",
      "tensor(0.0526)\n",
      "iteration 791, train loss: 0.054553, validation loss: 0.052618\n",
      "tensor(0.0526)\n",
      "iteration 792, train loss: 0.054546, validation loss: 0.052617\n",
      "tensor(0.0526)\n",
      "iteration 793, train loss: 0.054489, validation loss: 0.052618\n",
      "tensor(0.0526)\n",
      "iteration 794, train loss: 0.054682, validation loss: 0.052617\n",
      "tensor(0.0526)\n",
      "iteration 795, train loss: 0.054487, validation loss: 0.052614\n",
      "tensor(0.0526)\n",
      "iteration 796, train loss: 0.054506, validation loss: 0.052613\n",
      "tensor(0.0526)\n",
      "iteration 797, train loss: 0.05464, validation loss: 0.052613\n",
      "tensor(0.0526)\n",
      "iteration 798, train loss: 0.054605, validation loss: 0.052615\n",
      "tensor(0.0526)\n",
      "iteration 799, train loss: 0.054565, validation loss: \u001b[92m0.052612\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 800, train loss: \u001b[92m0.054421\u001b[0m, validation loss: \u001b[92m0.052608\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 801, train loss: 0.0545, validation loss: \u001b[92m0.052605\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 802, train loss: 0.054609, validation loss: 0.052605\n",
      "tensor(0.0526)\n",
      "iteration 803, train loss: \u001b[92m0.054413\u001b[0m, validation loss: 0.052609\n",
      "tensor(0.0526)\n",
      "iteration 804, train loss: 0.054541, validation loss: 0.052614\n",
      "tensor(0.0526)\n",
      "iteration 805, train loss: 0.054424, validation loss: 0.052621\n",
      "tensor(0.0526)\n",
      "iteration 806, train loss: \u001b[92m0.054386\u001b[0m, validation loss: 0.052622\n",
      "tensor(0.0526)\n",
      "iteration 807, train loss: 0.054544, validation loss: 0.052619\n",
      "tensor(0.0526)\n",
      "iteration 808, train loss: 0.054542, validation loss: 0.052611\n",
      "tensor(0.0526)\n",
      "iteration 809, train loss: 0.054471, validation loss: 0.052605\n",
      "tensor(0.0526)\n",
      "iteration 810, train loss: 0.054429, validation loss: \u001b[92m0.0526\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 811, train loss: 0.054503, validation loss: \u001b[92m0.052597\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 812, train loss: 0.054472, validation loss: 0.052597\n",
      "tensor(0.0526)\n",
      "iteration 813, train loss: 0.054517, validation loss: 0.0526\n",
      "tensor(0.0526)\n",
      "iteration 814, train loss: 0.054591, validation loss: 0.052605\n",
      "tensor(0.0526)\n",
      "iteration 815, train loss: 0.054503, validation loss: 0.052612\n",
      "tensor(0.0526)\n",
      "iteration 816, train loss: 0.054492, validation loss: 0.05262\n",
      "tensor(0.0526)\n",
      "iteration 817, train loss: 0.054475, validation loss: 0.052623\n",
      "tensor(0.0526)\n",
      "iteration 818, train loss: 0.054471, validation loss: 0.05262\n",
      "tensor(0.0526)\n",
      "iteration 819, train loss: 0.05445, validation loss: 0.052615\n",
      "tensor(0.0526)\n",
      "iteration 820, train loss: 0.054422, validation loss: 0.052606\n",
      "tensor(0.0526)\n",
      "iteration 821, train loss: 0.054558, validation loss: 0.052598\n",
      "tensor(0.0526)\n",
      "iteration 822, train loss: 0.054426, validation loss: \u001b[92m0.052595\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 823, train loss: 0.054589, validation loss: 0.052596\n",
      "tensor(0.0526)\n",
      "iteration 824, train loss: 0.05447, validation loss: 0.052597\n",
      "tensor(0.0526)\n",
      "iteration 825, train loss: 0.054478, validation loss: 0.052598\n",
      "tensor(0.0526)\n",
      "iteration 826, train loss: 0.05452, validation loss: 0.052599\n",
      "tensor(0.0526)\n",
      "iteration 827, train loss: 0.05448, validation loss: 0.052599\n",
      "tensor(0.0526)\n",
      "iteration 828, train loss: 0.054465, validation loss: 0.052597\n",
      "tensor(0.0526)\n",
      "iteration 829, train loss: 0.054468, validation loss: \u001b[92m0.052594\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 830, train loss: \u001b[92m0.054357\u001b[0m, validation loss: \u001b[92m0.052593\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 831, train loss: 0.054427, validation loss: \u001b[92m0.052592\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 832, train loss: 0.054425, validation loss: \u001b[92m0.052589\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 833, train loss: 0.054492, validation loss: \u001b[92m0.052586\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 834, train loss: 0.054493, validation loss: 0.052586\n",
      "tensor(0.0526)\n",
      "iteration 835, train loss: 0.054427, validation loss: 0.052587\n",
      "tensor(0.0526)\n",
      "iteration 836, train loss: 0.054484, validation loss: 0.052591\n",
      "tensor(0.0526)\n",
      "iteration 837, train loss: 0.054401, validation loss: 0.052595\n",
      "tensor(0.0526)\n",
      "iteration 838, train loss: 0.054536, validation loss: 0.0526\n",
      "tensor(0.0526)\n",
      "iteration 839, train loss: 0.054432, validation loss: 0.052603\n",
      "tensor(0.0526)\n",
      "iteration 840, train loss: 0.054453, validation loss: 0.052604\n",
      "tensor(0.0526)\n",
      "iteration 841, train loss: 0.054447, validation loss: 0.0526\n",
      "tensor(0.0526)\n",
      "iteration 842, train loss: 0.05452, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 843, train loss: 0.054465, validation loss: 0.052586\n",
      "tensor(0.0526)\n",
      "iteration 844, train loss: 0.054466, validation loss: \u001b[92m0.052583\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 845, train loss: 0.054405, validation loss: \u001b[92m0.052581\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 846, train loss: 0.054497, validation loss: \u001b[92m0.052581\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 847, train loss: \u001b[92m0.054352\u001b[0m, validation loss: 0.052582\n",
      "tensor(0.0526)\n",
      "iteration 848, train loss: 0.054555, validation loss: 0.052588\n",
      "tensor(0.0526)\n",
      "iteration 849, train loss: 0.054397, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 850, train loss: 0.054374, validation loss: 0.052594\n",
      "tensor(0.0526)\n",
      "iteration 851, train loss: 0.054427, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 852, train loss: 0.054391, validation loss: 0.052593\n",
      "tensor(0.0526)\n",
      "iteration 853, train loss: 0.054379, validation loss: 0.052593\n",
      "tensor(0.0526)\n",
      "iteration 854, train loss: \u001b[92m0.054269\u001b[0m, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 855, train loss: 0.05433, validation loss: 0.052593\n",
      "tensor(0.0526)\n",
      "iteration 856, train loss: 0.054393, validation loss: 0.052596\n",
      "tensor(0.0526)\n",
      "iteration 857, train loss: \u001b[92m0.054257\u001b[0m, validation loss: 0.0526\n",
      "tensor(0.0526)\n",
      "iteration 858, train loss: 0.05437, validation loss: 0.052601\n",
      "tensor(0.0526)\n",
      "iteration 859, train loss: 0.054308, validation loss: 0.052601\n",
      "tensor(0.0526)\n",
      "iteration 860, train loss: 0.054451, validation loss: 0.052598\n",
      "tensor(0.0526)\n",
      "iteration 861, train loss: 0.054443, validation loss: 0.052593\n",
      "tensor(0.0526)\n",
      "iteration 862, train loss: 0.054325, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 863, train loss: 0.054507, validation loss: 0.052591\n",
      "tensor(0.0526)\n",
      "iteration 864, train loss: 0.054495, validation loss: 0.05259\n",
      "tensor(0.0526)\n",
      "iteration 865, train loss: 0.054368, validation loss: 0.052593\n",
      "tensor(0.0526)\n",
      "iteration 866, train loss: \u001b[92m0.054229\u001b[0m, validation loss: 0.052595\n",
      "tensor(0.0526)\n",
      "iteration 867, train loss: 0.054368, validation loss: 0.052597\n",
      "tensor(0.0526)\n",
      "iteration 868, train loss: 0.054345, validation loss: 0.052596\n",
      "tensor(0.0526)\n",
      "iteration 869, train loss: 0.054312, validation loss: 0.05259\n",
      "tensor(0.0526)\n",
      "iteration 870, train loss: 0.054389, validation loss: 0.052584\n",
      "tensor(0.0526)\n",
      "iteration 871, train loss: 0.054408, validation loss: \u001b[92m0.052579\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 872, train loss: 0.054275, validation loss: \u001b[92m0.052577\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 873, train loss: 0.054239, validation loss: \u001b[92m0.052576\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 874, train loss: 0.05445, validation loss: 0.052579\n",
      "tensor(0.0526)\n",
      "iteration 875, train loss: 0.054377, validation loss: 0.052587\n",
      "tensor(0.0526)\n",
      "iteration 876, train loss: 0.054282, validation loss: 0.052592\n",
      "tensor(0.0526)\n",
      "iteration 877, train loss: 0.054267, validation loss: 0.052588\n",
      "tensor(0.0526)\n",
      "iteration 878, train loss: 0.054348, validation loss: 0.05258\n",
      "tensor(0.0526)\n",
      "iteration 879, train loss: 0.054279, validation loss: \u001b[92m0.052572\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 880, train loss: 0.054308, validation loss: \u001b[92m0.052566\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 881, train loss: 0.054304, validation loss: \u001b[92m0.052564\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 882, train loss: 0.054353, validation loss: \u001b[92m0.052563\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 883, train loss: 0.054323, validation loss: 0.052563\n",
      "tensor(0.0526)\n",
      "iteration 884, train loss: \u001b[92m0.054178\u001b[0m, validation loss: 0.052564\n",
      "tensor(0.0526)\n",
      "iteration 885, train loss: 0.054309, validation loss: 0.052567\n",
      "tensor(0.0526)\n",
      "iteration 886, train loss: 0.05422, validation loss: 0.052568\n",
      "tensor(0.0526)\n",
      "iteration 887, train loss: 0.054358, validation loss: 0.052565\n",
      "tensor(0.0526)\n",
      "iteration 888, train loss: 0.054278, validation loss: \u001b[92m0.052561\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 889, train loss: 0.054439, validation loss: \u001b[92m0.052557\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 890, train loss: 0.054288, validation loss: \u001b[92m0.052553\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 891, train loss: 0.054294, validation loss: \u001b[92m0.052551\u001b[0m\n",
      "tensor(0.0526)\n",
      "iteration 892, train loss: 0.054472, validation loss: 0.052551\n",
      "tensor(0.0526)\n",
      "iteration 893, train loss: 0.054337, validation loss: 0.052551\n",
      "tensor(0.0526)\n",
      "iteration 894, train loss: 0.054247, validation loss: 0.052553\n",
      "tensor(0.0526)\n",
      "iteration 895, train loss: 0.054287, validation loss: 0.052554\n",
      "tensor(0.0526)\n",
      "iteration 896, train loss: 0.054311, validation loss: 0.052555\n",
      "tensor(0.0526)\n",
      "iteration 897, train loss: 0.054224, validation loss: 0.052552\n",
      "tensor(0.0525)\n",
      "iteration 898, train loss: 0.05437, validation loss: \u001b[92m0.052548\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 899, train loss: 0.054256, validation loss: \u001b[92m0.052543\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 900, train loss: 0.054428, validation loss: \u001b[92m0.052539\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 901, train loss: 0.054306, validation loss: \u001b[92m0.052537\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 902, train loss: 0.054204, validation loss: \u001b[92m0.052536\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 903, train loss: 0.054345, validation loss: 0.052543\n",
      "tensor(0.0526)\n",
      "iteration 904, train loss: 0.054265, validation loss: 0.052554\n",
      "tensor(0.0526)\n",
      "iteration 905, train loss: 0.054355, validation loss: 0.052563\n",
      "tensor(0.0526)\n",
      "iteration 906, train loss: 0.054364, validation loss: 0.052568\n",
      "tensor(0.0526)\n",
      "iteration 907, train loss: 0.054285, validation loss: 0.052561\n",
      "tensor(0.0525)\n",
      "iteration 908, train loss: 0.054362, validation loss: 0.052544\n",
      "tensor(0.0525)\n",
      "iteration 909, train loss: 0.054239, validation loss: \u001b[92m0.052532\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 910, train loss: 0.054325, validation loss: \u001b[92m0.052526\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 911, train loss: 0.054305, validation loss: \u001b[92m0.052524\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 912, train loss: 0.054283, validation loss: \u001b[92m0.052524\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 913, train loss: 0.054233, validation loss: 0.052528\n",
      "tensor(0.0525)\n",
      "iteration 914, train loss: 0.054327, validation loss: 0.052538\n",
      "tensor(0.0526)\n",
      "iteration 915, train loss: 0.054264, validation loss: 0.052556\n",
      "tensor(0.0526)\n",
      "iteration 916, train loss: 0.054295, validation loss: 0.052573\n",
      "tensor(0.0526)\n",
      "iteration 917, train loss: 0.054364, validation loss: 0.052573\n",
      "tensor(0.0526)\n",
      "iteration 918, train loss: 0.054247, validation loss: 0.052558\n",
      "tensor(0.0525)\n",
      "iteration 919, train loss: \u001b[92m0.054166\u001b[0m, validation loss: 0.052541\n",
      "tensor(0.0525)\n",
      "iteration 920, train loss: 0.054416, validation loss: 0.052524\n",
      "tensor(0.0525)\n",
      "iteration 921, train loss: 0.054186, validation loss: \u001b[92m0.052517\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 922, train loss: 0.054193, validation loss: \u001b[92m0.052516\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 923, train loss: 0.054251, validation loss: \u001b[92m0.052515\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 924, train loss: 0.054248, validation loss: 0.052519\n",
      "tensor(0.0525)\n",
      "iteration 925, train loss: 0.054188, validation loss: 0.052529\n",
      "tensor(0.0525)\n",
      "iteration 926, train loss: 0.054256, validation loss: 0.052546\n",
      "tensor(0.0526)\n",
      "iteration 927, train loss: 0.054236, validation loss: 0.052556\n",
      "tensor(0.0526)\n",
      "iteration 928, train loss: 0.054226, validation loss: 0.052554\n",
      "tensor(0.0525)\n",
      "iteration 929, train loss: 0.054225, validation loss: 0.05254\n",
      "tensor(0.0525)\n",
      "iteration 930, train loss: 0.054238, validation loss: 0.052523\n",
      "tensor(0.0525)\n",
      "iteration 931, train loss: 0.05428, validation loss: \u001b[92m0.05251\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 932, train loss: 0.054181, validation loss: \u001b[92m0.052502\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 933, train loss: 0.054184, validation loss: \u001b[92m0.052498\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 934, train loss: \u001b[92m0.054154\u001b[0m, validation loss: 0.052499\n",
      "tensor(0.0525)\n",
      "iteration 935, train loss: 0.054168, validation loss: 0.052503\n",
      "tensor(0.0525)\n",
      "iteration 936, train loss: 0.054291, validation loss: 0.052511\n",
      "tensor(0.0525)\n",
      "iteration 937, train loss: \u001b[92m0.054106\u001b[0m, validation loss: 0.052522\n",
      "tensor(0.0525)\n",
      "iteration 938, train loss: 0.05419, validation loss: 0.052532\n",
      "tensor(0.0525)\n",
      "iteration 939, train loss: 0.054229, validation loss: 0.052537\n",
      "tensor(0.0525)\n",
      "iteration 940, train loss: 0.05414, validation loss: 0.052531\n",
      "tensor(0.0525)\n",
      "iteration 941, train loss: 0.054248, validation loss: 0.052519\n",
      "tensor(0.0525)\n",
      "iteration 942, train loss: 0.054221, validation loss: 0.052504\n",
      "tensor(0.0525)\n",
      "iteration 943, train loss: 0.054215, validation loss: \u001b[92m0.052495\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 944, train loss: 0.054231, validation loss: \u001b[92m0.052488\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 945, train loss: 0.054155, validation loss: \u001b[92m0.052486\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 946, train loss: 0.054153, validation loss: 0.052487\n",
      "tensor(0.0525)\n",
      "iteration 947, train loss: 0.054277, validation loss: 0.052496\n",
      "tensor(0.0525)\n",
      "iteration 948, train loss: 0.054293, validation loss: 0.052504\n",
      "tensor(0.0525)\n",
      "iteration 949, train loss: 0.05421, validation loss: 0.052512\n",
      "tensor(0.0525)\n",
      "iteration 950, train loss: 0.054247, validation loss: 0.052517\n",
      "tensor(0.0525)\n",
      "iteration 951, train loss: 0.054236, validation loss: 0.052517\n",
      "tensor(0.0525)\n",
      "iteration 952, train loss: 0.054148, validation loss: 0.052514\n",
      "tensor(0.0525)\n",
      "iteration 953, train loss: 0.054188, validation loss: 0.052508\n",
      "tensor(0.0525)\n",
      "iteration 954, train loss: 0.054187, validation loss: 0.052496\n",
      "tensor(0.0525)\n",
      "iteration 955, train loss: 0.054165, validation loss: 0.052488\n",
      "tensor(0.0525)\n",
      "iteration 956, train loss: 0.054156, validation loss: \u001b[92m0.052485\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 957, train loss: 0.054131, validation loss: 0.052488\n",
      "tensor(0.0525)\n",
      "iteration 958, train loss: 0.054275, validation loss: 0.052497\n",
      "tensor(0.0525)\n",
      "iteration 959, train loss: 0.05424, validation loss: 0.052506\n",
      "tensor(0.0525)\n",
      "iteration 960, train loss: 0.054134, validation loss: 0.052513\n",
      "tensor(0.0525)\n",
      "iteration 961, train loss: 0.054282, validation loss: 0.052512\n",
      "tensor(0.0525)\n",
      "iteration 962, train loss: 0.054201, validation loss: 0.052509\n",
      "tensor(0.0525)\n",
      "iteration 963, train loss: 0.054289, validation loss: 0.052497\n",
      "tensor(0.0525)\n",
      "iteration 964, train loss: 0.054253, validation loss: 0.052493\n",
      "tensor(0.0525)\n",
      "iteration 965, train loss: 0.054148, validation loss: 0.052493\n",
      "tensor(0.0525)\n",
      "iteration 966, train loss: 0.054198, validation loss: 0.052497\n",
      "tensor(0.0525)\n",
      "iteration 967, train loss: 0.0542, validation loss: 0.052499\n",
      "tensor(0.0525)\n",
      "iteration 968, train loss: 0.054231, validation loss: 0.052501\n",
      "tensor(0.0525)\n",
      "iteration 969, train loss: 0.054218, validation loss: 0.052502\n",
      "tensor(0.0525)\n",
      "iteration 970, train loss: 0.054255, validation loss: 0.052501\n",
      "tensor(0.0525)\n",
      "iteration 971, train loss: 0.054139, validation loss: 0.052499\n",
      "tensor(0.0525)\n",
      "iteration 972, train loss: 0.054258, validation loss: 0.0525\n",
      "tensor(0.0525)\n",
      "iteration 973, train loss: 0.0542, validation loss: 0.052502\n",
      "tensor(0.0525)\n",
      "iteration 974, train loss: 0.05415, validation loss: 0.052503\n",
      "tensor(0.0525)\n",
      "iteration 975, train loss: 0.054265, validation loss: 0.0525\n",
      "tensor(0.0525)\n",
      "iteration 976, train loss: 0.054123, validation loss: 0.052492\n",
      "tensor(0.0525)\n",
      "iteration 977, train loss: \u001b[92m0.05408\u001b[0m, validation loss: \u001b[92m0.052485\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 978, train loss: 0.054165, validation loss: \u001b[92m0.052484\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 979, train loss: \u001b[92m0.05408\u001b[0m, validation loss: \u001b[92m0.052483\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 980, train loss: 0.05412, validation loss: 0.052489\n",
      "tensor(0.0525)\n",
      "iteration 981, train loss: 0.054104, validation loss: 0.0525\n",
      "tensor(0.0525)\n",
      "iteration 982, train loss: 0.05416, validation loss: 0.05251\n",
      "tensor(0.0525)\n",
      "iteration 983, train loss: 0.054119, validation loss: 0.052521\n",
      "tensor(0.0525)\n",
      "iteration 984, train loss: 0.05409, validation loss: 0.052525\n",
      "tensor(0.0525)\n",
      "iteration 985, train loss: 0.054216, validation loss: 0.052521\n",
      "tensor(0.0525)\n",
      "iteration 986, train loss: 0.054227, validation loss: 0.052508\n",
      "tensor(0.0525)\n",
      "iteration 987, train loss: 0.054134, validation loss: 0.052486\n",
      "tensor(0.0525)\n",
      "iteration 988, train loss: 0.054192, validation loss: \u001b[92m0.052471\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 989, train loss: \u001b[92m0.054068\u001b[0m, validation loss: \u001b[92m0.052463\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 990, train loss: 0.054094, validation loss: \u001b[92m0.052463\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 991, train loss: \u001b[92m0.054058\u001b[0m, validation loss: 0.05247\n",
      "tensor(0.0525)\n",
      "iteration 992, train loss: \u001b[92m0.054043\u001b[0m, validation loss: 0.05248\n",
      "tensor(0.0525)\n",
      "iteration 993, train loss: 0.054189, validation loss: 0.052493\n",
      "tensor(0.0525)\n",
      "iteration 994, train loss: \u001b[92m0.053996\u001b[0m, validation loss: 0.052502\n",
      "tensor(0.0525)\n",
      "iteration 995, train loss: 0.054244, validation loss: 0.052512\n",
      "tensor(0.0525)\n",
      "iteration 996, train loss: 0.054195, validation loss: 0.05251\n",
      "tensor(0.0525)\n",
      "iteration 997, train loss: 0.054125, validation loss: 0.052497\n",
      "tensor(0.0525)\n",
      "iteration 998, train loss: 0.054131, validation loss: 0.052487\n",
      "tensor(0.0525)\n",
      "iteration 999, train loss: 0.054131, validation loss: 0.052472\n",
      "tensor(0.0525)\n",
      "iteration 1000, train loss: 0.054117, validation loss: \u001b[92m0.052461\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 1001, train loss: 0.054247, validation loss: 0.052461\n",
      "tensor(0.0525)\n",
      "iteration 1002, train loss: 0.054047, validation loss: 0.05247\n",
      "tensor(0.0525)\n",
      "iteration 1003, train loss: 0.054223, validation loss: 0.052492\n",
      "tensor(0.0525)\n",
      "iteration 1004, train loss: 0.054157, validation loss: 0.052523\n",
      "tensor(0.0525)\n",
      "iteration 1005, train loss: 0.05408, validation loss: 0.052543\n",
      "tensor(0.0525)\n",
      "iteration 1006, train loss: 0.054067, validation loss: 0.052541\n",
      "tensor(0.0525)\n",
      "iteration 1007, train loss: 0.054108, validation loss: 0.052523\n",
      "tensor(0.0525)\n",
      "iteration 1008, train loss: 0.054142, validation loss: 0.05249\n",
      "tensor(0.0525)\n",
      "iteration 1009, train loss: \u001b[92m0.053968\u001b[0m, validation loss: \u001b[92m0.05246\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1010, train loss: 0.054102, validation loss: \u001b[92m0.05244\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1011, train loss: 0.053993, validation loss: \u001b[92m0.052434\u001b[0m\n",
      "tensor(0.0525)\n",
      "iteration 1012, train loss: 0.054132, validation loss: 0.052453\n",
      "tensor(0.0525)\n",
      "iteration 1013, train loss: 0.05416, validation loss: 0.052482\n",
      "tensor(0.0525)\n",
      "iteration 1014, train loss: 0.054257, validation loss: 0.052507\n",
      "tensor(0.0525)\n",
      "iteration 1015, train loss: 0.054008, validation loss: 0.052513\n",
      "tensor(0.0525)\n",
      "iteration 1016, train loss: 0.054138, validation loss: 0.052503\n",
      "tensor(0.0525)\n",
      "iteration 1017, train loss: 0.054103, validation loss: 0.052484\n",
      "tensor(0.0525)\n",
      "iteration 1018, train loss: 0.05403, validation loss: 0.052459\n",
      "tensor(0.0524)\n",
      "iteration 1019, train loss: 0.054047, validation loss: 0.052439\n",
      "tensor(0.0524)\n",
      "iteration 1020, train loss: 0.054096, validation loss: \u001b[92m0.052428\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1021, train loss: 0.054176, validation loss: 0.052436\n",
      "tensor(0.0525)\n",
      "iteration 1022, train loss: 0.054087, validation loss: 0.052457\n",
      "tensor(0.0525)\n",
      "iteration 1023, train loss: 0.054112, validation loss: 0.05248\n",
      "tensor(0.0525)\n",
      "iteration 1024, train loss: 0.05403, validation loss: 0.052496\n",
      "tensor(0.0525)\n",
      "iteration 1025, train loss: 0.054197, validation loss: 0.052503\n",
      "tensor(0.0525)\n",
      "iteration 1026, train loss: 0.054199, validation loss: 0.052496\n",
      "tensor(0.0525)\n",
      "iteration 1027, train loss: 0.054073, validation loss: 0.052465\n",
      "tensor(0.0524)\n",
      "iteration 1028, train loss: 0.054059, validation loss: 0.052432\n",
      "tensor(0.0524)\n",
      "iteration 1029, train loss: 0.054085, validation loss: \u001b[92m0.052412\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1030, train loss: 0.05408, validation loss: 0.052414\n",
      "tensor(0.0524)\n",
      "iteration 1031, train loss: 0.054103, validation loss: 0.052435\n",
      "tensor(0.0525)\n",
      "iteration 1032, train loss: 0.054125, validation loss: 0.052455\n",
      "tensor(0.0525)\n",
      "iteration 1033, train loss: 0.054149, validation loss: 0.05247\n",
      "tensor(0.0525)\n",
      "iteration 1034, train loss: 0.054096, validation loss: 0.052482\n",
      "tensor(0.0525)\n",
      "iteration 1035, train loss: 0.054067, validation loss: 0.052484\n",
      "tensor(0.0525)\n",
      "iteration 1036, train loss: 0.053984, validation loss: 0.05247\n",
      "tensor(0.0524)\n",
      "iteration 1037, train loss: 0.054151, validation loss: 0.052436\n",
      "tensor(0.0524)\n",
      "iteration 1038, train loss: 0.054061, validation loss: 0.052415\n",
      "tensor(0.0524)\n",
      "iteration 1039, train loss: 0.054107, validation loss: \u001b[92m0.052399\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1040, train loss: 0.05411, validation loss: 0.052399\n",
      "tensor(0.0524)\n",
      "iteration 1041, train loss: 0.054085, validation loss: 0.052403\n",
      "tensor(0.0524)\n",
      "iteration 1042, train loss: 0.054048, validation loss: 0.052423\n",
      "tensor(0.0524)\n",
      "iteration 1043, train loss: 0.054012, validation loss: 0.052449\n",
      "tensor(0.0525)\n",
      "iteration 1044, train loss: 0.054063, validation loss: 0.052469\n",
      "tensor(0.0525)\n",
      "iteration 1045, train loss: 0.054162, validation loss: 0.052463\n",
      "tensor(0.0524)\n",
      "iteration 1046, train loss: 0.054042, validation loss: 0.052433\n",
      "tensor(0.0524)\n",
      "iteration 1047, train loss: \u001b[92m0.053951\u001b[0m, validation loss: 0.052401\n",
      "tensor(0.0524)\n",
      "iteration 1048, train loss: 0.054066, validation loss: \u001b[92m0.052378\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1049, train loss: 0.05403, validation loss: \u001b[92m0.052367\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1050, train loss: \u001b[92m0.053948\u001b[0m, validation loss: \u001b[92m0.052367\u001b[0m\n",
      "tensor(0.0524)\n",
      "iteration 1051, train loss: 0.053954, validation loss: 0.052376\n",
      "tensor(0.0524)\n",
      "iteration 1052, train loss: 0.053992, validation loss: 0.052396\n",
      "tensor(0.0524)\n",
      "iteration 1053, train loss: 0.054056, validation loss: 0.052423\n",
      "tensor(0.0524)\n",
      "iteration 1054, train loss: 0.054047, validation loss: 0.052438\n",
      "tensor(0.0524)\n",
      "iteration 1055, train loss: 0.053953, validation loss: 0.052426\n",
      "tensor(0.0524)\n",
      "iteration 1056, train loss: 0.054048, validation loss: 0.05239\n",
      "tensor(0.0524)\n",
      "iteration 1057, train loss: 0.054006, validation loss: \u001b[92m0.05236\u001b[0m\n",
      "tensor(0.0523)\n",
      "iteration 1058, train loss: 0.054067, validation loss: \u001b[92m0.052337\u001b[0m\n",
      "tensor(0.0523)\n",
      "iteration 1059, train loss: \u001b[92m0.053882\u001b[0m, validation loss: \u001b[92m0.052323\u001b[0m\n",
      "tensor(0.0523)\n",
      "iteration 1060, train loss: 0.053958, validation loss: 0.052329\n",
      "tensor(0.0523)\n",
      "iteration 1061, train loss: 0.054038, validation loss: 0.052348\n",
      "tensor(0.0524)\n",
      "iteration 1062, train loss: 0.054061, validation loss: 0.052371\n",
      "tensor(0.0524)\n",
      "iteration 1063, train loss: 0.054007, validation loss: 0.052386\n",
      "tensor(0.0524)\n",
      "iteration 1064, train loss: \u001b[92m0.053839\u001b[0m, validation loss: 0.052363\n",
      "tensor(0.0523)\n",
      "iteration 1065, train loss: 0.053971, validation loss: \u001b[92m0.052313\u001b[0m\n",
      "tensor(0.0523)\n",
      "iteration 1066, train loss: 0.053949, validation loss: \u001b[92m0.052274\u001b[0m\n",
      "tensor(0.0522)\n",
      "iteration 1067, train loss: 0.053894, validation loss: \u001b[92m0.052248\u001b[0m\n",
      "tensor(0.0522)\n",
      "iteration 1068, train loss: 0.053906, validation loss: \u001b[92m0.05223\u001b[0m\n",
      "tensor(0.0522)\n",
      "iteration 1069, train loss: 0.053975, validation loss: \u001b[92m0.052221\u001b[0m\n",
      "tensor(0.0522)\n",
      "iteration 1070, train loss: 0.053974, validation loss: \u001b[92m0.052199\u001b[0m\n",
      "tensor(0.0522)\n",
      "iteration 1071, train loss: \u001b[92m0.053799\u001b[0m, validation loss: \u001b[92m0.052166\u001b[0m\n",
      "tensor(0.0521)\n",
      "iteration 1072, train loss: \u001b[92m0.05376\u001b[0m, validation loss: \u001b[92m0.052094\u001b[0m\n",
      "tensor(0.0520)\n",
      "iteration 1073, train loss: 0.053911, validation loss: \u001b[92m0.051989\u001b[0m\n",
      "tensor(0.0519)\n",
      "iteration 1074, train loss: 0.053786, validation loss: \u001b[92m0.051857\u001b[0m\n",
      "tensor(0.0517)\n",
      "iteration 1075, train loss: \u001b[92m0.053646\u001b[0m, validation loss: \u001b[92m0.051664\u001b[0m\n",
      "tensor(0.0514)\n",
      "iteration 1076, train loss: \u001b[92m0.053627\u001b[0m, validation loss: \u001b[92m0.051358\u001b[0m\n",
      "tensor(0.0509)\n",
      "iteration 1077, train loss: \u001b[92m0.053278\u001b[0m, validation loss: \u001b[92m0.050942\u001b[0m\n",
      "tensor(0.0503)\n",
      "iteration 1078, train loss: \u001b[92m0.052955\u001b[0m, validation loss: \u001b[92m0.050274\u001b[0m\n",
      "tensor(0.0488)\n",
      "iteration 1079, train loss: \u001b[92m0.052459\u001b[0m, validation loss: \u001b[92m0.048841\u001b[0m\n",
      "tensor(0.0463)\n",
      "iteration 1080, train loss: \u001b[92m0.051063\u001b[0m, validation loss: \u001b[92m0.046267\u001b[0m\n",
      "tensor(0.0423)\n",
      "iteration 1081, train loss: \u001b[92m0.049408\u001b[0m, validation loss: \u001b[92m0.042308\u001b[0m\n",
      "tensor(0.0380)\n",
      "iteration 1082, train loss: \u001b[92m0.045316\u001b[0m, validation loss: \u001b[92m0.038016\u001b[0m\n",
      "tensor(0.0337)\n",
      "iteration 1083, train loss: \u001b[92m0.041871\u001b[0m, validation loss: \u001b[92m0.033712\u001b[0m\n",
      "tensor(0.0295)\n",
      "iteration 1084, train loss: \u001b[92m0.038403\u001b[0m, validation loss: \u001b[92m0.029472\u001b[0m\n",
      "tensor(0.0249)\n",
      "iteration 1085, train loss: \u001b[92m0.03282\u001b[0m, validation loss: \u001b[92m0.024924\u001b[0m\n",
      "tensor(0.0220)\n",
      "iteration 1086, train loss: \u001b[92m0.028863\u001b[0m, validation loss: \u001b[92m0.021978\u001b[0m\n",
      "tensor(0.0202)\n",
      "iteration 1087, train loss: \u001b[92m0.025314\u001b[0m, validation loss: \u001b[92m0.020247\u001b[0m\n",
      "tensor(0.0196)\n",
      "iteration 1088, train loss: \u001b[92m0.023144\u001b[0m, validation loss: \u001b[92m0.019626\u001b[0m\n",
      "tensor(0.0180)\n",
      "iteration 1089, train loss: \u001b[92m0.02239\u001b[0m, validation loss: \u001b[92m0.018008\u001b[0m\n",
      "tensor(0.0163)\n",
      "iteration 1090, train loss: \u001b[92m0.020987\u001b[0m, validation loss: \u001b[92m0.016314\u001b[0m\n",
      "tensor(0.0179)\n",
      "iteration 1091, train loss: \u001b[92m0.019479\u001b[0m, validation loss: 0.017854\n",
      "tensor(0.0158)\n",
      "iteration 1092, train loss: 0.02005, validation loss: \u001b[92m0.015812\u001b[0m\n",
      "tensor(0.0164)\n",
      "iteration 1093, train loss: \u001b[92m0.018637\u001b[0m, validation loss: 0.016356\n",
      "tensor(0.0155)\n",
      "iteration 1094, train loss: 0.01942, validation loss: \u001b[92m0.015516\u001b[0m\n",
      "tensor(0.0172)\n",
      "iteration 1095, train loss: \u001b[92m0.017834\u001b[0m, validation loss: 0.017194\n",
      "tensor(0.0147)\n",
      "iteration 1096, train loss: 0.018267, validation loss: \u001b[92m0.014654\u001b[0m\n",
      "tensor(0.0139)\n",
      "iteration 1097, train loss: \u001b[92m0.016879\u001b[0m, validation loss: \u001b[92m0.013914\u001b[0m\n",
      "tensor(0.0133)\n",
      "iteration 1098, train loss: 0.017077, validation loss: \u001b[92m0.013273\u001b[0m\n",
      "tensor(0.0137)\n",
      "iteration 1099, train loss: \u001b[92m0.016274\u001b[0m, validation loss: 0.013741\n",
      "tensor(0.0132)\n",
      "iteration 1100, train loss: \u001b[92m0.016101\u001b[0m, validation loss: \u001b[92m0.013208\u001b[0m\n",
      "tensor(0.0122)\n",
      "iteration 1101, train loss: \u001b[92m0.01541\u001b[0m, validation loss: \u001b[92m0.012207\u001b[0m\n",
      "tensor(0.0117)\n",
      "iteration 1102, train loss: \u001b[92m0.015357\u001b[0m, validation loss: \u001b[92m0.011706\u001b[0m\n",
      "tensor(0.0116)\n",
      "iteration 1103, train loss: \u001b[92m0.01507\u001b[0m, validation loss: \u001b[92m0.011597\u001b[0m\n",
      "tensor(0.0118)\n",
      "iteration 1104, train loss: \u001b[92m0.014795\u001b[0m, validation loss: 0.011771\n",
      "tensor(0.0114)\n",
      "iteration 1105, train loss: \u001b[92m0.014463\u001b[0m, validation loss: \u001b[92m0.011413\u001b[0m\n",
      "tensor(0.0109)\n",
      "iteration 1106, train loss: \u001b[92m0.013951\u001b[0m, validation loss: \u001b[92m0.010877\u001b[0m\n",
      "tensor(0.0108)\n",
      "iteration 1107, train loss: \u001b[92m0.013948\u001b[0m, validation loss: \u001b[92m0.010783\u001b[0m\n",
      "tensor(0.0109)\n",
      "iteration 1108, train loss: \u001b[92m0.013877\u001b[0m, validation loss: 0.010895\n",
      "tensor(0.0112)\n",
      "iteration 1109, train loss: \u001b[92m0.013779\u001b[0m, validation loss: 0.0112\n",
      "tensor(0.0105)\n",
      "iteration 1110, train loss: \u001b[92m0.013263\u001b[0m, validation loss: \u001b[92m0.010457\u001b[0m\n",
      "tensor(0.0102)\n",
      "iteration 1111, train loss: \u001b[92m0.01319\u001b[0m, validation loss: \u001b[92m0.010164\u001b[0m\n",
      "tensor(0.0100)\n",
      "iteration 1112, train loss: \u001b[92m0.012931\u001b[0m, validation loss: \u001b[92m0.009998\u001b[0m\n",
      "tensor(0.0098)\n",
      "iteration 1113, train loss: 0.013034, validation loss: \u001b[92m0.009839\u001b[0m\n",
      "tensor(0.0098)\n",
      "iteration 1114, train loss: 0.013112, validation loss: \u001b[92m0.009836\u001b[0m\n",
      "tensor(0.0096)\n",
      "iteration 1115, train loss: \u001b[92m0.012818\u001b[0m, validation loss: \u001b[92m0.009603\u001b[0m\n",
      "tensor(0.0094)\n",
      "iteration 1116, train loss: 0.012979, validation loss: \u001b[92m0.009434\u001b[0m\n",
      "tensor(0.0094)\n",
      "iteration 1117, train loss: \u001b[92m0.012583\u001b[0m, validation loss: \u001b[92m0.009357\u001b[0m\n",
      "tensor(0.0094)\n",
      "iteration 1118, train loss: \u001b[92m0.012582\u001b[0m, validation loss: \u001b[92m0.009355\u001b[0m\n",
      "tensor(0.0096)\n",
      "iteration 1119, train loss: 0.013285, validation loss: 0.009569\n",
      "tensor(0.0095)\n",
      "iteration 1120, train loss: 0.012709, validation loss: 0.009511\n",
      "tensor(0.0093)\n",
      "iteration 1121, train loss: \u001b[92m0.012419\u001b[0m, validation loss: \u001b[92m0.009338\u001b[0m\n",
      "tensor(0.0093)\n",
      "iteration 1122, train loss: 0.012541, validation loss: 0.009345\n",
      "tensor(0.0093)\n",
      "iteration 1123, train loss: 0.012508, validation loss: \u001b[92m0.009265\u001b[0m\n",
      "tensor(0.0092)\n",
      "iteration 1124, train loss: \u001b[92m0.012381\u001b[0m, validation loss: \u001b[92m0.009218\u001b[0m\n",
      "tensor(0.0093)\n",
      "iteration 1125, train loss: \u001b[92m0.012281\u001b[0m, validation loss: 0.009304\n",
      "tensor(0.0091)\n",
      "iteration 1126, train loss: 0.012313, validation loss: \u001b[92m0.009144\u001b[0m\n",
      "tensor(0.0089)\n",
      "iteration 1127, train loss: 0.012362, validation loss: \u001b[92m0.008943\u001b[0m\n",
      "tensor(0.0088)\n",
      "iteration 1128, train loss: \u001b[92m0.012127\u001b[0m, validation loss: \u001b[92m0.008794\u001b[0m\n",
      "tensor(0.0087)\n",
      "iteration 1129, train loss: 0.012145, validation loss: \u001b[92m0.008748\u001b[0m\n",
      "tensor(0.0088)\n",
      "iteration 1130, train loss: 0.012414, validation loss: 0.008844\n",
      "tensor(0.0090)\n",
      "iteration 1131, train loss: \u001b[92m0.011945\u001b[0m, validation loss: 0.00899\n",
      "tensor(0.0088)\n",
      "iteration 1132, train loss: 0.012082, validation loss: 0.008841\n",
      "tensor(0.0087)\n",
      "iteration 1133, train loss: 0.012245, validation loss: \u001b[92m0.008739\u001b[0m\n",
      "tensor(0.0086)\n",
      "iteration 1134, train loss: 0.011973, validation loss: \u001b[92m0.008645\u001b[0m\n",
      "tensor(0.0087)\n",
      "iteration 1135, train loss: 0.01206, validation loss: 0.008668\n",
      "tensor(0.0087)\n",
      "iteration 1136, train loss: \u001b[92m0.011893\u001b[0m, validation loss: 0.008722\n",
      "tensor(0.0088)\n",
      "iteration 1137, train loss: 0.012227, validation loss: 0.00881\n",
      "tensor(0.0088)\n",
      "iteration 1138, train loss: 0.012177, validation loss: 0.008811\n",
      "tensor(0.0087)\n",
      "iteration 1139, train loss: 0.012019, validation loss: 0.008675\n",
      "tensor(0.0086)\n",
      "iteration 1140, train loss: \u001b[92m0.01183\u001b[0m, validation loss: \u001b[92m0.008592\u001b[0m\n",
      "tensor(0.0085)\n",
      "iteration 1141, train loss: 0.0119, validation loss: \u001b[92m0.008457\u001b[0m\n",
      "tensor(0.0084)\n",
      "iteration 1142, train loss: 0.011905, validation loss: \u001b[92m0.008386\u001b[0m\n",
      "tensor(0.0085)\n",
      "iteration 1143, train loss: 0.01192, validation loss: 0.008475\n",
      "tensor(0.0086)\n",
      "iteration 1144, train loss: \u001b[92m0.011792\u001b[0m, validation loss: 0.008597\n",
      "tensor(0.0087)\n",
      "iteration 1145, train loss: \u001b[92m0.011575\u001b[0m, validation loss: 0.00868\n",
      "tensor(0.0084)\n",
      "iteration 1146, train loss: 0.011814, validation loss: \u001b[92m0.008374\u001b[0m\n",
      "tensor(0.0083)\n",
      "iteration 1147, train loss: 0.011665, validation loss: \u001b[92m0.008302\u001b[0m\n",
      "tensor(0.0083)\n",
      "iteration 1148, train loss: 0.011824, validation loss: 0.008325\n",
      "tensor(0.0085)\n",
      "iteration 1149, train loss: 0.01173, validation loss: 0.008481\n",
      "tensor(0.0086)\n",
      "iteration 1150, train loss: 0.011726, validation loss: 0.008565\n",
      "tensor(0.0084)\n",
      "iteration 1151, train loss: 0.011831, validation loss: 0.008363\n",
      "tensor(0.0083)\n",
      "iteration 1152, train loss: 0.011753, validation loss: \u001b[92m0.008283\u001b[0m\n",
      "tensor(0.0083)\n",
      "iteration 1153, train loss: 0.011749, validation loss: 0.008301\n",
      "tensor(0.0084)\n",
      "iteration 1154, train loss: 0.011624, validation loss: 0.008362\n",
      "tensor(0.0083)\n",
      "iteration 1155, train loss: 0.011727, validation loss: 0.008318\n",
      "tensor(0.0082)\n",
      "iteration 1156, train loss: \u001b[92m0.011559\u001b[0m, validation loss: \u001b[92m0.008237\u001b[0m\n",
      "tensor(0.0082)\n",
      "iteration 1157, train loss: \u001b[92m0.011441\u001b[0m, validation loss: \u001b[92m0.008235\u001b[0m\n",
      "tensor(0.0082)\n",
      "iteration 1158, train loss: 0.011776, validation loss: \u001b[92m0.008232\u001b[0m\n",
      "tensor(0.0083)\n",
      "iteration 1159, train loss: 0.011864, validation loss: 0.008314\n",
      "tensor(0.0083)\n",
      "iteration 1160, train loss: 0.011613, validation loss: 0.008263\n",
      "tensor(0.0081)\n",
      "iteration 1161, train loss: 0.011573, validation loss: \u001b[92m0.008142\u001b[0m\n",
      "tensor(0.0081)\n",
      "iteration 1162, train loss: 0.011611, validation loss: \u001b[92m0.008094\u001b[0m\n",
      "tensor(0.0082)\n",
      "iteration 1163, train loss: 0.011548, validation loss: 0.008151\n",
      "tensor(0.0082)\n",
      "iteration 1164, train loss: \u001b[92m0.011401\u001b[0m, validation loss: 0.00824\n",
      "tensor(0.0081)\n",
      "iteration 1165, train loss: 0.0117, validation loss: 0.008147\n",
      "tensor(0.0081)\n",
      "iteration 1166, train loss: 0.011676, validation loss: 0.008142\n",
      "tensor(0.0081)\n",
      "iteration 1167, train loss: \u001b[92m0.011286\u001b[0m, validation loss: \u001b[92m0.008083\u001b[0m\n",
      "tensor(0.0082)\n",
      "iteration 1168, train loss: 0.011503, validation loss: 0.008168\n",
      "tensor(0.0081)\n",
      "iteration 1169, train loss: 0.011604, validation loss: 0.008088\n",
      "tensor(0.0081)\n",
      "iteration 1170, train loss: 0.01144, validation loss: 0.008095\n",
      "tensor(0.0080)\n",
      "iteration 1171, train loss: 0.011416, validation loss: \u001b[92m0.008045\u001b[0m\n",
      "tensor(0.0081)\n",
      "iteration 1172, train loss: 0.011562, validation loss: 0.008072\n",
      "tensor(0.0082)\n",
      "iteration 1173, train loss: 0.011418, validation loss: 0.00816\n",
      "tensor(0.0083)\n",
      "iteration 1174, train loss: 0.011299, validation loss: 0.00828\n",
      "tensor(0.0083)\n",
      "iteration 1175, train loss: 0.01151, validation loss: 0.008303\n",
      "tensor(0.0081)\n",
      "iteration 1176, train loss: 0.011427, validation loss: 0.008056\n",
      "tensor(0.0080)\n",
      "iteration 1177, train loss: 0.011499, validation loss: \u001b[92m0.007994\u001b[0m\n",
      "tensor(0.0081)\n",
      "iteration 1178, train loss: \u001b[92m0.011244\u001b[0m, validation loss: 0.008137\n",
      "tensor(0.0082)\n",
      "iteration 1179, train loss: 0.011249, validation loss: 0.008197\n",
      "tensor(0.0081)\n",
      "iteration 1180, train loss: 0.011306, validation loss: 0.008134\n",
      "tensor(0.0081)\n",
      "iteration 1181, train loss: 0.011386, validation loss: 0.008051\n",
      "tensor(0.0079)\n",
      "iteration 1182, train loss: 0.011342, validation loss: \u001b[92m0.007926\u001b[0m\n",
      "tensor(0.0078)\n",
      "iteration 1183, train loss: 0.011351, validation loss: \u001b[92m0.007792\u001b[0m\n",
      "tensor(0.0078)\n",
      "iteration 1184, train loss: \u001b[92m0.011237\u001b[0m, validation loss: 0.007795\n",
      "tensor(0.0080)\n",
      "iteration 1185, train loss: 0.011633, validation loss: 0.008011\n",
      "tensor(0.0082)\n",
      "iteration 1186, train loss: 0.011274, validation loss: 0.008218\n",
      "tensor(0.0079)\n",
      "iteration 1187, train loss: 0.011322, validation loss: 0.007918\n",
      "tensor(0.0078)\n",
      "iteration 1188, train loss: 0.011259, validation loss: 0.007821\n",
      "tensor(0.0078)\n",
      "iteration 1189, train loss: 0.011342, validation loss: \u001b[92m0.007783\u001b[0m\n",
      "tensor(0.0078)\n",
      "iteration 1190, train loss: \u001b[92m0.011216\u001b[0m, validation loss: 0.007827\n",
      "tensor(0.0079)\n",
      "iteration 1191, train loss: 0.011235, validation loss: 0.007906\n",
      "tensor(0.0079)\n",
      "iteration 1192, train loss: 0.011275, validation loss: 0.007859\n",
      "tensor(0.0078)\n",
      "iteration 1193, train loss: 0.011247, validation loss: \u001b[92m0.007775\u001b[0m\n",
      "tensor(0.0077)\n",
      "iteration 1194, train loss: 0.011236, validation loss: \u001b[92m0.007716\u001b[0m\n",
      "tensor(0.0076)\n",
      "iteration 1195, train loss: 0.011314, validation loss: \u001b[92m0.007631\u001b[0m\n",
      "tensor(0.0076)\n",
      "iteration 1196, train loss: \u001b[92m0.011054\u001b[0m, validation loss: 0.007632\n",
      "tensor(0.0078)\n",
      "iteration 1197, train loss: 0.011147, validation loss: 0.007805\n",
      "tensor(0.0080)\n",
      "iteration 1198, train loss: 0.011098, validation loss: 0.007963\n",
      "tensor(0.0077)\n",
      "iteration 1199, train loss: \u001b[92m0.010975\u001b[0m, validation loss: 0.00771\n",
      "tensor(0.0076)\n",
      "iteration 1200, train loss: 0.011159, validation loss: \u001b[92m0.007584\u001b[0m\n",
      "tensor(0.0077)\n",
      "iteration 1201, train loss: \u001b[92m0.010922\u001b[0m, validation loss: 0.007732\n",
      "tensor(0.0078)\n",
      "iteration 1202, train loss: 0.011033, validation loss: 0.007847\n",
      "tensor(0.0076)\n",
      "iteration 1203, train loss: 0.011031, validation loss: 0.007632\n",
      "tensor(0.0075)\n",
      "iteration 1204, train loss: 0.011128, validation loss: \u001b[92m0.007541\u001b[0m\n",
      "tensor(0.0076)\n",
      "iteration 1205, train loss: 0.01109, validation loss: 0.007568\n",
      "tensor(0.0078)\n",
      "iteration 1206, train loss: 0.011028, validation loss: 0.007819\n",
      "tensor(0.0078)\n",
      "iteration 1207, train loss: 0.010976, validation loss: 0.007795\n",
      "tensor(0.0075)\n",
      "iteration 1208, train loss: 0.011053, validation loss: \u001b[92m0.007503\u001b[0m\n",
      "tensor(0.0074)\n",
      "iteration 1209, train loss: 0.011137, validation loss: \u001b[92m0.00744\u001b[0m\n",
      "tensor(0.0075)\n",
      "iteration 1210, train loss: 0.011249, validation loss: 0.007496\n",
      "tensor(0.0076)\n",
      "iteration 1211, train loss: 0.010974, validation loss: 0.007601\n",
      "tensor(0.0076)\n",
      "iteration 1212, train loss: 0.011007, validation loss: 0.0076\n",
      "tensor(0.0075)\n",
      "iteration 1213, train loss: 0.011015, validation loss: 0.007502\n",
      "tensor(0.0075)\n",
      "iteration 1214, train loss: 0.010962, validation loss: 0.007468\n",
      "tensor(0.0077)\n",
      "iteration 1215, train loss: 0.011107, validation loss: 0.007668\n",
      "tensor(0.0076)\n",
      "iteration 1216, train loss: \u001b[92m0.010736\u001b[0m, validation loss: 0.007639\n",
      "tensor(0.0075)\n",
      "iteration 1217, train loss: 0.010945, validation loss: 0.007504\n",
      "tensor(0.0074)\n",
      "iteration 1218, train loss: 0.01115, validation loss: \u001b[92m0.007421\u001b[0m\n",
      "tensor(0.0077)\n",
      "iteration 1219, train loss: 0.011028, validation loss: 0.00769\n",
      "tensor(0.0075)\n",
      "iteration 1220, train loss: 0.011074, validation loss: 0.007545\n",
      "tensor(0.0073)\n",
      "iteration 1221, train loss: 0.011003, validation loss: \u001b[92m0.007348\u001b[0m\n",
      "tensor(0.0073)\n",
      "iteration 1222, train loss: 0.010878, validation loss: \u001b[92m0.007324\u001b[0m\n",
      "tensor(0.0078)\n",
      "iteration 1223, train loss: 0.010858, validation loss: 0.007762\n",
      "tensor(0.0077)\n",
      "iteration 1224, train loss: \u001b[92m0.010723\u001b[0m, validation loss: 0.00774\n",
      "tensor(0.0072)\n",
      "iteration 1225, train loss: 0.01078, validation loss: \u001b[92m0.007234\u001b[0m\n",
      "tensor(0.0072)\n",
      "iteration 1226, train loss: 0.011016, validation loss: \u001b[92m0.007189\u001b[0m\n",
      "tensor(0.0073)\n",
      "iteration 1227, train loss: 0.010835, validation loss: 0.007339\n",
      "tensor(0.0081)\n",
      "iteration 1228, train loss: \u001b[92m0.010676\u001b[0m, validation loss: 0.008089\n",
      "tensor(0.0075)\n",
      "iteration 1229, train loss: 0.011186, validation loss: 0.007476\n",
      "tensor(0.0071)\n",
      "iteration 1230, train loss: 0.010922, validation loss: \u001b[92m0.007138\u001b[0m\n",
      "tensor(0.0071)\n",
      "iteration 1231, train loss: \u001b[92m0.010587\u001b[0m, validation loss: \u001b[92m0.007111\u001b[0m\n",
      "tensor(0.0074)\n",
      "iteration 1232, train loss: 0.010751, validation loss: 0.007404\n",
      "tensor(0.0079)\n",
      "iteration 1233, train loss: 0.010667, validation loss: 0.007939\n",
      "tensor(0.0073)\n",
      "iteration 1234, train loss: 0.011018, validation loss: 0.007297\n",
      "tensor(0.0070)\n",
      "iteration 1235, train loss: 0.010704, validation loss: \u001b[92m0.007044\u001b[0m\n",
      "tensor(0.0071)\n",
      "iteration 1236, train loss: 0.010978, validation loss: 0.007125\n",
      "tensor(0.0076)\n",
      "iteration 1237, train loss: 0.010723, validation loss: 0.007638\n",
      "tensor(0.0075)\n",
      "iteration 1238, train loss: 0.010785, validation loss: 0.007456\n",
      "tensor(0.0070)\n",
      "iteration 1239, train loss: \u001b[92m0.010537\u001b[0m, validation loss: \u001b[92m0.006983\u001b[0m\n",
      "tensor(0.0069)\n",
      "iteration 1240, train loss: 0.010549, validation loss: \u001b[92m0.006943\u001b[0m\n",
      "tensor(0.0072)\n",
      "iteration 1241, train loss: 0.010537, validation loss: 0.007233\n",
      "tensor(0.0074)\n",
      "iteration 1242, train loss: \u001b[92m0.010294\u001b[0m, validation loss: 0.007426\n",
      "tensor(0.0071)\n",
      "iteration 1243, train loss: 0.010699, validation loss: 0.007117\n",
      "tensor(0.0069)\n",
      "iteration 1244, train loss: 0.01053, validation loss: \u001b[92m0.006851\u001b[0m\n",
      "tensor(0.0069)\n",
      "iteration 1245, train loss: 0.010333, validation loss: 0.006922\n",
      "tensor(0.0073)\n",
      "iteration 1246, train loss: 0.010478, validation loss: 0.007326\n",
      "tensor(0.0071)\n",
      "iteration 1247, train loss: 0.010323, validation loss: 0.007135\n",
      "tensor(0.0067)\n",
      "iteration 1248, train loss: 0.010305, validation loss: \u001b[92m0.006721\u001b[0m\n",
      "tensor(0.0067)\n",
      "iteration 1249, train loss: \u001b[92m0.010275\u001b[0m, validation loss: 0.006734\n",
      "tensor(0.0071)\n",
      "iteration 1250, train loss: \u001b[92m0.010183\u001b[0m, validation loss: 0.007142\n",
      "tensor(0.0076)\n",
      "iteration 1251, train loss: \u001b[92m0.009958\u001b[0m, validation loss: 0.007607\n",
      "tensor(0.0065)\n",
      "iteration 1252, train loss: 0.010341, validation loss: \u001b[92m0.006533\u001b[0m\n",
      "tensor(0.0067)\n",
      "iteration 1253, train loss: 0.010383, validation loss: 0.006687\n",
      "tensor(0.0074)\n",
      "iteration 1254, train loss: \u001b[92m0.009631\u001b[0m, validation loss: 0.007381\n",
      "tensor(0.0065)\n",
      "iteration 1255, train loss: 0.010202, validation loss: \u001b[92m0.006504\u001b[0m\n",
      "tensor(0.0067)\n",
      "iteration 1256, train loss: 0.009907, validation loss: 0.006657\n",
      "tensor(0.0070)\n",
      "iteration 1257, train loss: 0.01001, validation loss: 0.006965\n",
      "tensor(0.0066)\n",
      "iteration 1258, train loss: 0.009858, validation loss: 0.006634\n",
      "tensor(0.0065)\n",
      "iteration 1259, train loss: 0.009896, validation loss: \u001b[92m0.00647\u001b[0m\n",
      "tensor(0.0066)\n",
      "iteration 1260, train loss: 0.009813, validation loss: 0.006632\n",
      "tensor(0.0068)\n",
      "iteration 1261, train loss: 0.009684, validation loss: 0.006755\n",
      "tensor(0.0065)\n",
      "iteration 1262, train loss: 0.009769, validation loss: 0.00649\n",
      "tensor(0.0063)\n",
      "iteration 1263, train loss: \u001b[92m0.009578\u001b[0m, validation loss: \u001b[92m0.006279\u001b[0m\n",
      "tensor(0.0063)\n",
      "iteration 1264, train loss: \u001b[92m0.009441\u001b[0m, validation loss: 0.006308\n",
      "tensor(0.0068)\n",
      "iteration 1265, train loss: 0.00955, validation loss: 0.006781\n",
      "tensor(0.0064)\n",
      "iteration 1266, train loss: 0.009894, validation loss: 0.006428\n",
      "tensor(0.0062)\n",
      "iteration 1267, train loss: 0.009454, validation loss: \u001b[92m0.006235\u001b[0m\n",
      "tensor(0.0065)\n",
      "iteration 1268, train loss: 0.00988, validation loss: 0.006453\n",
      "tensor(0.0068)\n",
      "iteration 1269, train loss: 0.009625, validation loss: 0.006753\n",
      "tensor(0.0063)\n",
      "iteration 1270, train loss: 0.009546, validation loss: 0.006267\n",
      "tensor(0.0061)\n",
      "iteration 1271, train loss: \u001b[92m0.009384\u001b[0m, validation loss: \u001b[92m0.00611\u001b[0m\n",
      "tensor(0.0067)\n",
      "iteration 1272, train loss: 0.009895, validation loss: 0.006664\n",
      "tensor(0.0067)\n",
      "iteration 1273, train loss: 0.009458, validation loss: 0.006683\n",
      "tensor(0.0061)\n",
      "iteration 1274, train loss: 0.009547, validation loss: 0.00611\n",
      "tensor(0.0061)\n",
      "iteration 1275, train loss: 0.009631, validation loss: \u001b[92m0.006052\u001b[0m\n",
      "tensor(0.0062)\n",
      "iteration 1276, train loss: 0.009536, validation loss: 0.006184\n",
      "tensor(0.0062)\n",
      "iteration 1277, train loss: 0.009596, validation loss: 0.006166\n",
      "tensor(0.0060)\n",
      "iteration 1278, train loss: \u001b[92m0.009184\u001b[0m, validation loss: \u001b[92m0.006034\u001b[0m\n",
      "tensor(0.0063)\n",
      "iteration 1279, train loss: 0.009333, validation loss: 0.006292\n",
      "tensor(0.0062)\n",
      "iteration 1280, train loss: 0.009224, validation loss: 0.006241\n",
      "tensor(0.0060)\n",
      "iteration 1281, train loss: 0.009201, validation loss: \u001b[92m0.005966\u001b[0m\n",
      "tensor(0.0060)\n",
      "iteration 1282, train loss: 0.009261, validation loss: 0.005975\n",
      "tensor(0.0062)\n",
      "iteration 1283, train loss: 0.009242, validation loss: 0.006245\n",
      "tensor(0.0064)\n",
      "iteration 1284, train loss: 0.009256, validation loss: 0.006412\n",
      "tensor(0.0060)\n",
      "iteration 1285, train loss: 0.009401, validation loss: 0.005974\n",
      "tensor(0.0060)\n",
      "iteration 1286, train loss: 0.009366, validation loss: 0.00601\n",
      "tensor(0.0069)\n",
      "iteration 1287, train loss: 0.00928, validation loss: 0.00687\n",
      "tensor(0.0069)\n",
      "iteration 1288, train loss: 0.009651, validation loss: 0.006921\n",
      "tensor(0.0059)\n",
      "iteration 1289, train loss: 0.009463, validation loss: \u001b[92m0.00587\u001b[0m\n",
      "tensor(0.0060)\n",
      "iteration 1290, train loss: 0.009344, validation loss: 0.005951\n",
      "tensor(0.0069)\n",
      "iteration 1291, train loss: 0.009519, validation loss: 0.00687\n",
      "tensor(0.0067)\n",
      "iteration 1292, train loss: 0.009616, validation loss: 0.006717\n",
      "tensor(0.0059)\n",
      "iteration 1293, train loss: 0.009464, validation loss: \u001b[92m0.005853\u001b[0m\n",
      "tensor(0.0059)\n",
      "iteration 1294, train loss: 0.009201, validation loss: 0.005948\n",
      "tensor(0.0060)\n",
      "iteration 1295, train loss: 0.009419, validation loss: 0.006006\n",
      "tensor(0.0066)\n",
      "iteration 1296, train loss: 0.009231, validation loss: 0.006614\n",
      "tensor(0.0059)\n",
      "iteration 1297, train loss: 0.009396, validation loss: 0.005929\n",
      "tensor(0.0058)\n",
      "iteration 1298, train loss: 0.009213, validation loss: \u001b[92m0.005823\u001b[0m\n",
      "tensor(0.0061)\n",
      "iteration 1299, train loss: \u001b[92m0.009116\u001b[0m, validation loss: 0.006084\n",
      "tensor(0.0063)\n",
      "iteration 1300, train loss: \u001b[92m0.009081\u001b[0m, validation loss: 0.00625\n",
      "tensor(0.0060)\n",
      "iteration 1301, train loss: 0.009216, validation loss: 0.00602\n",
      "tensor(0.0058)\n",
      "iteration 1302, train loss: \u001b[92m0.009073\u001b[0m, validation loss: 0.005833\n",
      "tensor(0.0059)\n",
      "iteration 1303, train loss: 0.009201, validation loss: 0.005925\n",
      "tensor(0.0062)\n",
      "iteration 1304, train loss: \u001b[92m0.008932\u001b[0m, validation loss: 0.006176\n",
      "tensor(0.0059)\n",
      "iteration 1305, train loss: 0.009046, validation loss: 0.005905\n",
      "tensor(0.0058)\n",
      "iteration 1306, train loss: \u001b[92m0.008859\u001b[0m, validation loss: \u001b[92m0.005762\u001b[0m\n",
      "tensor(0.0058)\n",
      "iteration 1307, train loss: 0.009014, validation loss: 0.005842\n",
      "tensor(0.0060)\n",
      "iteration 1308, train loss: 0.008921, validation loss: 0.006031\n",
      "tensor(0.0061)\n",
      "iteration 1309, train loss: 0.009023, validation loss: 0.006054\n",
      "tensor(0.0057)\n",
      "iteration 1310, train loss: 0.009298, validation loss: \u001b[92m0.005723\u001b[0m\n",
      "tensor(0.0059)\n",
      "iteration 1311, train loss: 0.009124, validation loss: 0.005855\n",
      "tensor(0.0060)\n",
      "iteration 1312, train loss: \u001b[92m0.008852\u001b[0m, validation loss: 0.005981\n",
      "tensor(0.0058)\n",
      "iteration 1313, train loss: 0.008934, validation loss: 0.005788\n",
      "tensor(0.0058)\n",
      "iteration 1314, train loss: \u001b[92m0.008811\u001b[0m, validation loss: 0.005769\n",
      "tensor(0.0058)\n",
      "iteration 1315, train loss: \u001b[92m0.008746\u001b[0m, validation loss: 0.005847\n",
      "tensor(0.0059)\n",
      "iteration 1316, train loss: 0.00895, validation loss: 0.00593\n",
      "tensor(0.0059)\n",
      "iteration 1317, train loss: 0.008866, validation loss: 0.005916\n",
      "tensor(0.0058)\n",
      "iteration 1318, train loss: 0.008894, validation loss: 0.005814\n",
      "tensor(0.0057)\n",
      "iteration 1319, train loss: 0.008803, validation loss: \u001b[92m0.005714\u001b[0m\n",
      "tensor(0.0057)\n",
      "iteration 1320, train loss: 0.008943, validation loss: 0.005723\n",
      "tensor(0.0061)\n",
      "iteration 1321, train loss: 0.008809, validation loss: 0.006111\n",
      "tensor(0.0060)\n",
      "iteration 1322, train loss: 0.008915, validation loss: 0.005955\n",
      "tensor(0.0057)\n",
      "iteration 1323, train loss: 0.008871, validation loss: \u001b[92m0.005664\u001b[0m\n",
      "tensor(0.0058)\n",
      "iteration 1324, train loss: 0.008765, validation loss: 0.005786\n",
      "tensor(0.0059)\n",
      "iteration 1325, train loss: 0.008883, validation loss: 0.005881\n",
      "tensor(0.0057)\n",
      "iteration 1326, train loss: 0.008763, validation loss: 0.005671\n",
      "tensor(0.0057)\n",
      "iteration 1327, train loss: 0.00887, validation loss: 0.005723\n",
      "tensor(0.0058)\n",
      "iteration 1328, train loss: 0.008785, validation loss: 0.005827\n",
      "tensor(0.0060)\n",
      "iteration 1329, train loss: 0.008952, validation loss: 0.00604\n",
      "tensor(0.0057)\n",
      "iteration 1330, train loss: 0.008961, validation loss: 0.005688\n",
      "tensor(0.0057)\n",
      "iteration 1331, train loss: 0.008813, validation loss: \u001b[92m0.005658\u001b[0m\n",
      "tensor(0.0057)\n",
      "iteration 1332, train loss: 0.008825, validation loss: 0.005736\n",
      "tensor(0.0061)\n",
      "iteration 1333, train loss: \u001b[92m0.008684\u001b[0m, validation loss: 0.006115\n",
      "tensor(0.0058)\n",
      "iteration 1334, train loss: 0.00892, validation loss: 0.00578\n",
      "tensor(0.0058)\n",
      "iteration 1335, train loss: \u001b[92m0.008588\u001b[0m, validation loss: 0.005765\n",
      "tensor(0.0056)\n",
      "iteration 1336, train loss: 0.008705, validation loss: \u001b[92m0.005601\u001b[0m\n",
      "tensor(0.0056)\n",
      "iteration 1337, train loss: 0.008728, validation loss: 0.005623\n",
      "tensor(0.0060)\n",
      "iteration 1338, train loss: 0.008768, validation loss: 0.00602\n",
      "tensor(0.0057)\n",
      "iteration 1339, train loss: 0.008947, validation loss: 0.005661\n",
      "tensor(0.0058)\n",
      "iteration 1340, train loss: 0.008799, validation loss: 0.005766\n",
      "tensor(0.0056)\n",
      "iteration 1341, train loss: 0.008827, validation loss: 0.005628\n",
      "tensor(0.0056)\n",
      "iteration 1342, train loss: 0.008688, validation loss: 0.005647\n",
      "tensor(0.0057)\n",
      "iteration 1343, train loss: \u001b[92m0.00856\u001b[0m, validation loss: 0.005651\n",
      "tensor(0.0060)\n",
      "iteration 1344, train loss: 0.008879, validation loss: 0.005977\n",
      "tensor(0.0056)\n",
      "iteration 1345, train loss: 0.008893, validation loss: 0.005635\n",
      "tensor(0.0056)\n",
      "iteration 1346, train loss: 0.008823, validation loss: 0.005606\n",
      "tensor(0.0058)\n",
      "iteration 1347, train loss: 0.008567, validation loss: 0.005844\n",
      "tensor(0.0057)\n",
      "iteration 1348, train loss: 0.008678, validation loss: 0.005668\n",
      "tensor(0.0056)\n",
      "iteration 1349, train loss: 0.008659, validation loss: 0.005649\n",
      "tensor(0.0057)\n",
      "iteration 1350, train loss: 0.00875, validation loss: 0.005715\n",
      "tensor(0.0056)\n",
      "iteration 1351, train loss: 0.008702, validation loss: 0.005635\n",
      "tensor(0.0056)\n",
      "iteration 1352, train loss: 0.008585, validation loss: \u001b[92m0.005574\u001b[0m\n",
      "tensor(0.0057)\n",
      "iteration 1353, train loss: 0.008561, validation loss: 0.00573\n",
      "tensor(0.0058)\n",
      "iteration 1354, train loss: \u001b[92m0.008497\u001b[0m, validation loss: 0.005801\n",
      "tensor(0.0056)\n",
      "iteration 1355, train loss: \u001b[92m0.008449\u001b[0m, validation loss: 0.005575\n",
      "tensor(0.0055)\n",
      "iteration 1356, train loss: 0.008612, validation loss: \u001b[92m0.005512\u001b[0m\n",
      "tensor(0.0060)\n",
      "iteration 1357, train loss: 0.008557, validation loss: 0.005995\n",
      "tensor(0.0057)\n",
      "iteration 1358, train loss: 0.008743, validation loss: 0.005664\n",
      "tensor(0.0056)\n",
      "iteration 1359, train loss: 0.008452, validation loss: 0.005556\n",
      "tensor(0.0060)\n",
      "iteration 1360, train loss: 0.008761, validation loss: 0.005965\n",
      "tensor(0.0059)\n",
      "iteration 1361, train loss: 0.008746, validation loss: 0.005878\n",
      "tensor(0.0055)\n",
      "iteration 1362, train loss: 0.008561, validation loss: \u001b[92m0.005488\u001b[0m\n",
      "tensor(0.0056)\n",
      "iteration 1363, train loss: 0.008521, validation loss: 0.005575\n",
      "tensor(0.0062)\n",
      "iteration 1364, train loss: 0.008597, validation loss: 0.006166\n",
      "tensor(0.0056)\n",
      "iteration 1365, train loss: 0.008884, validation loss: 0.005582\n",
      "tensor(0.0055)\n",
      "iteration 1366, train loss: 0.008461, validation loss: 0.005489\n",
      "tensor(0.0055)\n",
      "iteration 1367, train loss: 0.008625, validation loss: 0.005541\n",
      "tensor(0.0058)\n",
      "iteration 1368, train loss: 0.008515, validation loss: 0.005821\n",
      "tensor(0.0057)\n",
      "iteration 1369, train loss: 0.008777, validation loss: 0.005686\n",
      "tensor(0.0055)\n",
      "iteration 1370, train loss: 0.008459, validation loss: 0.00551\n",
      "tensor(0.0056)\n",
      "iteration 1371, train loss: 0.008617, validation loss: 0.005638\n",
      "tensor(0.0056)\n",
      "iteration 1372, train loss: 0.00854, validation loss: 0.005636\n",
      "tensor(0.0055)\n",
      "iteration 1373, train loss: 0.008866, validation loss: \u001b[92m0.005456\u001b[0m\n",
      "tensor(0.0056)\n",
      "iteration 1374, train loss: 0.008531, validation loss: 0.00565\n",
      "tensor(0.0058)\n",
      "iteration 1375, train loss: 0.008461, validation loss: 0.005833\n",
      "tensor(0.0055)\n",
      "iteration 1376, train loss: 0.008465, validation loss: \u001b[92m0.005454\u001b[0m\n",
      "tensor(0.0054)\n",
      "iteration 1377, train loss: \u001b[92m0.008213\u001b[0m, validation loss: \u001b[92m0.005418\u001b[0m\n",
      "tensor(0.0057)\n",
      "iteration 1378, train loss: 0.008481, validation loss: 0.005685\n",
      "tensor(0.0058)\n",
      "iteration 1379, train loss: 0.008473, validation loss: 0.005766\n",
      "tensor(0.0055)\n",
      "iteration 1380, train loss: 0.008435, validation loss: 0.005549\n",
      "tensor(0.0055)\n",
      "iteration 1381, train loss: 0.008752, validation loss: 0.005456\n",
      "tensor(0.0059)\n",
      "iteration 1382, train loss: 0.008602, validation loss: 0.00589\n",
      "tensor(0.0056)\n",
      "iteration 1383, train loss: 0.008727, validation loss: 0.005649\n",
      "tensor(0.0055)\n",
      "iteration 1384, train loss: 0.008305, validation loss: 0.005463\n",
      "tensor(0.0055)\n",
      "iteration 1385, train loss: 0.008593, validation loss: 0.005518\n",
      "tensor(0.0063)\n",
      "iteration 1386, train loss: 0.008236, validation loss: 0.006299\n",
      "tensor(0.0055)\n",
      "iteration 1387, train loss: 0.008708, validation loss: 0.005483\n",
      "tensor(0.0055)\n",
      "iteration 1388, train loss: 0.008441, validation loss: 0.005501\n",
      "tensor(0.0056)\n",
      "iteration 1389, train loss: 0.008612, validation loss: 0.005622\n",
      "tensor(0.0057)\n",
      "iteration 1390, train loss: 0.008329, validation loss: 0.005678\n",
      "tensor(0.0054)\n",
      "iteration 1391, train loss: 0.00855, validation loss: 0.00542\n",
      "tensor(0.0054)\n",
      "iteration 1392, train loss: 0.008579, validation loss: 0.005438\n",
      "tensor(0.0056)\n",
      "iteration 1393, train loss: 0.008351, validation loss: 0.005582\n",
      "tensor(0.0054)\n",
      "iteration 1394, train loss: 0.008469, validation loss: \u001b[92m0.00539\u001b[0m\n",
      "tensor(0.0054)\n",
      "iteration 1395, train loss: 0.008215, validation loss: \u001b[92m0.005376\u001b[0m\n",
      "tensor(0.0056)\n",
      "iteration 1396, train loss: 0.008331, validation loss: 0.005609\n",
      "tensor(0.0056)\n",
      "iteration 1397, train loss: 0.008254, validation loss: 0.005636\n",
      "tensor(0.0055)\n",
      "iteration 1398, train loss: 0.008411, validation loss: 0.00546\n",
      "tensor(0.0055)\n",
      "iteration 1399, train loss: 0.00841, validation loss: 0.005464\n",
      "tensor(0.0058)\n",
      "iteration 1400, train loss: 0.008372, validation loss: 0.00582\n",
      "tensor(0.0054)\n",
      "iteration 1401, train loss: 0.008443, validation loss: 0.005413\n",
      "tensor(0.0055)\n",
      "iteration 1402, train loss: \u001b[92m0.008132\u001b[0m, validation loss: 0.005472\n",
      "tensor(0.0055)\n",
      "iteration 1403, train loss: 0.008306, validation loss: 0.005468\n",
      "tensor(0.0057)\n",
      "iteration 1404, train loss: 0.008198, validation loss: 0.005698\n",
      "tensor(0.0053)\n",
      "iteration 1405, train loss: 0.008788, validation loss: \u001b[92m0.005334\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1406, train loss: 0.008387, validation loss: 0.005337\n",
      "tensor(0.0054)\n",
      "iteration 1407, train loss: 0.008306, validation loss: 0.005433\n",
      "tensor(0.0057)\n",
      "iteration 1408, train loss: 0.008285, validation loss: 0.005717\n",
      "tensor(0.0055)\n",
      "iteration 1409, train loss: 0.008256, validation loss: 0.005472\n",
      "tensor(0.0054)\n",
      "iteration 1410, train loss: 0.008266, validation loss: 0.005379\n",
      "tensor(0.0054)\n",
      "iteration 1411, train loss: 0.008377, validation loss: 0.005382\n",
      "tensor(0.0057)\n",
      "iteration 1412, train loss: 0.008296, validation loss: 0.005689\n",
      "tensor(0.0054)\n",
      "iteration 1413, train loss: 0.00825, validation loss: 0.005357\n",
      "tensor(0.0054)\n",
      "iteration 1414, train loss: 0.008145, validation loss: 0.005401\n",
      "tensor(0.0055)\n",
      "iteration 1415, train loss: 0.008639, validation loss: 0.005544\n",
      "tensor(0.0057)\n",
      "iteration 1416, train loss: 0.008326, validation loss: 0.005717\n",
      "tensor(0.0054)\n",
      "iteration 1417, train loss: 0.008353, validation loss: 0.005352\n",
      "tensor(0.0053)\n",
      "iteration 1418, train loss: 0.008185, validation loss: \u001b[92m0.005301\u001b[0m\n",
      "tensor(0.0057)\n",
      "iteration 1419, train loss: 0.008313, validation loss: 0.005684\n",
      "tensor(0.0056)\n",
      "iteration 1420, train loss: 0.008307, validation loss: 0.005585\n",
      "tensor(0.0053)\n",
      "iteration 1421, train loss: \u001b[92m0.008123\u001b[0m, validation loss: 0.005334\n",
      "tensor(0.0054)\n",
      "iteration 1422, train loss: 0.008448, validation loss: 0.00539\n",
      "tensor(0.0056)\n",
      "iteration 1423, train loss: 0.008242, validation loss: 0.005624\n",
      "tensor(0.0053)\n",
      "iteration 1424, train loss: 0.008313, validation loss: 0.00532\n",
      "tensor(0.0053)\n",
      "iteration 1425, train loss: 0.00821, validation loss: 0.005306\n",
      "tensor(0.0055)\n",
      "iteration 1426, train loss: \u001b[92m0.00805\u001b[0m, validation loss: 0.005485\n",
      "tensor(0.0054)\n",
      "iteration 1427, train loss: 0.008173, validation loss: 0.005359\n",
      "tensor(0.0053)\n",
      "iteration 1428, train loss: 0.008105, validation loss: \u001b[92m0.005277\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1429, train loss: 0.008193, validation loss: 0.00532\n",
      "tensor(0.0055)\n",
      "iteration 1430, train loss: 0.008138, validation loss: 0.005452\n",
      "tensor(0.0053)\n",
      "iteration 1431, train loss: 0.008147, validation loss: 0.005349\n",
      "tensor(0.0053)\n",
      "iteration 1432, train loss: 0.008187, validation loss: \u001b[92m0.005265\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1433, train loss: 0.008372, validation loss: 0.005337\n",
      "tensor(0.0055)\n",
      "iteration 1434, train loss: 0.00829, validation loss: 0.00547\n",
      "tensor(0.0054)\n",
      "iteration 1435, train loss: 0.008211, validation loss: 0.005399\n",
      "tensor(0.0054)\n",
      "iteration 1436, train loss: 0.008235, validation loss: 0.005356\n",
      "tensor(0.0055)\n",
      "iteration 1437, train loss: 0.008349, validation loss: 0.005544\n",
      "tensor(0.0056)\n",
      "iteration 1438, train loss: 0.008201, validation loss: 0.005593\n",
      "tensor(0.0053)\n",
      "iteration 1439, train loss: \u001b[92m0.007984\u001b[0m, validation loss: 0.005308\n",
      "tensor(0.0054)\n",
      "iteration 1440, train loss: 0.008428, validation loss: 0.005352\n",
      "tensor(0.0059)\n",
      "iteration 1441, train loss: 0.008289, validation loss: 0.00591\n",
      "tensor(0.0057)\n",
      "iteration 1442, train loss: 0.008337, validation loss: 0.005704\n",
      "tensor(0.0054)\n",
      "iteration 1443, train loss: 0.008256, validation loss: 0.005365\n",
      "tensor(0.0052)\n",
      "iteration 1444, train loss: 0.008443, validation loss: \u001b[92m0.005235\u001b[0m\n",
      "tensor(0.0058)\n",
      "iteration 1445, train loss: 0.008175, validation loss: 0.005778\n",
      "tensor(0.0056)\n",
      "iteration 1446, train loss: 0.008351, validation loss: 0.00558\n",
      "tensor(0.0054)\n",
      "iteration 1447, train loss: 0.008115, validation loss: 0.00542\n",
      "tensor(0.0052)\n",
      "iteration 1448, train loss: 0.008272, validation loss: 0.005242\n",
      "tensor(0.0058)\n",
      "iteration 1449, train loss: 0.008157, validation loss: 0.005763\n",
      "tensor(0.0056)\n",
      "iteration 1450, train loss: 0.008319, validation loss: 0.005644\n",
      "tensor(0.0053)\n",
      "iteration 1451, train loss: 0.008215, validation loss: 0.005266\n",
      "tensor(0.0055)\n",
      "iteration 1452, train loss: 0.008233, validation loss: 0.005519\n",
      "tensor(0.0059)\n",
      "iteration 1453, train loss: 0.008577, validation loss: 0.005859\n",
      "tensor(0.0059)\n",
      "iteration 1454, train loss: 0.008316, validation loss: 0.005878\n",
      "tensor(0.0052)\n",
      "iteration 1455, train loss: 0.008343, validation loss: \u001b[92m0.005225\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1456, train loss: 0.008257, validation loss: 0.005333\n",
      "tensor(0.0053)\n",
      "iteration 1457, train loss: 0.008312, validation loss: 0.005335\n",
      "tensor(0.0059)\n",
      "iteration 1458, train loss: 0.008007, validation loss: 0.00586\n",
      "tensor(0.0053)\n",
      "iteration 1459, train loss: 0.008336, validation loss: 0.005254\n",
      "tensor(0.0053)\n",
      "iteration 1460, train loss: 0.008021, validation loss: 0.005259\n",
      "tensor(0.0053)\n",
      "iteration 1461, train loss: 0.008074, validation loss: 0.005312\n",
      "tensor(0.0055)\n",
      "iteration 1462, train loss: \u001b[92m0.007974\u001b[0m, validation loss: 0.0055\n",
      "tensor(0.0054)\n",
      "iteration 1463, train loss: 0.008251, validation loss: 0.005402\n",
      "tensor(0.0053)\n",
      "iteration 1464, train loss: 0.008085, validation loss: 0.005316\n",
      "tensor(0.0052)\n",
      "iteration 1465, train loss: 0.008261, validation loss: \u001b[92m0.005209\u001b[0m\n",
      "tensor(0.0054)\n",
      "iteration 1466, train loss: 0.00802, validation loss: 0.005358\n",
      "tensor(0.0054)\n",
      "iteration 1467, train loss: 0.00803, validation loss: 0.00541\n",
      "tensor(0.0052)\n",
      "iteration 1468, train loss: 0.008064, validation loss: 0.005239\n",
      "tensor(0.0053)\n",
      "iteration 1469, train loss: \u001b[92m0.007967\u001b[0m, validation loss: 0.005256\n",
      "tensor(0.0053)\n",
      "iteration 1470, train loss: 0.008145, validation loss: 0.005263\n",
      "tensor(0.0053)\n",
      "iteration 1471, train loss: \u001b[92m0.00788\u001b[0m, validation loss: 0.005346\n",
      "tensor(0.0053)\n",
      "iteration 1472, train loss: 0.008053, validation loss: 0.005313\n",
      "tensor(0.0052)\n",
      "iteration 1473, train loss: 0.007981, validation loss: 0.005214\n",
      "tensor(0.0052)\n",
      "iteration 1474, train loss: \u001b[92m0.007846\u001b[0m, validation loss: \u001b[92m0.005199\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1475, train loss: 0.007933, validation loss: 0.005222\n",
      "tensor(0.0054)\n",
      "iteration 1476, train loss: \u001b[92m0.007821\u001b[0m, validation loss: 0.005355\n",
      "tensor(0.0054)\n",
      "iteration 1477, train loss: 0.007857, validation loss: 0.005358\n",
      "tensor(0.0052)\n",
      "iteration 1478, train loss: 0.007977, validation loss: \u001b[92m0.005197\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1479, train loss: \u001b[92m0.007756\u001b[0m, validation loss: 0.005207\n",
      "tensor(0.0052)\n",
      "iteration 1480, train loss: 0.008041, validation loss: \u001b[92m0.005184\u001b[0m\n",
      "tensor(0.0054)\n",
      "iteration 1481, train loss: 0.007833, validation loss: 0.00535\n",
      "tensor(0.0053)\n",
      "iteration 1482, train loss: 0.008003, validation loss: 0.005318\n",
      "tensor(0.0052)\n",
      "iteration 1483, train loss: 0.007908, validation loss: \u001b[92m0.005173\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1484, train loss: 0.007993, validation loss: 0.005207\n",
      "tensor(0.0054)\n",
      "iteration 1485, train loss: 0.008168, validation loss: 0.005392\n",
      "tensor(0.0053)\n",
      "iteration 1486, train loss: 0.008088, validation loss: 0.005277\n",
      "tensor(0.0051)\n",
      "iteration 1487, train loss: 0.008007, validation loss: \u001b[92m0.005149\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1488, train loss: 0.00783, validation loss: 0.005162\n",
      "tensor(0.0052)\n",
      "iteration 1489, train loss: 0.008033, validation loss: 0.005244\n",
      "tensor(0.0052)\n",
      "iteration 1490, train loss: 0.0078, validation loss: 0.005239\n",
      "tensor(0.0052)\n",
      "iteration 1491, train loss: 0.007966, validation loss: 0.005195\n",
      "tensor(0.0052)\n",
      "iteration 1492, train loss: 0.007877, validation loss: 0.005206\n",
      "tensor(0.0053)\n",
      "iteration 1493, train loss: 0.00802, validation loss: 0.005292\n",
      "tensor(0.0052)\n",
      "iteration 1494, train loss: 0.007887, validation loss: 0.005189\n",
      "tensor(0.0052)\n",
      "iteration 1495, train loss: \u001b[92m0.007736\u001b[0m, validation loss: 0.005194\n",
      "tensor(0.0052)\n",
      "iteration 1496, train loss: \u001b[92m0.007602\u001b[0m, validation loss: 0.005179\n",
      "tensor(0.0054)\n",
      "iteration 1497, train loss: 0.007743, validation loss: 0.005394\n",
      "tensor(0.0052)\n",
      "iteration 1498, train loss: 0.007928, validation loss: 0.005206\n",
      "tensor(0.0051)\n",
      "iteration 1499, train loss: \u001b[92m0.00751\u001b[0m, validation loss: \u001b[92m0.005139\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1500, train loss: 0.00789, validation loss: 0.005145\n",
      "tensor(0.0053)\n",
      "iteration 1501, train loss: 0.007675, validation loss: 0.005313\n",
      "tensor(0.0052)\n",
      "iteration 1502, train loss: 0.007828, validation loss: 0.005231\n",
      "tensor(0.0052)\n",
      "iteration 1503, train loss: 0.007881, validation loss: 0.005168\n",
      "tensor(0.0051)\n",
      "iteration 1504, train loss: 0.007831, validation loss: \u001b[92m0.005138\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1505, train loss: 0.007776, validation loss: 0.005217\n",
      "tensor(0.0052)\n",
      "iteration 1506, train loss: 0.007782, validation loss: 0.005151\n",
      "tensor(0.0051)\n",
      "iteration 1507, train loss: 0.007661, validation loss: 0.00514\n",
      "tensor(0.0052)\n",
      "iteration 1508, train loss: 0.007811, validation loss: 0.005248\n",
      "tensor(0.0053)\n",
      "iteration 1509, train loss: 0.00779, validation loss: 0.005301\n",
      "tensor(0.0051)\n",
      "iteration 1510, train loss: 0.00771, validation loss: \u001b[92m0.005116\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1511, train loss: 0.007758, validation loss: \u001b[92m0.00511\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1512, train loss: 0.007942, validation loss: 0.005316\n",
      "tensor(0.0053)\n",
      "iteration 1513, train loss: 0.007817, validation loss: 0.005273\n",
      "tensor(0.0051)\n",
      "iteration 1514, train loss: 0.007778, validation loss: 0.005125\n",
      "tensor(0.0051)\n",
      "iteration 1515, train loss: 0.007824, validation loss: 0.005128\n",
      "tensor(0.0053)\n",
      "iteration 1516, train loss: 0.007722, validation loss: 0.005286\n",
      "tensor(0.0052)\n",
      "iteration 1517, train loss: 0.007955, validation loss: 0.005176\n",
      "tensor(0.0052)\n",
      "iteration 1518, train loss: 0.007695, validation loss: 0.005231\n",
      "tensor(0.0052)\n",
      "iteration 1519, train loss: 0.007801, validation loss: 0.005175\n",
      "tensor(0.0052)\n",
      "iteration 1520, train loss: 0.00766, validation loss: 0.005171\n",
      "tensor(0.0052)\n",
      "iteration 1521, train loss: 0.007708, validation loss: 0.005191\n",
      "tensor(0.0051)\n",
      "iteration 1522, train loss: 0.007688, validation loss: \u001b[92m0.005092\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1523, train loss: 0.007685, validation loss: \u001b[92m0.005089\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1524, train loss: 0.007802, validation loss: 0.005132\n",
      "tensor(0.0052)\n",
      "iteration 1525, train loss: 0.007586, validation loss: 0.005162\n",
      "tensor(0.0051)\n",
      "iteration 1526, train loss: 0.007707, validation loss: 0.00511\n",
      "tensor(0.0051)\n",
      "iteration 1527, train loss: 0.007696, validation loss: 0.0051\n",
      "tensor(0.0051)\n",
      "iteration 1528, train loss: 0.007649, validation loss: 0.005108\n",
      "tensor(0.0052)\n",
      "iteration 1529, train loss: 0.007592, validation loss: 0.005152\n",
      "tensor(0.0053)\n",
      "iteration 1530, train loss: \u001b[92m0.007485\u001b[0m, validation loss: 0.005265\n",
      "tensor(0.0052)\n",
      "iteration 1531, train loss: 0.007664, validation loss: 0.005176\n",
      "tensor(0.0051)\n",
      "iteration 1532, train loss: 0.007673, validation loss: 0.005114\n",
      "tensor(0.0051)\n",
      "iteration 1533, train loss: 0.007836, validation loss: \u001b[92m0.005088\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1534, train loss: 0.007718, validation loss: 0.005126\n",
      "tensor(0.0054)\n",
      "iteration 1535, train loss: 0.007548, validation loss: 0.005397\n",
      "tensor(0.0051)\n",
      "iteration 1536, train loss: 0.007939, validation loss: 0.005145\n",
      "tensor(0.0051)\n",
      "iteration 1537, train loss: 0.007691, validation loss: \u001b[92m0.005086\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1538, train loss: 0.007618, validation loss: 0.005278\n",
      "tensor(0.0053)\n",
      "iteration 1539, train loss: 0.007783, validation loss: 0.005255\n",
      "tensor(0.0052)\n",
      "iteration 1540, train loss: 0.00774, validation loss: 0.005153\n",
      "tensor(0.0052)\n",
      "iteration 1541, train loss: 0.007724, validation loss: 0.005166\n",
      "tensor(0.0052)\n",
      "iteration 1542, train loss: 0.007623, validation loss: 0.005164\n",
      "tensor(0.0052)\n",
      "iteration 1543, train loss: 0.007558, validation loss: 0.005194\n",
      "tensor(0.0051)\n",
      "iteration 1544, train loss: 0.007563, validation loss: \u001b[92m0.00508\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1545, train loss: 0.007539, validation loss: \u001b[92m0.005071\u001b[0m\n",
      "tensor(0.0052)\n",
      "iteration 1546, train loss: 0.007733, validation loss: 0.005189\n",
      "tensor(0.0052)\n",
      "iteration 1547, train loss: 0.007752, validation loss: 0.00522\n",
      "tensor(0.0051)\n",
      "iteration 1548, train loss: 0.007577, validation loss: 0.0051\n",
      "tensor(0.0051)\n",
      "iteration 1549, train loss: 0.007687, validation loss: 0.005133\n",
      "tensor(0.0051)\n",
      "iteration 1550, train loss: 0.007606, validation loss: 0.005078\n",
      "tensor(0.0052)\n",
      "iteration 1551, train loss: 0.007683, validation loss: 0.005247\n",
      "tensor(0.0053)\n",
      "iteration 1552, train loss: 0.007517, validation loss: 0.005342\n",
      "tensor(0.0051)\n",
      "iteration 1553, train loss: 0.007667, validation loss: 0.005096\n",
      "tensor(0.0052)\n",
      "iteration 1554, train loss: 0.00766, validation loss: 0.005191\n",
      "tensor(0.0053)\n",
      "iteration 1555, train loss: 0.007746, validation loss: 0.005325\n",
      "tensor(0.0052)\n",
      "iteration 1556, train loss: 0.007736, validation loss: 0.005169\n",
      "tensor(0.0053)\n",
      "iteration 1557, train loss: 0.007536, validation loss: 0.005257\n",
      "tensor(0.0052)\n",
      "iteration 1558, train loss: 0.007669, validation loss: 0.005178\n",
      "tensor(0.0052)\n",
      "iteration 1559, train loss: 0.007729, validation loss: 0.005206\n",
      "tensor(0.0054)\n",
      "iteration 1560, train loss: 0.00758, validation loss: 0.005393\n",
      "tensor(0.0051)\n",
      "iteration 1561, train loss: 0.007657, validation loss: 0.005121\n",
      "tensor(0.0052)\n",
      "iteration 1562, train loss: 0.007633, validation loss: 0.005221\n",
      "tensor(0.0053)\n",
      "iteration 1563, train loss: 0.007809, validation loss: 0.005258\n",
      "tensor(0.0053)\n",
      "iteration 1564, train loss: 0.007557, validation loss: 0.0053\n",
      "tensor(0.0052)\n",
      "iteration 1565, train loss: 0.007547, validation loss: 0.005203\n",
      "tensor(0.0051)\n",
      "iteration 1566, train loss: 0.007594, validation loss: 0.005148\n",
      "tensor(0.0050)\n",
      "iteration 1567, train loss: 0.007737, validation loss: \u001b[92m0.005042\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1568, train loss: 0.007554, validation loss: 0.005261\n",
      "tensor(0.0053)\n",
      "iteration 1569, train loss: 0.007496, validation loss: 0.005281\n",
      "tensor(0.0051)\n",
      "iteration 1570, train loss: 0.00773, validation loss: 0.00507\n",
      "tensor(0.0052)\n",
      "iteration 1571, train loss: \u001b[92m0.00748\u001b[0m, validation loss: 0.005157\n",
      "tensor(0.0053)\n",
      "iteration 1572, train loss: \u001b[92m0.007417\u001b[0m, validation loss: 0.00535\n",
      "tensor(0.0052)\n",
      "iteration 1573, train loss: 0.007606, validation loss: 0.005181\n",
      "tensor(0.0051)\n",
      "iteration 1574, train loss: 0.00743, validation loss: 0.00512\n",
      "tensor(0.0052)\n",
      "iteration 1575, train loss: 0.007507, validation loss: 0.005157\n",
      "tensor(0.0052)\n",
      "iteration 1576, train loss: 0.007537, validation loss: 0.005225\n",
      "tensor(0.0052)\n",
      "iteration 1577, train loss: 0.007671, validation loss: 0.00521\n",
      "tensor(0.0051)\n",
      "iteration 1578, train loss: \u001b[92m0.007411\u001b[0m, validation loss: 0.005071\n",
      "tensor(0.0051)\n",
      "iteration 1579, train loss: \u001b[92m0.007285\u001b[0m, validation loss: 0.00508\n",
      "tensor(0.0050)\n",
      "iteration 1580, train loss: 0.007562, validation loss: \u001b[92m0.005038\u001b[0m\n",
      "tensor(0.0053)\n",
      "iteration 1581, train loss: 0.007352, validation loss: 0.0053\n",
      "tensor(0.0050)\n",
      "iteration 1582, train loss: 0.007582, validation loss: 0.005039\n",
      "tensor(0.0050)\n",
      "iteration 1583, train loss: 0.007379, validation loss: 0.005042\n",
      "tensor(0.0051)\n",
      "iteration 1584, train loss: 0.007386, validation loss: 0.005055\n",
      "tensor(0.0052)\n",
      "iteration 1585, train loss: 0.007383, validation loss: 0.005154\n",
      "tensor(0.0051)\n",
      "iteration 1586, train loss: 0.007579, validation loss: 0.005145\n",
      "tensor(0.0051)\n",
      "iteration 1587, train loss: 0.007467, validation loss: 0.005077\n",
      "tensor(0.0051)\n",
      "iteration 1588, train loss: 0.007471, validation loss: 0.005053\n",
      "tensor(0.0052)\n",
      "iteration 1589, train loss: 0.007326, validation loss: 0.005223\n",
      "tensor(0.0052)\n",
      "iteration 1590, train loss: 0.007384, validation loss: 0.005207\n",
      "tensor(0.0050)\n",
      "iteration 1591, train loss: 0.007514, validation loss: 0.005043\n",
      "tensor(0.0051)\n",
      "iteration 1592, train loss: \u001b[92m0.007283\u001b[0m, validation loss: 0.005071\n",
      "tensor(0.0050)\n",
      "iteration 1593, train loss: 0.007541, validation loss: 0.005045\n",
      "tensor(0.0052)\n",
      "iteration 1594, train loss: 0.007421, validation loss: 0.005153\n",
      "tensor(0.0051)\n",
      "iteration 1595, train loss: 0.007327, validation loss: 0.005082\n",
      "tensor(0.0050)\n",
      "iteration 1596, train loss: 0.007317, validation loss: \u001b[92m0.005025\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1597, train loss: \u001b[92m0.007167\u001b[0m, validation loss: 0.005091\n",
      "tensor(0.0051)\n",
      "iteration 1598, train loss: 0.007362, validation loss: 0.005085\n",
      "tensor(0.0052)\n",
      "iteration 1599, train loss: 0.007425, validation loss: 0.00515\n",
      "tensor(0.0051)\n",
      "iteration 1600, train loss: 0.007287, validation loss: 0.005056\n",
      "tensor(0.0051)\n",
      "iteration 1601, train loss: 0.007298, validation loss: 0.00506\n",
      "tensor(0.0050)\n",
      "iteration 1602, train loss: 0.007318, validation loss: \u001b[92m0.005017\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1603, train loss: 0.007286, validation loss: 0.00511\n",
      "tensor(0.0050)\n",
      "iteration 1604, train loss: 0.007283, validation loss: 0.005017\n",
      "tensor(0.0050)\n",
      "iteration 1605, train loss: 0.007254, validation loss: 0.00502\n",
      "tensor(0.0050)\n",
      "iteration 1606, train loss: 0.007327, validation loss: 0.005037\n",
      "tensor(0.0051)\n",
      "iteration 1607, train loss: 0.007279, validation loss: 0.005052\n",
      "tensor(0.0050)\n",
      "iteration 1608, train loss: 0.007247, validation loss: 0.005024\n",
      "tensor(0.0050)\n",
      "iteration 1609, train loss: 0.007352, validation loss: 0.005025\n",
      "tensor(0.0050)\n",
      "iteration 1610, train loss: 0.007282, validation loss: \u001b[92m0.005014\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1611, train loss: 0.007227, validation loss: 0.005119\n",
      "tensor(0.0050)\n",
      "iteration 1612, train loss: 0.007364, validation loss: 0.005043\n",
      "tensor(0.0051)\n",
      "iteration 1613, train loss: 0.007277, validation loss: 0.005068\n",
      "tensor(0.0050)\n",
      "iteration 1614, train loss: 0.007394, validation loss: \u001b[92m0.004998\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1615, train loss: 0.007319, validation loss: 0.005145\n",
      "tensor(0.0050)\n",
      "iteration 1616, train loss: 0.007205, validation loss: 0.005004\n",
      "tensor(0.0050)\n",
      "iteration 1617, train loss: \u001b[92m0.007163\u001b[0m, validation loss: 0.005002\n",
      "tensor(0.0050)\n",
      "iteration 1618, train loss: 0.007169, validation loss: \u001b[92m0.004998\u001b[0m\n",
      "tensor(0.0051)\n",
      "iteration 1619, train loss: 0.007245, validation loss: 0.005131\n",
      "tensor(0.0050)\n",
      "iteration 1620, train loss: 0.007246, validation loss: 0.00502\n",
      "tensor(0.0050)\n",
      "iteration 1621, train loss: 0.007175, validation loss: 0.005018\n",
      "tensor(0.0050)\n",
      "iteration 1622, train loss: 0.007219, validation loss: \u001b[92m0.004996\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1623, train loss: \u001b[92m0.007085\u001b[0m, validation loss: 0.005047\n",
      "tensor(0.0050)\n",
      "iteration 1624, train loss: 0.00717, validation loss: \u001b[92m0.004986\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1625, train loss: 0.007243, validation loss: 0.004993\n",
      "tensor(0.0050)\n",
      "iteration 1626, train loss: \u001b[92m0.007065\u001b[0m, validation loss: 0.004991\n",
      "tensor(0.0050)\n",
      "iteration 1627, train loss: 0.007226, validation loss: 0.005001\n",
      "tensor(0.0050)\n",
      "iteration 1628, train loss: 0.007113, validation loss: 0.004999\n",
      "tensor(0.0050)\n",
      "iteration 1629, train loss: 0.007185, validation loss: \u001b[92m0.00497\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1630, train loss: \u001b[92m0.007044\u001b[0m, validation loss: 0.004975\n",
      "tensor(0.0050)\n",
      "iteration 1631, train loss: 0.007105, validation loss: 0.004989\n",
      "tensor(0.0050)\n",
      "iteration 1632, train loss: 0.007132, validation loss: \u001b[92m0.004966\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1633, train loss: \u001b[92m0.006968\u001b[0m, validation loss: \u001b[92m0.004961\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1634, train loss: 0.007095, validation loss: 0.00497\n",
      "tensor(0.0050)\n",
      "iteration 1635, train loss: 0.007001, validation loss: 0.004995\n",
      "tensor(0.0050)\n",
      "iteration 1636, train loss: \u001b[92m0.00694\u001b[0m, validation loss: 0.004982\n",
      "tensor(0.0049)\n",
      "iteration 1637, train loss: 0.007109, validation loss: \u001b[92m0.004942\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1638, train loss: 0.007026, validation loss: 0.004947\n",
      "tensor(0.0049)\n",
      "iteration 1639, train loss: 0.007122, validation loss: \u001b[92m0.004935\u001b[0m\n",
      "tensor(0.0050)\n",
      "iteration 1640, train loss: 0.007114, validation loss: 0.004978\n",
      "tensor(0.0049)\n",
      "iteration 1641, train loss: 0.006961, validation loss: 0.004943\n",
      "tensor(0.0049)\n",
      "iteration 1642, train loss: 0.007046, validation loss: \u001b[92m0.004907\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1643, train loss: 0.007139, validation loss: 0.004943\n",
      "tensor(0.0049)\n",
      "iteration 1644, train loss: 0.007239, validation loss: 0.004914\n",
      "tensor(0.0049)\n",
      "iteration 1645, train loss: 0.007003, validation loss: 0.004938\n",
      "tensor(0.0049)\n",
      "iteration 1646, train loss: \u001b[92m0.006937\u001b[0m, validation loss: 0.004915\n",
      "tensor(0.0049)\n",
      "iteration 1647, train loss: 0.007114, validation loss: \u001b[92m0.004902\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1648, train loss: 0.006967, validation loss: \u001b[92m0.004893\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1649, train loss: \u001b[92m0.006917\u001b[0m, validation loss: \u001b[92m0.004877\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1650, train loss: \u001b[92m0.006857\u001b[0m, validation loss: \u001b[92m0.004876\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1651, train loss: 0.006961, validation loss: 0.004884\n",
      "tensor(0.0049)\n",
      "iteration 1652, train loss: 0.007013, validation loss: \u001b[92m0.004855\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1653, train loss: 0.007033, validation loss: \u001b[92m0.004853\u001b[0m\n",
      "tensor(0.0048)\n",
      "iteration 1654, train loss: 0.007004, validation loss: \u001b[92m0.004846\u001b[0m\n",
      "tensor(0.0049)\n",
      "iteration 1655, train loss: 0.007033, validation loss: 0.004926\n",
      "tensor(0.0049)\n",
      "iteration 1656, train loss: 0.006989, validation loss: 0.004871\n",
      "tensor(0.0049)\n",
      "iteration 1657, train loss: 0.007126, validation loss: 0.004864\n",
      "tensor(0.0048)\n",
      "iteration 1658, train loss: 0.00689, validation loss: \u001b[92m0.00482\u001b[0m\n",
      "tensor(0.0048)\n",
      "iteration 1659, train loss: \u001b[92m0.006821\u001b[0m, validation loss: 0.004832\n",
      "tensor(0.0048)\n",
      "iteration 1660, train loss: 0.006942, validation loss: \u001b[92m0.004808\u001b[0m\n",
      "tensor(0.0048)\n",
      "iteration 1661, train loss: 0.006948, validation loss: 0.004815\n",
      "tensor(0.0048)\n",
      "iteration 1662, train loss: 0.006894, validation loss: \u001b[92m0.004787\u001b[0m\n",
      "tensor(0.0048)\n",
      "iteration 1663, train loss: \u001b[92m0.006819\u001b[0m, validation loss: \u001b[92m0.004783\u001b[0m\n",
      "tensor(0.0048)\n",
      "iteration 1664, train loss: 0.006866, validation loss: 0.004795\n",
      "tensor(0.0048)\n",
      "iteration 1665, train loss: 0.00684, validation loss: \u001b[92m0.004752\u001b[0m\n",
      "tensor(0.0047)\n",
      "iteration 1666, train loss: \u001b[92m0.006819\u001b[0m, validation loss: \u001b[92m0.004738\u001b[0m\n",
      "tensor(0.0047)\n",
      "iteration 1667, train loss: 0.006908, validation loss: \u001b[92m0.004737\u001b[0m\n",
      "tensor(0.0047)\n",
      "iteration 1668, train loss: \u001b[92m0.006798\u001b[0m, validation loss: \u001b[92m0.004709\u001b[0m\n",
      "tensor(0.0047)\n",
      "iteration 1669, train loss: 0.006804, validation loss: \u001b[92m0.004683\u001b[0m\n",
      "tensor(0.0047)\n",
      "iteration 1670, train loss: \u001b[92m0.006733\u001b[0m, validation loss: 0.004687\n",
      "tensor(0.0047)\n",
      "iteration 1671, train loss: 0.006789, validation loss: \u001b[92m0.00466\u001b[0m\n",
      "tensor(0.0046)\n",
      "iteration 1672, train loss: 0.00678, validation loss: \u001b[92m0.00463\u001b[0m\n",
      "tensor(0.0046)\n",
      "iteration 1673, train loss: 0.006866, validation loss: \u001b[92m0.004609\u001b[0m\n",
      "tensor(0.0046)\n",
      "iteration 1674, train loss: 0.006769, validation loss: \u001b[92m0.004582\u001b[0m\n",
      "tensor(0.0046)\n",
      "iteration 1675, train loss: 0.00674, validation loss: 0.004583\n",
      "tensor(0.0046)\n",
      "iteration 1676, train loss: 0.006763, validation loss: 0.004587\n",
      "tensor(0.0045)\n",
      "iteration 1677, train loss: 0.006806, validation loss: \u001b[92m0.004549\u001b[0m\n",
      "tensor(0.0046)\n",
      "iteration 1678, train loss: 0.006739, validation loss: 0.004552\n",
      "tensor(0.0045)\n",
      "iteration 1679, train loss: \u001b[92m0.006549\u001b[0m, validation loss: \u001b[92m0.004513\u001b[0m\n",
      "tensor(0.0045)\n",
      "iteration 1680, train loss: 0.006661, validation loss: \u001b[92m0.004492\u001b[0m\n",
      "tensor(0.0045)\n",
      "iteration 1681, train loss: 0.006628, validation loss: 0.004505\n",
      "tensor(0.0045)\n",
      "iteration 1682, train loss: 0.006608, validation loss: \u001b[92m0.004479\u001b[0m\n",
      "tensor(0.0045)\n",
      "iteration 1683, train loss: 0.006581, validation loss: 0.004482\n",
      "tensor(0.0045)\n",
      "iteration 1684, train loss: 0.006568, validation loss: 0.004489\n",
      "tensor(0.0045)\n",
      "iteration 1685, train loss: 0.006622, validation loss: \u001b[92m0.004462\u001b[0m\n",
      "tensor(0.0045)\n",
      "iteration 1686, train loss: 0.006575, validation loss: \u001b[92m0.004453\u001b[0m\n",
      "tensor(0.0044)\n",
      "iteration 1687, train loss: \u001b[92m0.006531\u001b[0m, validation loss: \u001b[92m0.004418\u001b[0m\n",
      "tensor(0.0044)\n",
      "iteration 1688, train loss: 0.006646, validation loss: 0.004439\n",
      "tensor(0.0044)\n",
      "iteration 1689, train loss: \u001b[92m0.006494\u001b[0m, validation loss: \u001b[92m0.004379\u001b[0m\n",
      "tensor(0.0045)\n",
      "iteration 1690, train loss: \u001b[92m0.006314\u001b[0m, validation loss: 0.004461\n",
      "tensor(0.0043)\n",
      "iteration 1691, train loss: 0.006598, validation loss: \u001b[92m0.004325\u001b[0m\n",
      "tensor(0.0044)\n",
      "iteration 1692, train loss: 0.006352, validation loss: 0.004393\n",
      "tensor(0.0043)\n",
      "iteration 1693, train loss: 0.006543, validation loss: \u001b[92m0.004297\u001b[0m\n",
      "tensor(0.0043)\n",
      "iteration 1694, train loss: 0.006476, validation loss: 0.004322\n",
      "tensor(0.0044)\n",
      "iteration 1695, train loss: 0.006417, validation loss: 0.004424\n",
      "tensor(0.0044)\n",
      "iteration 1696, train loss: 0.006351, validation loss: 0.004393\n",
      "tensor(0.0042)\n",
      "iteration 1697, train loss: 0.006543, validation loss: \u001b[92m0.004238\u001b[0m\n",
      "tensor(0.0042)\n",
      "iteration 1698, train loss: 0.006357, validation loss: \u001b[92m0.004202\u001b[0m\n",
      "tensor(0.0043)\n",
      "iteration 1699, train loss: 0.00644, validation loss: 0.004313\n",
      "tensor(0.0044)\n",
      "iteration 1700, train loss: 0.006401, validation loss: 0.004373\n",
      "tensor(0.0042)\n",
      "iteration 1701, train loss: \u001b[92m0.006269\u001b[0m, validation loss: 0.004207\n",
      "tensor(0.0042)\n",
      "iteration 1702, train loss: \u001b[92m0.00617\u001b[0m, validation loss: \u001b[92m0.00416\u001b[0m\n",
      "tensor(0.0042)\n",
      "iteration 1703, train loss: 0.006322, validation loss: 0.004231\n",
      "tensor(0.0042)\n",
      "iteration 1704, train loss: 0.006308, validation loss: 0.004162\n",
      "tensor(0.0042)\n",
      "iteration 1705, train loss: \u001b[92m0.006163\u001b[0m, validation loss: 0.004226\n",
      "tensor(0.0041)\n",
      "iteration 1706, train loss: 0.006252, validation loss: \u001b[92m0.004138\u001b[0m\n",
      "tensor(0.0043)\n",
      "iteration 1707, train loss: \u001b[92m0.005993\u001b[0m, validation loss: 0.00434\n",
      "tensor(0.0041)\n",
      "iteration 1708, train loss: 0.006183, validation loss: \u001b[92m0.004096\u001b[0m\n",
      "tensor(0.0042)\n",
      "iteration 1709, train loss: 0.006104, validation loss: 0.004155\n",
      "tensor(0.0043)\n",
      "iteration 1710, train loss: 0.00623, validation loss: 0.004324\n",
      "tensor(0.0043)\n",
      "iteration 1711, train loss: 0.006029, validation loss: 0.004274\n",
      "tensor(0.0041)\n",
      "iteration 1712, train loss: 0.006064, validation loss: 0.004149\n",
      "tensor(0.0040)\n",
      "iteration 1713, train loss: 0.00623, validation loss: \u001b[92m0.004036\u001b[0m\n",
      "tensor(0.0043)\n",
      "iteration 1714, train loss: 0.006027, validation loss: 0.004286\n",
      "tensor(0.0041)\n",
      "iteration 1715, train loss: 0.006127, validation loss: 0.004055\n",
      "tensor(0.0040)\n",
      "iteration 1716, train loss: 0.006015, validation loss: \u001b[92m0.004034\u001b[0m\n",
      "tensor(0.0042)\n",
      "iteration 1717, train loss: \u001b[92m0.005979\u001b[0m, validation loss: 0.004172\n",
      "tensor(0.0042)\n",
      "iteration 1718, train loss: 0.006037, validation loss: 0.004228\n",
      "tensor(0.0040)\n",
      "iteration 1719, train loss: \u001b[92m0.005887\u001b[0m, validation loss: 0.004048\n",
      "tensor(0.0040)\n",
      "iteration 1720, train loss: \u001b[92m0.005809\u001b[0m, validation loss: \u001b[92m0.003983\u001b[0m\n",
      "tensor(0.0041)\n",
      "iteration 1721, train loss: \u001b[92m0.005779\u001b[0m, validation loss: 0.004138\n",
      "tensor(0.0041)\n",
      "iteration 1722, train loss: 0.006045, validation loss: 0.00407\n",
      "tensor(0.0041)\n",
      "iteration 1723, train loss: \u001b[92m0.005683\u001b[0m, validation loss: 0.004075\n",
      "tensor(0.0040)\n",
      "iteration 1724, train loss: 0.005853, validation loss: 0.003985\n",
      "tensor(0.0040)\n",
      "iteration 1725, train loss: 0.005719, validation loss: \u001b[92m0.003976\u001b[0m\n",
      "tensor(0.0040)\n",
      "iteration 1726, train loss: 0.005872, validation loss: 0.004039\n",
      "tensor(0.0040)\n",
      "iteration 1727, train loss: 0.005822, validation loss: 0.004019\n",
      "tensor(0.0040)\n",
      "iteration 1728, train loss: 0.005759, validation loss: 0.004041\n",
      "tensor(0.0040)\n",
      "iteration 1729, train loss: 0.005743, validation loss: 0.003977\n",
      "tensor(0.0039)\n",
      "iteration 1730, train loss: 0.005729, validation loss: \u001b[92m0.003881\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1731, train loss: \u001b[92m0.005674\u001b[0m, validation loss: 0.003919\n",
      "tensor(0.0041)\n",
      "iteration 1732, train loss: \u001b[92m0.005647\u001b[0m, validation loss: 0.004064\n",
      "tensor(0.0039)\n",
      "iteration 1733, train loss: 0.005652, validation loss: 0.003928\n",
      "tensor(0.0040)\n",
      "iteration 1734, train loss: \u001b[92m0.005601\u001b[0m, validation loss: 0.003956\n",
      "tensor(0.0039)\n",
      "iteration 1735, train loss: 0.005727, validation loss: 0.003928\n",
      "tensor(0.0039)\n",
      "iteration 1736, train loss: 0.005724, validation loss: 0.003908\n",
      "tensor(0.0040)\n",
      "iteration 1737, train loss: \u001b[92m0.00556\u001b[0m, validation loss: 0.003968\n",
      "tensor(0.0038)\n",
      "iteration 1738, train loss: 0.005574, validation loss: \u001b[92m0.003819\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1739, train loss: 0.005622, validation loss: 0.003888\n",
      "tensor(0.0040)\n",
      "iteration 1740, train loss: \u001b[92m0.005558\u001b[0m, validation loss: 0.00395\n",
      "tensor(0.0040)\n",
      "iteration 1741, train loss: 0.005587, validation loss: 0.00395\n",
      "tensor(0.0038)\n",
      "iteration 1742, train loss: \u001b[92m0.005513\u001b[0m, validation loss: \u001b[92m0.003802\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1743, train loss: 0.005562, validation loss: 0.00391\n",
      "tensor(0.0039)\n",
      "iteration 1744, train loss: \u001b[92m0.005436\u001b[0m, validation loss: 0.003925\n",
      "tensor(0.0039)\n",
      "iteration 1745, train loss: 0.005512, validation loss: 0.003876\n",
      "tensor(0.0039)\n",
      "iteration 1746, train loss: \u001b[92m0.005395\u001b[0m, validation loss: 0.00391\n",
      "tensor(0.0039)\n",
      "iteration 1747, train loss: 0.0054, validation loss: 0.003908\n",
      "tensor(0.0038)\n",
      "iteration 1748, train loss: 0.0055, validation loss: 0.003804\n",
      "tensor(0.0038)\n",
      "iteration 1749, train loss: 0.00547, validation loss: 0.003845\n",
      "tensor(0.0040)\n",
      "iteration 1750, train loss: 0.005423, validation loss: 0.003997\n",
      "tensor(0.0037)\n",
      "iteration 1751, train loss: 0.005424, validation loss: \u001b[92m0.00374\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1752, train loss: 0.005462, validation loss: 0.003883\n",
      "tensor(0.0040)\n",
      "iteration 1753, train loss: 0.005447, validation loss: 0.003966\n",
      "tensor(0.0037)\n",
      "iteration 1754, train loss: 0.005452, validation loss: \u001b[92m0.00373\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1755, train loss: 0.00546, validation loss: \u001b[92m0.003719\u001b[0m\n",
      "tensor(0.0040)\n",
      "iteration 1756, train loss: 0.00551, validation loss: 0.004049\n",
      "tensor(0.0039)\n",
      "iteration 1757, train loss: \u001b[92m0.005387\u001b[0m, validation loss: 0.003919\n",
      "tensor(0.0037)\n",
      "iteration 1758, train loss: 0.005441, validation loss: \u001b[92m0.003693\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1759, train loss: 0.00544, validation loss: 0.003892\n",
      "tensor(0.0039)\n",
      "iteration 1760, train loss: \u001b[92m0.005334\u001b[0m, validation loss: 0.00391\n",
      "tensor(0.0037)\n",
      "iteration 1761, train loss: 0.005364, validation loss: 0.003723\n",
      "tensor(0.0037)\n",
      "iteration 1762, train loss: \u001b[92m0.005272\u001b[0m, validation loss: 0.003711\n",
      "tensor(0.0037)\n",
      "iteration 1763, train loss: 0.005347, validation loss: 0.003749\n",
      "tensor(0.0039)\n",
      "iteration 1764, train loss: \u001b[92m0.005267\u001b[0m, validation loss: 0.003883\n",
      "tensor(0.0038)\n",
      "iteration 1765, train loss: 0.005278, validation loss: 0.003772\n",
      "tensor(0.0037)\n",
      "iteration 1766, train loss: 0.005303, validation loss: 0.00371\n",
      "tensor(0.0038)\n",
      "iteration 1767, train loss: 0.005361, validation loss: 0.003782\n",
      "tensor(0.0040)\n",
      "iteration 1768, train loss: 0.00537, validation loss: 0.004\n",
      "tensor(0.0037)\n",
      "iteration 1769, train loss: 0.005443, validation loss: \u001b[92m0.003679\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1770, train loss: \u001b[92m0.005188\u001b[0m, validation loss: \u001b[92m0.003676\u001b[0m\n",
      "tensor(0.0040)\n",
      "iteration 1771, train loss: 0.005334, validation loss: 0.00396\n",
      "tensor(0.0038)\n",
      "iteration 1772, train loss: 0.00539, validation loss: 0.00385\n",
      "tensor(0.0036)\n",
      "iteration 1773, train loss: 0.005206, validation loss: \u001b[92m0.003637\u001b[0m\n",
      "tensor(0.0038)\n",
      "iteration 1774, train loss: 0.005362, validation loss: 0.003844\n",
      "tensor(0.0039)\n",
      "iteration 1775, train loss: 0.005251, validation loss: 0.003945\n",
      "tensor(0.0037)\n",
      "iteration 1776, train loss: 0.00531, validation loss: 0.003652\n",
      "tensor(0.0037)\n",
      "iteration 1777, train loss: 0.005409, validation loss: 0.003683\n",
      "tensor(0.0040)\n",
      "iteration 1778, train loss: 0.005318, validation loss: 0.003973\n",
      "tensor(0.0039)\n",
      "iteration 1779, train loss: 0.005197, validation loss: 0.003907\n",
      "tensor(0.0037)\n",
      "iteration 1780, train loss: 0.005363, validation loss: 0.003676\n",
      "tensor(0.0036)\n",
      "iteration 1781, train loss: 0.005207, validation loss: \u001b[92m0.003624\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1782, train loss: \u001b[92m0.005178\u001b[0m, validation loss: 0.003725\n",
      "tensor(0.0039)\n",
      "iteration 1783, train loss: \u001b[92m0.005131\u001b[0m, validation loss: 0.003869\n",
      "tensor(0.0037)\n",
      "iteration 1784, train loss: 0.00526, validation loss: 0.00371\n",
      "tensor(0.0037)\n",
      "iteration 1785, train loss: 0.005229, validation loss: 0.003677\n",
      "tensor(0.0038)\n",
      "iteration 1786, train loss: 0.005159, validation loss: 0.003783\n",
      "tensor(0.0037)\n",
      "iteration 1787, train loss: 0.00521, validation loss: 0.003744\n",
      "tensor(0.0037)\n",
      "iteration 1788, train loss: 0.00527, validation loss: 0.003652\n",
      "tensor(0.0039)\n",
      "iteration 1789, train loss: 0.00532, validation loss: 0.00388\n",
      "tensor(0.0038)\n",
      "iteration 1790, train loss: 0.005352, validation loss: 0.003815\n",
      "tensor(0.0037)\n",
      "iteration 1791, train loss: 0.005147, validation loss: 0.003691\n",
      "tensor(0.0036)\n",
      "iteration 1792, train loss: 0.005267, validation loss: 0.003635\n",
      "tensor(0.0037)\n",
      "iteration 1793, train loss: \u001b[92m0.005052\u001b[0m, validation loss: 0.00374\n",
      "tensor(0.0037)\n",
      "iteration 1794, train loss: 0.00526, validation loss: 0.003699\n",
      "tensor(0.0037)\n",
      "iteration 1795, train loss: 0.005134, validation loss: 0.00369\n",
      "tensor(0.0038)\n",
      "iteration 1796, train loss: 0.005228, validation loss: 0.003779\n",
      "tensor(0.0037)\n",
      "iteration 1797, train loss: 0.005206, validation loss: 0.003662\n",
      "tensor(0.0038)\n",
      "iteration 1798, train loss: 0.005247, validation loss: 0.003815\n",
      "tensor(0.0036)\n",
      "iteration 1799, train loss: 0.00517, validation loss: \u001b[92m0.003604\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1800, train loss: 0.005082, validation loss: 0.003701\n",
      "tensor(0.0038)\n",
      "iteration 1801, train loss: 0.005227, validation loss: 0.003824\n",
      "tensor(0.0038)\n",
      "iteration 1802, train loss: 0.005188, validation loss: 0.003777\n",
      "tensor(0.0036)\n",
      "iteration 1803, train loss: 0.005242, validation loss: \u001b[92m0.003581\u001b[0m\n",
      "tensor(0.0038)\n",
      "iteration 1804, train loss: 0.005105, validation loss: 0.003758\n",
      "tensor(0.0037)\n",
      "iteration 1805, train loss: 0.005269, validation loss: 0.003708\n",
      "tensor(0.0037)\n",
      "iteration 1806, train loss: \u001b[92m0.005024\u001b[0m, validation loss: 0.003721\n",
      "tensor(0.0036)\n",
      "iteration 1807, train loss: 0.005183, validation loss: \u001b[92m0.003581\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1808, train loss: 0.005087, validation loss: 0.003665\n",
      "tensor(0.0037)\n",
      "iteration 1809, train loss: 0.005154, validation loss: 0.003652\n",
      "tensor(0.0037)\n",
      "iteration 1810, train loss: 0.005088, validation loss: 0.003703\n",
      "tensor(0.0037)\n",
      "iteration 1811, train loss: 0.005121, validation loss: 0.003681\n",
      "tensor(0.0037)\n",
      "iteration 1812, train loss: 0.005103, validation loss: 0.003662\n",
      "tensor(0.0036)\n",
      "iteration 1813, train loss: 0.005204, validation loss: \u001b[92m0.003564\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1814, train loss: 0.005151, validation loss: 0.003905\n",
      "tensor(0.0038)\n",
      "iteration 1815, train loss: 0.005253, validation loss: 0.003764\n",
      "tensor(0.0035)\n",
      "iteration 1816, train loss: 0.00521, validation loss: \u001b[92m0.003544\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1817, train loss: 0.005093, validation loss: 0.003587\n",
      "tensor(0.0040)\n",
      "iteration 1818, train loss: 0.005109, validation loss: 0.003988\n",
      "tensor(0.0038)\n",
      "iteration 1819, train loss: 0.005222, validation loss: 0.003773\n",
      "tensor(0.0036)\n",
      "iteration 1820, train loss: 0.005138, validation loss: 0.003553\n",
      "tensor(0.0036)\n",
      "iteration 1821, train loss: 0.005216, validation loss: 0.00359\n",
      "tensor(0.0040)\n",
      "iteration 1822, train loss: 0.005057, validation loss: 0.003955\n",
      "tensor(0.0037)\n",
      "iteration 1823, train loss: 0.005139, validation loss: 0.003672\n",
      "tensor(0.0035)\n",
      "iteration 1824, train loss: \u001b[92m0.004987\u001b[0m, validation loss: \u001b[92m0.003529\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1825, train loss: 0.005148, validation loss: 0.00355\n",
      "tensor(0.0038)\n",
      "iteration 1826, train loss: 0.005034, validation loss: 0.003759\n",
      "tensor(0.0037)\n",
      "iteration 1827, train loss: 0.005017, validation loss: 0.003669\n",
      "tensor(0.0035)\n",
      "iteration 1828, train loss: 0.005065, validation loss: \u001b[92m0.003477\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1829, train loss: 0.005039, validation loss: 0.003591\n",
      "tensor(0.0037)\n",
      "iteration 1830, train loss: 0.005041, validation loss: 0.00369\n",
      "tensor(0.0036)\n",
      "iteration 1831, train loss: 0.005056, validation loss: 0.003569\n",
      "tensor(0.0035)\n",
      "iteration 1832, train loss: 0.005018, validation loss: 0.003545\n",
      "tensor(0.0036)\n",
      "iteration 1833, train loss: \u001b[92m0.004951\u001b[0m, validation loss: 0.003552\n",
      "tensor(0.0035)\n",
      "iteration 1834, train loss: 0.004988, validation loss: 0.003545\n",
      "tensor(0.0035)\n",
      "iteration 1835, train loss: \u001b[92m0.004934\u001b[0m, validation loss: 0.003541\n",
      "tensor(0.0038)\n",
      "iteration 1836, train loss: 0.005111, validation loss: 0.003758\n",
      "tensor(0.0037)\n",
      "iteration 1837, train loss: 0.00516, validation loss: 0.003738\n",
      "tensor(0.0036)\n",
      "iteration 1838, train loss: 0.004991, validation loss: 0.003554\n",
      "tensor(0.0035)\n",
      "iteration 1839, train loss: 0.004968, validation loss: \u001b[92m0.003468\u001b[0m\n",
      "tensor(0.0039)\n",
      "iteration 1840, train loss: 0.005114, validation loss: 0.003871\n",
      "tensor(0.0036)\n",
      "iteration 1841, train loss: 0.005246, validation loss: 0.003594\n",
      "tensor(0.0036)\n",
      "iteration 1842, train loss: \u001b[92m0.004892\u001b[0m, validation loss: 0.003577\n",
      "tensor(0.0035)\n",
      "iteration 1843, train loss: 0.00513, validation loss: 0.003539\n",
      "tensor(0.0037)\n",
      "iteration 1844, train loss: 0.004998, validation loss: 0.003729\n",
      "tensor(0.0035)\n",
      "iteration 1845, train loss: 0.005133, validation loss: \u001b[92m0.003452\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1846, train loss: \u001b[92m0.004875\u001b[0m, validation loss: 0.003508\n",
      "tensor(0.0036)\n",
      "iteration 1847, train loss: 0.004926, validation loss: 0.003616\n",
      "tensor(0.0035)\n",
      "iteration 1848, train loss: 0.004966, validation loss: 0.003544\n",
      "tensor(0.0035)\n",
      "iteration 1849, train loss: \u001b[92m0.004826\u001b[0m, validation loss: 0.003518\n",
      "tensor(0.0036)\n",
      "iteration 1850, train loss: 0.004995, validation loss: 0.003575\n",
      "tensor(0.0037)\n",
      "iteration 1851, train loss: 0.004922, validation loss: 0.003684\n",
      "tensor(0.0035)\n",
      "iteration 1852, train loss: 0.00494, validation loss: 0.003529\n",
      "tensor(0.0034)\n",
      "iteration 1853, train loss: 0.004961, validation loss: \u001b[92m0.003446\u001b[0m\n",
      "tensor(0.0037)\n",
      "iteration 1854, train loss: 0.005001, validation loss: 0.003662\n",
      "tensor(0.0036)\n",
      "iteration 1855, train loss: 0.005055, validation loss: 0.003618\n",
      "tensor(0.0034)\n",
      "iteration 1856, train loss: 0.005044, validation loss: \u001b[92m0.003443\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1857, train loss: 0.004855, validation loss: 0.0036\n",
      "tensor(0.0036)\n",
      "iteration 1858, train loss: 0.005015, validation loss: 0.003589\n",
      "tensor(0.0036)\n",
      "iteration 1859, train loss: 0.004995, validation loss: 0.003597\n",
      "tensor(0.0034)\n",
      "iteration 1860, train loss: 0.005038, validation loss: \u001b[92m0.003435\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1861, train loss: 0.004888, validation loss: 0.00361\n",
      "tensor(0.0036)\n",
      "iteration 1862, train loss: 0.004937, validation loss: 0.003552\n",
      "tensor(0.0036)\n",
      "iteration 1863, train loss: 0.00485, validation loss: 0.003611\n",
      "tensor(0.0034)\n",
      "iteration 1864, train loss: 0.004993, validation loss: 0.003446\n",
      "tensor(0.0037)\n",
      "iteration 1865, train loss: 0.004887, validation loss: 0.003677\n",
      "tensor(0.0035)\n",
      "iteration 1866, train loss: 0.004975, validation loss: 0.003529\n",
      "tensor(0.0035)\n",
      "iteration 1867, train loss: 0.004877, validation loss: 0.003543\n",
      "tensor(0.0035)\n",
      "iteration 1868, train loss: 0.004993, validation loss: 0.003454\n",
      "tensor(0.0034)\n",
      "iteration 1869, train loss: \u001b[92m0.004823\u001b[0m, validation loss: 0.00344\n",
      "tensor(0.0035)\n",
      "iteration 1870, train loss: 0.004921, validation loss: 0.003499\n",
      "tensor(0.0036)\n",
      "iteration 1871, train loss: 0.004871, validation loss: 0.003629\n",
      "tensor(0.0035)\n",
      "iteration 1872, train loss: 0.00484, validation loss: 0.003497\n",
      "tensor(0.0034)\n",
      "iteration 1873, train loss: 0.004933, validation loss: \u001b[92m0.003415\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1874, train loss: 0.004972, validation loss: 0.003523\n",
      "tensor(0.0037)\n",
      "iteration 1875, train loss: 0.004907, validation loss: 0.003748\n",
      "tensor(0.0034)\n",
      "iteration 1876, train loss: 0.004869, validation loss: \u001b[92m0.003397\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 1877, train loss: \u001b[92m0.0048\u001b[0m, validation loss: 0.003406\n",
      "tensor(0.0035)\n",
      "iteration 1878, train loss: 0.005018, validation loss: 0.003504\n",
      "tensor(0.0039)\n",
      "iteration 1879, train loss: 0.004841, validation loss: 0.003851\n",
      "tensor(0.0035)\n",
      "iteration 1880, train loss: 0.004953, validation loss: 0.003486\n",
      "tensor(0.0034)\n",
      "iteration 1881, train loss: \u001b[92m0.004773\u001b[0m, validation loss: \u001b[92m0.003383\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 1882, train loss: 0.004832, validation loss: \u001b[92m0.003353\u001b[0m\n",
      "tensor(0.0038)\n",
      "iteration 1883, train loss: \u001b[92m0.004773\u001b[0m, validation loss: 0.003813\n",
      "tensor(0.0035)\n",
      "iteration 1884, train loss: 0.005037, validation loss: 0.003542\n",
      "tensor(0.0034)\n",
      "iteration 1885, train loss: 0.004879, validation loss: 0.003363\n",
      "tensor(0.0033)\n",
      "iteration 1886, train loss: 0.004823, validation loss: \u001b[92m0.00335\u001b[0m\n",
      "tensor(0.0036)\n",
      "iteration 1887, train loss: 0.0048, validation loss: 0.003638\n",
      "tensor(0.0036)\n",
      "iteration 1888, train loss: 0.00485, validation loss: 0.003639\n",
      "tensor(0.0033)\n",
      "iteration 1889, train loss: 0.004874, validation loss: \u001b[92m0.003337\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1890, train loss: 0.004908, validation loss: \u001b[92m0.003334\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1891, train loss: 0.00488, validation loss: 0.003516\n",
      "tensor(0.0036)\n",
      "iteration 1892, train loss: 0.004874, validation loss: 0.003615\n",
      "tensor(0.0033)\n",
      "iteration 1893, train loss: 0.004977, validation loss: 0.003349\n",
      "tensor(0.0034)\n",
      "iteration 1894, train loss: 0.004813, validation loss: 0.003352\n",
      "tensor(0.0035)\n",
      "iteration 1895, train loss: 0.004986, validation loss: 0.003502\n",
      "tensor(0.0037)\n",
      "iteration 1896, train loss: 0.004801, validation loss: 0.003703\n",
      "tensor(0.0034)\n",
      "iteration 1897, train loss: 0.004952, validation loss: 0.003377\n",
      "tensor(0.0033)\n",
      "iteration 1898, train loss: 0.004777, validation loss: \u001b[92m0.003325\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 1899, train loss: 0.004786, validation loss: 0.003379\n",
      "tensor(0.0036)\n",
      "iteration 1900, train loss: \u001b[92m0.004726\u001b[0m, validation loss: 0.003562\n",
      "tensor(0.0034)\n",
      "iteration 1901, train loss: 0.004889, validation loss: 0.003444\n",
      "tensor(0.0033)\n",
      "iteration 1902, train loss: 0.004768, validation loss: \u001b[92m0.00331\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1903, train loss: 0.004797, validation loss: 0.003327\n",
      "tensor(0.0036)\n",
      "iteration 1904, train loss: 0.004911, validation loss: 0.00356\n",
      "tensor(0.0039)\n",
      "iteration 1905, train loss: 0.0048, validation loss: 0.003867\n",
      "tensor(0.0034)\n",
      "iteration 1906, train loss: 0.005035, validation loss: 0.003397\n",
      "tensor(0.0034)\n",
      "iteration 1907, train loss: 0.004813, validation loss: 0.003358\n",
      "tensor(0.0035)\n",
      "iteration 1908, train loss: 0.004923, validation loss: 0.003498\n",
      "tensor(0.0035)\n",
      "iteration 1909, train loss: 0.004736, validation loss: 0.003514\n",
      "tensor(0.0034)\n",
      "iteration 1910, train loss: 0.004825, validation loss: 0.003434\n",
      "tensor(0.0033)\n",
      "iteration 1911, train loss: 0.00482, validation loss: 0.003312\n",
      "tensor(0.0034)\n",
      "iteration 1912, train loss: 0.004773, validation loss: 0.00337\n",
      "tensor(0.0034)\n",
      "iteration 1913, train loss: 0.004836, validation loss: 0.003428\n",
      "tensor(0.0035)\n",
      "iteration 1914, train loss: \u001b[92m0.004695\u001b[0m, validation loss: 0.003471\n",
      "tensor(0.0034)\n",
      "iteration 1915, train loss: 0.004703, validation loss: 0.003385\n",
      "tensor(0.0034)\n",
      "iteration 1916, train loss: 0.004823, validation loss: 0.003374\n",
      "tensor(0.0034)\n",
      "iteration 1917, train loss: 0.004742, validation loss: 0.003434\n",
      "tensor(0.0034)\n",
      "iteration 1918, train loss: \u001b[92m0.004653\u001b[0m, validation loss: 0.003438\n",
      "tensor(0.0033)\n",
      "iteration 1919, train loss: 0.004759, validation loss: 0.00332\n",
      "tensor(0.0034)\n",
      "iteration 1920, train loss: 0.004767, validation loss: 0.003377\n",
      "tensor(0.0034)\n",
      "iteration 1921, train loss: 0.004764, validation loss: 0.003424\n",
      "tensor(0.0034)\n",
      "iteration 1922, train loss: 0.004703, validation loss: 0.003367\n",
      "tensor(0.0033)\n",
      "iteration 1923, train loss: 0.004742, validation loss: \u001b[92m0.003304\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 1924, train loss: 0.00472, validation loss: 0.003365\n",
      "tensor(0.0035)\n",
      "iteration 1925, train loss: 0.004717, validation loss: 0.003515\n",
      "tensor(0.0035)\n",
      "iteration 1926, train loss: 0.004672, validation loss: 0.003455\n",
      "tensor(0.0033)\n",
      "iteration 1927, train loss: 0.004759, validation loss: \u001b[92m0.003299\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1928, train loss: 0.004692, validation loss: \u001b[92m0.003289\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1929, train loss: 0.004672, validation loss: 0.003536\n",
      "tensor(0.0034)\n",
      "iteration 1930, train loss: 0.0048, validation loss: 0.003411\n",
      "tensor(0.0033)\n",
      "iteration 1931, train loss: 0.004721, validation loss: 0.003333\n",
      "tensor(0.0033)\n",
      "iteration 1932, train loss: 0.004686, validation loss: 0.003305\n",
      "tensor(0.0033)\n",
      "iteration 1933, train loss: 0.00478, validation loss: 0.003325\n",
      "tensor(0.0034)\n",
      "iteration 1934, train loss: 0.004729, validation loss: 0.003385\n",
      "tensor(0.0034)\n",
      "iteration 1935, train loss: 0.004728, validation loss: 0.003351\n",
      "tensor(0.0033)\n",
      "iteration 1936, train loss: 0.004657, validation loss: 0.003298\n",
      "tensor(0.0034)\n",
      "iteration 1937, train loss: 0.004753, validation loss: 0.003351\n",
      "tensor(0.0034)\n",
      "iteration 1938, train loss: 0.004672, validation loss: 0.003376\n",
      "tensor(0.0033)\n",
      "iteration 1939, train loss: 0.004689, validation loss: \u001b[92m0.003276\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1940, train loss: \u001b[92m0.004625\u001b[0m, validation loss: \u001b[92m0.003271\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1941, train loss: 0.004647, validation loss: \u001b[92m0.003261\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 1942, train loss: 0.004804, validation loss: 0.003395\n",
      "tensor(0.0034)\n",
      "iteration 1943, train loss: 0.004642, validation loss: 0.003394\n",
      "tensor(0.0032)\n",
      "iteration 1944, train loss: \u001b[92m0.004605\u001b[0m, validation loss: \u001b[92m0.003242\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 1945, train loss: 0.00482, validation loss: \u001b[92m0.00324\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1946, train loss: 0.00481, validation loss: 0.003525\n",
      "tensor(0.0036)\n",
      "iteration 1947, train loss: 0.004697, validation loss: 0.003602\n",
      "tensor(0.0032)\n",
      "iteration 1948, train loss: 0.004755, validation loss: \u001b[92m0.003234\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 1949, train loss: 0.004621, validation loss: \u001b[92m0.003228\u001b[0m\n",
      "tensor(0.0035)\n",
      "iteration 1950, train loss: 0.004706, validation loss: 0.00347\n",
      "tensor(0.0036)\n",
      "iteration 1951, train loss: 0.004714, validation loss: 0.003577\n",
      "tensor(0.0033)\n",
      "iteration 1952, train loss: 0.00477, validation loss: 0.003273\n",
      "tensor(0.0032)\n",
      "iteration 1953, train loss: 0.004639, validation loss: 0.003236\n",
      "tensor(0.0033)\n",
      "iteration 1954, train loss: 0.004728, validation loss: 0.003267\n",
      "tensor(0.0037)\n",
      "iteration 1955, train loss: 0.004681, validation loss: 0.003746\n",
      "tensor(0.0035)\n",
      "iteration 1956, train loss: 0.004742, validation loss: 0.003523\n",
      "tensor(0.0032)\n",
      "iteration 1957, train loss: 0.004727, validation loss: 0.003231\n",
      "tensor(0.0032)\n",
      "iteration 1958, train loss: 0.004797, validation loss: 0.003232\n",
      "tensor(0.0036)\n",
      "iteration 1959, train loss: 0.004757, validation loss: 0.003561\n",
      "tensor(0.0036)\n",
      "iteration 1960, train loss: 0.004785, validation loss: 0.003584\n",
      "tensor(0.0032)\n",
      "iteration 1961, train loss: 0.004807, validation loss: 0.003229\n",
      "tensor(0.0032)\n",
      "iteration 1962, train loss: 0.004629, validation loss: 0.003247\n",
      "tensor(0.0034)\n",
      "iteration 1963, train loss: 0.004712, validation loss: 0.003407\n",
      "tensor(0.0035)\n",
      "iteration 1964, train loss: \u001b[92m0.004583\u001b[0m, validation loss: 0.003511\n",
      "tensor(0.0034)\n",
      "iteration 1965, train loss: 0.004694, validation loss: 0.003405\n",
      "tensor(0.0032)\n",
      "iteration 1966, train loss: 0.004613, validation loss: \u001b[92m0.003206\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1967, train loss: 0.00459, validation loss: 0.003309\n",
      "tensor(0.0034)\n",
      "iteration 1968, train loss: 0.004703, validation loss: 0.003427\n",
      "tensor(0.0034)\n",
      "iteration 1969, train loss: 0.004675, validation loss: 0.003394\n",
      "tensor(0.0033)\n",
      "iteration 1970, train loss: \u001b[92m0.004513\u001b[0m, validation loss: 0.003304\n",
      "tensor(0.0032)\n",
      "iteration 1971, train loss: 0.004713, validation loss: 0.003223\n",
      "tensor(0.0033)\n",
      "iteration 1972, train loss: 0.00466, validation loss: 0.003342\n",
      "tensor(0.0033)\n",
      "iteration 1973, train loss: 0.004749, validation loss: 0.003337\n",
      "tensor(0.0034)\n",
      "iteration 1974, train loss: 0.004528, validation loss: 0.003388\n",
      "tensor(0.0033)\n",
      "iteration 1975, train loss: 0.004572, validation loss: 0.003294\n",
      "tensor(0.0032)\n",
      "iteration 1976, train loss: 0.004601, validation loss: \u001b[92m0.003198\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 1977, train loss: 0.004575, validation loss: 0.003302\n",
      "tensor(0.0033)\n",
      "iteration 1978, train loss: 0.004553, validation loss: 0.003329\n",
      "tensor(0.0034)\n",
      "iteration 1979, train loss: 0.004689, validation loss: 0.003396\n",
      "tensor(0.0033)\n",
      "iteration 1980, train loss: 0.004662, validation loss: 0.003321\n",
      "tensor(0.0033)\n",
      "iteration 1981, train loss: 0.004584, validation loss: 0.003257\n",
      "tensor(0.0033)\n",
      "iteration 1982, train loss: 0.004612, validation loss: 0.003313\n",
      "tensor(0.0033)\n",
      "iteration 1983, train loss: 0.004659, validation loss: 0.003256\n",
      "tensor(0.0033)\n",
      "iteration 1984, train loss: 0.004655, validation loss: 0.003274\n",
      "tensor(0.0033)\n",
      "iteration 1985, train loss: 0.004558, validation loss: 0.00326\n",
      "tensor(0.0034)\n",
      "iteration 1986, train loss: 0.004563, validation loss: 0.003399\n",
      "tensor(0.0033)\n",
      "iteration 1987, train loss: 0.004749, validation loss: 0.003306\n",
      "tensor(0.0033)\n",
      "iteration 1988, train loss: \u001b[92m0.004511\u001b[0m, validation loss: 0.003262\n",
      "tensor(0.0032)\n",
      "iteration 1989, train loss: 0.004549, validation loss: 0.003248\n",
      "tensor(0.0033)\n",
      "iteration 1990, train loss: \u001b[92m0.004471\u001b[0m, validation loss: 0.003275\n",
      "tensor(0.0032)\n",
      "iteration 1991, train loss: 0.004597, validation loss: 0.003233\n",
      "tensor(0.0032)\n",
      "iteration 1992, train loss: 0.004539, validation loss: 0.003227\n",
      "tensor(0.0032)\n",
      "iteration 1993, train loss: \u001b[92m0.004429\u001b[0m, validation loss: 0.003231\n",
      "tensor(0.0032)\n",
      "iteration 1994, train loss: 0.004554, validation loss: 0.003224\n",
      "tensor(0.0033)\n",
      "iteration 1995, train loss: 0.004538, validation loss: 0.003272\n",
      "tensor(0.0033)\n",
      "iteration 1996, train loss: 0.004491, validation loss: 0.003301\n",
      "tensor(0.0032)\n",
      "iteration 1997, train loss: 0.004617, validation loss: \u001b[92m0.003178\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 1998, train loss: 0.004575, validation loss: 0.003201\n",
      "tensor(0.0032)\n",
      "iteration 1999, train loss: 0.004451, validation loss: \u001b[92m0.003165\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 2000, train loss: 0.004575, validation loss: 0.003256\n",
      "tensor(0.0034)\n",
      "iteration 2001, train loss: 0.004577, validation loss: 0.003438\n",
      "tensor(0.0033)\n",
      "iteration 2002, train loss: 0.004565, validation loss: 0.003348\n",
      "tensor(0.0032)\n",
      "iteration 2003, train loss: 0.004471, validation loss: 0.003178\n",
      "tensor(0.0032)\n",
      "iteration 2004, train loss: 0.004638, validation loss: 0.00319\n",
      "tensor(0.0035)\n",
      "iteration 2005, train loss: 0.004624, validation loss: 0.003536\n",
      "tensor(0.0033)\n",
      "iteration 2006, train loss: 0.004722, validation loss: 0.003343\n",
      "tensor(0.0032)\n",
      "iteration 2007, train loss: 0.0045, validation loss: 0.00319\n",
      "tensor(0.0031)\n",
      "iteration 2008, train loss: 0.004722, validation loss: \u001b[92m0.003143\u001b[0m\n",
      "tensor(0.0033)\n",
      "iteration 2009, train loss: 0.004482, validation loss: 0.003295\n",
      "tensor(0.0034)\n",
      "iteration 2010, train loss: 0.004512, validation loss: 0.003367\n",
      "tensor(0.0032)\n",
      "iteration 2011, train loss: 0.004555, validation loss: 0.003237\n",
      "tensor(0.0032)\n",
      "iteration 2012, train loss: 0.004455, validation loss: 0.003175\n",
      "tensor(0.0032)\n",
      "iteration 2013, train loss: 0.004527, validation loss: 0.00319\n",
      "tensor(0.0033)\n",
      "iteration 2014, train loss: 0.004476, validation loss: 0.003288\n",
      "tensor(0.0033)\n",
      "iteration 2015, train loss: 0.004498, validation loss: 0.003291\n",
      "tensor(0.0032)\n",
      "iteration 2016, train loss: 0.004573, validation loss: 0.003198\n",
      "tensor(0.0032)\n",
      "iteration 2017, train loss: 0.004534, validation loss: 0.003196\n",
      "tensor(0.0033)\n",
      "iteration 2018, train loss: 0.004443, validation loss: 0.003273\n",
      "tensor(0.0032)\n",
      "iteration 2019, train loss: 0.004588, validation loss: 0.003213\n",
      "tensor(0.0032)\n",
      "iteration 2020, train loss: 0.00454, validation loss: 0.003153\n",
      "tensor(0.0032)\n",
      "iteration 2021, train loss: 0.004524, validation loss: 0.003159\n",
      "tensor(0.0034)\n",
      "iteration 2022, train loss: 0.00455, validation loss: 0.003397\n",
      "tensor(0.0034)\n",
      "iteration 2023, train loss: 0.004687, validation loss: 0.003385\n",
      "tensor(0.0031)\n",
      "iteration 2024, train loss: 0.00463, validation loss: \u001b[92m0.003107\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2025, train loss: 0.004591, validation loss: 0.003138\n",
      "tensor(0.0035)\n",
      "iteration 2026, train loss: 0.004626, validation loss: 0.003516\n",
      "tensor(0.0034)\n",
      "iteration 2027, train loss: 0.004712, validation loss: 0.00344\n",
      "tensor(0.0031)\n",
      "iteration 2028, train loss: 0.00464, validation loss: 0.003115\n",
      "tensor(0.0031)\n",
      "iteration 2029, train loss: 0.004619, validation loss: 0.003138\n",
      "tensor(0.0034)\n",
      "iteration 2030, train loss: 0.004651, validation loss: 0.00343\n",
      "tensor(0.0035)\n",
      "iteration 2031, train loss: 0.004484, validation loss: 0.003479\n",
      "tensor(0.0032)\n",
      "iteration 2032, train loss: 0.00457, validation loss: 0.003162\n",
      "tensor(0.0031)\n",
      "iteration 2033, train loss: 0.004452, validation loss: 0.003136\n",
      "tensor(0.0031)\n",
      "iteration 2034, train loss: 0.004579, validation loss: 0.003149\n",
      "tensor(0.0035)\n",
      "iteration 2035, train loss: \u001b[92m0.004396\u001b[0m, validation loss: 0.003529\n",
      "tensor(0.0033)\n",
      "iteration 2036, train loss: 0.004537, validation loss: 0.003256\n",
      "tensor(0.0032)\n",
      "iteration 2037, train loss: 0.004481, validation loss: 0.003157\n",
      "tensor(0.0031)\n",
      "iteration 2038, train loss: 0.004523, validation loss: 0.003111\n",
      "tensor(0.0032)\n",
      "iteration 2039, train loss: 0.004445, validation loss: 0.00321\n",
      "tensor(0.0034)\n",
      "iteration 2040, train loss: 0.004563, validation loss: 0.003384\n",
      "tensor(0.0032)\n",
      "iteration 2041, train loss: 0.004492, validation loss: 0.003209\n",
      "tensor(0.0031)\n",
      "iteration 2042, train loss: \u001b[92m0.004358\u001b[0m, validation loss: 0.003118\n",
      "tensor(0.0032)\n",
      "iteration 2043, train loss: 0.004393, validation loss: 0.003179\n",
      "tensor(0.0033)\n",
      "iteration 2044, train loss: 0.00443, validation loss: 0.003338\n",
      "tensor(0.0033)\n",
      "iteration 2045, train loss: 0.004464, validation loss: 0.003293\n",
      "tensor(0.0031)\n",
      "iteration 2046, train loss: 0.004362, validation loss: 0.003114\n",
      "tensor(0.0031)\n",
      "iteration 2047, train loss: 0.004454, validation loss: \u001b[92m0.003084\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 2048, train loss: 0.004573, validation loss: 0.003244\n",
      "tensor(0.0034)\n",
      "iteration 2049, train loss: 0.004429, validation loss: 0.003408\n",
      "tensor(0.0033)\n",
      "iteration 2050, train loss: 0.004507, validation loss: 0.003253\n",
      "tensor(0.0031)\n",
      "iteration 2051, train loss: 0.004367, validation loss: 0.003088\n",
      "tensor(0.0031)\n",
      "iteration 2052, train loss: 0.004454, validation loss: \u001b[92m0.003084\u001b[0m\n",
      "tensor(0.0034)\n",
      "iteration 2053, train loss: 0.0044, validation loss: 0.003367\n",
      "tensor(0.0034)\n",
      "iteration 2054, train loss: 0.00457, validation loss: 0.003363\n",
      "tensor(0.0031)\n",
      "iteration 2055, train loss: 0.004503, validation loss: 0.003121\n",
      "tensor(0.0031)\n",
      "iteration 2056, train loss: 0.004519, validation loss: 0.0031\n",
      "tensor(0.0033)\n",
      "iteration 2057, train loss: 0.004697, validation loss: 0.00328\n",
      "tensor(0.0035)\n",
      "iteration 2058, train loss: 0.00443, validation loss: 0.003464\n",
      "tensor(0.0032)\n",
      "iteration 2059, train loss: 0.004584, validation loss: 0.003173\n",
      "tensor(0.0031)\n",
      "iteration 2060, train loss: 0.004413, validation loss: 0.003085\n",
      "tensor(0.0031)\n",
      "iteration 2061, train loss: 0.00439, validation loss: 0.003128\n",
      "tensor(0.0033)\n",
      "iteration 2062, train loss: 0.00459, validation loss: 0.003326\n",
      "tensor(0.0033)\n",
      "iteration 2063, train loss: 0.004425, validation loss: 0.003285\n",
      "tensor(0.0031)\n",
      "iteration 2064, train loss: 0.00445, validation loss: 0.003102\n",
      "tensor(0.0031)\n",
      "iteration 2065, train loss: 0.004422, validation loss: \u001b[92m0.003063\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 2066, train loss: 0.004465, validation loss: 0.003246\n",
      "tensor(0.0032)\n",
      "iteration 2067, train loss: 0.004426, validation loss: 0.003199\n",
      "tensor(0.0032)\n",
      "iteration 2068, train loss: \u001b[92m0.004352\u001b[0m, validation loss: 0.003192\n",
      "tensor(0.0032)\n",
      "iteration 2069, train loss: 0.004406, validation loss: 0.003188\n",
      "tensor(0.0031)\n",
      "iteration 2070, train loss: 0.004419, validation loss: 0.003136\n",
      "tensor(0.0031)\n",
      "iteration 2071, train loss: \u001b[92m0.004315\u001b[0m, validation loss: 0.003139\n",
      "tensor(0.0031)\n",
      "iteration 2072, train loss: 0.00443, validation loss: 0.003129\n",
      "tensor(0.0032)\n",
      "iteration 2073, train loss: 0.004329, validation loss: 0.00316\n",
      "tensor(0.0031)\n",
      "iteration 2074, train loss: 0.004508, validation loss: 0.003114\n",
      "tensor(0.0031)\n",
      "iteration 2075, train loss: 0.004373, validation loss: 0.003079\n",
      "tensor(0.0032)\n",
      "iteration 2076, train loss: 0.004355, validation loss: 0.003215\n",
      "tensor(0.0033)\n",
      "iteration 2077, train loss: 0.004343, validation loss: 0.003331\n",
      "tensor(0.0032)\n",
      "iteration 2078, train loss: 0.004474, validation loss: 0.003188\n",
      "tensor(0.0030)\n",
      "iteration 2079, train loss: 0.004398, validation loss: \u001b[92m0.003046\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2080, train loss: 0.004412, validation loss: 0.003053\n",
      "tensor(0.0033)\n",
      "iteration 2081, train loss: 0.004321, validation loss: 0.003311\n",
      "tensor(0.0033)\n",
      "iteration 2082, train loss: 0.004354, validation loss: 0.003322\n",
      "tensor(0.0031)\n",
      "iteration 2083, train loss: 0.004522, validation loss: 0.003081\n",
      "tensor(0.0030)\n",
      "iteration 2084, train loss: 0.004496, validation loss: \u001b[92m0.003046\u001b[0m\n",
      "tensor(0.0032)\n",
      "iteration 2085, train loss: 0.004331, validation loss: 0.003164\n",
      "tensor(0.0033)\n",
      "iteration 2086, train loss: 0.004432, validation loss: 0.003284\n",
      "tensor(0.0032)\n",
      "iteration 2087, train loss: 0.004508, validation loss: 0.003174\n",
      "tensor(0.0030)\n",
      "iteration 2088, train loss: 0.004515, validation loss: \u001b[92m0.003031\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2089, train loss: 0.004422, validation loss: 0.003101\n",
      "tensor(0.0032)\n",
      "iteration 2090, train loss: 0.004364, validation loss: 0.003199\n",
      "tensor(0.0031)\n",
      "iteration 2091, train loss: 0.00436, validation loss: 0.003118\n",
      "tensor(0.0031)\n",
      "iteration 2092, train loss: 0.004379, validation loss: 0.003083\n",
      "tensor(0.0031)\n",
      "iteration 2093, train loss: \u001b[92m0.004295\u001b[0m, validation loss: 0.003066\n",
      "tensor(0.0032)\n",
      "iteration 2094, train loss: 0.004402, validation loss: 0.003156\n",
      "tensor(0.0032)\n",
      "iteration 2095, train loss: 0.004425, validation loss: 0.003176\n",
      "tensor(0.0031)\n",
      "iteration 2096, train loss: 0.004367, validation loss: 0.003144\n",
      "tensor(0.0030)\n",
      "iteration 2097, train loss: 0.004354, validation loss: 0.003032\n",
      "tensor(0.0031)\n",
      "iteration 2098, train loss: 0.004347, validation loss: 0.003086\n",
      "tensor(0.0032)\n",
      "iteration 2099, train loss: 0.004369, validation loss: 0.003153\n",
      "tensor(0.0032)\n",
      "iteration 2100, train loss: 0.004437, validation loss: 0.003183\n",
      "tensor(0.0031)\n",
      "iteration 2101, train loss: 0.004324, validation loss: 0.003111\n",
      "tensor(0.0030)\n",
      "iteration 2102, train loss: 0.004338, validation loss: \u001b[92m0.003019\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2103, train loss: 0.004321, validation loss: 0.003053\n",
      "tensor(0.0032)\n",
      "iteration 2104, train loss: 0.004341, validation loss: 0.003181\n",
      "tensor(0.0032)\n",
      "iteration 2105, train loss: 0.00434, validation loss: 0.003176\n",
      "tensor(0.0030)\n",
      "iteration 2106, train loss: 0.004346, validation loss: 0.003024\n",
      "tensor(0.0030)\n",
      "iteration 2107, train loss: 0.004346, validation loss: \u001b[92m0.003015\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2108, train loss: 0.004462, validation loss: 0.003105\n",
      "tensor(0.0034)\n",
      "iteration 2109, train loss: 0.004302, validation loss: 0.00343\n",
      "tensor(0.0032)\n",
      "iteration 2110, train loss: 0.004448, validation loss: 0.003183\n",
      "tensor(0.0030)\n",
      "iteration 2111, train loss: 0.004356, validation loss: 0.003033\n",
      "tensor(0.0030)\n",
      "iteration 2112, train loss: 0.004435, validation loss: 0.003029\n",
      "tensor(0.0031)\n",
      "iteration 2113, train loss: 0.004348, validation loss: 0.003083\n",
      "tensor(0.0032)\n",
      "iteration 2114, train loss: 0.004397, validation loss: 0.003245\n",
      "tensor(0.0032)\n",
      "iteration 2115, train loss: 0.004339, validation loss: 0.003151\n",
      "tensor(0.0030)\n",
      "iteration 2116, train loss: 0.004315, validation loss: 0.003038\n",
      "tensor(0.0031)\n",
      "iteration 2117, train loss: 0.004359, validation loss: 0.003056\n",
      "tensor(0.0031)\n",
      "iteration 2118, train loss: 0.004389, validation loss: 0.003065\n",
      "tensor(0.0032)\n",
      "iteration 2119, train loss: \u001b[92m0.004266\u001b[0m, validation loss: 0.003248\n",
      "tensor(0.0031)\n",
      "iteration 2120, train loss: 0.004373, validation loss: 0.003136\n",
      "tensor(0.0030)\n",
      "iteration 2121, train loss: 0.004278, validation loss: \u001b[92m0.003011\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2122, train loss: 0.004359, validation loss: 0.003062\n",
      "tensor(0.0032)\n",
      "iteration 2123, train loss: \u001b[92m0.004224\u001b[0m, validation loss: 0.003183\n",
      "tensor(0.0031)\n",
      "iteration 2124, train loss: 0.004284, validation loss: 0.003094\n",
      "tensor(0.0030)\n",
      "iteration 2125, train loss: 0.004306, validation loss: 0.003033\n",
      "tensor(0.0030)\n",
      "iteration 2126, train loss: 0.004367, validation loss: 0.003035\n",
      "tensor(0.0031)\n",
      "iteration 2127, train loss: 0.004304, validation loss: 0.003112\n",
      "tensor(0.0031)\n",
      "iteration 2128, train loss: 0.004353, validation loss: 0.003093\n",
      "tensor(0.0030)\n",
      "iteration 2129, train loss: 0.004303, validation loss: 0.003021\n",
      "tensor(0.0030)\n",
      "iteration 2130, train loss: 0.00425, validation loss: \u001b[92m0.00301\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2131, train loss: 0.004264, validation loss: 0.003053\n",
      "tensor(0.0031)\n",
      "iteration 2132, train loss: 0.004312, validation loss: 0.003114\n",
      "tensor(0.0031)\n",
      "iteration 2133, train loss: 0.004251, validation loss: 0.003082\n",
      "tensor(0.0030)\n",
      "iteration 2134, train loss: \u001b[92m0.004215\u001b[0m, validation loss: \u001b[92m0.003009\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2135, train loss: 0.004278, validation loss: \u001b[92m0.002989\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2136, train loss: 0.004433, validation loss: 0.003078\n",
      "tensor(0.0032)\n",
      "iteration 2137, train loss: 0.004296, validation loss: 0.003221\n",
      "tensor(0.0031)\n",
      "iteration 2138, train loss: 0.004237, validation loss: 0.003077\n",
      "tensor(0.0030)\n",
      "iteration 2139, train loss: 0.004371, validation loss: 0.00299\n",
      "tensor(0.0030)\n",
      "iteration 2140, train loss: 0.004423, validation loss: 0.002997\n",
      "tensor(0.0031)\n",
      "iteration 2141, train loss: 0.004353, validation loss: 0.003145\n",
      "tensor(0.0031)\n",
      "iteration 2142, train loss: 0.004284, validation loss: 0.003141\n",
      "tensor(0.0030)\n",
      "iteration 2143, train loss: 0.004396, validation loss: 0.003012\n",
      "tensor(0.0030)\n",
      "iteration 2144, train loss: 0.004305, validation loss: 0.00301\n",
      "tensor(0.0030)\n",
      "iteration 2145, train loss: 0.004273, validation loss: 0.003035\n",
      "tensor(0.0031)\n",
      "iteration 2146, train loss: 0.004352, validation loss: 0.003114\n",
      "tensor(0.0032)\n",
      "iteration 2147, train loss: 0.004337, validation loss: 0.003191\n",
      "tensor(0.0030)\n",
      "iteration 2148, train loss: 0.004412, validation loss: 0.003019\n",
      "tensor(0.0031)\n",
      "iteration 2149, train loss: 0.004326, validation loss: 0.003065\n",
      "tensor(0.0031)\n",
      "iteration 2150, train loss: 0.004371, validation loss: 0.003067\n",
      "tensor(0.0030)\n",
      "iteration 2151, train loss: 0.004395, validation loss: 0.003027\n",
      "tensor(0.0031)\n",
      "iteration 2152, train loss: 0.004276, validation loss: 0.003133\n",
      "tensor(0.0030)\n",
      "iteration 2153, train loss: 0.00441, validation loss: 0.003005\n",
      "tensor(0.0031)\n",
      "iteration 2154, train loss: 0.00431, validation loss: 0.003087\n",
      "tensor(0.0031)\n",
      "iteration 2155, train loss: 0.00442, validation loss: 0.003066\n",
      "tensor(0.0030)\n",
      "iteration 2156, train loss: \u001b[92m0.004201\u001b[0m, validation loss: 0.003038\n",
      "tensor(0.0030)\n",
      "iteration 2157, train loss: 0.004351, validation loss: 0.003023\n",
      "tensor(0.0031)\n",
      "iteration 2158, train loss: 0.004267, validation loss: 0.003079\n",
      "tensor(0.0032)\n",
      "iteration 2159, train loss: 0.004252, validation loss: 0.003191\n",
      "tensor(0.0031)\n",
      "iteration 2160, train loss: 0.004317, validation loss: 0.003102\n",
      "tensor(0.0030)\n",
      "iteration 2161, train loss: 0.004314, validation loss: \u001b[92m0.00298\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2162, train loss: 0.004286, validation loss: 0.00304\n",
      "tensor(0.0031)\n",
      "iteration 2163, train loss: 0.004419, validation loss: 0.003099\n",
      "tensor(0.0031)\n",
      "iteration 2164, train loss: 0.0043, validation loss: 0.00308\n",
      "tensor(0.0030)\n",
      "iteration 2165, train loss: 0.004271, validation loss: 0.002993\n",
      "tensor(0.0030)\n",
      "iteration 2166, train loss: 0.004312, validation loss: \u001b[92m0.002961\u001b[0m\n",
      "tensor(0.0031)\n",
      "iteration 2167, train loss: 0.004372, validation loss: 0.003081\n",
      "tensor(0.0032)\n",
      "iteration 2168, train loss: 0.00437, validation loss: 0.003197\n",
      "tensor(0.0030)\n",
      "iteration 2169, train loss: 0.004275, validation loss: 0.003039\n",
      "tensor(0.0029)\n",
      "iteration 2170, train loss: 0.00425, validation loss: \u001b[92m0.002943\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2171, train loss: 0.004368, validation loss: 0.003018\n",
      "tensor(0.0033)\n",
      "iteration 2172, train loss: 0.004205, validation loss: 0.003255\n",
      "tensor(0.0030)\n",
      "iteration 2173, train loss: 0.004293, validation loss: 0.003037\n",
      "tensor(0.0030)\n",
      "iteration 2174, train loss: 0.004267, validation loss: 0.002961\n",
      "tensor(0.0030)\n",
      "iteration 2175, train loss: 0.004267, validation loss: 0.002951\n",
      "tensor(0.0031)\n",
      "iteration 2176, train loss: 0.004248, validation loss: 0.003101\n",
      "tensor(0.0031)\n",
      "iteration 2177, train loss: 0.0043, validation loss: 0.003063\n",
      "tensor(0.0030)\n",
      "iteration 2178, train loss: 0.00432, validation loss: 0.002986\n",
      "tensor(0.0030)\n",
      "iteration 2179, train loss: 0.004238, validation loss: 0.002953\n",
      "tensor(0.0029)\n",
      "iteration 2180, train loss: 0.004275, validation loss: 0.002947\n",
      "tensor(0.0031)\n",
      "iteration 2181, train loss: \u001b[92m0.004123\u001b[0m, validation loss: 0.003072\n",
      "tensor(0.0032)\n",
      "iteration 2182, train loss: 0.004233, validation loss: 0.003166\n",
      "tensor(0.0030)\n",
      "iteration 2183, train loss: 0.004291, validation loss: 0.002972\n",
      "tensor(0.0029)\n",
      "iteration 2184, train loss: 0.004159, validation loss: \u001b[92m0.002941\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2185, train loss: 0.004232, validation loss: 0.003037\n",
      "tensor(0.0031)\n",
      "iteration 2186, train loss: 0.004338, validation loss: 0.003081\n",
      "tensor(0.0029)\n",
      "iteration 2187, train loss: 0.004269, validation loss: 0.002945\n",
      "tensor(0.0029)\n",
      "iteration 2188, train loss: 0.004207, validation loss: 0.002946\n",
      "tensor(0.0031)\n",
      "iteration 2189, train loss: 0.004241, validation loss: 0.003113\n",
      "tensor(0.0033)\n",
      "iteration 2190, train loss: 0.004314, validation loss: 0.003258\n",
      "tensor(0.0030)\n",
      "iteration 2191, train loss: 0.004234, validation loss: 0.003022\n",
      "tensor(0.0029)\n",
      "iteration 2192, train loss: 0.00418, validation loss: 0.002944\n",
      "tensor(0.0030)\n",
      "iteration 2193, train loss: 0.004325, validation loss: 0.002957\n",
      "tensor(0.0033)\n",
      "iteration 2194, train loss: \u001b[92m0.004104\u001b[0m, validation loss: 0.003293\n",
      "tensor(0.0032)\n",
      "iteration 2195, train loss: 0.004352, validation loss: 0.003154\n",
      "tensor(0.0029)\n",
      "iteration 2196, train loss: 0.004179, validation loss: \u001b[92m0.002933\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2197, train loss: 0.004282, validation loss: 0.002935\n",
      "tensor(0.0031)\n",
      "iteration 2198, train loss: 0.004284, validation loss: 0.003063\n",
      "tensor(0.0031)\n",
      "iteration 2199, train loss: 0.004236, validation loss: 0.003125\n",
      "tensor(0.0030)\n",
      "iteration 2200, train loss: 0.00426, validation loss: 0.002984\n",
      "tensor(0.0029)\n",
      "iteration 2201, train loss: 0.004273, validation loss: \u001b[92m0.00293\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2202, train loss: 0.004237, validation loss: 0.002932\n",
      "tensor(0.0030)\n",
      "iteration 2203, train loss: \u001b[92m0.004101\u001b[0m, validation loss: 0.003049\n",
      "tensor(0.0032)\n",
      "iteration 2204, train loss: 0.004161, validation loss: 0.003175\n",
      "tensor(0.0030)\n",
      "iteration 2205, train loss: 0.004271, validation loss: 0.002983\n",
      "tensor(0.0030)\n",
      "iteration 2206, train loss: 0.004111, validation loss: 0.002956\n",
      "tensor(0.0030)\n",
      "iteration 2207, train loss: 0.004295, validation loss: 0.002986\n",
      "tensor(0.0031)\n",
      "iteration 2208, train loss: 0.004203, validation loss: 0.003078\n",
      "tensor(0.0030)\n",
      "iteration 2209, train loss: 0.00415, validation loss: 0.003\n",
      "tensor(0.0029)\n",
      "iteration 2210, train loss: 0.004203, validation loss: \u001b[92m0.002909\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2211, train loss: 0.004338, validation loss: 0.002919\n",
      "tensor(0.0032)\n",
      "iteration 2212, train loss: 0.004299, validation loss: 0.003204\n",
      "tensor(0.0031)\n",
      "iteration 2213, train loss: 0.004261, validation loss: 0.003117\n",
      "tensor(0.0029)\n",
      "iteration 2214, train loss: 0.004267, validation loss: 0.002938\n",
      "tensor(0.0030)\n",
      "iteration 2215, train loss: 0.004361, validation loss: 0.002966\n",
      "tensor(0.0030)\n",
      "iteration 2216, train loss: 0.004259, validation loss: 0.003018\n",
      "tensor(0.0030)\n",
      "iteration 2217, train loss: 0.00419, validation loss: 0.002971\n",
      "tensor(0.0029)\n",
      "iteration 2218, train loss: 0.004334, validation loss: 0.002933\n",
      "tensor(0.0030)\n",
      "iteration 2219, train loss: 0.004157, validation loss: 0.002978\n",
      "tensor(0.0030)\n",
      "iteration 2220, train loss: 0.004195, validation loss: 0.003004\n",
      "tensor(0.0030)\n",
      "iteration 2221, train loss: 0.004161, validation loss: 0.002958\n",
      "tensor(0.0030)\n",
      "iteration 2222, train loss: 0.004126, validation loss: 0.002972\n",
      "tensor(0.0029)\n",
      "iteration 2223, train loss: 0.004206, validation loss: 0.00294\n",
      "tensor(0.0030)\n",
      "iteration 2224, train loss: 0.004262, validation loss: 0.003033\n",
      "tensor(0.0030)\n",
      "iteration 2225, train loss: 0.004216, validation loss: 0.002989\n",
      "tensor(0.0029)\n",
      "iteration 2226, train loss: 0.004236, validation loss: 0.002947\n",
      "tensor(0.0030)\n",
      "iteration 2227, train loss: 0.004188, validation loss: 0.002963\n",
      "tensor(0.0029)\n",
      "iteration 2228, train loss: 0.004193, validation loss: 0.002948\n",
      "tensor(0.0031)\n",
      "iteration 2229, train loss: 0.004235, validation loss: 0.003056\n",
      "tensor(0.0030)\n",
      "iteration 2230, train loss: 0.004155, validation loss: 0.003007\n",
      "tensor(0.0030)\n",
      "iteration 2231, train loss: 0.004243, validation loss: 0.002983\n",
      "tensor(0.0030)\n",
      "iteration 2232, train loss: 0.004148, validation loss: 0.003001\n",
      "tensor(0.0029)\n",
      "iteration 2233, train loss: 0.004273, validation loss: 0.002945\n",
      "tensor(0.0030)\n",
      "iteration 2234, train loss: 0.00415, validation loss: 0.00296\n",
      "tensor(0.0030)\n",
      "iteration 2235, train loss: 0.004155, validation loss: 0.003001\n",
      "tensor(0.0031)\n",
      "iteration 2236, train loss: 0.004226, validation loss: 0.003132\n",
      "tensor(0.0030)\n",
      "iteration 2237, train loss: 0.004338, validation loss: 0.002961\n",
      "tensor(0.0029)\n",
      "iteration 2238, train loss: 0.004218, validation loss: 0.002937\n",
      "tensor(0.0030)\n",
      "iteration 2239, train loss: 0.00429, validation loss: 0.002968\n",
      "tensor(0.0031)\n",
      "iteration 2240, train loss: 0.004275, validation loss: 0.003148\n",
      "tensor(0.0031)\n",
      "iteration 2241, train loss: 0.004296, validation loss: 0.003102\n",
      "tensor(0.0029)\n",
      "iteration 2242, train loss: 0.004189, validation loss: 0.002936\n",
      "tensor(0.0029)\n",
      "iteration 2243, train loss: 0.004124, validation loss: \u001b[92m0.002892\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2244, train loss: 0.00417, validation loss: 0.00303\n",
      "tensor(0.0030)\n",
      "iteration 2245, train loss: 0.004223, validation loss: 0.003023\n",
      "tensor(0.0029)\n",
      "iteration 2246, train loss: 0.004142, validation loss: 0.002909\n",
      "tensor(0.0030)\n",
      "iteration 2247, train loss: 0.004163, validation loss: 0.002976\n",
      "tensor(0.0030)\n",
      "iteration 2248, train loss: 0.004281, validation loss: 0.003023\n",
      "tensor(0.0032)\n",
      "iteration 2249, train loss: 0.004147, validation loss: 0.003168\n",
      "tensor(0.0029)\n",
      "iteration 2250, train loss: 0.004264, validation loss: 0.002901\n",
      "tensor(0.0030)\n",
      "iteration 2251, train loss: 0.004232, validation loss: 0.002996\n",
      "tensor(0.0031)\n",
      "iteration 2252, train loss: 0.004247, validation loss: 0.003071\n",
      "tensor(0.0030)\n",
      "iteration 2253, train loss: 0.004219, validation loss: 0.003045\n",
      "tensor(0.0029)\n",
      "iteration 2254, train loss: 0.004122, validation loss: 0.002937\n",
      "tensor(0.0029)\n",
      "iteration 2255, train loss: 0.004263, validation loss: 0.002904\n",
      "tensor(0.0029)\n",
      "iteration 2256, train loss: 0.004158, validation loss: 0.002911\n",
      "tensor(0.0031)\n",
      "iteration 2257, train loss: 0.0042, validation loss: 0.003057\n",
      "tensor(0.0030)\n",
      "iteration 2258, train loss: 0.004155, validation loss: 0.003038\n",
      "tensor(0.0029)\n",
      "iteration 2259, train loss: 0.004144, validation loss: \u001b[92m0.00289\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2260, train loss: 0.004104, validation loss: \u001b[92m0.002861\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2261, train loss: 0.004161, validation loss: 0.002959\n",
      "tensor(0.0031)\n",
      "iteration 2262, train loss: 0.004129, validation loss: 0.003115\n",
      "tensor(0.0030)\n",
      "iteration 2263, train loss: 0.004241, validation loss: 0.002974\n",
      "tensor(0.0030)\n",
      "iteration 2264, train loss: \u001b[92m0.004085\u001b[0m, validation loss: 0.002959\n",
      "tensor(0.0029)\n",
      "iteration 2265, train loss: 0.004184, validation loss: 0.002882\n",
      "tensor(0.0029)\n",
      "iteration 2266, train loss: 0.004182, validation loss: 0.002945\n",
      "tensor(0.0030)\n",
      "iteration 2267, train loss: 0.004116, validation loss: 0.002969\n",
      "tensor(0.0029)\n",
      "iteration 2268, train loss: 0.004211, validation loss: 0.002895\n",
      "tensor(0.0029)\n",
      "iteration 2269, train loss: \u001b[92m0.004057\u001b[0m, validation loss: 0.002914\n",
      "tensor(0.0029)\n",
      "iteration 2270, train loss: 0.004068, validation loss: 0.002933\n",
      "tensor(0.0030)\n",
      "iteration 2271, train loss: 0.004188, validation loss: 0.002987\n",
      "tensor(0.0029)\n",
      "iteration 2272, train loss: 0.004228, validation loss: 0.002897\n",
      "tensor(0.0029)\n",
      "iteration 2273, train loss: 0.004183, validation loss: 0.002936\n",
      "tensor(0.0031)\n",
      "iteration 2274, train loss: 0.004126, validation loss: 0.003088\n",
      "tensor(0.0031)\n",
      "iteration 2275, train loss: 0.00427, validation loss: 0.003082\n",
      "tensor(0.0030)\n",
      "iteration 2276, train loss: 0.004242, validation loss: 0.002989\n",
      "tensor(0.0029)\n",
      "iteration 2277, train loss: 0.004217, validation loss: 0.00292\n",
      "tensor(0.0029)\n",
      "iteration 2278, train loss: 0.004242, validation loss: 0.002867\n",
      "tensor(0.0032)\n",
      "iteration 2279, train loss: 0.004071, validation loss: 0.003187\n",
      "tensor(0.0031)\n",
      "iteration 2280, train loss: 0.00419, validation loss: 0.003142\n",
      "tensor(0.0029)\n",
      "iteration 2281, train loss: 0.004247, validation loss: 0.0029\n",
      "tensor(0.0030)\n",
      "iteration 2282, train loss: \u001b[92m0.004025\u001b[0m, validation loss: 0.002979\n",
      "tensor(0.0029)\n",
      "iteration 2283, train loss: 0.004336, validation loss: 0.002868\n",
      "tensor(0.0031)\n",
      "iteration 2284, train loss: 0.004227, validation loss: 0.003087\n",
      "tensor(0.0032)\n",
      "iteration 2285, train loss: 0.004159, validation loss: 0.003171\n",
      "tensor(0.0030)\n",
      "iteration 2286, train loss: 0.004186, validation loss: 0.002951\n",
      "tensor(0.0029)\n",
      "iteration 2287, train loss: 0.00406, validation loss: 0.002863\n",
      "tensor(0.0029)\n",
      "iteration 2288, train loss: 0.004229, validation loss: 0.002866\n",
      "tensor(0.0030)\n",
      "iteration 2289, train loss: 0.004198, validation loss: 0.002981\n",
      "tensor(0.0031)\n",
      "iteration 2290, train loss: 0.004059, validation loss: 0.003053\n",
      "tensor(0.0029)\n",
      "iteration 2291, train loss: 0.004117, validation loss: 0.002945\n",
      "tensor(0.0029)\n",
      "iteration 2292, train loss: 0.004156, validation loss: \u001b[92m0.002855\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2293, train loss: 0.004223, validation loss: \u001b[92m0.002843\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2294, train loss: 0.004066, validation loss: 0.002923\n",
      "tensor(0.0029)\n",
      "iteration 2295, train loss: 0.004042, validation loss: 0.002896\n",
      "tensor(0.0029)\n",
      "iteration 2296, train loss: 0.004112, validation loss: 0.0029\n",
      "tensor(0.0029)\n",
      "iteration 2297, train loss: 0.00413, validation loss: 0.002918\n",
      "tensor(0.0029)\n",
      "iteration 2298, train loss: 0.004106, validation loss: 0.002923\n",
      "tensor(0.0030)\n",
      "iteration 2299, train loss: 0.004044, validation loss: 0.002953\n",
      "tensor(0.0029)\n",
      "iteration 2300, train loss: 0.004205, validation loss: 0.002861\n",
      "tensor(0.0028)\n",
      "iteration 2301, train loss: 0.004096, validation loss: \u001b[92m0.002838\u001b[0m\n",
      "tensor(0.0030)\n",
      "iteration 2302, train loss: 0.004129, validation loss: 0.002959\n",
      "tensor(0.0030)\n",
      "iteration 2303, train loss: 0.00405, validation loss: 0.002968\n",
      "tensor(0.0028)\n",
      "iteration 2304, train loss: 0.004103, validation loss: 0.002838\n",
      "tensor(0.0029)\n",
      "iteration 2305, train loss: 0.004056, validation loss: 0.002885\n",
      "tensor(0.0030)\n",
      "iteration 2306, train loss: 0.00423, validation loss: 0.003004\n",
      "tensor(0.0030)\n",
      "iteration 2307, train loss: 0.004143, validation loss: 0.003028\n",
      "tensor(0.0029)\n",
      "iteration 2308, train loss: 0.004093, validation loss: 0.002912\n",
      "tensor(0.0028)\n",
      "iteration 2309, train loss: 0.004043, validation loss: \u001b[92m0.002819\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2310, train loss: 0.004159, validation loss: 0.002923\n",
      "tensor(0.0029)\n",
      "iteration 2311, train loss: 0.004169, validation loss: 0.002914\n",
      "tensor(0.0029)\n",
      "iteration 2312, train loss: 0.004054, validation loss: 0.002911\n",
      "tensor(0.0029)\n",
      "iteration 2313, train loss: 0.004088, validation loss: 0.002908\n",
      "tensor(0.0029)\n",
      "iteration 2314, train loss: 0.004093, validation loss: 0.002905\n",
      "tensor(0.0029)\n",
      "iteration 2315, train loss: 0.004078, validation loss: 0.002925\n",
      "tensor(0.0029)\n",
      "iteration 2316, train loss: \u001b[92m0.004024\u001b[0m, validation loss: 0.00288\n",
      "tensor(0.0029)\n",
      "iteration 2317, train loss: 0.00418, validation loss: 0.002894\n",
      "tensor(0.0029)\n",
      "iteration 2318, train loss: 0.004104, validation loss: 0.002911\n",
      "tensor(0.0029)\n",
      "iteration 2319, train loss: 0.004039, validation loss: 0.002877\n",
      "tensor(0.0029)\n",
      "iteration 2320, train loss: 0.004035, validation loss: 0.002864\n",
      "tensor(0.0028)\n",
      "iteration 2321, train loss: 0.00405, validation loss: 0.002833\n",
      "tensor(0.0028)\n",
      "iteration 2322, train loss: \u001b[92m0.00401\u001b[0m, validation loss: 0.002834\n",
      "tensor(0.0029)\n",
      "iteration 2323, train loss: 0.004062, validation loss: 0.002937\n",
      "tensor(0.0029)\n",
      "iteration 2324, train loss: \u001b[92m0.00398\u001b[0m, validation loss: 0.002925\n",
      "tensor(0.0028)\n",
      "iteration 2325, train loss: 0.004063, validation loss: 0.002835\n",
      "tensor(0.0028)\n",
      "iteration 2326, train loss: 0.004041, validation loss: 0.002821\n",
      "tensor(0.0029)\n",
      "iteration 2327, train loss: 0.004104, validation loss: 0.002855\n",
      "tensor(0.0030)\n",
      "iteration 2328, train loss: 0.004032, validation loss: 0.003038\n",
      "tensor(0.0029)\n",
      "iteration 2329, train loss: 0.004132, validation loss: 0.002945\n",
      "tensor(0.0028)\n",
      "iteration 2330, train loss: 0.004037, validation loss: \u001b[92m0.002815\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2331, train loss: 0.004076, validation loss: 0.002816\n",
      "tensor(0.0030)\n",
      "iteration 2332, train loss: 0.00415, validation loss: 0.002959\n",
      "tensor(0.0029)\n",
      "iteration 2333, train loss: 0.004047, validation loss: 0.002945\n",
      "tensor(0.0029)\n",
      "iteration 2334, train loss: 0.004102, validation loss: 0.002852\n",
      "tensor(0.0029)\n",
      "iteration 2335, train loss: 0.004131, validation loss: 0.00287\n",
      "tensor(0.0029)\n",
      "iteration 2336, train loss: 0.004002, validation loss: 0.002889\n",
      "tensor(0.0028)\n",
      "iteration 2337, train loss: 0.00401, validation loss: 0.002828\n",
      "tensor(0.0029)\n",
      "iteration 2338, train loss: 0.004076, validation loss: 0.002871\n",
      "tensor(0.0028)\n",
      "iteration 2339, train loss: 0.004223, validation loss: 0.002834\n",
      "tensor(0.0030)\n",
      "iteration 2340, train loss: 0.004065, validation loss: 0.002996\n",
      "tensor(0.0031)\n",
      "iteration 2341, train loss: 0.004014, validation loss: 0.003053\n",
      "tensor(0.0028)\n",
      "iteration 2342, train loss: 0.004105, validation loss: 0.002816\n",
      "tensor(0.0028)\n",
      "iteration 2343, train loss: 0.004083, validation loss: 0.002849\n",
      "tensor(0.0029)\n",
      "iteration 2344, train loss: 0.004146, validation loss: 0.002859\n",
      "tensor(0.0029)\n",
      "iteration 2345, train loss: 0.004065, validation loss: 0.002896\n",
      "tensor(0.0029)\n",
      "iteration 2346, train loss: 0.004099, validation loss: 0.002925\n",
      "tensor(0.0028)\n",
      "iteration 2347, train loss: 0.004129, validation loss: \u001b[92m0.002814\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2348, train loss: 0.004088, validation loss: 0.002823\n",
      "tensor(0.0028)\n",
      "iteration 2349, train loss: 0.004118, validation loss: \u001b[92m0.002813\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2350, train loss: 0.004062, validation loss: 0.002834\n",
      "tensor(0.0030)\n",
      "iteration 2351, train loss: 0.004059, validation loss: 0.003016\n",
      "tensor(0.0029)\n",
      "iteration 2352, train loss: 0.004149, validation loss: 0.002883\n",
      "tensor(0.0028)\n",
      "iteration 2353, train loss: 0.004074, validation loss: \u001b[92m0.0028\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2354, train loss: 0.004103, validation loss: 0.002817\n",
      "tensor(0.0030)\n",
      "iteration 2355, train loss: 0.003982, validation loss: 0.002997\n",
      "tensor(0.0029)\n",
      "iteration 2356, train loss: \u001b[92m0.003978\u001b[0m, validation loss: 0.002936\n",
      "tensor(0.0028)\n",
      "iteration 2357, train loss: 0.004044, validation loss: \u001b[92m0.002797\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2358, train loss: 0.004043, validation loss: 0.002825\n",
      "tensor(0.0029)\n",
      "iteration 2359, train loss: 0.004017, validation loss: 0.002866\n",
      "tensor(0.0029)\n",
      "iteration 2360, train loss: 0.004007, validation loss: 0.002912\n",
      "tensor(0.0029)\n",
      "iteration 2361, train loss: 0.004042, validation loss: 0.002891\n",
      "tensor(0.0028)\n",
      "iteration 2362, train loss: 0.003987, validation loss: 0.002799\n",
      "tensor(0.0028)\n",
      "iteration 2363, train loss: 0.003978, validation loss: 0.002817\n",
      "tensor(0.0028)\n",
      "iteration 2364, train loss: \u001b[92m0.003945\u001b[0m, validation loss: 0.002846\n",
      "tensor(0.0028)\n",
      "iteration 2365, train loss: 0.004097, validation loss: 0.002825\n",
      "tensor(0.0028)\n",
      "iteration 2366, train loss: 0.004019, validation loss: 0.002812\n",
      "tensor(0.0028)\n",
      "iteration 2367, train loss: 0.004112, validation loss: 0.002817\n",
      "tensor(0.0028)\n",
      "iteration 2368, train loss: 0.00406, validation loss: 0.002814\n",
      "tensor(0.0029)\n",
      "iteration 2369, train loss: 0.00398, validation loss: 0.002856\n",
      "tensor(0.0028)\n",
      "iteration 2370, train loss: 0.004125, validation loss: 0.002829\n",
      "tensor(0.0028)\n",
      "iteration 2371, train loss: 0.004027, validation loss: 0.002837\n",
      "tensor(0.0028)\n",
      "iteration 2372, train loss: 0.004037, validation loss: 0.002841\n",
      "tensor(0.0028)\n",
      "iteration 2373, train loss: 0.004054, validation loss: 0.002824\n",
      "tensor(0.0029)\n",
      "iteration 2374, train loss: 0.004075, validation loss: 0.002876\n",
      "tensor(0.0029)\n",
      "iteration 2375, train loss: \u001b[92m0.003921\u001b[0m, validation loss: 0.002933\n",
      "tensor(0.0029)\n",
      "iteration 2376, train loss: 0.004045, validation loss: 0.002944\n",
      "tensor(0.0028)\n",
      "iteration 2377, train loss: 0.004133, validation loss: \u001b[92m0.002772\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2378, train loss: 0.003997, validation loss: 0.002821\n",
      "tensor(0.0029)\n",
      "iteration 2379, train loss: 0.004107, validation loss: 0.002944\n",
      "tensor(0.0030)\n",
      "iteration 2380, train loss: 0.00403, validation loss: 0.003024\n",
      "tensor(0.0028)\n",
      "iteration 2381, train loss: 0.004068, validation loss: 0.002805\n",
      "tensor(0.0028)\n",
      "iteration 2382, train loss: 0.004053, validation loss: 0.002796\n",
      "tensor(0.0028)\n",
      "iteration 2383, train loss: 0.003944, validation loss: 0.00282\n",
      "tensor(0.0029)\n",
      "iteration 2384, train loss: \u001b[92m0.003895\u001b[0m, validation loss: 0.002924\n",
      "tensor(0.0028)\n",
      "iteration 2385, train loss: 0.004079, validation loss: 0.002803\n",
      "tensor(0.0028)\n",
      "iteration 2386, train loss: 0.003983, validation loss: 0.002782\n",
      "tensor(0.0028)\n",
      "iteration 2387, train loss: 0.003911, validation loss: 0.00278\n",
      "tensor(0.0029)\n",
      "iteration 2388, train loss: 0.004027, validation loss: 0.002893\n",
      "tensor(0.0029)\n",
      "iteration 2389, train loss: 0.003925, validation loss: 0.0029\n",
      "tensor(0.0028)\n",
      "iteration 2390, train loss: 0.003999, validation loss: 0.002813\n",
      "tensor(0.0028)\n",
      "iteration 2391, train loss: 0.003953, validation loss: 0.002819\n",
      "tensor(0.0029)\n",
      "iteration 2392, train loss: 0.004102, validation loss: 0.00286\n",
      "tensor(0.0030)\n",
      "iteration 2393, train loss: 0.004, validation loss: 0.003033\n",
      "tensor(0.0028)\n",
      "iteration 2394, train loss: 0.004203, validation loss: 0.002815\n",
      "tensor(0.0028)\n",
      "iteration 2395, train loss: 0.004076, validation loss: 0.00279\n",
      "tensor(0.0029)\n",
      "iteration 2396, train loss: 0.004064, validation loss: 0.002889\n",
      "tensor(0.0029)\n",
      "iteration 2397, train loss: 0.004063, validation loss: 0.002854\n",
      "tensor(0.0028)\n",
      "iteration 2398, train loss: 0.003932, validation loss: 0.00283\n",
      "tensor(0.0028)\n",
      "iteration 2399, train loss: 0.003997, validation loss: \u001b[92m0.002764\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2400, train loss: 0.004069, validation loss: 0.002774\n",
      "tensor(0.0028)\n",
      "iteration 2401, train loss: 0.004021, validation loss: 0.00284\n",
      "tensor(0.0028)\n",
      "iteration 2402, train loss: 0.004004, validation loss: 0.002825\n",
      "tensor(0.0028)\n",
      "iteration 2403, train loss: 0.004043, validation loss: 0.002845\n",
      "tensor(0.0028)\n",
      "iteration 2404, train loss: 0.003986, validation loss: 0.002818\n",
      "tensor(0.0028)\n",
      "iteration 2405, train loss: 0.003968, validation loss: 0.002774\n",
      "tensor(0.0027)\n",
      "iteration 2406, train loss: 0.003937, validation loss: \u001b[92m0.002745\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2407, train loss: 0.003984, validation loss: 0.002779\n",
      "tensor(0.0028)\n",
      "iteration 2408, train loss: 0.00404, validation loss: 0.002842\n",
      "tensor(0.0030)\n",
      "iteration 2409, train loss: 0.004113, validation loss: 0.002965\n",
      "tensor(0.0029)\n",
      "iteration 2410, train loss: 0.004082, validation loss: 0.002894\n",
      "tensor(0.0027)\n",
      "iteration 2411, train loss: 0.004036, validation loss: \u001b[92m0.002743\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2412, train loss: 0.004006, validation loss: 0.002799\n",
      "tensor(0.0030)\n",
      "iteration 2413, train loss: 0.004121, validation loss: 0.002986\n",
      "tensor(0.0030)\n",
      "iteration 2414, train loss: 0.004079, validation loss: 0.002984\n",
      "tensor(0.0028)\n",
      "iteration 2415, train loss: 0.004047, validation loss: 0.002814\n",
      "tensor(0.0028)\n",
      "iteration 2416, train loss: 0.00405, validation loss: 0.002751\n",
      "tensor(0.0029)\n",
      "iteration 2417, train loss: 0.003917, validation loss: 0.002911\n",
      "tensor(0.0029)\n",
      "iteration 2418, train loss: 0.004054, validation loss: 0.002912\n",
      "tensor(0.0028)\n",
      "iteration 2419, train loss: 0.004096, validation loss: 0.002839\n",
      "tensor(0.0028)\n",
      "iteration 2420, train loss: 0.004012, validation loss: 0.002829\n",
      "tensor(0.0027)\n",
      "iteration 2421, train loss: 0.004009, validation loss: \u001b[92m0.002741\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2422, train loss: \u001b[92m0.003874\u001b[0m, validation loss: 0.002819\n",
      "tensor(0.0029)\n",
      "iteration 2423, train loss: 0.004134, validation loss: 0.002863\n",
      "tensor(0.0028)\n",
      "iteration 2424, train loss: 0.003958, validation loss: 0.002841\n",
      "tensor(0.0028)\n",
      "iteration 2425, train loss: 0.00402, validation loss: 0.002769\n",
      "tensor(0.0028)\n",
      "iteration 2426, train loss: 0.003932, validation loss: 0.00277\n",
      "tensor(0.0028)\n",
      "iteration 2427, train loss: 0.00399, validation loss: 0.002782\n",
      "tensor(0.0028)\n",
      "iteration 2428, train loss: 0.003985, validation loss: 0.00278\n",
      "tensor(0.0028)\n",
      "iteration 2429, train loss: 0.003909, validation loss: 0.00279\n",
      "tensor(0.0028)\n",
      "iteration 2430, train loss: 0.00394, validation loss: 0.002778\n",
      "tensor(0.0029)\n",
      "iteration 2431, train loss: 0.00398, validation loss: 0.002924\n",
      "tensor(0.0028)\n",
      "iteration 2432, train loss: 0.004039, validation loss: 0.002829\n",
      "tensor(0.0028)\n",
      "iteration 2433, train loss: 0.004104, validation loss: 0.002773\n",
      "tensor(0.0028)\n",
      "iteration 2434, train loss: 0.004084, validation loss: 0.002815\n",
      "tensor(0.0028)\n",
      "iteration 2435, train loss: 0.004041, validation loss: 0.002803\n",
      "tensor(0.0029)\n",
      "iteration 2436, train loss: 0.004009, validation loss: 0.002872\n",
      "tensor(0.0028)\n",
      "iteration 2437, train loss: 0.003999, validation loss: 0.002782\n",
      "tensor(0.0028)\n",
      "iteration 2438, train loss: 0.003975, validation loss: 0.002764\n",
      "tensor(0.0028)\n",
      "iteration 2439, train loss: 0.003944, validation loss: 0.002839\n",
      "tensor(0.0028)\n",
      "iteration 2440, train loss: 0.004054, validation loss: 0.002793\n",
      "tensor(0.0028)\n",
      "iteration 2441, train loss: \u001b[92m0.003874\u001b[0m, validation loss: 0.002757\n",
      "tensor(0.0028)\n",
      "iteration 2442, train loss: 0.003891, validation loss: 0.002805\n",
      "tensor(0.0028)\n",
      "iteration 2443, train loss: 0.003959, validation loss: 0.0028\n",
      "tensor(0.0028)\n",
      "iteration 2444, train loss: \u001b[92m0.003862\u001b[0m, validation loss: 0.002808\n",
      "tensor(0.0028)\n",
      "iteration 2445, train loss: 0.00398, validation loss: 0.002805\n",
      "tensor(0.0028)\n",
      "iteration 2446, train loss: 0.003911, validation loss: 0.002787\n",
      "tensor(0.0028)\n",
      "iteration 2447, train loss: 0.004037, validation loss: 0.002773\n",
      "tensor(0.0028)\n",
      "iteration 2448, train loss: 0.003896, validation loss: 0.002751\n",
      "tensor(0.0027)\n",
      "iteration 2449, train loss: 0.003956, validation loss: \u001b[92m0.002728\u001b[0m\n",
      "tensor(0.0029)\n",
      "iteration 2450, train loss: 0.00401, validation loss: 0.002855\n",
      "tensor(0.0028)\n",
      "iteration 2451, train loss: 0.004038, validation loss: 0.002841\n",
      "tensor(0.0027)\n",
      "iteration 2452, train loss: 0.00403, validation loss: 0.002748\n",
      "tensor(0.0028)\n",
      "iteration 2453, train loss: 0.003899, validation loss: 0.002751\n",
      "tensor(0.0027)\n",
      "iteration 2454, train loss: 0.004034, validation loss: 0.002745\n",
      "tensor(0.0030)\n",
      "iteration 2455, train loss: 0.003942, validation loss: 0.003041\n",
      "tensor(0.0028)\n",
      "iteration 2456, train loss: 0.004094, validation loss: 0.002788\n",
      "tensor(0.0028)\n",
      "iteration 2457, train loss: 0.003917, validation loss: 0.002756\n",
      "tensor(0.0028)\n",
      "iteration 2458, train loss: 0.004037, validation loss: 0.002758\n",
      "tensor(0.0029)\n",
      "iteration 2459, train loss: 0.003916, validation loss: 0.002918\n",
      "tensor(0.0029)\n",
      "iteration 2460, train loss: 0.003949, validation loss: 0.002884\n",
      "tensor(0.0028)\n",
      "iteration 2461, train loss: 0.003974, validation loss: 0.002786\n",
      "tensor(0.0028)\n",
      "iteration 2462, train loss: 0.004017, validation loss: 0.002767\n",
      "tensor(0.0028)\n",
      "iteration 2463, train loss: 0.00406, validation loss: 0.002825\n",
      "tensor(0.0032)\n",
      "iteration 2464, train loss: 0.004003, validation loss: 0.003182\n",
      "tensor(0.0027)\n",
      "iteration 2465, train loss: 0.004237, validation loss: 0.002735\n",
      "tensor(0.0028)\n",
      "iteration 2466, train loss: 0.003886, validation loss: 0.002848\n",
      "tensor(0.0028)\n",
      "iteration 2467, train loss: 0.004146, validation loss: 0.002847\n",
      "tensor(0.0031)\n",
      "iteration 2468, train loss: 0.003936, validation loss: 0.003084\n",
      "tensor(0.0028)\n",
      "iteration 2469, train loss: 0.004051, validation loss: 0.002828\n",
      "tensor(0.0027)\n",
      "iteration 2470, train loss: 0.003911, validation loss: \u001b[92m0.002721\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2471, train loss: 0.004011, validation loss: 0.00275\n",
      "tensor(0.0028)\n",
      "iteration 2472, train loss: 0.003946, validation loss: 0.002849\n",
      "tensor(0.0028)\n",
      "iteration 2473, train loss: 0.003875, validation loss: 0.002759\n",
      "tensor(0.0028)\n",
      "iteration 2474, train loss: 0.003868, validation loss: 0.002779\n",
      "tensor(0.0027)\n",
      "iteration 2475, train loss: 0.003945, validation loss: \u001b[92m0.002719\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2476, train loss: 0.003923, validation loss: 0.002747\n",
      "tensor(0.0028)\n",
      "iteration 2477, train loss: 0.003878, validation loss: 0.00277\n",
      "tensor(0.0028)\n",
      "iteration 2478, train loss: 0.003913, validation loss: 0.002827\n",
      "tensor(0.0027)\n",
      "iteration 2479, train loss: \u001b[92m0.003855\u001b[0m, validation loss: 0.002747\n",
      "tensor(0.0027)\n",
      "iteration 2480, train loss: 0.003962, validation loss: 0.002733\n",
      "tensor(0.0028)\n",
      "iteration 2481, train loss: 0.003911, validation loss: 0.002839\n",
      "tensor(0.0028)\n",
      "iteration 2482, train loss: 0.003991, validation loss: 0.002805\n",
      "tensor(0.0028)\n",
      "iteration 2483, train loss: 0.003968, validation loss: 0.00276\n",
      "tensor(0.0028)\n",
      "iteration 2484, train loss: \u001b[92m0.003852\u001b[0m, validation loss: 0.002793\n",
      "tensor(0.0028)\n",
      "iteration 2485, train loss: 0.003955, validation loss: 0.002761\n",
      "tensor(0.0029)\n",
      "iteration 2486, train loss: 0.003993, validation loss: 0.002861\n",
      "tensor(0.0029)\n",
      "iteration 2487, train loss: 0.004011, validation loss: 0.00286\n",
      "tensor(0.0027)\n",
      "iteration 2488, train loss: 0.003989, validation loss: 0.002729\n",
      "tensor(0.0027)\n",
      "iteration 2489, train loss: 0.003907, validation loss: 0.002746\n",
      "tensor(0.0027)\n",
      "iteration 2490, train loss: 0.003925, validation loss: \u001b[92m0.002714\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2491, train loss: 0.003883, validation loss: 0.002841\n",
      "tensor(0.0029)\n",
      "iteration 2492, train loss: 0.004044, validation loss: 0.002871\n",
      "tensor(0.0027)\n",
      "iteration 2493, train loss: 0.003925, validation loss: 0.002718\n",
      "tensor(0.0027)\n",
      "iteration 2494, train loss: 0.003909, validation loss: \u001b[92m0.002693\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2495, train loss: \u001b[92m0.003796\u001b[0m, validation loss: 0.002696\n",
      "tensor(0.0027)\n",
      "iteration 2496, train loss: 0.003885, validation loss: 0.002731\n",
      "tensor(0.0028)\n",
      "iteration 2497, train loss: 0.004028, validation loss: 0.002836\n",
      "tensor(0.0027)\n",
      "iteration 2498, train loss: 0.0039, validation loss: 0.002727\n",
      "tensor(0.0027)\n",
      "iteration 2499, train loss: 0.003841, validation loss: 0.002708\n",
      "tensor(0.0027)\n",
      "iteration 2500, train loss: 0.003929, validation loss: 0.002707\n",
      "tensor(0.0028)\n",
      "iteration 2501, train loss: 0.003862, validation loss: 0.002848\n",
      "tensor(0.0028)\n",
      "iteration 2502, train loss: 0.003851, validation loss: 0.002848\n",
      "tensor(0.0027)\n",
      "iteration 2503, train loss: 0.003927, validation loss: 0.002717\n",
      "tensor(0.0027)\n",
      "iteration 2504, train loss: 0.003889, validation loss: 0.002702\n",
      "tensor(0.0027)\n",
      "iteration 2505, train loss: 0.003963, validation loss: 0.002741\n",
      "tensor(0.0028)\n",
      "iteration 2506, train loss: 0.003933, validation loss: 0.002795\n",
      "tensor(0.0028)\n",
      "iteration 2507, train loss: 0.003835, validation loss: 0.002761\n",
      "tensor(0.0027)\n",
      "iteration 2508, train loss: 0.003915, validation loss: \u001b[92m0.002688\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2509, train loss: 0.003882, validation loss: 0.002706\n",
      "tensor(0.0028)\n",
      "iteration 2510, train loss: 0.003949, validation loss: 0.00277\n",
      "tensor(0.0027)\n",
      "iteration 2511, train loss: 0.003859, validation loss: 0.002692\n",
      "tensor(0.0027)\n",
      "iteration 2512, train loss: 0.003866, validation loss: 0.002707\n",
      "tensor(0.0027)\n",
      "iteration 2513, train loss: 0.003918, validation loss: 0.002697\n",
      "tensor(0.0028)\n",
      "iteration 2514, train loss: 0.00389, validation loss: 0.00279\n",
      "tensor(0.0028)\n",
      "iteration 2515, train loss: 0.00394, validation loss: 0.002775\n",
      "tensor(0.0027)\n",
      "iteration 2516, train loss: 0.003818, validation loss: 0.002732\n",
      "tensor(0.0027)\n",
      "iteration 2517, train loss: 0.003874, validation loss: 0.0027\n",
      "tensor(0.0027)\n",
      "iteration 2518, train loss: 0.003875, validation loss: 0.00273\n",
      "tensor(0.0028)\n",
      "iteration 2519, train loss: 0.003837, validation loss: 0.002752\n",
      "tensor(0.0028)\n",
      "iteration 2520, train loss: 0.003887, validation loss: 0.00277\n",
      "tensor(0.0027)\n",
      "iteration 2521, train loss: 0.003838, validation loss: 0.002728\n",
      "tensor(0.0027)\n",
      "iteration 2522, train loss: 0.0039, validation loss: 0.002711\n",
      "tensor(0.0027)\n",
      "iteration 2523, train loss: 0.003957, validation loss: 0.00271\n",
      "tensor(0.0027)\n",
      "iteration 2524, train loss: 0.003897, validation loss: 0.002726\n",
      "tensor(0.0028)\n",
      "iteration 2525, train loss: 0.003874, validation loss: 0.002792\n",
      "tensor(0.0027)\n",
      "iteration 2526, train loss: 0.003876, validation loss: 0.002746\n",
      "tensor(0.0027)\n",
      "iteration 2527, train loss: \u001b[92m0.003794\u001b[0m, validation loss: \u001b[92m0.002674\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2528, train loss: 0.003899, validation loss: 0.002691\n",
      "tensor(0.0028)\n",
      "iteration 2529, train loss: 0.00393, validation loss: 0.002751\n",
      "tensor(0.0029)\n",
      "iteration 2530, train loss: 0.003813, validation loss: 0.002906\n",
      "tensor(0.0027)\n",
      "iteration 2531, train loss: 0.003997, validation loss: 0.002737\n",
      "tensor(0.0027)\n",
      "iteration 2532, train loss: 0.003896, validation loss: 0.00271\n",
      "tensor(0.0027)\n",
      "iteration 2533, train loss: 0.003945, validation loss: 0.00273\n",
      "tensor(0.0029)\n",
      "iteration 2534, train loss: 0.004049, validation loss: 0.002943\n",
      "tensor(0.0029)\n",
      "iteration 2535, train loss: 0.003967, validation loss: 0.002854\n",
      "tensor(0.0027)\n",
      "iteration 2536, train loss: 0.003907, validation loss: 0.002704\n",
      "tensor(0.0027)\n",
      "iteration 2537, train loss: 0.003862, validation loss: 0.002693\n",
      "tensor(0.0028)\n",
      "iteration 2538, train loss: 0.003835, validation loss: 0.002794\n",
      "tensor(0.0028)\n",
      "iteration 2539, train loss: 0.004018, validation loss: 0.002761\n",
      "tensor(0.0028)\n",
      "iteration 2540, train loss: 0.003925, validation loss: 0.002777\n",
      "tensor(0.0027)\n",
      "iteration 2541, train loss: 0.003961, validation loss: 0.002733\n",
      "tensor(0.0028)\n",
      "iteration 2542, train loss: 0.003871, validation loss: 0.002783\n",
      "tensor(0.0027)\n",
      "iteration 2543, train loss: 0.003953, validation loss: 0.002701\n",
      "tensor(0.0027)\n",
      "iteration 2544, train loss: 0.003839, validation loss: \u001b[92m0.002664\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2545, train loss: 0.003954, validation loss: 0.002782\n",
      "tensor(0.0028)\n",
      "iteration 2546, train loss: 0.003868, validation loss: 0.002793\n",
      "tensor(0.0027)\n",
      "iteration 2547, train loss: 0.003897, validation loss: 0.002723\n",
      "tensor(0.0027)\n",
      "iteration 2548, train loss: 0.003917, validation loss: 0.00269\n",
      "tensor(0.0027)\n",
      "iteration 2549, train loss: 0.003825, validation loss: 0.002678\n",
      "tensor(0.0027)\n",
      "iteration 2550, train loss: 0.003839, validation loss: 0.002712\n",
      "tensor(0.0027)\n",
      "iteration 2551, train loss: 0.003873, validation loss: 0.002682\n",
      "tensor(0.0027)\n",
      "iteration 2552, train loss: 0.00385, validation loss: \u001b[92m0.002657\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2553, train loss: 0.00387, validation loss: 0.002679\n",
      "tensor(0.0027)\n",
      "iteration 2554, train loss: 0.003841, validation loss: 0.002699\n",
      "tensor(0.0027)\n",
      "iteration 2555, train loss: 0.003849, validation loss: 0.002728\n",
      "tensor(0.0028)\n",
      "iteration 2556, train loss: 0.00388, validation loss: 0.00276\n",
      "tensor(0.0027)\n",
      "iteration 2557, train loss: 0.003919, validation loss: 0.002713\n",
      "tensor(0.0027)\n",
      "iteration 2558, train loss: 0.003814, validation loss: 0.00268\n",
      "tensor(0.0027)\n",
      "iteration 2559, train loss: 0.003926, validation loss: \u001b[92m0.002655\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2560, train loss: 0.003916, validation loss: 0.002732\n",
      "tensor(0.0028)\n",
      "iteration 2561, train loss: 0.00382, validation loss: 0.002771\n",
      "tensor(0.0027)\n",
      "iteration 2562, train loss: 0.003902, validation loss: 0.002722\n",
      "tensor(0.0027)\n",
      "iteration 2563, train loss: 0.003851, validation loss: 0.002669\n",
      "tensor(0.0027)\n",
      "iteration 2564, train loss: 0.003934, validation loss: 0.002664\n",
      "tensor(0.0028)\n",
      "iteration 2565, train loss: 0.003898, validation loss: 0.002769\n",
      "tensor(0.0028)\n",
      "iteration 2566, train loss: 0.003871, validation loss: 0.002777\n",
      "tensor(0.0026)\n",
      "iteration 2567, train loss: 0.00385, validation loss: \u001b[92m0.002647\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2568, train loss: 0.003858, validation loss: 0.002691\n",
      "tensor(0.0027)\n",
      "iteration 2569, train loss: 0.003893, validation loss: 0.002739\n",
      "tensor(0.0029)\n",
      "iteration 2570, train loss: 0.003902, validation loss: 0.002947\n",
      "tensor(0.0029)\n",
      "iteration 2571, train loss: 0.003884, validation loss: 0.00288\n",
      "tensor(0.0027)\n",
      "iteration 2572, train loss: 0.003886, validation loss: 0.002699\n",
      "tensor(0.0027)\n",
      "iteration 2573, train loss: 0.003881, validation loss: 0.002654\n",
      "tensor(0.0028)\n",
      "iteration 2574, train loss: 0.003927, validation loss: 0.002849\n",
      "tensor(0.0027)\n",
      "iteration 2575, train loss: 0.003961, validation loss: 0.002709\n",
      "tensor(0.0028)\n",
      "iteration 2576, train loss: 0.003796, validation loss: 0.00275\n",
      "tensor(0.0027)\n",
      "iteration 2577, train loss: 0.003882, validation loss: 0.00268\n",
      "tensor(0.0028)\n",
      "iteration 2578, train loss: 0.003829, validation loss: 0.002836\n",
      "tensor(0.0027)\n",
      "iteration 2579, train loss: 0.00394, validation loss: 0.002687\n",
      "tensor(0.0027)\n",
      "iteration 2580, train loss: 0.003894, validation loss: 0.00268\n",
      "tensor(0.0028)\n",
      "iteration 2581, train loss: 0.003865, validation loss: 0.002807\n",
      "tensor(0.0028)\n",
      "iteration 2582, train loss: 0.003865, validation loss: 0.002832\n",
      "tensor(0.0027)\n",
      "iteration 2583, train loss: 0.003904, validation loss: 0.002677\n",
      "tensor(0.0027)\n",
      "iteration 2584, train loss: 0.003841, validation loss: 0.002654\n",
      "tensor(0.0026)\n",
      "iteration 2585, train loss: \u001b[92m0.00379\u001b[0m, validation loss: \u001b[92m0.002623\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2586, train loss: \u001b[92m0.003727\u001b[0m, validation loss: 0.002765\n",
      "tensor(0.0028)\n",
      "iteration 2587, train loss: 0.003798, validation loss: 0.002836\n",
      "tensor(0.0027)\n",
      "iteration 2588, train loss: 0.003909, validation loss: 0.002652\n",
      "tensor(0.0027)\n",
      "iteration 2589, train loss: 0.003778, validation loss: 0.00269\n",
      "tensor(0.0026)\n",
      "iteration 2590, train loss: 0.003964, validation loss: \u001b[92m0.002621\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2591, train loss: 0.003809, validation loss: 0.002785\n",
      "tensor(0.0028)\n",
      "iteration 2592, train loss: 0.003832, validation loss: 0.002815\n",
      "tensor(0.0027)\n",
      "iteration 2593, train loss: 0.003904, validation loss: 0.002666\n",
      "tensor(0.0026)\n",
      "iteration 2594, train loss: 0.003829, validation loss: 0.002642\n",
      "tensor(0.0027)\n",
      "iteration 2595, train loss: 0.003859, validation loss: 0.002717\n",
      "tensor(0.0027)\n",
      "iteration 2596, train loss: 0.003857, validation loss: 0.002747\n",
      "tensor(0.0027)\n",
      "iteration 2597, train loss: 0.003816, validation loss: 0.002699\n",
      "tensor(0.0027)\n",
      "iteration 2598, train loss: 0.003814, validation loss: 0.002713\n",
      "tensor(0.0026)\n",
      "iteration 2599, train loss: 0.00379, validation loss: 0.002636\n",
      "tensor(0.0027)\n",
      "iteration 2600, train loss: 0.00385, validation loss: 0.002681\n",
      "tensor(0.0026)\n",
      "iteration 2601, train loss: 0.00383, validation loss: 0.002635\n",
      "tensor(0.0026)\n",
      "iteration 2602, train loss: 0.003817, validation loss: 0.002647\n",
      "tensor(0.0027)\n",
      "iteration 2603, train loss: 0.003861, validation loss: 0.002694\n",
      "tensor(0.0027)\n",
      "iteration 2604, train loss: 0.003814, validation loss: 0.002715\n",
      "tensor(0.0027)\n",
      "iteration 2605, train loss: 0.003789, validation loss: 0.00269\n",
      "tensor(0.0026)\n",
      "iteration 2606, train loss: 0.003896, validation loss: \u001b[92m0.002618\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2607, train loss: 0.00383, validation loss: 0.002694\n",
      "tensor(0.0028)\n",
      "iteration 2608, train loss: 0.003744, validation loss: 0.002799\n",
      "tensor(0.0027)\n",
      "iteration 2609, train loss: 0.003778, validation loss: 0.002693\n",
      "tensor(0.0026)\n",
      "iteration 2610, train loss: 0.003829, validation loss: 0.002629\n",
      "tensor(0.0026)\n",
      "iteration 2611, train loss: 0.003819, validation loss: 0.002648\n",
      "tensor(0.0027)\n",
      "iteration 2612, train loss: 0.00388, validation loss: 0.002743\n",
      "tensor(0.0027)\n",
      "iteration 2613, train loss: \u001b[92m0.003715\u001b[0m, validation loss: 0.002721\n",
      "tensor(0.0026)\n",
      "iteration 2614, train loss: 0.00382, validation loss: 0.002642\n",
      "tensor(0.0026)\n",
      "iteration 2615, train loss: 0.003762, validation loss: \u001b[92m0.002615\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2616, train loss: 0.003728, validation loss: 0.002661\n",
      "tensor(0.0027)\n",
      "iteration 2617, train loss: 0.003786, validation loss: 0.002718\n",
      "tensor(0.0027)\n",
      "iteration 2618, train loss: 0.003901, validation loss: 0.002658\n",
      "tensor(0.0027)\n",
      "iteration 2619, train loss: 0.003879, validation loss: 0.002673\n",
      "tensor(0.0028)\n",
      "iteration 2620, train loss: 0.003909, validation loss: 0.002788\n",
      "tensor(0.0028)\n",
      "iteration 2621, train loss: 0.003876, validation loss: 0.002824\n",
      "tensor(0.0026)\n",
      "iteration 2622, train loss: 0.00389, validation loss: \u001b[92m0.00261\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2623, train loss: 0.003768, validation loss: 0.002697\n",
      "tensor(0.0027)\n",
      "iteration 2624, train loss: 0.003956, validation loss: 0.002687\n",
      "tensor(0.0029)\n",
      "iteration 2625, train loss: 0.003749, validation loss: 0.002864\n",
      "tensor(0.0027)\n",
      "iteration 2626, train loss: 0.003851, validation loss: 0.002743\n",
      "tensor(0.0027)\n",
      "iteration 2627, train loss: 0.003918, validation loss: 0.002656\n",
      "tensor(0.0027)\n",
      "iteration 2628, train loss: 0.00389, validation loss: 0.002673\n",
      "tensor(0.0028)\n",
      "iteration 2629, train loss: 0.003861, validation loss: 0.002755\n",
      "tensor(0.0029)\n",
      "iteration 2630, train loss: 0.003862, validation loss: 0.002944\n",
      "tensor(0.0026)\n",
      "iteration 2631, train loss: 0.003893, validation loss: 0.002621\n",
      "tensor(0.0026)\n",
      "iteration 2632, train loss: 0.003856, validation loss: 0.002626\n",
      "tensor(0.0027)\n",
      "iteration 2633, train loss: 0.003876, validation loss: 0.002664\n",
      "tensor(0.0029)\n",
      "iteration 2634, train loss: 0.003821, validation loss: 0.002852\n",
      "tensor(0.0027)\n",
      "iteration 2635, train loss: 0.003806, validation loss: 0.002735\n",
      "tensor(0.0026)\n",
      "iteration 2636, train loss: 0.003807, validation loss: 0.002613\n",
      "tensor(0.0026)\n",
      "iteration 2637, train loss: 0.003795, validation loss: 0.002636\n",
      "tensor(0.0027)\n",
      "iteration 2638, train loss: 0.003785, validation loss: 0.002734\n",
      "tensor(0.0027)\n",
      "iteration 2639, train loss: 0.003795, validation loss: 0.002685\n",
      "tensor(0.0026)\n",
      "iteration 2640, train loss: 0.003759, validation loss: 0.002612\n",
      "tensor(0.0026)\n",
      "iteration 2641, train loss: 0.00379, validation loss: \u001b[92m0.002609\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2642, train loss: 0.003763, validation loss: 0.002723\n",
      "tensor(0.0028)\n",
      "iteration 2643, train loss: 0.003789, validation loss: 0.002794\n",
      "tensor(0.0027)\n",
      "iteration 2644, train loss: 0.003826, validation loss: 0.002653\n",
      "tensor(0.0027)\n",
      "iteration 2645, train loss: 0.003752, validation loss: 0.002655\n",
      "tensor(0.0026)\n",
      "iteration 2646, train loss: 0.003835, validation loss: 0.002625\n",
      "tensor(0.0027)\n",
      "iteration 2647, train loss: 0.003852, validation loss: 0.002747\n",
      "tensor(0.0027)\n",
      "iteration 2648, train loss: 0.003888, validation loss: 0.00266\n",
      "tensor(0.0026)\n",
      "iteration 2649, train loss: 0.003773, validation loss: 0.002632\n",
      "tensor(0.0026)\n",
      "iteration 2650, train loss: 0.003778, validation loss: \u001b[92m0.002606\u001b[0m\n",
      "tensor(0.0028)\n",
      "iteration 2651, train loss: 0.003758, validation loss: 0.00275\n",
      "tensor(0.0027)\n",
      "iteration 2652, train loss: 0.003815, validation loss: 0.002735\n",
      "tensor(0.0026)\n",
      "iteration 2653, train loss: 0.003871, validation loss: 0.002622\n",
      "tensor(0.0027)\n",
      "iteration 2654, train loss: 0.003848, validation loss: 0.002652\n",
      "tensor(0.0027)\n",
      "iteration 2655, train loss: 0.00401, validation loss: 0.002727\n",
      "tensor(0.0029)\n",
      "iteration 2656, train loss: 0.00372, validation loss: 0.0029\n",
      "tensor(0.0026)\n",
      "iteration 2657, train loss: 0.003832, validation loss: 0.002633\n",
      "tensor(0.0027)\n",
      "iteration 2658, train loss: \u001b[92m0.003699\u001b[0m, validation loss: 0.002659\n",
      "tensor(0.0026)\n",
      "iteration 2659, train loss: 0.003934, validation loss: 0.002642\n",
      "tensor(0.0027)\n",
      "iteration 2660, train loss: 0.003746, validation loss: 0.002715\n",
      "tensor(0.0027)\n",
      "iteration 2661, train loss: 0.003742, validation loss: 0.002671\n",
      "tensor(0.0026)\n",
      "iteration 2662, train loss: 0.003882, validation loss: 0.002628\n",
      "tensor(0.0027)\n",
      "iteration 2663, train loss: 0.003845, validation loss: 0.00268\n",
      "tensor(0.0028)\n",
      "iteration 2664, train loss: 0.003817, validation loss: 0.002763\n",
      "tensor(0.0027)\n",
      "iteration 2665, train loss: 0.003824, validation loss: 0.002691\n",
      "tensor(0.0026)\n",
      "iteration 2666, train loss: 0.003763, validation loss: 0.002615\n",
      "tensor(0.0026)\n",
      "iteration 2667, train loss: 0.003876, validation loss: 0.002647\n",
      "tensor(0.0028)\n",
      "iteration 2668, train loss: 0.003789, validation loss: 0.002831\n",
      "tensor(0.0027)\n",
      "iteration 2669, train loss: 0.003835, validation loss: 0.00268\n",
      "tensor(0.0026)\n",
      "iteration 2670, train loss: 0.003734, validation loss: \u001b[92m0.002588\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2671, train loss: 0.003806, validation loss: 0.002614\n",
      "tensor(0.0027)\n",
      "iteration 2672, train loss: 0.003778, validation loss: 0.00272\n",
      "tensor(0.0028)\n",
      "iteration 2673, train loss: 0.003719, validation loss: 0.002771\n",
      "tensor(0.0026)\n",
      "iteration 2674, train loss: 0.003702, validation loss: 0.00262\n",
      "tensor(0.0026)\n",
      "iteration 2675, train loss: 0.003829, validation loss: \u001b[92m0.002586\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2676, train loss: 0.003785, validation loss: 0.002665\n",
      "tensor(0.0027)\n",
      "iteration 2677, train loss: 0.003735, validation loss: 0.002683\n",
      "tensor(0.0027)\n",
      "iteration 2678, train loss: \u001b[92m0.003696\u001b[0m, validation loss: 0.002656\n",
      "tensor(0.0026)\n",
      "iteration 2679, train loss: 0.003745, validation loss: 0.00264\n",
      "tensor(0.0026)\n",
      "iteration 2680, train loss: 0.003777, validation loss: 0.002598\n",
      "tensor(0.0026)\n",
      "iteration 2681, train loss: 0.003747, validation loss: 0.002608\n",
      "tensor(0.0026)\n",
      "iteration 2682, train loss: 0.003828, validation loss: 0.002631\n",
      "tensor(0.0027)\n",
      "iteration 2683, train loss: \u001b[92m0.003695\u001b[0m, validation loss: 0.002657\n",
      "tensor(0.0027)\n",
      "iteration 2684, train loss: 0.003768, validation loss: 0.002657\n",
      "tensor(0.0026)\n",
      "iteration 2685, train loss: 0.003783, validation loss: 0.002648\n",
      "tensor(0.0026)\n",
      "iteration 2686, train loss: 0.00383, validation loss: 0.002587\n",
      "tensor(0.0026)\n",
      "iteration 2687, train loss: 0.003828, validation loss: 0.002618\n",
      "tensor(0.0027)\n",
      "iteration 2688, train loss: \u001b[92m0.003664\u001b[0m, validation loss: 0.00266\n",
      "tensor(0.0027)\n",
      "iteration 2689, train loss: 0.003749, validation loss: 0.002674\n",
      "tensor(0.0027)\n",
      "iteration 2690, train loss: 0.003784, validation loss: 0.00274\n",
      "tensor(0.0026)\n",
      "iteration 2691, train loss: 0.00392, validation loss: \u001b[92m0.002573\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2692, train loss: 0.003707, validation loss: 0.002669\n",
      "tensor(0.0027)\n",
      "iteration 2693, train loss: 0.003941, validation loss: 0.002669\n",
      "tensor(0.0028)\n",
      "iteration 2694, train loss: 0.003751, validation loss: 0.002786\n",
      "tensor(0.0026)\n",
      "iteration 2695, train loss: 0.00383, validation loss: 0.002592\n",
      "tensor(0.0026)\n",
      "iteration 2696, train loss: 0.003744, validation loss: 0.002576\n",
      "tensor(0.0026)\n",
      "iteration 2697, train loss: 0.003716, validation loss: 0.00259\n",
      "tensor(0.0027)\n",
      "iteration 2698, train loss: 0.003792, validation loss: 0.002721\n",
      "tensor(0.0027)\n",
      "iteration 2699, train loss: 0.003842, validation loss: 0.002722\n",
      "tensor(0.0026)\n",
      "iteration 2700, train loss: 0.003755, validation loss: 0.002605\n",
      "tensor(0.0026)\n",
      "iteration 2701, train loss: 0.003788, validation loss: 0.002629\n",
      "tensor(0.0026)\n",
      "iteration 2702, train loss: 0.003815, validation loss: 0.00261\n",
      "tensor(0.0027)\n",
      "iteration 2703, train loss: 0.003736, validation loss: 0.002744\n",
      "tensor(0.0026)\n",
      "iteration 2704, train loss: 0.003853, validation loss: 0.002593\n",
      "tensor(0.0026)\n",
      "iteration 2705, train loss: \u001b[92m0.003662\u001b[0m, validation loss: 0.002615\n",
      "tensor(0.0026)\n",
      "iteration 2706, train loss: 0.00386, validation loss: 0.002583\n",
      "tensor(0.0028)\n",
      "iteration 2707, train loss: 0.003788, validation loss: 0.002767\n",
      "tensor(0.0026)\n",
      "iteration 2708, train loss: 0.003849, validation loss: 0.002611\n",
      "tensor(0.0026)\n",
      "iteration 2709, train loss: 0.003796, validation loss: 0.002607\n",
      "tensor(0.0026)\n",
      "iteration 2710, train loss: 0.003818, validation loss: 0.002638\n",
      "tensor(0.0027)\n",
      "iteration 2711, train loss: 0.003687, validation loss: 0.002674\n",
      "tensor(0.0027)\n",
      "iteration 2712, train loss: 0.0038, validation loss: 0.002657\n",
      "tensor(0.0026)\n",
      "iteration 2713, train loss: 0.003685, validation loss: 0.002599\n",
      "tensor(0.0026)\n",
      "iteration 2714, train loss: 0.00371, validation loss: \u001b[92m0.002563\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2715, train loss: 0.003839, validation loss: 0.002629\n",
      "tensor(0.0026)\n",
      "iteration 2716, train loss: 0.003691, validation loss: 0.002617\n",
      "tensor(0.0027)\n",
      "iteration 2717, train loss: 0.003752, validation loss: 0.002656\n",
      "tensor(0.0026)\n",
      "iteration 2718, train loss: 0.003712, validation loss: 0.002634\n",
      "tensor(0.0026)\n",
      "iteration 2719, train loss: 0.003856, validation loss: 0.002634\n",
      "tensor(0.0026)\n",
      "iteration 2720, train loss: 0.003748, validation loss: 0.002581\n",
      "tensor(0.0026)\n",
      "iteration 2721, train loss: 0.003762, validation loss: 0.002564\n",
      "tensor(0.0026)\n",
      "iteration 2722, train loss: \u001b[92m0.003644\u001b[0m, validation loss: 0.002627\n",
      "tensor(0.0027)\n",
      "iteration 2723, train loss: 0.003677, validation loss: 0.002692\n",
      "tensor(0.0027)\n",
      "iteration 2724, train loss: 0.003759, validation loss: 0.002711\n",
      "tensor(0.0026)\n",
      "iteration 2725, train loss: 0.003661, validation loss: 0.002607\n",
      "tensor(0.0026)\n",
      "iteration 2726, train loss: 0.003762, validation loss: \u001b[92m0.00255\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2727, train loss: 0.003684, validation loss: 0.00261\n",
      "tensor(0.0027)\n",
      "iteration 2728, train loss: 0.003793, validation loss: 0.002679\n",
      "tensor(0.0026)\n",
      "iteration 2729, train loss: 0.003791, validation loss: 0.002631\n",
      "tensor(0.0026)\n",
      "iteration 2730, train loss: 0.003718, validation loss: 0.002611\n",
      "tensor(0.0026)\n",
      "iteration 2731, train loss: 0.003899, validation loss: 0.002572\n",
      "tensor(0.0026)\n",
      "iteration 2732, train loss: 0.003791, validation loss: 0.002606\n",
      "tensor(0.0026)\n",
      "iteration 2733, train loss: \u001b[92m0.003618\u001b[0m, validation loss: 0.002649\n",
      "tensor(0.0027)\n",
      "iteration 2734, train loss: 0.003764, validation loss: 0.002695\n",
      "tensor(0.0026)\n",
      "iteration 2735, train loss: 0.003748, validation loss: 0.002599\n",
      "tensor(0.0026)\n",
      "iteration 2736, train loss: 0.003767, validation loss: 0.002556\n",
      "tensor(0.0026)\n",
      "iteration 2737, train loss: 0.003763, validation loss: 0.002573\n",
      "tensor(0.0026)\n",
      "iteration 2738, train loss: 0.003772, validation loss: 0.002609\n",
      "tensor(0.0027)\n",
      "iteration 2739, train loss: 0.003745, validation loss: 0.002702\n",
      "tensor(0.0027)\n",
      "iteration 2740, train loss: 0.003631, validation loss: 0.002724\n",
      "tensor(0.0026)\n",
      "iteration 2741, train loss: 0.003622, validation loss: 0.002614\n",
      "tensor(0.0026)\n",
      "iteration 2742, train loss: 0.0037, validation loss: 0.002555\n",
      "tensor(0.0025)\n",
      "iteration 2743, train loss: 0.003676, validation loss: \u001b[92m0.002548\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2744, train loss: 0.003659, validation loss: 0.002675\n",
      "tensor(0.0027)\n",
      "iteration 2745, train loss: 0.003697, validation loss: 0.002704\n",
      "tensor(0.0026)\n",
      "iteration 2746, train loss: 0.003725, validation loss: 0.002565\n",
      "tensor(0.0026)\n",
      "iteration 2747, train loss: 0.003678, validation loss: 0.002558\n",
      "tensor(0.0026)\n",
      "iteration 2748, train loss: 0.003796, validation loss: 0.002554\n",
      "tensor(0.0027)\n",
      "iteration 2749, train loss: 0.003643, validation loss: 0.002675\n",
      "tensor(0.0026)\n",
      "iteration 2750, train loss: 0.003683, validation loss: 0.002634\n",
      "tensor(0.0026)\n",
      "iteration 2751, train loss: 0.003654, validation loss: 0.002616\n",
      "tensor(0.0026)\n",
      "iteration 2752, train loss: 0.003782, validation loss: 0.002605\n",
      "tensor(0.0026)\n",
      "iteration 2753, train loss: 0.0037, validation loss: 0.002574\n",
      "tensor(0.0026)\n",
      "iteration 2754, train loss: 0.003727, validation loss: 0.002625\n",
      "tensor(0.0027)\n",
      "iteration 2755, train loss: 0.003662, validation loss: 0.002731\n",
      "tensor(0.0026)\n",
      "iteration 2756, train loss: 0.003804, validation loss: 0.002611\n",
      "tensor(0.0026)\n",
      "iteration 2757, train loss: 0.003719, validation loss: 0.002568\n",
      "tensor(0.0026)\n",
      "iteration 2758, train loss: 0.003745, validation loss: 0.002563\n",
      "tensor(0.0026)\n",
      "iteration 2759, train loss: 0.003621, validation loss: 0.00258\n",
      "tensor(0.0027)\n",
      "iteration 2760, train loss: 0.003689, validation loss: 0.002675\n",
      "tensor(0.0026)\n",
      "iteration 2761, train loss: 0.003633, validation loss: 0.00265\n",
      "tensor(0.0026)\n",
      "iteration 2762, train loss: 0.003722, validation loss: 0.002561\n",
      "tensor(0.0026)\n",
      "iteration 2763, train loss: 0.003668, validation loss: 0.002555\n",
      "tensor(0.0025)\n",
      "iteration 2764, train loss: 0.003704, validation loss: \u001b[92m0.002545\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2765, train loss: 0.00378, validation loss: 0.002631\n",
      "tensor(0.0026)\n",
      "iteration 2766, train loss: 0.003774, validation loss: 0.002622\n",
      "tensor(0.0026)\n",
      "iteration 2767, train loss: 0.003698, validation loss: 0.002578\n",
      "tensor(0.0025)\n",
      "iteration 2768, train loss: 0.003623, validation loss: \u001b[92m0.002539\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2769, train loss: 0.003631, validation loss: \u001b[92m0.002531\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2770, train loss: 0.003681, validation loss: 0.002595\n",
      "tensor(0.0026)\n",
      "iteration 2771, train loss: 0.003676, validation loss: 0.002643\n",
      "tensor(0.0026)\n",
      "iteration 2772, train loss: 0.003647, validation loss: 0.002603\n",
      "tensor(0.0026)\n",
      "iteration 2773, train loss: 0.003644, validation loss: 0.002558\n",
      "tensor(0.0025)\n",
      "iteration 2774, train loss: 0.003733, validation loss: 0.00255\n",
      "tensor(0.0025)\n",
      "iteration 2775, train loss: 0.003625, validation loss: 0.002538\n",
      "tensor(0.0026)\n",
      "iteration 2776, train loss: 0.003679, validation loss: 0.002575\n",
      "tensor(0.0026)\n",
      "iteration 2777, train loss: 0.003689, validation loss: 0.002602\n",
      "tensor(0.0026)\n",
      "iteration 2778, train loss: 0.003701, validation loss: 0.002603\n",
      "tensor(0.0025)\n",
      "iteration 2779, train loss: 0.003672, validation loss: \u001b[92m0.002522\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2780, train loss: 0.003664, validation loss: 0.002532\n",
      "tensor(0.0026)\n",
      "iteration 2781, train loss: \u001b[92m0.003611\u001b[0m, validation loss: 0.002634\n",
      "tensor(0.0026)\n",
      "iteration 2782, train loss: 0.003694, validation loss: 0.002562\n",
      "tensor(0.0026)\n",
      "iteration 2783, train loss: 0.003735, validation loss: 0.002645\n",
      "tensor(0.0026)\n",
      "iteration 2784, train loss: 0.003723, validation loss: 0.002621\n",
      "tensor(0.0025)\n",
      "iteration 2785, train loss: 0.003628, validation loss: 0.002547\n",
      "tensor(0.0026)\n",
      "iteration 2786, train loss: 0.003619, validation loss: 0.002572\n",
      "tensor(0.0026)\n",
      "iteration 2787, train loss: 0.003631, validation loss: 0.002559\n",
      "tensor(0.0026)\n",
      "iteration 2788, train loss: 0.00364, validation loss: 0.002566\n",
      "tensor(0.0026)\n",
      "iteration 2789, train loss: \u001b[92m0.003603\u001b[0m, validation loss: 0.002578\n",
      "tensor(0.0025)\n",
      "iteration 2790, train loss: 0.003661, validation loss: 0.002536\n",
      "tensor(0.0027)\n",
      "iteration 2791, train loss: 0.003771, validation loss: 0.002702\n",
      "tensor(0.0027)\n",
      "iteration 2792, train loss: 0.003696, validation loss: 0.002729\n",
      "tensor(0.0026)\n",
      "iteration 2793, train loss: 0.00377, validation loss: 0.00258\n",
      "tensor(0.0026)\n",
      "iteration 2794, train loss: 0.003615, validation loss: 0.002561\n",
      "tensor(0.0025)\n",
      "iteration 2795, train loss: 0.003694, validation loss: 0.002526\n",
      "tensor(0.0027)\n",
      "iteration 2796, train loss: 0.003759, validation loss: 0.002683\n",
      "tensor(0.0027)\n",
      "iteration 2797, train loss: 0.003714, validation loss: 0.002669\n",
      "tensor(0.0026)\n",
      "iteration 2798, train loss: 0.003705, validation loss: 0.002566\n",
      "tensor(0.0025)\n",
      "iteration 2799, train loss: 0.003641, validation loss: 0.002531\n",
      "tensor(0.0027)\n",
      "iteration 2800, train loss: 0.003707, validation loss: 0.002654\n",
      "tensor(0.0026)\n",
      "iteration 2801, train loss: 0.003692, validation loss: 0.002579\n",
      "tensor(0.0026)\n",
      "iteration 2802, train loss: 0.003687, validation loss: 0.002596\n",
      "tensor(0.0026)\n",
      "iteration 2803, train loss: 0.003805, validation loss: 0.002551\n",
      "tensor(0.0028)\n",
      "iteration 2804, train loss: 0.003631, validation loss: 0.002764\n",
      "tensor(0.0025)\n",
      "iteration 2805, train loss: 0.003901, validation loss: \u001b[92m0.002512\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2806, train loss: 0.003632, validation loss: 0.002572\n",
      "tensor(0.0025)\n",
      "iteration 2807, train loss: 0.003784, validation loss: 0.002521\n",
      "tensor(0.0027)\n",
      "iteration 2808, train loss: 0.003681, validation loss: 0.002722\n",
      "tensor(0.0028)\n",
      "iteration 2809, train loss: 0.003782, validation loss: 0.002803\n",
      "tensor(0.0025)\n",
      "iteration 2810, train loss: 0.003828, validation loss: 0.002531\n",
      "tensor(0.0026)\n",
      "iteration 2811, train loss: 0.003609, validation loss: 0.002614\n",
      "tensor(0.0025)\n",
      "iteration 2812, train loss: 0.003756, validation loss: 0.002526\n",
      "tensor(0.0028)\n",
      "iteration 2813, train loss: 0.003712, validation loss: 0.002762\n",
      "tensor(0.0026)\n",
      "iteration 2814, train loss: 0.003812, validation loss: 0.002628\n",
      "tensor(0.0025)\n",
      "iteration 2815, train loss: \u001b[92m0.003593\u001b[0m, validation loss: 0.002523\n",
      "tensor(0.0025)\n",
      "iteration 2816, train loss: 0.003648, validation loss: 0.002549\n",
      "tensor(0.0025)\n",
      "iteration 2817, train loss: 0.003718, validation loss: 0.00253\n",
      "tensor(0.0026)\n",
      "iteration 2818, train loss: 0.003617, validation loss: 0.002613\n",
      "tensor(0.0027)\n",
      "iteration 2819, train loss: 0.003643, validation loss: 0.002651\n",
      "tensor(0.0025)\n",
      "iteration 2820, train loss: 0.003675, validation loss: 0.002537\n",
      "tensor(0.0025)\n",
      "iteration 2821, train loss: 0.003659, validation loss: 0.002541\n",
      "tensor(0.0025)\n",
      "iteration 2822, train loss: 0.003742, validation loss: 0.002549\n",
      "tensor(0.0026)\n",
      "iteration 2823, train loss: 0.003626, validation loss: 0.00255\n",
      "tensor(0.0027)\n",
      "iteration 2824, train loss: \u001b[92m0.003546\u001b[0m, validation loss: 0.002678\n",
      "tensor(0.0026)\n",
      "iteration 2825, train loss: 0.003677, validation loss: 0.002644\n",
      "tensor(0.0025)\n",
      "iteration 2826, train loss: 0.00379, validation loss: 0.002518\n",
      "tensor(0.0025)\n",
      "iteration 2827, train loss: 0.003627, validation loss: \u001b[92m0.002501\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2828, train loss: 0.003752, validation loss: 0.002553\n",
      "tensor(0.0026)\n",
      "iteration 2829, train loss: 0.003614, validation loss: 0.002637\n",
      "tensor(0.0026)\n",
      "iteration 2830, train loss: 0.003619, validation loss: 0.002612\n",
      "tensor(0.0025)\n",
      "iteration 2831, train loss: 0.003644, validation loss: 0.00255\n",
      "tensor(0.0026)\n",
      "iteration 2832, train loss: 0.003682, validation loss: 0.002553\n",
      "tensor(0.0026)\n",
      "iteration 2833, train loss: 0.00379, validation loss: 0.002577\n",
      "tensor(0.0026)\n",
      "iteration 2834, train loss: 0.003654, validation loss: 0.002585\n",
      "tensor(0.0025)\n",
      "iteration 2835, train loss: 0.003729, validation loss: 0.002534\n",
      "tensor(0.0026)\n",
      "iteration 2836, train loss: 0.003597, validation loss: 0.002634\n",
      "tensor(0.0026)\n",
      "iteration 2837, train loss: 0.003677, validation loss: 0.002588\n",
      "tensor(0.0026)\n",
      "iteration 2838, train loss: 0.003702, validation loss: 0.002562\n",
      "tensor(0.0026)\n",
      "iteration 2839, train loss: 0.003673, validation loss: 0.002599\n",
      "tensor(0.0025)\n",
      "iteration 2840, train loss: 0.003693, validation loss: 0.002546\n",
      "tensor(0.0026)\n",
      "iteration 2841, train loss: 0.003723, validation loss: 0.002562\n",
      "tensor(0.0025)\n",
      "iteration 2842, train loss: 0.003625, validation loss: 0.002522\n",
      "tensor(0.0025)\n",
      "iteration 2843, train loss: 0.003603, validation loss: 0.002521\n",
      "tensor(0.0026)\n",
      "iteration 2844, train loss: 0.003578, validation loss: 0.002598\n",
      "tensor(0.0026)\n",
      "iteration 2845, train loss: 0.00363, validation loss: 0.00259\n",
      "tensor(0.0025)\n",
      "iteration 2846, train loss: 0.003676, validation loss: \u001b[92m0.002487\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2847, train loss: 0.003627, validation loss: 0.002516\n",
      "tensor(0.0026)\n",
      "iteration 2848, train loss: 0.003614, validation loss: 0.002554\n",
      "tensor(0.0026)\n",
      "iteration 2849, train loss: 0.003645, validation loss: 0.002647\n",
      "tensor(0.0025)\n",
      "iteration 2850, train loss: 0.003866, validation loss: 0.002549\n",
      "tensor(0.0025)\n",
      "iteration 2851, train loss: 0.003632, validation loss: 0.002532\n",
      "tensor(0.0025)\n",
      "iteration 2852, train loss: 0.003724, validation loss: 0.002504\n",
      "tensor(0.0027)\n",
      "iteration 2853, train loss: 0.003653, validation loss: 0.002749\n",
      "tensor(0.0026)\n",
      "iteration 2854, train loss: 0.003831, validation loss: 0.002583\n",
      "tensor(0.0026)\n",
      "iteration 2855, train loss: 0.003651, validation loss: 0.002598\n",
      "tensor(0.0025)\n",
      "iteration 2856, train loss: 0.003764, validation loss: 0.002528\n",
      "tensor(0.0026)\n",
      "iteration 2857, train loss: 0.003578, validation loss: 0.002644\n",
      "tensor(0.0025)\n",
      "iteration 2858, train loss: 0.003757, validation loss: 0.002537\n",
      "tensor(0.0026)\n",
      "iteration 2859, train loss: 0.003612, validation loss: 0.002551\n",
      "tensor(0.0026)\n",
      "iteration 2860, train loss: 0.00372, validation loss: 0.002612\n",
      "tensor(0.0027)\n",
      "iteration 2861, train loss: 0.003678, validation loss: 0.002657\n",
      "tensor(0.0025)\n",
      "iteration 2862, train loss: 0.003657, validation loss: 0.002534\n",
      "tensor(0.0025)\n",
      "iteration 2863, train loss: 0.003631, validation loss: \u001b[92m0.002478\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2864, train loss: 0.00373, validation loss: 0.002509\n",
      "tensor(0.0025)\n",
      "iteration 2865, train loss: 0.003586, validation loss: 0.002534\n",
      "tensor(0.0026)\n",
      "iteration 2866, train loss: 0.003611, validation loss: 0.002578\n",
      "tensor(0.0026)\n",
      "iteration 2867, train loss: \u001b[92m0.003545\u001b[0m, validation loss: 0.002607\n",
      "tensor(0.0025)\n",
      "iteration 2868, train loss: 0.003739, validation loss: 0.002488\n",
      "tensor(0.0025)\n",
      "iteration 2869, train loss: 0.003723, validation loss: 0.002497\n",
      "tensor(0.0025)\n",
      "iteration 2870, train loss: 0.003643, validation loss: 0.002533\n",
      "tensor(0.0026)\n",
      "iteration 2871, train loss: 0.003577, validation loss: 0.002629\n",
      "tensor(0.0025)\n",
      "iteration 2872, train loss: 0.003683, validation loss: 0.002535\n",
      "tensor(0.0025)\n",
      "iteration 2873, train loss: 0.003616, validation loss: 0.002508\n",
      "tensor(0.0026)\n",
      "iteration 2874, train loss: 0.003692, validation loss: 0.002561\n",
      "tensor(0.0027)\n",
      "iteration 2875, train loss: 0.00368, validation loss: 0.002654\n",
      "tensor(0.0026)\n",
      "iteration 2876, train loss: 0.003602, validation loss: 0.002587\n",
      "tensor(0.0025)\n",
      "iteration 2877, train loss: 0.003687, validation loss: 0.002531\n",
      "tensor(0.0026)\n",
      "iteration 2878, train loss: 0.003789, validation loss: 0.002587\n",
      "tensor(0.0026)\n",
      "iteration 2879, train loss: 0.003718, validation loss: 0.002619\n",
      "tensor(0.0025)\n",
      "iteration 2880, train loss: 0.003711, validation loss: 0.002531\n",
      "tensor(0.0027)\n",
      "iteration 2881, train loss: 0.003657, validation loss: 0.002655\n",
      "tensor(0.0027)\n",
      "iteration 2882, train loss: 0.003657, validation loss: 0.002692\n",
      "tensor(0.0025)\n",
      "iteration 2883, train loss: 0.003752, validation loss: 0.002496\n",
      "tensor(0.0026)\n",
      "iteration 2884, train loss: 0.003586, validation loss: 0.002628\n",
      "tensor(0.0025)\n",
      "iteration 2885, train loss: 0.003681, validation loss: 0.002536\n",
      "tensor(0.0026)\n",
      "iteration 2886, train loss: 0.003691, validation loss: 0.002554\n",
      "tensor(0.0026)\n",
      "iteration 2887, train loss: 0.003717, validation loss: 0.002618\n",
      "tensor(0.0026)\n",
      "iteration 2888, train loss: 0.003687, validation loss: 0.002586\n",
      "tensor(0.0025)\n",
      "iteration 2889, train loss: 0.003626, validation loss: 0.002548\n",
      "tensor(0.0026)\n",
      "iteration 2890, train loss: 0.003634, validation loss: 0.00261\n",
      "tensor(0.0025)\n",
      "iteration 2891, train loss: 0.003668, validation loss: 0.00253\n",
      "tensor(0.0027)\n",
      "iteration 2892, train loss: 0.003636, validation loss: 0.002699\n",
      "tensor(0.0026)\n",
      "iteration 2893, train loss: 0.003758, validation loss: 0.002551\n",
      "tensor(0.0025)\n",
      "iteration 2894, train loss: 0.003603, validation loss: \u001b[92m0.002464\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2895, train loss: 0.003665, validation loss: 0.002586\n",
      "tensor(0.0026)\n",
      "iteration 2896, train loss: 0.003624, validation loss: 0.00262\n",
      "tensor(0.0026)\n",
      "iteration 2897, train loss: 0.003708, validation loss: 0.002602\n",
      "tensor(0.0026)\n",
      "iteration 2898, train loss: 0.003707, validation loss: 0.002575\n",
      "tensor(0.0025)\n",
      "iteration 2899, train loss: 0.003689, validation loss: 0.002542\n",
      "tensor(0.0026)\n",
      "iteration 2900, train loss: 0.003637, validation loss: 0.002569\n",
      "tensor(0.0026)\n",
      "iteration 2901, train loss: 0.003606, validation loss: 0.002636\n",
      "tensor(0.0025)\n",
      "iteration 2902, train loss: 0.00367, validation loss: 0.002525\n",
      "tensor(0.0026)\n",
      "iteration 2903, train loss: 0.003551, validation loss: 0.002588\n",
      "tensor(0.0025)\n",
      "iteration 2904, train loss: 0.003761, validation loss: 0.002536\n",
      "tensor(0.0025)\n",
      "iteration 2905, train loss: 0.003595, validation loss: 0.002529\n",
      "tensor(0.0026)\n",
      "iteration 2906, train loss: 0.003641, validation loss: 0.002604\n",
      "tensor(0.0025)\n",
      "iteration 2907, train loss: 0.00373, validation loss: 0.002546\n",
      "tensor(0.0027)\n",
      "iteration 2908, train loss: 0.003648, validation loss: 0.002678\n",
      "tensor(0.0027)\n",
      "iteration 2909, train loss: 0.003616, validation loss: 0.002678\n",
      "tensor(0.0025)\n",
      "iteration 2910, train loss: 0.003708, validation loss: 0.002491\n",
      "tensor(0.0025)\n",
      "iteration 2911, train loss: 0.003634, validation loss: 0.002529\n",
      "tensor(0.0025)\n",
      "iteration 2912, train loss: 0.003641, validation loss: 0.002504\n",
      "tensor(0.0026)\n",
      "iteration 2913, train loss: 0.003609, validation loss: 0.002643\n",
      "tensor(0.0026)\n",
      "iteration 2914, train loss: 0.003725, validation loss: 0.002584\n",
      "tensor(0.0025)\n",
      "iteration 2915, train loss: 0.003631, validation loss: 0.002522\n",
      "tensor(0.0025)\n",
      "iteration 2916, train loss: 0.003732, validation loss: \u001b[92m0.002457\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2917, train loss: 0.003625, validation loss: 0.002562\n",
      "tensor(0.0025)\n",
      "iteration 2918, train loss: 0.003552, validation loss: 0.002538\n",
      "tensor(0.0025)\n",
      "iteration 2919, train loss: 0.003594, validation loss: 0.002482\n",
      "tensor(0.0025)\n",
      "iteration 2920, train loss: 0.003547, validation loss: 0.002493\n",
      "tensor(0.0025)\n",
      "iteration 2921, train loss: 0.003644, validation loss: 0.002479\n",
      "tensor(0.0026)\n",
      "iteration 2922, train loss: 0.003594, validation loss: 0.002588\n",
      "tensor(0.0025)\n",
      "iteration 2923, train loss: 0.003718, validation loss: 0.002513\n",
      "tensor(0.0025)\n",
      "iteration 2924, train loss: 0.003593, validation loss: 0.002501\n",
      "tensor(0.0025)\n",
      "iteration 2925, train loss: 0.003582, validation loss: \u001b[92m0.002457\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2926, train loss: 0.003582, validation loss: 0.002663\n",
      "tensor(0.0025)\n",
      "iteration 2927, train loss: 0.003706, validation loss: 0.002511\n",
      "tensor(0.0025)\n",
      "iteration 2928, train loss: 0.003666, validation loss: 0.002519\n",
      "tensor(0.0025)\n",
      "iteration 2929, train loss: 0.003738, validation loss: 0.002484\n",
      "tensor(0.0027)\n",
      "iteration 2930, train loss: 0.003676, validation loss: 0.002674\n",
      "tensor(0.0026)\n",
      "iteration 2931, train loss: 0.003694, validation loss: 0.00263\n",
      "tensor(0.0025)\n",
      "iteration 2932, train loss: 0.003671, validation loss: 0.002546\n",
      "tensor(0.0025)\n",
      "iteration 2933, train loss: 0.003743, validation loss: 0.002516\n",
      "tensor(0.0025)\n",
      "iteration 2934, train loss: 0.003669, validation loss: 0.002473\n",
      "tensor(0.0026)\n",
      "iteration 2935, train loss: 0.003701, validation loss: 0.002625\n",
      "tensor(0.0025)\n",
      "iteration 2936, train loss: 0.003697, validation loss: 0.002539\n",
      "tensor(0.0025)\n",
      "iteration 2937, train loss: 0.00362, validation loss: 0.002508\n",
      "tensor(0.0025)\n",
      "iteration 2938, train loss: 0.003683, validation loss: 0.002477\n",
      "tensor(0.0026)\n",
      "iteration 2939, train loss: 0.003596, validation loss: 0.002601\n",
      "tensor(0.0025)\n",
      "iteration 2940, train loss: 0.003636, validation loss: 0.002508\n",
      "tensor(0.0025)\n",
      "iteration 2941, train loss: 0.00356, validation loss: 0.00248\n",
      "tensor(0.0025)\n",
      "iteration 2942, train loss: 0.003669, validation loss: \u001b[92m0.002452\u001b[0m\n",
      "tensor(0.0026)\n",
      "iteration 2943, train loss: \u001b[92m0.003522\u001b[0m, validation loss: 0.002557\n",
      "tensor(0.0025)\n",
      "iteration 2944, train loss: 0.003585, validation loss: 0.002545\n",
      "tensor(0.0025)\n",
      "iteration 2945, train loss: 0.003593, validation loss: 0.002501\n",
      "tensor(0.0025)\n",
      "iteration 2946, train loss: 0.003532, validation loss: 0.002514\n",
      "tensor(0.0025)\n",
      "iteration 2947, train loss: 0.003573, validation loss: 0.002471\n",
      "tensor(0.0025)\n",
      "iteration 2948, train loss: 0.003594, validation loss: 0.002537\n",
      "tensor(0.0025)\n",
      "iteration 2949, train loss: 0.003592, validation loss: 0.002472\n",
      "tensor(0.0025)\n",
      "iteration 2950, train loss: \u001b[92m0.003516\u001b[0m, validation loss: 0.002469\n",
      "tensor(0.0025)\n",
      "iteration 2951, train loss: 0.003571, validation loss: 0.002519\n",
      "tensor(0.0025)\n",
      "iteration 2952, train loss: 0.00364, validation loss: 0.002493\n",
      "tensor(0.0025)\n",
      "iteration 2953, train loss: 0.003557, validation loss: 0.002495\n",
      "tensor(0.0025)\n",
      "iteration 2954, train loss: 0.00358, validation loss: 0.002479\n",
      "tensor(0.0025)\n",
      "iteration 2955, train loss: 0.003533, validation loss: 0.002465\n",
      "tensor(0.0025)\n",
      "iteration 2956, train loss: \u001b[92m0.00351\u001b[0m, validation loss: 0.002469\n",
      "tensor(0.0026)\n",
      "iteration 2957, train loss: 0.003543, validation loss: 0.002571\n",
      "tensor(0.0025)\n",
      "iteration 2958, train loss: 0.003686, validation loss: 0.002488\n",
      "tensor(0.0025)\n",
      "iteration 2959, train loss: 0.003522, validation loss: 0.00247\n",
      "tensor(0.0025)\n",
      "iteration 2960, train loss: 0.003556, validation loss: 0.002472\n",
      "tensor(0.0025)\n",
      "iteration 2961, train loss: \u001b[92m0.00351\u001b[0m, validation loss: 0.002479\n",
      "tensor(0.0025)\n",
      "iteration 2962, train loss: \u001b[92m0.003439\u001b[0m, validation loss: 0.002472\n",
      "tensor(0.0026)\n",
      "iteration 2963, train loss: 0.003577, validation loss: 0.002556\n",
      "tensor(0.0026)\n",
      "iteration 2964, train loss: 0.003549, validation loss: 0.002577\n",
      "tensor(0.0025)\n",
      "iteration 2965, train loss: 0.003588, validation loss: 0.002482\n",
      "tensor(0.0025)\n",
      "iteration 2966, train loss: 0.003521, validation loss: 0.002545\n",
      "tensor(0.0025)\n",
      "iteration 2967, train loss: 0.003779, validation loss: 0.002464\n",
      "tensor(0.0028)\n",
      "iteration 2968, train loss: 0.003642, validation loss: 0.002767\n",
      "tensor(0.0025)\n",
      "iteration 2969, train loss: 0.003756, validation loss: 0.00255\n",
      "tensor(0.0025)\n",
      "iteration 2970, train loss: 0.003557, validation loss: 0.002535\n",
      "tensor(0.0024)\n",
      "iteration 2971, train loss: 0.003714, validation loss: \u001b[92m0.002449\u001b[0m\n",
      "tensor(0.0027)\n",
      "iteration 2972, train loss: 0.003634, validation loss: 0.002665\n",
      "tensor(0.0026)\n",
      "iteration 2973, train loss: 0.003678, validation loss: 0.002642\n",
      "tensor(0.0025)\n",
      "iteration 2974, train loss: 0.003561, validation loss: 0.002496\n",
      "tensor(0.0025)\n",
      "iteration 2975, train loss: 0.003606, validation loss: 0.002477\n",
      "tensor(0.0025)\n",
      "iteration 2976, train loss: 0.003531, validation loss: 0.002462\n",
      "tensor(0.0026)\n",
      "iteration 2977, train loss: 0.003568, validation loss: 0.002638\n",
      "tensor(0.0025)\n",
      "iteration 2978, train loss: 0.003768, validation loss: 0.002466\n",
      "tensor(0.0024)\n",
      "iteration 2979, train loss: 0.003532, validation loss: \u001b[92m0.002446\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2980, train loss: 0.003549, validation loss: 0.002496\n",
      "tensor(0.0025)\n",
      "iteration 2981, train loss: 0.003606, validation loss: 0.002527\n",
      "tensor(0.0025)\n",
      "iteration 2982, train loss: 0.003655, validation loss: 0.002501\n",
      "tensor(0.0024)\n",
      "iteration 2983, train loss: 0.003527, validation loss: \u001b[92m0.002441\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 2984, train loss: 0.003478, validation loss: \u001b[92m0.002435\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 2985, train loss: 0.003593, validation loss: 0.002483\n",
      "tensor(0.0025)\n",
      "iteration 2986, train loss: 0.003541, validation loss: 0.00251\n",
      "tensor(0.0026)\n",
      "iteration 2987, train loss: 0.003486, validation loss: 0.002564\n",
      "tensor(0.0024)\n",
      "iteration 2988, train loss: 0.003602, validation loss: 0.002441\n",
      "tensor(0.0024)\n",
      "iteration 2989, train loss: \u001b[92m0.003435\u001b[0m, validation loss: 0.002447\n",
      "tensor(0.0025)\n",
      "iteration 2990, train loss: 0.003482, validation loss: 0.002471\n",
      "tensor(0.0025)\n",
      "iteration 2991, train loss: 0.00357, validation loss: 0.002509\n",
      "tensor(0.0025)\n",
      "iteration 2992, train loss: 0.003547, validation loss: 0.002486\n",
      "tensor(0.0024)\n",
      "iteration 2993, train loss: 0.003544, validation loss: \u001b[92m0.002418\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 2994, train loss: 0.003621, validation loss: 0.002436\n",
      "tensor(0.0026)\n",
      "iteration 2995, train loss: 0.003515, validation loss: 0.002609\n",
      "tensor(0.0025)\n",
      "iteration 2996, train loss: 0.003603, validation loss: 0.002522\n",
      "tensor(0.0025)\n",
      "iteration 2997, train loss: 0.003551, validation loss: 0.00245\n",
      "tensor(0.0024)\n",
      "iteration 2998, train loss: 0.003649, validation loss: 0.002423\n",
      "tensor(0.0026)\n",
      "iteration 2999, train loss: 0.003469, validation loss: 0.002558\n",
      "tensor(0.0025)\n",
      "iteration 3000, train loss: 0.003655, validation loss: 0.002522\n",
      "tensor(0.0025)\n",
      "iteration 3001, train loss: 0.003561, validation loss: 0.002483\n",
      "tensor(0.0025)\n",
      "iteration 3002, train loss: 0.003511, validation loss: 0.002463\n",
      "tensor(0.0024)\n",
      "iteration 3003, train loss: 0.003535, validation loss: 0.00245\n",
      "tensor(0.0025)\n",
      "iteration 3004, train loss: 0.003479, validation loss: 0.00245\n",
      "tensor(0.0025)\n",
      "iteration 3005, train loss: 0.00359, validation loss: 0.002466\n",
      "tensor(0.0025)\n",
      "iteration 3006, train loss: 0.003494, validation loss: 0.002504\n",
      "tensor(0.0025)\n",
      "iteration 3007, train loss: 0.003473, validation loss: 0.002458\n",
      "tensor(0.0024)\n",
      "iteration 3008, train loss: 0.003539, validation loss: 0.002437\n",
      "tensor(0.0025)\n",
      "iteration 3009, train loss: 0.003499, validation loss: 0.00246\n",
      "tensor(0.0025)\n",
      "iteration 3010, train loss: 0.003603, validation loss: 0.002487\n",
      "tensor(0.0025)\n",
      "iteration 3011, train loss: 0.00347, validation loss: 0.002489\n",
      "tensor(0.0025)\n",
      "iteration 3012, train loss: 0.003583, validation loss: 0.002469\n",
      "tensor(0.0024)\n",
      "iteration 3013, train loss: 0.003587, validation loss: 0.002438\n",
      "tensor(0.0025)\n",
      "iteration 3014, train loss: 0.003538, validation loss: 0.002532\n",
      "tensor(0.0025)\n",
      "iteration 3015, train loss: 0.003587, validation loss: 0.002526\n",
      "tensor(0.0025)\n",
      "iteration 3016, train loss: 0.003637, validation loss: 0.002531\n",
      "tensor(0.0026)\n",
      "iteration 3017, train loss: 0.003435, validation loss: 0.002647\n",
      "tensor(0.0024)\n",
      "iteration 3018, train loss: 0.003671, validation loss: \u001b[92m0.002417\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 3019, train loss: 0.003519, validation loss: 0.002529\n",
      "tensor(0.0025)\n",
      "iteration 3020, train loss: 0.003685, validation loss: 0.002533\n",
      "tensor(0.0026)\n",
      "iteration 3021, train loss: 0.003719, validation loss: 0.002608\n",
      "tensor(0.0025)\n",
      "iteration 3022, train loss: 0.003613, validation loss: 0.002538\n",
      "tensor(0.0025)\n",
      "iteration 3023, train loss: 0.003636, validation loss: 0.002458\n",
      "tensor(0.0025)\n",
      "iteration 3024, train loss: 0.003577, validation loss: 0.002488\n",
      "tensor(0.0027)\n",
      "iteration 3025, train loss: 0.00355, validation loss: 0.002668\n",
      "tensor(0.0025)\n",
      "iteration 3026, train loss: 0.003635, validation loss: 0.002482\n",
      "tensor(0.0025)\n",
      "iteration 3027, train loss: 0.003504, validation loss: 0.002455\n",
      "tensor(0.0025)\n",
      "iteration 3028, train loss: 0.00368, validation loss: 0.002471\n",
      "tensor(0.0025)\n",
      "iteration 3029, train loss: \u001b[92m0.003418\u001b[0m, validation loss: 0.002544\n",
      "tensor(0.0024)\n",
      "iteration 3030, train loss: 0.003644, validation loss: 0.002442\n",
      "tensor(0.0024)\n",
      "iteration 3031, train loss: 0.003601, validation loss: 0.002435\n",
      "tensor(0.0024)\n",
      "iteration 3032, train loss: 0.003498, validation loss: 0.002446\n",
      "tensor(0.0025)\n",
      "iteration 3033, train loss: 0.003494, validation loss: 0.002529\n",
      "tensor(0.0025)\n",
      "iteration 3034, train loss: 0.00353, validation loss: 0.002469\n",
      "tensor(0.0024)\n",
      "iteration 3035, train loss: 0.003591, validation loss: 0.002419\n",
      "tensor(0.0024)\n",
      "iteration 3036, train loss: 0.003573, validation loss: 0.002437\n",
      "tensor(0.0024)\n",
      "iteration 3037, train loss: 0.003513, validation loss: 0.002433\n",
      "tensor(0.0026)\n",
      "iteration 3038, train loss: 0.003545, validation loss: 0.002596\n",
      "tensor(0.0026)\n",
      "iteration 3039, train loss: 0.00364, validation loss: 0.002552\n",
      "tensor(0.0025)\n",
      "iteration 3040, train loss: 0.003593, validation loss: 0.002485\n",
      "tensor(0.0025)\n",
      "iteration 3041, train loss: 0.003602, validation loss: 0.002483\n",
      "tensor(0.0025)\n",
      "iteration 3042, train loss: 0.003754, validation loss: 0.002544\n",
      "tensor(0.0025)\n",
      "iteration 3043, train loss: 0.003506, validation loss: 0.002532\n",
      "tensor(0.0024)\n",
      "iteration 3044, train loss: 0.003679, validation loss: 0.002449\n",
      "tensor(0.0025)\n",
      "iteration 3045, train loss: 0.003441, validation loss: 0.002495\n",
      "tensor(0.0025)\n",
      "iteration 3046, train loss: 0.003542, validation loss: 0.00248\n",
      "tensor(0.0025)\n",
      "iteration 3047, train loss: 0.003508, validation loss: 0.002542\n",
      "tensor(0.0024)\n",
      "iteration 3048, train loss: 0.003545, validation loss: 0.002448\n",
      "tensor(0.0025)\n",
      "iteration 3049, train loss: 0.003538, validation loss: 0.002478\n",
      "tensor(0.0024)\n",
      "iteration 3050, train loss: 0.003514, validation loss: 0.002429\n",
      "tensor(0.0026)\n",
      "iteration 3051, train loss: 0.003505, validation loss: 0.002555\n",
      "tensor(0.0025)\n",
      "iteration 3052, train loss: 0.003532, validation loss: 0.002503\n",
      "tensor(0.0024)\n",
      "iteration 3053, train loss: 0.003602, validation loss: \u001b[92m0.002398\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3054, train loss: 0.003487, validation loss: 0.0024\n",
      "tensor(0.0025)\n",
      "iteration 3055, train loss: 0.003433, validation loss: 0.002462\n",
      "tensor(0.0025)\n",
      "iteration 3056, train loss: 0.003468, validation loss: 0.002504\n",
      "tensor(0.0024)\n",
      "iteration 3057, train loss: 0.00345, validation loss: 0.002434\n",
      "tensor(0.0024)\n",
      "iteration 3058, train loss: 0.003501, validation loss: 0.002401\n",
      "tensor(0.0024)\n",
      "iteration 3059, train loss: 0.003546, validation loss: 0.002402\n",
      "tensor(0.0025)\n",
      "iteration 3060, train loss: 0.003623, validation loss: 0.002526\n",
      "tensor(0.0025)\n",
      "iteration 3061, train loss: 0.003561, validation loss: 0.002542\n",
      "tensor(0.0025)\n",
      "iteration 3062, train loss: 0.003502, validation loss: 0.002453\n",
      "tensor(0.0024)\n",
      "iteration 3063, train loss: 0.003507, validation loss: 0.002414\n",
      "tensor(0.0025)\n",
      "iteration 3064, train loss: 0.003467, validation loss: 0.002511\n",
      "tensor(0.0025)\n",
      "iteration 3065, train loss: 0.003453, validation loss: 0.002458\n",
      "tensor(0.0024)\n",
      "iteration 3066, train loss: 0.003508, validation loss: 0.002409\n",
      "tensor(0.0025)\n",
      "iteration 3067, train loss: 0.003522, validation loss: 0.002481\n",
      "tensor(0.0025)\n",
      "iteration 3068, train loss: 0.003578, validation loss: 0.002499\n",
      "tensor(0.0025)\n",
      "iteration 3069, train loss: 0.003515, validation loss: 0.002534\n",
      "tensor(0.0024)\n",
      "iteration 3070, train loss: 0.003532, validation loss: 0.002407\n",
      "tensor(0.0024)\n",
      "iteration 3071, train loss: 0.003482, validation loss: 0.002423\n",
      "tensor(0.0025)\n",
      "iteration 3072, train loss: 0.003659, validation loss: 0.002455\n",
      "tensor(0.0026)\n",
      "iteration 3073, train loss: 0.003565, validation loss: 0.002612\n",
      "tensor(0.0025)\n",
      "iteration 3074, train loss: 0.003607, validation loss: 0.002451\n",
      "tensor(0.0024)\n",
      "iteration 3075, train loss: 0.003512, validation loss: 0.002408\n",
      "tensor(0.0024)\n",
      "iteration 3076, train loss: 0.003457, validation loss: 0.002404\n",
      "tensor(0.0025)\n",
      "iteration 3077, train loss: 0.003449, validation loss: 0.002474\n",
      "tensor(0.0025)\n",
      "iteration 3078, train loss: 0.003555, validation loss: 0.002457\n",
      "tensor(0.0024)\n",
      "iteration 3079, train loss: \u001b[92m0.003416\u001b[0m, validation loss: 0.002426\n",
      "tensor(0.0024)\n",
      "iteration 3080, train loss: 0.00351, validation loss: 0.0024\n",
      "tensor(0.0024)\n",
      "iteration 3081, train loss: 0.003627, validation loss: 0.002448\n",
      "tensor(0.0024)\n",
      "iteration 3082, train loss: 0.003519, validation loss: 0.002441\n",
      "tensor(0.0024)\n",
      "iteration 3083, train loss: \u001b[92m0.003399\u001b[0m, validation loss: 0.002442\n",
      "tensor(0.0024)\n",
      "iteration 3084, train loss: 0.003582, validation loss: 0.002424\n",
      "tensor(0.0024)\n",
      "iteration 3085, train loss: 0.003537, validation loss: 0.002433\n",
      "tensor(0.0024)\n",
      "iteration 3086, train loss: 0.003483, validation loss: 0.002408\n",
      "tensor(0.0024)\n",
      "iteration 3087, train loss: 0.003436, validation loss: 0.002402\n",
      "tensor(0.0024)\n",
      "iteration 3088, train loss: 0.00353, validation loss: \u001b[92m0.002394\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3089, train loss: 0.003471, validation loss: \u001b[92m0.002384\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3090, train loss: 0.003434, validation loss: 0.002393\n",
      "tensor(0.0024)\n",
      "iteration 3091, train loss: 0.003511, validation loss: 0.002402\n",
      "tensor(0.0025)\n",
      "iteration 3092, train loss: 0.003439, validation loss: 0.002455\n",
      "tensor(0.0024)\n",
      "iteration 3093, train loss: 0.003427, validation loss: 0.002438\n",
      "tensor(0.0024)\n",
      "iteration 3094, train loss: 0.003443, validation loss: 0.002439\n",
      "tensor(0.0024)\n",
      "iteration 3095, train loss: 0.003477, validation loss: 0.002445\n",
      "tensor(0.0024)\n",
      "iteration 3096, train loss: 0.003492, validation loss: 0.002418\n",
      "tensor(0.0024)\n",
      "iteration 3097, train loss: 0.003548, validation loss: 0.002424\n",
      "tensor(0.0024)\n",
      "iteration 3098, train loss: 0.003507, validation loss: \u001b[92m0.002379\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3099, train loss: 0.003608, validation loss: 0.002441\n",
      "tensor(0.0025)\n",
      "iteration 3100, train loss: 0.003524, validation loss: 0.002494\n",
      "tensor(0.0025)\n",
      "iteration 3101, train loss: 0.003445, validation loss: 0.002461\n",
      "tensor(0.0025)\n",
      "iteration 3102, train loss: 0.00344, validation loss: 0.002544\n",
      "tensor(0.0024)\n",
      "iteration 3103, train loss: 0.003571, validation loss: 0.002444\n",
      "tensor(0.0024)\n",
      "iteration 3104, train loss: \u001b[92m0.003375\u001b[0m, validation loss: 0.002422\n",
      "tensor(0.0024)\n",
      "iteration 3105, train loss: 0.003511, validation loss: 0.002422\n",
      "tensor(0.0025)\n",
      "iteration 3106, train loss: 0.003566, validation loss: 0.002474\n",
      "tensor(0.0025)\n",
      "iteration 3107, train loss: 0.0035, validation loss: 0.00248\n",
      "tensor(0.0024)\n",
      "iteration 3108, train loss: 0.003512, validation loss: 0.00239\n",
      "tensor(0.0024)\n",
      "iteration 3109, train loss: 0.003509, validation loss: 0.002386\n",
      "tensor(0.0024)\n",
      "iteration 3110, train loss: 0.00352, validation loss: 0.002407\n",
      "tensor(0.0025)\n",
      "iteration 3111, train loss: 0.003408, validation loss: 0.002459\n",
      "tensor(0.0025)\n",
      "iteration 3112, train loss: 0.003443, validation loss: 0.002462\n",
      "tensor(0.0024)\n",
      "iteration 3113, train loss: 0.003492, validation loss: 0.002422\n",
      "tensor(0.0024)\n",
      "iteration 3114, train loss: 0.003525, validation loss: 0.002399\n",
      "tensor(0.0025)\n",
      "iteration 3115, train loss: 0.003536, validation loss: 0.002503\n",
      "tensor(0.0025)\n",
      "iteration 3116, train loss: 0.003519, validation loss: 0.002529\n",
      "tensor(0.0024)\n",
      "iteration 3117, train loss: 0.003519, validation loss: 0.002395\n",
      "tensor(0.0024)\n",
      "iteration 3118, train loss: 0.003487, validation loss: 0.002393\n",
      "tensor(0.0024)\n",
      "iteration 3119, train loss: 0.003618, validation loss: 0.002449\n",
      "tensor(0.0026)\n",
      "iteration 3120, train loss: 0.003401, validation loss: 0.002605\n",
      "tensor(0.0024)\n",
      "iteration 3121, train loss: 0.003678, validation loss: 0.002389\n",
      "tensor(0.0025)\n",
      "iteration 3122, train loss: 0.003405, validation loss: 0.002464\n",
      "tensor(0.0024)\n",
      "iteration 3123, train loss: 0.003657, validation loss: 0.002441\n",
      "tensor(0.0025)\n",
      "iteration 3124, train loss: 0.003473, validation loss: 0.002498\n",
      "tensor(0.0024)\n",
      "iteration 3125, train loss: 0.003567, validation loss: 0.002407\n",
      "tensor(0.0024)\n",
      "iteration 3126, train loss: \u001b[92m0.003328\u001b[0m, validation loss: 0.0024\n",
      "tensor(0.0024)\n",
      "iteration 3127, train loss: 0.00349, validation loss: 0.002397\n",
      "tensor(0.0025)\n",
      "iteration 3128, train loss: 0.003468, validation loss: 0.002452\n",
      "tensor(0.0024)\n",
      "iteration 3129, train loss: 0.003477, validation loss: 0.002395\n",
      "tensor(0.0024)\n",
      "iteration 3130, train loss: 0.003511, validation loss: 0.002382\n",
      "tensor(0.0024)\n",
      "iteration 3131, train loss: 0.003493, validation loss: 0.002412\n",
      "tensor(0.0024)\n",
      "iteration 3132, train loss: 0.003411, validation loss: 0.002381\n",
      "tensor(0.0024)\n",
      "iteration 3133, train loss: 0.003419, validation loss: 0.002407\n",
      "tensor(0.0024)\n",
      "iteration 3134, train loss: 0.0035, validation loss: 0.002402\n",
      "tensor(0.0024)\n",
      "iteration 3135, train loss: 0.003468, validation loss: 0.00243\n",
      "tensor(0.0025)\n",
      "iteration 3136, train loss: 0.003454, validation loss: 0.002491\n",
      "tensor(0.0024)\n",
      "iteration 3137, train loss: 0.00361, validation loss: 0.002421\n",
      "tensor(0.0024)\n",
      "iteration 3138, train loss: 0.00349, validation loss: 0.002436\n",
      "tensor(0.0024)\n",
      "iteration 3139, train loss: 0.003489, validation loss: 0.002425\n",
      "tensor(0.0024)\n",
      "iteration 3140, train loss: 0.003472, validation loss: 0.002423\n",
      "tensor(0.0024)\n",
      "iteration 3141, train loss: 0.003533, validation loss: 0.002402\n",
      "tensor(0.0024)\n",
      "iteration 3142, train loss: 0.003401, validation loss: 0.002402\n",
      "tensor(0.0024)\n",
      "iteration 3143, train loss: 0.003459, validation loss: 0.002435\n",
      "tensor(0.0024)\n",
      "iteration 3144, train loss: 0.003497, validation loss: 0.002387\n",
      "tensor(0.0024)\n",
      "iteration 3145, train loss: 0.003437, validation loss: \u001b[92m0.002361\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3146, train loss: 0.003397, validation loss: 0.002393\n",
      "tensor(0.0025)\n",
      "iteration 3147, train loss: 0.003418, validation loss: 0.002515\n",
      "tensor(0.0025)\n",
      "iteration 3148, train loss: 0.003518, validation loss: 0.002518\n",
      "tensor(0.0024)\n",
      "iteration 3149, train loss: 0.003443, validation loss: 0.002405\n",
      "tensor(0.0024)\n",
      "iteration 3150, train loss: 0.003406, validation loss: 0.002377\n",
      "tensor(0.0024)\n",
      "iteration 3151, train loss: 0.003553, validation loss: 0.002366\n",
      "tensor(0.0025)\n",
      "iteration 3152, train loss: 0.003431, validation loss: 0.00248\n",
      "tensor(0.0025)\n",
      "iteration 3153, train loss: 0.00341, validation loss: 0.002482\n",
      "tensor(0.0024)\n",
      "iteration 3154, train loss: 0.003558, validation loss: 0.00242\n",
      "tensor(0.0024)\n",
      "iteration 3155, train loss: 0.003424, validation loss: 0.002438\n",
      "tensor(0.0025)\n",
      "iteration 3156, train loss: 0.003551, validation loss: 0.00249\n",
      "tensor(0.0025)\n",
      "iteration 3157, train loss: 0.003505, validation loss: 0.002452\n",
      "tensor(0.0024)\n",
      "iteration 3158, train loss: 0.003451, validation loss: 0.002419\n",
      "tensor(0.0024)\n",
      "iteration 3159, train loss: 0.003529, validation loss: 0.002425\n",
      "tensor(0.0024)\n",
      "iteration 3160, train loss: 0.003503, validation loss: 0.002371\n",
      "tensor(0.0024)\n",
      "iteration 3161, train loss: 0.003439, validation loss: 0.002384\n",
      "tensor(0.0024)\n",
      "iteration 3162, train loss: 0.003558, validation loss: 0.002369\n",
      "tensor(0.0024)\n",
      "iteration 3163, train loss: 0.003448, validation loss: 0.002439\n",
      "tensor(0.0026)\n",
      "iteration 3164, train loss: 0.00347, validation loss: 0.00261\n",
      "tensor(0.0024)\n",
      "iteration 3165, train loss: 0.003548, validation loss: 0.002422\n",
      "tensor(0.0023)\n",
      "iteration 3166, train loss: 0.003426, validation loss: \u001b[92m0.00235\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3167, train loss: 0.00354, validation loss: 0.002372\n",
      "tensor(0.0023)\n",
      "iteration 3168, train loss: 0.00341, validation loss: \u001b[92m0.002346\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 3169, train loss: 0.003399, validation loss: 0.002455\n",
      "tensor(0.0025)\n",
      "iteration 3170, train loss: 0.003423, validation loss: 0.002504\n",
      "tensor(0.0024)\n",
      "iteration 3171, train loss: 0.003486, validation loss: 0.002394\n",
      "tensor(0.0024)\n",
      "iteration 3172, train loss: 0.00344, validation loss: 0.002387\n",
      "tensor(0.0024)\n",
      "iteration 3173, train loss: 0.003522, validation loss: 0.002373\n",
      "tensor(0.0024)\n",
      "iteration 3174, train loss: 0.003486, validation loss: 0.002441\n",
      "tensor(0.0025)\n",
      "iteration 3175, train loss: 0.003511, validation loss: 0.002543\n",
      "tensor(0.0024)\n",
      "iteration 3176, train loss: 0.003509, validation loss: 0.002394\n",
      "tensor(0.0023)\n",
      "iteration 3177, train loss: 0.00347, validation loss: \u001b[92m0.00234\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3178, train loss: 0.003509, validation loss: 0.002353\n",
      "tensor(0.0025)\n",
      "iteration 3179, train loss: 0.003467, validation loss: 0.00245\n",
      "tensor(0.0025)\n",
      "iteration 3180, train loss: 0.003387, validation loss: 0.002529\n",
      "tensor(0.0024)\n",
      "iteration 3181, train loss: 0.00344, validation loss: 0.00242\n",
      "tensor(0.0024)\n",
      "iteration 3182, train loss: 0.003417, validation loss: 0.002394\n",
      "tensor(0.0024)\n",
      "iteration 3183, train loss: 0.003544, validation loss: 0.00238\n",
      "tensor(0.0025)\n",
      "iteration 3184, train loss: 0.003465, validation loss: 0.002457\n",
      "tensor(0.0024)\n",
      "iteration 3185, train loss: 0.00337, validation loss: 0.002442\n",
      "tensor(0.0024)\n",
      "iteration 3186, train loss: 0.003447, validation loss: 0.002371\n",
      "tensor(0.0023)\n",
      "iteration 3187, train loss: 0.003437, validation loss: \u001b[92m0.002333\u001b[0m\n",
      "tensor(0.0025)\n",
      "iteration 3188, train loss: 0.003532, validation loss: 0.002488\n",
      "tensor(0.0025)\n",
      "iteration 3189, train loss: 0.003469, validation loss: 0.002483\n",
      "tensor(0.0024)\n",
      "iteration 3190, train loss: 0.003528, validation loss: 0.002435\n",
      "tensor(0.0024)\n",
      "iteration 3191, train loss: 0.003468, validation loss: 0.002435\n",
      "tensor(0.0024)\n",
      "iteration 3192, train loss: 0.003578, validation loss: 0.002411\n",
      "tensor(0.0025)\n",
      "iteration 3193, train loss: 0.003501, validation loss: 0.002531\n",
      "tensor(0.0024)\n",
      "iteration 3194, train loss: 0.003563, validation loss: 0.002362\n",
      "tensor(0.0024)\n",
      "iteration 3195, train loss: 0.003483, validation loss: 0.002437\n",
      "tensor(0.0024)\n",
      "iteration 3196, train loss: 0.003512, validation loss: 0.002444\n",
      "tensor(0.0024)\n",
      "iteration 3197, train loss: 0.003397, validation loss: 0.002371\n",
      "tensor(0.0023)\n",
      "iteration 3198, train loss: 0.003428, validation loss: \u001b[92m0.002327\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3199, train loss: 0.003446, validation loss: 0.002334\n",
      "tensor(0.0025)\n",
      "iteration 3200, train loss: 0.003465, validation loss: 0.00248\n",
      "tensor(0.0026)\n",
      "iteration 3201, train loss: 0.003532, validation loss: 0.002568\n",
      "tensor(0.0024)\n",
      "iteration 3202, train loss: 0.003466, validation loss: 0.002428\n",
      "tensor(0.0023)\n",
      "iteration 3203, train loss: 0.003512, validation loss: 0.002334\n",
      "tensor(0.0024)\n",
      "iteration 3204, train loss: 0.003426, validation loss: 0.002418\n",
      "tensor(0.0024)\n",
      "iteration 3205, train loss: 0.00343, validation loss: 0.002428\n",
      "tensor(0.0025)\n",
      "iteration 3206, train loss: 0.003494, validation loss: 0.002505\n",
      "tensor(0.0025)\n",
      "iteration 3207, train loss: 0.003465, validation loss: 0.002457\n",
      "tensor(0.0023)\n",
      "iteration 3208, train loss: 0.003524, validation loss: 0.00233\n",
      "tensor(0.0024)\n",
      "iteration 3209, train loss: 0.003434, validation loss: 0.002408\n",
      "tensor(0.0024)\n",
      "iteration 3210, train loss: 0.003521, validation loss: 0.002363\n",
      "tensor(0.0025)\n",
      "iteration 3211, train loss: 0.003433, validation loss: 0.002504\n",
      "tensor(0.0025)\n",
      "iteration 3212, train loss: 0.003404, validation loss: 0.002534\n",
      "tensor(0.0024)\n",
      "iteration 3213, train loss: 0.003457, validation loss: 0.002388\n",
      "tensor(0.0023)\n",
      "iteration 3214, train loss: 0.003459, validation loss: 0.002346\n",
      "tensor(0.0023)\n",
      "iteration 3215, train loss: 0.003527, validation loss: 0.002348\n",
      "tensor(0.0024)\n",
      "iteration 3216, train loss: 0.003339, validation loss: 0.00236\n",
      "tensor(0.0024)\n",
      "iteration 3217, train loss: 0.003454, validation loss: 0.002439\n",
      "tensor(0.0024)\n",
      "iteration 3218, train loss: 0.003409, validation loss: 0.002399\n",
      "tensor(0.0024)\n",
      "iteration 3219, train loss: 0.003399, validation loss: 0.002378\n",
      "tensor(0.0023)\n",
      "iteration 3220, train loss: 0.003419, validation loss: 0.002338\n",
      "tensor(0.0023)\n",
      "iteration 3221, train loss: 0.003468, validation loss: \u001b[92m0.002326\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3222, train loss: 0.00345, validation loss: 0.002385\n",
      "tensor(0.0024)\n",
      "iteration 3223, train loss: 0.00345, validation loss: 0.002393\n",
      "tensor(0.0023)\n",
      "iteration 3224, train loss: 0.003342, validation loss: 0.002345\n",
      "tensor(0.0023)\n",
      "iteration 3225, train loss: 0.003403, validation loss: 0.002343\n",
      "tensor(0.0024)\n",
      "iteration 3226, train loss: 0.003412, validation loss: 0.002393\n",
      "tensor(0.0024)\n",
      "iteration 3227, train loss: 0.003446, validation loss: 0.002357\n",
      "tensor(0.0024)\n",
      "iteration 3228, train loss: 0.003343, validation loss: 0.002355\n",
      "tensor(0.0024)\n",
      "iteration 3229, train loss: 0.003456, validation loss: 0.002351\n",
      "tensor(0.0024)\n",
      "iteration 3230, train loss: 0.003435, validation loss: 0.002383\n",
      "tensor(0.0024)\n",
      "iteration 3231, train loss: 0.003411, validation loss: 0.002363\n",
      "tensor(0.0024)\n",
      "iteration 3232, train loss: 0.003355, validation loss: 0.002369\n",
      "tensor(0.0024)\n",
      "iteration 3233, train loss: 0.003408, validation loss: 0.002423\n",
      "tensor(0.0023)\n",
      "iteration 3234, train loss: 0.003449, validation loss: \u001b[92m0.002322\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3235, train loss: 0.003574, validation loss: 0.002376\n",
      "tensor(0.0024)\n",
      "iteration 3236, train loss: 0.003396, validation loss: 0.002393\n",
      "tensor(0.0024)\n",
      "iteration 3237, train loss: 0.003374, validation loss: 0.00239\n",
      "tensor(0.0024)\n",
      "iteration 3238, train loss: 0.003455, validation loss: 0.002389\n",
      "tensor(0.0023)\n",
      "iteration 3239, train loss: 0.003406, validation loss: \u001b[92m0.00232\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3240, train loss: 0.003356, validation loss: 0.002357\n",
      "tensor(0.0024)\n",
      "iteration 3241, train loss: 0.003347, validation loss: 0.002447\n",
      "tensor(0.0024)\n",
      "iteration 3242, train loss: 0.003423, validation loss: 0.002397\n",
      "tensor(0.0023)\n",
      "iteration 3243, train loss: 0.003452, validation loss: \u001b[92m0.002317\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3244, train loss: 0.003438, validation loss: 0.002338\n",
      "tensor(0.0024)\n",
      "iteration 3245, train loss: 0.003503, validation loss: 0.002399\n",
      "tensor(0.0024)\n",
      "iteration 3246, train loss: 0.003401, validation loss: 0.002405\n",
      "tensor(0.0024)\n",
      "iteration 3247, train loss: 0.003438, validation loss: 0.002362\n",
      "tensor(0.0023)\n",
      "iteration 3248, train loss: 0.003462, validation loss: 0.002335\n",
      "tensor(0.0023)\n",
      "iteration 3249, train loss: 0.003359, validation loss: 0.002326\n",
      "tensor(0.0024)\n",
      "iteration 3250, train loss: 0.003413, validation loss: 0.002353\n",
      "tensor(0.0024)\n",
      "iteration 3251, train loss: 0.003391, validation loss: 0.002409\n",
      "tensor(0.0024)\n",
      "iteration 3252, train loss: 0.003512, validation loss: 0.002368\n",
      "tensor(0.0023)\n",
      "iteration 3253, train loss: 0.003419, validation loss: 0.002324\n",
      "tensor(0.0025)\n",
      "iteration 3254, train loss: 0.00341, validation loss: 0.002456\n",
      "tensor(0.0025)\n",
      "iteration 3255, train loss: 0.00345, validation loss: 0.002483\n",
      "tensor(0.0024)\n",
      "iteration 3256, train loss: 0.003402, validation loss: 0.002398\n",
      "tensor(0.0024)\n",
      "iteration 3257, train loss: 0.003368, validation loss: 0.002382\n",
      "tensor(0.0024)\n",
      "iteration 3258, train loss: 0.003483, validation loss: 0.002374\n",
      "tensor(0.0025)\n",
      "iteration 3259, train loss: 0.00345, validation loss: 0.002536\n",
      "tensor(0.0023)\n",
      "iteration 3260, train loss: 0.00352, validation loss: 0.002325\n",
      "tensor(0.0024)\n",
      "iteration 3261, train loss: 0.003358, validation loss: 0.002402\n",
      "tensor(0.0024)\n",
      "iteration 3262, train loss: 0.003426, validation loss: 0.002352\n",
      "tensor(0.0024)\n",
      "iteration 3263, train loss: 0.003366, validation loss: 0.002384\n",
      "tensor(0.0024)\n",
      "iteration 3264, train loss: 0.003374, validation loss: 0.002353\n",
      "tensor(0.0023)\n",
      "iteration 3265, train loss: 0.003386, validation loss: 0.002336\n",
      "tensor(0.0024)\n",
      "iteration 3266, train loss: 0.00339, validation loss: 0.002405\n",
      "tensor(0.0024)\n",
      "iteration 3267, train loss: 0.003329, validation loss: 0.00239\n",
      "tensor(0.0024)\n",
      "iteration 3268, train loss: 0.003413, validation loss: 0.002385\n",
      "tensor(0.0023)\n",
      "iteration 3269, train loss: 0.003409, validation loss: 0.002324\n",
      "tensor(0.0024)\n",
      "iteration 3270, train loss: 0.003377, validation loss: 0.002354\n",
      "tensor(0.0024)\n",
      "iteration 3271, train loss: 0.003369, validation loss: 0.0024\n",
      "tensor(0.0025)\n",
      "iteration 3272, train loss: 0.003346, validation loss: 0.002488\n",
      "tensor(0.0023)\n",
      "iteration 3273, train loss: 0.003389, validation loss: 0.002332\n",
      "tensor(0.0023)\n",
      "iteration 3274, train loss: 0.003344, validation loss: \u001b[92m0.002301\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3275, train loss: \u001b[92m0.003323\u001b[0m, validation loss: 0.002301\n",
      "tensor(0.0024)\n",
      "iteration 3276, train loss: 0.003392, validation loss: 0.002374\n",
      "tensor(0.0024)\n",
      "iteration 3277, train loss: 0.003391, validation loss: 0.002359\n",
      "tensor(0.0023)\n",
      "iteration 3278, train loss: 0.003389, validation loss: 0.002317\n",
      "tensor(0.0023)\n",
      "iteration 3279, train loss: 0.003349, validation loss: 0.00232\n",
      "tensor(0.0024)\n",
      "iteration 3280, train loss: 0.003439, validation loss: 0.002393\n",
      "tensor(0.0024)\n",
      "iteration 3281, train loss: 0.003462, validation loss: 0.002418\n",
      "tensor(0.0023)\n",
      "iteration 3282, train loss: 0.003398, validation loss: 0.00234\n",
      "tensor(0.0023)\n",
      "iteration 3283, train loss: 0.003355, validation loss: 0.002336\n",
      "tensor(0.0023)\n",
      "iteration 3284, train loss: \u001b[92m0.003314\u001b[0m, validation loss: \u001b[92m0.002294\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3285, train loss: 0.003326, validation loss: 0.002304\n",
      "tensor(0.0023)\n",
      "iteration 3286, train loss: 0.003461, validation loss: 0.002333\n",
      "tensor(0.0024)\n",
      "iteration 3287, train loss: \u001b[92m0.003295\u001b[0m, validation loss: 0.002356\n",
      "tensor(0.0024)\n",
      "iteration 3288, train loss: 0.003373, validation loss: 0.002389\n",
      "tensor(0.0023)\n",
      "iteration 3289, train loss: 0.003303, validation loss: 0.002333\n",
      "tensor(0.0023)\n",
      "iteration 3290, train loss: 0.003375, validation loss: 0.002349\n",
      "tensor(0.0023)\n",
      "iteration 3291, train loss: 0.00343, validation loss: 0.00233\n",
      "tensor(0.0024)\n",
      "iteration 3292, train loss: 0.00339, validation loss: 0.002373\n",
      "tensor(0.0024)\n",
      "iteration 3293, train loss: 0.003439, validation loss: 0.002393\n",
      "tensor(0.0023)\n",
      "iteration 3294, train loss: 0.003403, validation loss: 0.00231\n",
      "tensor(0.0023)\n",
      "iteration 3295, train loss: 0.003434, validation loss: 0.002325\n",
      "tensor(0.0024)\n",
      "iteration 3296, train loss: 0.00336, validation loss: 0.002359\n",
      "tensor(0.0024)\n",
      "iteration 3297, train loss: 0.003502, validation loss: 0.002371\n",
      "tensor(0.0025)\n",
      "iteration 3298, train loss: 0.003481, validation loss: 0.002452\n",
      "tensor(0.0024)\n",
      "iteration 3299, train loss: 0.003339, validation loss: 0.002384\n",
      "tensor(0.0024)\n",
      "iteration 3300, train loss: 0.003379, validation loss: 0.002361\n",
      "tensor(0.0023)\n",
      "iteration 3301, train loss: 0.003342, validation loss: 0.002295\n",
      "tensor(0.0023)\n",
      "iteration 3302, train loss: 0.003345, validation loss: 0.002301\n",
      "tensor(0.0024)\n",
      "iteration 3303, train loss: 0.003296, validation loss: 0.002371\n",
      "tensor(0.0023)\n",
      "iteration 3304, train loss: 0.003417, validation loss: 0.002348\n",
      "tensor(0.0023)\n",
      "iteration 3305, train loss: 0.003479, validation loss: \u001b[92m0.002279\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3306, train loss: 0.003304, validation loss: 0.002282\n",
      "tensor(0.0023)\n",
      "iteration 3307, train loss: 0.003338, validation loss: 0.002307\n",
      "tensor(0.0024)\n",
      "iteration 3308, train loss: 0.003334, validation loss: 0.002371\n",
      "tensor(0.0023)\n",
      "iteration 3309, train loss: 0.003354, validation loss: 0.002344\n",
      "tensor(0.0023)\n",
      "iteration 3310, train loss: 0.003393, validation loss: \u001b[92m0.002277\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3311, train loss: 0.003374, validation loss: 0.002277\n",
      "tensor(0.0023)\n",
      "iteration 3312, train loss: 0.003307, validation loss: 0.002299\n",
      "tensor(0.0023)\n",
      "iteration 3313, train loss: 0.003336, validation loss: 0.002317\n",
      "tensor(0.0023)\n",
      "iteration 3314, train loss: 0.003368, validation loss: 0.002322\n",
      "tensor(0.0023)\n",
      "iteration 3315, train loss: 0.003346, validation loss: 0.002314\n",
      "tensor(0.0024)\n",
      "iteration 3316, train loss: 0.003463, validation loss: 0.002358\n",
      "tensor(0.0024)\n",
      "iteration 3317, train loss: 0.00336, validation loss: 0.00237\n",
      "tensor(0.0023)\n",
      "iteration 3318, train loss: 0.003344, validation loss: 0.002316\n",
      "tensor(0.0023)\n",
      "iteration 3319, train loss: 0.00335, validation loss: 0.002289\n",
      "tensor(0.0023)\n",
      "iteration 3320, train loss: 0.003392, validation loss: 0.002295\n",
      "tensor(0.0023)\n",
      "iteration 3321, train loss: 0.003381, validation loss: 0.002321\n",
      "tensor(0.0024)\n",
      "iteration 3322, train loss: \u001b[92m0.003279\u001b[0m, validation loss: 0.002394\n",
      "tensor(0.0023)\n",
      "iteration 3323, train loss: 0.003449, validation loss: 0.002304\n",
      "tensor(0.0023)\n",
      "iteration 3324, train loss: 0.003387, validation loss: 0.002311\n",
      "tensor(0.0023)\n",
      "iteration 3325, train loss: 0.003449, validation loss: 0.002311\n",
      "tensor(0.0024)\n",
      "iteration 3326, train loss: 0.003395, validation loss: 0.00241\n",
      "tensor(0.0024)\n",
      "iteration 3327, train loss: 0.003328, validation loss: 0.002431\n",
      "tensor(0.0023)\n",
      "iteration 3328, train loss: 0.003439, validation loss: 0.002311\n",
      "tensor(0.0023)\n",
      "iteration 3329, train loss: 0.003332, validation loss: \u001b[92m0.002277\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3330, train loss: 0.003484, validation loss: 0.002336\n",
      "tensor(0.0024)\n",
      "iteration 3331, train loss: 0.003382, validation loss: 0.002448\n",
      "tensor(0.0023)\n",
      "iteration 3332, train loss: 0.003382, validation loss: 0.002332\n",
      "tensor(0.0024)\n",
      "iteration 3333, train loss: 0.003326, validation loss: 0.002364\n",
      "tensor(0.0023)\n",
      "iteration 3334, train loss: 0.003355, validation loss: 0.002292\n",
      "tensor(0.0024)\n",
      "iteration 3335, train loss: 0.00335, validation loss: 0.00239\n",
      "tensor(0.0023)\n",
      "iteration 3336, train loss: 0.003371, validation loss: 0.002284\n",
      "tensor(0.0023)\n",
      "iteration 3337, train loss: 0.003356, validation loss: 0.002303\n",
      "tensor(0.0023)\n",
      "iteration 3338, train loss: 0.003389, validation loss: 0.00229\n",
      "tensor(0.0023)\n",
      "iteration 3339, train loss: 0.003304, validation loss: 0.002283\n",
      "tensor(0.0023)\n",
      "iteration 3340, train loss: 0.003351, validation loss: 0.00228\n",
      "tensor(0.0023)\n",
      "iteration 3341, train loss: 0.00346, validation loss: 0.002293\n",
      "tensor(0.0024)\n",
      "iteration 3342, train loss: 0.003339, validation loss: 0.002352\n",
      "tensor(0.0023)\n",
      "iteration 3343, train loss: 0.003317, validation loss: 0.002342\n",
      "tensor(0.0023)\n",
      "iteration 3344, train loss: 0.003378, validation loss: 0.002294\n",
      "tensor(0.0023)\n",
      "iteration 3345, train loss: 0.00336, validation loss: 0.002286\n",
      "tensor(0.0023)\n",
      "iteration 3346, train loss: 0.003418, validation loss: 0.002308\n",
      "tensor(0.0023)\n",
      "iteration 3347, train loss: 0.003312, validation loss: 0.002311\n",
      "tensor(0.0023)\n",
      "iteration 3348, train loss: 0.003322, validation loss: \u001b[92m0.002272\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3349, train loss: 0.003333, validation loss: 0.002363\n",
      "tensor(0.0023)\n",
      "iteration 3350, train loss: 0.003371, validation loss: 0.002323\n",
      "tensor(0.0023)\n",
      "iteration 3351, train loss: 0.003395, validation loss: 0.002319\n",
      "tensor(0.0023)\n",
      "iteration 3352, train loss: 0.003435, validation loss: 0.002295\n",
      "tensor(0.0023)\n",
      "iteration 3353, train loss: 0.003358, validation loss: 0.002321\n",
      "tensor(0.0023)\n",
      "iteration 3354, train loss: 0.003336, validation loss: 0.002278\n",
      "tensor(0.0023)\n",
      "iteration 3355, train loss: 0.003422, validation loss: 0.002281\n",
      "tensor(0.0023)\n",
      "iteration 3356, train loss: 0.003309, validation loss: 0.002315\n",
      "tensor(0.0024)\n",
      "iteration 3357, train loss: 0.003387, validation loss: 0.002365\n",
      "tensor(0.0024)\n",
      "iteration 3358, train loss: 0.003315, validation loss: 0.00238\n",
      "tensor(0.0023)\n",
      "iteration 3359, train loss: 0.003348, validation loss: 0.002272\n",
      "tensor(0.0023)\n",
      "iteration 3360, train loss: 0.003331, validation loss: 0.00228\n",
      "tensor(0.0023)\n",
      "iteration 3361, train loss: 0.003353, validation loss: 0.002337\n",
      "tensor(0.0023)\n",
      "iteration 3362, train loss: 0.003305, validation loss: 0.00234\n",
      "tensor(0.0023)\n",
      "iteration 3363, train loss: 0.003325, validation loss: 0.002344\n",
      "tensor(0.0023)\n",
      "iteration 3364, train loss: 0.003295, validation loss: 0.002302\n",
      "tensor(0.0023)\n",
      "iteration 3365, train loss: 0.00334, validation loss: 0.002327\n",
      "tensor(0.0023)\n",
      "iteration 3366, train loss: 0.003289, validation loss: 0.002298\n",
      "tensor(0.0023)\n",
      "iteration 3367, train loss: 0.003416, validation loss: 0.002277\n",
      "tensor(0.0023)\n",
      "iteration 3368, train loss: 0.003412, validation loss: 0.002337\n",
      "tensor(0.0024)\n",
      "iteration 3369, train loss: 0.003438, validation loss: 0.002396\n",
      "tensor(0.0023)\n",
      "iteration 3370, train loss: 0.003336, validation loss: 0.002302\n",
      "tensor(0.0023)\n",
      "iteration 3371, train loss: 0.003301, validation loss: \u001b[92m0.002269\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3372, train loss: 0.00338, validation loss: 0.002283\n",
      "tensor(0.0024)\n",
      "iteration 3373, train loss: 0.003368, validation loss: 0.002363\n",
      "tensor(0.0023)\n",
      "iteration 3374, train loss: 0.003315, validation loss: 0.00231\n",
      "tensor(0.0023)\n",
      "iteration 3375, train loss: 0.00329, validation loss: 0.002276\n",
      "tensor(0.0023)\n",
      "iteration 3376, train loss: 0.003353, validation loss: 0.00231\n",
      "tensor(0.0023)\n",
      "iteration 3377, train loss: 0.003329, validation loss: \u001b[92m0.002262\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3378, train loss: 0.003347, validation loss: 0.002276\n",
      "tensor(0.0023)\n",
      "iteration 3379, train loss: 0.003371, validation loss: 0.002318\n",
      "tensor(0.0023)\n",
      "iteration 3380, train loss: 0.003299, validation loss: 0.002304\n",
      "tensor(0.0023)\n",
      "iteration 3381, train loss: 0.003302, validation loss: 0.002267\n",
      "tensor(0.0023)\n",
      "iteration 3382, train loss: 0.003329, validation loss: 0.002297\n",
      "tensor(0.0024)\n",
      "iteration 3383, train loss: 0.003305, validation loss: 0.002388\n",
      "tensor(0.0024)\n",
      "iteration 3384, train loss: \u001b[92m0.003242\u001b[0m, validation loss: 0.002361\n",
      "tensor(0.0023)\n",
      "iteration 3385, train loss: 0.003258, validation loss: 0.0023\n",
      "tensor(0.0023)\n",
      "iteration 3386, train loss: 0.003281, validation loss: \u001b[92m0.002261\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3387, train loss: 0.00336, validation loss: 0.0023\n",
      "tensor(0.0023)\n",
      "iteration 3388, train loss: 0.003406, validation loss: 0.002341\n",
      "tensor(0.0024)\n",
      "iteration 3389, train loss: 0.003362, validation loss: 0.002425\n",
      "tensor(0.0024)\n",
      "iteration 3390, train loss: 0.003436, validation loss: 0.002373\n",
      "tensor(0.0024)\n",
      "iteration 3391, train loss: 0.003404, validation loss: 0.002372\n",
      "tensor(0.0023)\n",
      "iteration 3392, train loss: 0.003447, validation loss: 0.002289\n",
      "tensor(0.0023)\n",
      "iteration 3393, train loss: 0.003341, validation loss: 0.002269\n",
      "tensor(0.0022)\n",
      "iteration 3394, train loss: 0.003378, validation loss: \u001b[92m0.002243\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3395, train loss: 0.003407, validation loss: 0.002259\n",
      "tensor(0.0024)\n",
      "iteration 3396, train loss: 0.003379, validation loss: 0.002355\n",
      "tensor(0.0024)\n",
      "iteration 3397, train loss: 0.003376, validation loss: 0.002401\n",
      "tensor(0.0023)\n",
      "iteration 3398, train loss: 0.003369, validation loss: 0.00231\n",
      "tensor(0.0023)\n",
      "iteration 3399, train loss: 0.003275, validation loss: 0.002261\n",
      "tensor(0.0023)\n",
      "iteration 3400, train loss: 0.003296, validation loss: 0.002257\n",
      "tensor(0.0023)\n",
      "iteration 3401, train loss: \u001b[92m0.003236\u001b[0m, validation loss: 0.002272\n",
      "tensor(0.0024)\n",
      "iteration 3402, train loss: 0.003282, validation loss: 0.002351\n",
      "tensor(0.0023)\n",
      "iteration 3403, train loss: 0.003328, validation loss: 0.002287\n",
      "tensor(0.0023)\n",
      "iteration 3404, train loss: 0.003369, validation loss: 0.002267\n",
      "tensor(0.0023)\n",
      "iteration 3405, train loss: \u001b[92m0.003214\u001b[0m, validation loss: 0.002257\n",
      "tensor(0.0023)\n",
      "iteration 3406, train loss: 0.003235, validation loss: 0.002282\n",
      "tensor(0.0023)\n",
      "iteration 3407, train loss: 0.003333, validation loss: 0.002298\n",
      "tensor(0.0023)\n",
      "iteration 3408, train loss: 0.003298, validation loss: 0.002344\n",
      "tensor(0.0023)\n",
      "iteration 3409, train loss: 0.003309, validation loss: 0.002288\n",
      "tensor(0.0023)\n",
      "iteration 3410, train loss: 0.003342, validation loss: 0.002266\n",
      "tensor(0.0024)\n",
      "iteration 3411, train loss: 0.003262, validation loss: 0.002393\n",
      "tensor(0.0023)\n",
      "iteration 3412, train loss: 0.003339, validation loss: 0.00231\n",
      "tensor(0.0022)\n",
      "iteration 3413, train loss: 0.003262, validation loss: 0.002248\n",
      "tensor(0.0023)\n",
      "iteration 3414, train loss: 0.003356, validation loss: 0.002283\n",
      "tensor(0.0023)\n",
      "iteration 3415, train loss: 0.003285, validation loss: 0.002286\n",
      "tensor(0.0024)\n",
      "iteration 3416, train loss: 0.003342, validation loss: 0.002359\n",
      "tensor(0.0024)\n",
      "iteration 3417, train loss: 0.003264, validation loss: 0.002356\n",
      "tensor(0.0023)\n",
      "iteration 3418, train loss: 0.003264, validation loss: 0.002277\n",
      "tensor(0.0023)\n",
      "iteration 3419, train loss: 0.003286, validation loss: 0.002305\n",
      "tensor(0.0023)\n",
      "iteration 3420, train loss: 0.003285, validation loss: 0.002276\n",
      "tensor(0.0024)\n",
      "iteration 3421, train loss: 0.003374, validation loss: 0.002371\n",
      "tensor(0.0023)\n",
      "iteration 3422, train loss: 0.003334, validation loss: 0.002339\n",
      "tensor(0.0022)\n",
      "iteration 3423, train loss: 0.003309, validation loss: \u001b[92m0.00224\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3424, train loss: 0.003292, validation loss: 0.00226\n",
      "tensor(0.0023)\n",
      "iteration 3425, train loss: 0.003461, validation loss: 0.002311\n",
      "tensor(0.0025)\n",
      "iteration 3426, train loss: 0.003288, validation loss: 0.002462\n",
      "tensor(0.0023)\n",
      "iteration 3427, train loss: 0.003309, validation loss: 0.002307\n",
      "tensor(0.0022)\n",
      "iteration 3428, train loss: 0.003322, validation loss: 0.002241\n",
      "tensor(0.0022)\n",
      "iteration 3429, train loss: 0.003309, validation loss: \u001b[92m0.002235\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3430, train loss: 0.00331, validation loss: 0.002253\n",
      "tensor(0.0023)\n",
      "iteration 3431, train loss: 0.003266, validation loss: 0.002338\n",
      "tensor(0.0022)\n",
      "iteration 3432, train loss: 0.003381, validation loss: 0.002243\n",
      "tensor(0.0023)\n",
      "iteration 3433, train loss: 0.003218, validation loss: 0.002278\n",
      "tensor(0.0023)\n",
      "iteration 3434, train loss: 0.003329, validation loss: 0.002331\n",
      "tensor(0.0024)\n",
      "iteration 3435, train loss: 0.00332, validation loss: 0.002397\n",
      "tensor(0.0022)\n",
      "iteration 3436, train loss: 0.003365, validation loss: 0.002242\n",
      "tensor(0.0023)\n",
      "iteration 3437, train loss: 0.003334, validation loss: 0.002261\n",
      "tensor(0.0023)\n",
      "iteration 3438, train loss: 0.003371, validation loss: 0.00227\n",
      "tensor(0.0024)\n",
      "iteration 3439, train loss: 0.003238, validation loss: 0.002351\n",
      "tensor(0.0023)\n",
      "iteration 3440, train loss: 0.003266, validation loss: 0.002261\n",
      "tensor(0.0022)\n",
      "iteration 3441, train loss: 0.003321, validation loss: \u001b[92m0.002233\u001b[0m\n",
      "tensor(0.0024)\n",
      "iteration 3442, train loss: 0.003281, validation loss: 0.002358\n",
      "tensor(0.0023)\n",
      "iteration 3443, train loss: 0.003315, validation loss: 0.002287\n",
      "tensor(0.0023)\n",
      "iteration 3444, train loss: 0.003341, validation loss: 0.00229\n",
      "tensor(0.0023)\n",
      "iteration 3445, train loss: 0.003321, validation loss: 0.002311\n",
      "tensor(0.0024)\n",
      "iteration 3446, train loss: 0.003254, validation loss: 0.002387\n",
      "tensor(0.0022)\n",
      "iteration 3447, train loss: 0.003295, validation loss: 0.00225\n",
      "tensor(0.0022)\n",
      "iteration 3448, train loss: 0.003261, validation loss: 0.002245\n",
      "tensor(0.0023)\n",
      "iteration 3449, train loss: 0.003305, validation loss: 0.002313\n",
      "tensor(0.0024)\n",
      "iteration 3450, train loss: 0.003291, validation loss: 0.002433\n",
      "tensor(0.0024)\n",
      "iteration 3451, train loss: 0.003338, validation loss: 0.00236\n",
      "tensor(0.0024)\n",
      "iteration 3452, train loss: 0.003408, validation loss: 0.002411\n",
      "tensor(0.0023)\n",
      "iteration 3453, train loss: 0.003503, validation loss: 0.002313\n",
      "tensor(0.0024)\n",
      "iteration 3454, train loss: 0.003369, validation loss: 0.002421\n",
      "tensor(0.0022)\n",
      "iteration 3455, train loss: 0.003426, validation loss: 0.002238\n",
      "tensor(0.0023)\n",
      "iteration 3456, train loss: 0.003398, validation loss: 0.002253\n",
      "tensor(0.0024)\n",
      "iteration 3457, train loss: \u001b[92m0.003124\u001b[0m, validation loss: 0.00236\n",
      "tensor(0.0023)\n",
      "iteration 3458, train loss: 0.003327, validation loss: 0.002314\n",
      "tensor(0.0023)\n",
      "iteration 3459, train loss: 0.003364, validation loss: 0.002341\n",
      "tensor(0.0023)\n",
      "iteration 3460, train loss: 0.003349, validation loss: 0.00231\n",
      "tensor(0.0023)\n",
      "iteration 3461, train loss: 0.00331, validation loss: 0.002256\n",
      "tensor(0.0024)\n",
      "iteration 3462, train loss: 0.003299, validation loss: 0.002352\n",
      "tensor(0.0023)\n",
      "iteration 3463, train loss: 0.003266, validation loss: 0.002346\n",
      "tensor(0.0022)\n",
      "iteration 3464, train loss: 0.003269, validation loss: 0.002249\n",
      "tensor(0.0022)\n",
      "iteration 3465, train loss: 0.003271, validation loss: 0.002249\n",
      "tensor(0.0023)\n",
      "iteration 3466, train loss: 0.003386, validation loss: 0.002263\n",
      "tensor(0.0024)\n",
      "iteration 3467, train loss: 0.003315, validation loss: 0.002385\n",
      "tensor(0.0024)\n",
      "iteration 3468, train loss: 0.003276, validation loss: 0.00237\n",
      "tensor(0.0023)\n",
      "iteration 3469, train loss: 0.003326, validation loss: 0.002288\n",
      "tensor(0.0023)\n",
      "iteration 3470, train loss: 0.003306, validation loss: 0.002257\n",
      "tensor(0.0022)\n",
      "iteration 3471, train loss: 0.003352, validation loss: 0.002245\n",
      "tensor(0.0023)\n",
      "iteration 3472, train loss: 0.003171, validation loss: 0.002278\n",
      "tensor(0.0023)\n",
      "iteration 3473, train loss: 0.003255, validation loss: 0.002274\n",
      "tensor(0.0022)\n",
      "iteration 3474, train loss: 0.003385, validation loss: 0.002246\n",
      "tensor(0.0023)\n",
      "iteration 3475, train loss: 0.003257, validation loss: 0.002287\n",
      "tensor(0.0023)\n",
      "iteration 3476, train loss: 0.003287, validation loss: 0.002307\n",
      "tensor(0.0023)\n",
      "iteration 3477, train loss: 0.003222, validation loss: 0.002256\n",
      "tensor(0.0023)\n",
      "iteration 3478, train loss: 0.003309, validation loss: 0.002346\n",
      "tensor(0.0023)\n",
      "iteration 3479, train loss: 0.00332, validation loss: 0.002276\n",
      "tensor(0.0023)\n",
      "iteration 3480, train loss: 0.003272, validation loss: 0.002304\n",
      "tensor(0.0023)\n",
      "iteration 3481, train loss: 0.003372, validation loss: 0.002286\n",
      "tensor(0.0022)\n",
      "iteration 3482, train loss: 0.003344, validation loss: 0.002234\n",
      "tensor(0.0023)\n",
      "iteration 3483, train loss: 0.003288, validation loss: 0.002259\n",
      "tensor(0.0023)\n",
      "iteration 3484, train loss: 0.003302, validation loss: 0.002306\n",
      "tensor(0.0024)\n",
      "iteration 3485, train loss: 0.0033, validation loss: 0.002401\n",
      "tensor(0.0022)\n",
      "iteration 3486, train loss: 0.003405, validation loss: 0.002236\n",
      "tensor(0.0022)\n",
      "iteration 3487, train loss: 0.003239, validation loss: \u001b[92m0.002217\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3488, train loss: 0.003362, validation loss: 0.002275\n",
      "tensor(0.0023)\n",
      "iteration 3489, train loss: 0.003266, validation loss: 0.002296\n",
      "tensor(0.0023)\n",
      "iteration 3490, train loss: 0.003273, validation loss: 0.002297\n",
      "tensor(0.0023)\n",
      "iteration 3491, train loss: 0.003289, validation loss: 0.002299\n",
      "tensor(0.0023)\n",
      "iteration 3492, train loss: 0.003297, validation loss: 0.002264\n",
      "tensor(0.0023)\n",
      "iteration 3493, train loss: 0.003256, validation loss: 0.002272\n",
      "tensor(0.0023)\n",
      "iteration 3494, train loss: 0.003293, validation loss: 0.002253\n",
      "tensor(0.0023)\n",
      "iteration 3495, train loss: 0.003278, validation loss: 0.002291\n",
      "tensor(0.0022)\n",
      "iteration 3496, train loss: 0.003372, validation loss: 0.00224\n",
      "tensor(0.0022)\n",
      "iteration 3497, train loss: 0.003327, validation loss: \u001b[92m0.002214\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3498, train loss: 0.003339, validation loss: 0.002286\n",
      "tensor(0.0023)\n",
      "iteration 3499, train loss: 0.003334, validation loss: 0.002329\n",
      "tensor(0.0023)\n",
      "iteration 3500, train loss: 0.003282, validation loss: 0.002337\n",
      "tensor(0.0022)\n",
      "iteration 3501, train loss: 0.003271, validation loss: 0.002231\n",
      "tensor(0.0022)\n",
      "iteration 3502, train loss: 0.003367, validation loss: 0.002225\n",
      "tensor(0.0024)\n",
      "iteration 3503, train loss: 0.00334, validation loss: 0.002368\n",
      "tensor(0.0023)\n",
      "iteration 3504, train loss: 0.003313, validation loss: 0.002335\n",
      "tensor(0.0023)\n",
      "iteration 3505, train loss: 0.003253, validation loss: 0.002289\n",
      "tensor(0.0022)\n",
      "iteration 3506, train loss: 0.003232, validation loss: \u001b[92m0.002213\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3507, train loss: 0.003304, validation loss: 0.002295\n",
      "tensor(0.0023)\n",
      "iteration 3508, train loss: 0.003308, validation loss: 0.002289\n",
      "tensor(0.0023)\n",
      "iteration 3509, train loss: 0.003231, validation loss: 0.002276\n",
      "tensor(0.0022)\n",
      "iteration 3510, train loss: 0.003217, validation loss: 0.002229\n",
      "tensor(0.0023)\n",
      "iteration 3511, train loss: 0.003261, validation loss: 0.002316\n",
      "tensor(0.0022)\n",
      "iteration 3512, train loss: 0.003339, validation loss: 0.002233\n",
      "tensor(0.0023)\n",
      "iteration 3513, train loss: 0.003309, validation loss: 0.002277\n",
      "tensor(0.0024)\n",
      "iteration 3514, train loss: 0.003283, validation loss: 0.002434\n",
      "tensor(0.0023)\n",
      "iteration 3515, train loss: 0.003446, validation loss: 0.002332\n",
      "tensor(0.0022)\n",
      "iteration 3516, train loss: 0.00327, validation loss: 0.002241\n",
      "tensor(0.0022)\n",
      "iteration 3517, train loss: 0.003369, validation loss: \u001b[92m0.00221\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3518, train loss: 0.003301, validation loss: 0.002319\n",
      "tensor(0.0023)\n",
      "iteration 3519, train loss: 0.003213, validation loss: 0.002342\n",
      "tensor(0.0023)\n",
      "iteration 3520, train loss: 0.003273, validation loss: 0.002255\n",
      "tensor(0.0022)\n",
      "iteration 3521, train loss: 0.003297, validation loss: 0.002221\n",
      "tensor(0.0023)\n",
      "iteration 3522, train loss: 0.003302, validation loss: 0.002293\n",
      "tensor(0.0022)\n",
      "iteration 3523, train loss: 0.003358, validation loss: 0.002243\n",
      "tensor(0.0024)\n",
      "iteration 3524, train loss: 0.00322, validation loss: 0.002355\n",
      "tensor(0.0024)\n",
      "iteration 3525, train loss: 0.003314, validation loss: 0.002353\n",
      "tensor(0.0022)\n",
      "iteration 3526, train loss: 0.003308, validation loss: 0.002225\n",
      "tensor(0.0023)\n",
      "iteration 3527, train loss: 0.003273, validation loss: 0.002269\n",
      "tensor(0.0022)\n",
      "iteration 3528, train loss: 0.003337, validation loss: 0.00222\n",
      "tensor(0.0025)\n",
      "iteration 3529, train loss: 0.003282, validation loss: 0.002527\n",
      "tensor(0.0025)\n",
      "iteration 3530, train loss: 0.003377, validation loss: 0.002545\n",
      "tensor(0.0023)\n",
      "iteration 3531, train loss: 0.003341, validation loss: 0.002267\n",
      "tensor(0.0023)\n",
      "iteration 3532, train loss: 0.003326, validation loss: 0.002273\n",
      "tensor(0.0023)\n",
      "iteration 3533, train loss: 0.003494, validation loss: 0.002335\n",
      "tensor(0.0023)\n",
      "iteration 3534, train loss: 0.003408, validation loss: 0.002324\n",
      "tensor(0.0024)\n",
      "iteration 3535, train loss: 0.003317, validation loss: 0.002431\n",
      "tensor(0.0023)\n",
      "iteration 3536, train loss: 0.003448, validation loss: 0.002262\n",
      "tensor(0.0023)\n",
      "iteration 3537, train loss: 0.003299, validation loss: 0.002252\n",
      "tensor(0.0022)\n",
      "iteration 3538, train loss: 0.00334, validation loss: \u001b[92m0.002196\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3539, train loss: 0.003281, validation loss: 0.002258\n",
      "tensor(0.0023)\n",
      "iteration 3540, train loss: 0.003232, validation loss: 0.002333\n",
      "tensor(0.0023)\n",
      "iteration 3541, train loss: 0.003269, validation loss: 0.002274\n",
      "tensor(0.0022)\n",
      "iteration 3542, train loss: 0.003265, validation loss: 0.0022\n",
      "tensor(0.0022)\n",
      "iteration 3543, train loss: 0.00319, validation loss: \u001b[92m0.002186\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3544, train loss: 0.003245, validation loss: 0.002261\n",
      "tensor(0.0024)\n",
      "iteration 3545, train loss: 0.003344, validation loss: 0.002354\n",
      "tensor(0.0023)\n",
      "iteration 3546, train loss: 0.003323, validation loss: 0.00233\n",
      "tensor(0.0022)\n",
      "iteration 3547, train loss: 0.00334, validation loss: 0.002216\n",
      "tensor(0.0022)\n",
      "iteration 3548, train loss: 0.003342, validation loss: 0.002209\n",
      "tensor(0.0023)\n",
      "iteration 3549, train loss: 0.003352, validation loss: 0.002254\n",
      "tensor(0.0023)\n",
      "iteration 3550, train loss: 0.003241, validation loss: 0.002261\n",
      "tensor(0.0022)\n",
      "iteration 3551, train loss: 0.003227, validation loss: 0.002219\n",
      "tensor(0.0022)\n",
      "iteration 3552, train loss: 0.003247, validation loss: 0.002227\n",
      "tensor(0.0023)\n",
      "iteration 3553, train loss: 0.003223, validation loss: 0.002288\n",
      "tensor(0.0023)\n",
      "iteration 3554, train loss: 0.003235, validation loss: 0.002257\n",
      "tensor(0.0023)\n",
      "iteration 3555, train loss: 0.003253, validation loss: 0.002261\n",
      "tensor(0.0023)\n",
      "iteration 3556, train loss: 0.003262, validation loss: 0.002277\n",
      "tensor(0.0022)\n",
      "iteration 3557, train loss: 0.003319, validation loss: 0.002237\n",
      "tensor(0.0022)\n",
      "iteration 3558, train loss: 0.003215, validation loss: 0.002213\n",
      "tensor(0.0022)\n",
      "iteration 3559, train loss: 0.003384, validation loss: 0.002199\n",
      "tensor(0.0022)\n",
      "iteration 3560, train loss: 0.003344, validation loss: 0.002227\n",
      "tensor(0.0024)\n",
      "iteration 3561, train loss: 0.003348, validation loss: 0.002399\n",
      "tensor(0.0023)\n",
      "iteration 3562, train loss: 0.003314, validation loss: 0.002292\n",
      "tensor(0.0022)\n",
      "iteration 3563, train loss: 0.003164, validation loss: 0.002223\n",
      "tensor(0.0022)\n",
      "iteration 3564, train loss: 0.003278, validation loss: 0.0022\n",
      "tensor(0.0024)\n",
      "iteration 3565, train loss: 0.003308, validation loss: 0.002444\n",
      "tensor(0.0023)\n",
      "iteration 3566, train loss: 0.003409, validation loss: 0.002311\n",
      "tensor(0.0023)\n",
      "iteration 3567, train loss: 0.003235, validation loss: 0.00234\n",
      "tensor(0.0022)\n",
      "iteration 3568, train loss: 0.003497, validation loss: 0.002213\n",
      "tensor(0.0024)\n",
      "iteration 3569, train loss: 0.003212, validation loss: 0.002378\n",
      "tensor(0.0022)\n",
      "iteration 3570, train loss: 0.003422, validation loss: 0.002207\n",
      "tensor(0.0023)\n",
      "iteration 3571, train loss: 0.003195, validation loss: 0.002324\n",
      "tensor(0.0023)\n",
      "iteration 3572, train loss: 0.003288, validation loss: 0.002303\n",
      "tensor(0.0023)\n",
      "iteration 3573, train loss: 0.003272, validation loss: 0.002336\n",
      "tensor(0.0022)\n",
      "iteration 3574, train loss: 0.003331, validation loss: 0.002206\n",
      "tensor(0.0022)\n",
      "iteration 3575, train loss: 0.003405, validation loss: 0.002235\n",
      "tensor(0.0022)\n",
      "iteration 3576, train loss: 0.003361, validation loss: 0.002247\n",
      "tensor(0.0026)\n",
      "iteration 3577, train loss: 0.003328, validation loss: 0.00257\n",
      "tensor(0.0023)\n",
      "iteration 3578, train loss: 0.003405, validation loss: 0.002288\n",
      "tensor(0.0023)\n",
      "iteration 3579, train loss: 0.003178, validation loss: 0.0023\n",
      "tensor(0.0022)\n",
      "iteration 3580, train loss: 0.003374, validation loss: 0.002232\n",
      "tensor(0.0025)\n",
      "iteration 3581, train loss: 0.003347, validation loss: 0.002479\n",
      "tensor(0.0023)\n",
      "iteration 3582, train loss: 0.003476, validation loss: 0.002305\n",
      "tensor(0.0023)\n",
      "iteration 3583, train loss: 0.003327, validation loss: 0.002345\n",
      "tensor(0.0023)\n",
      "iteration 3584, train loss: 0.003374, validation loss: 0.002294\n",
      "tensor(0.0023)\n",
      "iteration 3585, train loss: 0.003315, validation loss: 0.002257\n",
      "tensor(0.0022)\n",
      "iteration 3586, train loss: 0.003331, validation loss: 0.002239\n",
      "tensor(0.0023)\n",
      "iteration 3587, train loss: 0.003266, validation loss: 0.002308\n",
      "tensor(0.0024)\n",
      "iteration 3588, train loss: 0.003322, validation loss: 0.002389\n",
      "tensor(0.0022)\n",
      "iteration 3589, train loss: 0.003345, validation loss: 0.002239\n",
      "tensor(0.0022)\n",
      "iteration 3590, train loss: 0.00324, validation loss: 0.00224\n",
      "tensor(0.0022)\n",
      "iteration 3591, train loss: 0.003298, validation loss: 0.00219\n",
      "tensor(0.0023)\n",
      "iteration 3592, train loss: 0.003252, validation loss: 0.002338\n",
      "tensor(0.0023)\n",
      "iteration 3593, train loss: 0.003184, validation loss: 0.002303\n",
      "tensor(0.0022)\n",
      "iteration 3594, train loss: 0.003239, validation loss: 0.002248\n",
      "tensor(0.0022)\n",
      "iteration 3595, train loss: 0.003249, validation loss: 0.002229\n",
      "tensor(0.0022)\n",
      "iteration 3596, train loss: 0.003197, validation loss: 0.002201\n",
      "tensor(0.0022)\n",
      "iteration 3597, train loss: 0.003187, validation loss: 0.002233\n",
      "tensor(0.0022)\n",
      "iteration 3598, train loss: 0.003256, validation loss: 0.002229\n",
      "tensor(0.0022)\n",
      "iteration 3599, train loss: 0.003278, validation loss: 0.002224\n",
      "tensor(0.0022)\n",
      "iteration 3600, train loss: 0.003163, validation loss: 0.002237\n",
      "tensor(0.0022)\n",
      "iteration 3601, train loss: 0.003185, validation loss: 0.002235\n",
      "tensor(0.0022)\n",
      "iteration 3602, train loss: 0.003147, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3603, train loss: 0.003258, validation loss: 0.00225\n",
      "tensor(0.0022)\n",
      "iteration 3604, train loss: 0.003316, validation loss: 0.002196\n",
      "tensor(0.0022)\n",
      "iteration 3605, train loss: 0.003263, validation loss: 0.002207\n",
      "tensor(0.0023)\n",
      "iteration 3606, train loss: 0.003339, validation loss: 0.002271\n",
      "tensor(0.0023)\n",
      "iteration 3607, train loss: 0.003194, validation loss: 0.002295\n",
      "tensor(0.0022)\n",
      "iteration 3608, train loss: 0.003247, validation loss: 0.002197\n",
      "tensor(0.0022)\n",
      "iteration 3609, train loss: 0.003237, validation loss: 0.002209\n",
      "tensor(0.0022)\n",
      "iteration 3610, train loss: 0.003268, validation loss: 0.002234\n",
      "tensor(0.0022)\n",
      "iteration 3611, train loss: 0.003204, validation loss: 0.002226\n",
      "tensor(0.0022)\n",
      "iteration 3612, train loss: 0.003149, validation loss: 0.00222\n",
      "tensor(0.0022)\n",
      "iteration 3613, train loss: 0.003206, validation loss: 0.002235\n",
      "tensor(0.0022)\n",
      "iteration 3614, train loss: 0.003175, validation loss: 0.002207\n",
      "tensor(0.0022)\n",
      "iteration 3615, train loss: 0.003235, validation loss: 0.002197\n",
      "tensor(0.0022)\n",
      "iteration 3616, train loss: 0.003248, validation loss: 0.002205\n",
      "tensor(0.0022)\n",
      "iteration 3617, train loss: 0.003219, validation loss: 0.002224\n",
      "tensor(0.0022)\n",
      "iteration 3618, train loss: 0.00318, validation loss: 0.002246\n",
      "tensor(0.0022)\n",
      "iteration 3619, train loss: 0.003277, validation loss: 0.002225\n",
      "tensor(0.0022)\n",
      "iteration 3620, train loss: 0.003244, validation loss: 0.002207\n",
      "tensor(0.0022)\n",
      "iteration 3621, train loss: 0.003203, validation loss: 0.00222\n",
      "tensor(0.0023)\n",
      "iteration 3622, train loss: \u001b[92m0.003087\u001b[0m, validation loss: 0.002272\n",
      "tensor(0.0023)\n",
      "iteration 3623, train loss: 0.003269, validation loss: 0.002262\n",
      "tensor(0.0022)\n",
      "iteration 3624, train loss: 0.003214, validation loss: 0.002196\n",
      "tensor(0.0022)\n",
      "iteration 3625, train loss: 0.003236, validation loss: 0.002203\n",
      "tensor(0.0022)\n",
      "iteration 3626, train loss: 0.003273, validation loss: \u001b[92m0.002183\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3627, train loss: 0.003305, validation loss: 0.002312\n",
      "tensor(0.0024)\n",
      "iteration 3628, train loss: 0.003195, validation loss: 0.002438\n",
      "tensor(0.0022)\n",
      "iteration 3629, train loss: 0.003347, validation loss: 0.002225\n",
      "tensor(0.0022)\n",
      "iteration 3630, train loss: 0.003205, validation loss: 0.002214\n",
      "tensor(0.0022)\n",
      "iteration 3631, train loss: 0.003258, validation loss: 0.002237\n",
      "tensor(0.0023)\n",
      "iteration 3632, train loss: 0.00319, validation loss: 0.002346\n",
      "tensor(0.0023)\n",
      "iteration 3633, train loss: 0.003239, validation loss: 0.002259\n",
      "tensor(0.0022)\n",
      "iteration 3634, train loss: 0.003252, validation loss: 0.002196\n",
      "tensor(0.0023)\n",
      "iteration 3635, train loss: 0.003206, validation loss: 0.002293\n",
      "tensor(0.0022)\n",
      "iteration 3636, train loss: 0.00335, validation loss: 0.002244\n",
      "tensor(0.0022)\n",
      "iteration 3637, train loss: 0.003159, validation loss: 0.002237\n",
      "tensor(0.0022)\n",
      "iteration 3638, train loss: 0.003247, validation loss: 0.002202\n",
      "tensor(0.0024)\n",
      "iteration 3639, train loss: 0.003135, validation loss: 0.002373\n",
      "tensor(0.0023)\n",
      "iteration 3640, train loss: 0.003271, validation loss: 0.002317\n",
      "tensor(0.0022)\n",
      "iteration 3641, train loss: 0.00319, validation loss: 0.002231\n",
      "tensor(0.0022)\n",
      "iteration 3642, train loss: 0.003209, validation loss: 0.002212\n",
      "tensor(0.0022)\n",
      "iteration 3643, train loss: 0.00331, validation loss: 0.002218\n",
      "tensor(0.0023)\n",
      "iteration 3644, train loss: 0.003215, validation loss: 0.002344\n",
      "tensor(0.0022)\n",
      "iteration 3645, train loss: 0.003273, validation loss: 0.002225\n",
      "tensor(0.0022)\n",
      "iteration 3646, train loss: 0.003314, validation loss: 0.002236\n",
      "tensor(0.0022)\n",
      "iteration 3647, train loss: 0.003249, validation loss: 0.002225\n",
      "tensor(0.0024)\n",
      "iteration 3648, train loss: 0.003197, validation loss: 0.002432\n",
      "tensor(0.0023)\n",
      "iteration 3649, train loss: 0.00324, validation loss: 0.002335\n",
      "tensor(0.0022)\n",
      "iteration 3650, train loss: 0.003234, validation loss: \u001b[92m0.002183\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3651, train loss: 0.003205, validation loss: \u001b[92m0.002164\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3652, train loss: 0.003191, validation loss: 0.002201\n",
      "tensor(0.0023)\n",
      "iteration 3653, train loss: 0.003205, validation loss: 0.002284\n",
      "tensor(0.0023)\n",
      "iteration 3654, train loss: 0.003252, validation loss: 0.002336\n",
      "tensor(0.0022)\n",
      "iteration 3655, train loss: 0.003224, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3656, train loss: 0.003213, validation loss: 0.00219\n",
      "tensor(0.0022)\n",
      "iteration 3657, train loss: 0.003166, validation loss: 0.002221\n",
      "tensor(0.0022)\n",
      "iteration 3658, train loss: 0.003258, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3659, train loss: 0.003249, validation loss: 0.002226\n",
      "tensor(0.0022)\n",
      "iteration 3660, train loss: 0.003199, validation loss: 0.002221\n",
      "tensor(0.0022)\n",
      "iteration 3661, train loss: 0.003163, validation loss: 0.002218\n",
      "tensor(0.0022)\n",
      "iteration 3662, train loss: 0.003104, validation loss: 0.002186\n",
      "tensor(0.0022)\n",
      "iteration 3663, train loss: 0.003191, validation loss: 0.002207\n",
      "tensor(0.0022)\n",
      "iteration 3664, train loss: 0.003189, validation loss: 0.002186\n",
      "tensor(0.0022)\n",
      "iteration 3665, train loss: 0.003155, validation loss: 0.002219\n",
      "tensor(0.0022)\n",
      "iteration 3666, train loss: 0.003171, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3667, train loss: 0.003198, validation loss: 0.002239\n",
      "tensor(0.0022)\n",
      "iteration 3668, train loss: 0.003256, validation loss: 0.002231\n",
      "tensor(0.0022)\n",
      "iteration 3669, train loss: 0.003184, validation loss: 0.002205\n",
      "tensor(0.0022)\n",
      "iteration 3670, train loss: 0.003127, validation loss: 0.002234\n",
      "tensor(0.0023)\n",
      "iteration 3671, train loss: 0.003252, validation loss: 0.00225\n",
      "tensor(0.0023)\n",
      "iteration 3672, train loss: 0.003321, validation loss: 0.002344\n",
      "tensor(0.0023)\n",
      "iteration 3673, train loss: 0.003345, validation loss: 0.00229\n",
      "tensor(0.0022)\n",
      "iteration 3674, train loss: 0.003172, validation loss: 0.002246\n",
      "tensor(0.0022)\n",
      "iteration 3675, train loss: 0.003209, validation loss: 0.002192\n",
      "tensor(0.0022)\n",
      "iteration 3676, train loss: 0.003206, validation loss: 0.002189\n",
      "tensor(0.0024)\n",
      "iteration 3677, train loss: 0.00323, validation loss: 0.002353\n",
      "tensor(0.0022)\n",
      "iteration 3678, train loss: 0.00325, validation loss: 0.002235\n",
      "tensor(0.0022)\n",
      "iteration 3679, train loss: 0.003303, validation loss: 0.002169\n",
      "tensor(0.0022)\n",
      "iteration 3680, train loss: 0.003116, validation loss: 0.002219\n",
      "tensor(0.0022)\n",
      "iteration 3681, train loss: 0.003134, validation loss: 0.002221\n",
      "tensor(0.0023)\n",
      "iteration 3682, train loss: 0.003201, validation loss: 0.002269\n",
      "tensor(0.0023)\n",
      "iteration 3683, train loss: 0.003221, validation loss: 0.002279\n",
      "tensor(0.0023)\n",
      "iteration 3684, train loss: 0.003281, validation loss: 0.002279\n",
      "tensor(0.0023)\n",
      "iteration 3685, train loss: 0.003242, validation loss: 0.002256\n",
      "tensor(0.0022)\n",
      "iteration 3686, train loss: 0.003213, validation loss: \u001b[92m0.00216\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3687, train loss: 0.003202, validation loss: 0.00216\n",
      "tensor(0.0022)\n",
      "iteration 3688, train loss: 0.003217, validation loss: 0.002231\n",
      "tensor(0.0023)\n",
      "iteration 3689, train loss: 0.003144, validation loss: 0.002293\n",
      "tensor(0.0022)\n",
      "iteration 3690, train loss: 0.003257, validation loss: 0.00222\n",
      "tensor(0.0022)\n",
      "iteration 3691, train loss: 0.003281, validation loss: 0.002195\n",
      "tensor(0.0022)\n",
      "iteration 3692, train loss: 0.003174, validation loss: 0.002205\n",
      "tensor(0.0022)\n",
      "iteration 3693, train loss: 0.003334, validation loss: 0.002209\n",
      "tensor(0.0022)\n",
      "iteration 3694, train loss: 0.003238, validation loss: 0.002184\n",
      "tensor(0.0022)\n",
      "iteration 3695, train loss: 0.003207, validation loss: 0.002166\n",
      "tensor(0.0022)\n",
      "iteration 3696, train loss: 0.003254, validation loss: 0.002218\n",
      "tensor(0.0022)\n",
      "iteration 3697, train loss: 0.003252, validation loss: 0.002224\n",
      "tensor(0.0022)\n",
      "iteration 3698, train loss: \u001b[92m0.00306\u001b[0m, validation loss: 0.002243\n",
      "tensor(0.0022)\n",
      "iteration 3699, train loss: 0.003097, validation loss: 0.002225\n",
      "tensor(0.0022)\n",
      "iteration 3700, train loss: 0.00323, validation loss: 0.002193\n",
      "tensor(0.0022)\n",
      "iteration 3701, train loss: 0.003191, validation loss: 0.00217\n",
      "tensor(0.0022)\n",
      "iteration 3702, train loss: 0.00326, validation loss: 0.002187\n",
      "tensor(0.0023)\n",
      "iteration 3703, train loss: 0.003125, validation loss: 0.002252\n",
      "tensor(0.0022)\n",
      "iteration 3704, train loss: 0.003152, validation loss: 0.002247\n",
      "tensor(0.0022)\n",
      "iteration 3705, train loss: 0.003235, validation loss: 0.002218\n",
      "tensor(0.0022)\n",
      "iteration 3706, train loss: 0.003252, validation loss: 0.002218\n",
      "tensor(0.0022)\n",
      "iteration 3707, train loss: 0.003284, validation loss: 0.00223\n",
      "tensor(0.0023)\n",
      "iteration 3708, train loss: 0.003251, validation loss: 0.002262\n",
      "tensor(0.0023)\n",
      "iteration 3709, train loss: 0.00312, validation loss: 0.002324\n",
      "tensor(0.0023)\n",
      "iteration 3710, train loss: 0.0032, validation loss: 0.002272\n",
      "tensor(0.0022)\n",
      "iteration 3711, train loss: 0.003176, validation loss: 0.002183\n",
      "tensor(0.0022)\n",
      "iteration 3712, train loss: 0.003172, validation loss: 0.002175\n",
      "tensor(0.0023)\n",
      "iteration 3713, train loss: 0.003298, validation loss: 0.002264\n",
      "tensor(0.0023)\n",
      "iteration 3714, train loss: 0.00321, validation loss: 0.002282\n",
      "tensor(0.0022)\n",
      "iteration 3715, train loss: 0.003213, validation loss: 0.002209\n",
      "tensor(0.0022)\n",
      "iteration 3716, train loss: 0.003187, validation loss: 0.002171\n",
      "tensor(0.0022)\n",
      "iteration 3717, train loss: 0.003158, validation loss: 0.002219\n",
      "tensor(0.0022)\n",
      "iteration 3718, train loss: 0.003249, validation loss: 0.002188\n",
      "tensor(0.0022)\n",
      "iteration 3719, train loss: 0.003243, validation loss: 0.002209\n",
      "tensor(0.0022)\n",
      "iteration 3720, train loss: 0.003245, validation loss: 0.002249\n",
      "tensor(0.0023)\n",
      "iteration 3721, train loss: 0.003194, validation loss: 0.002278\n",
      "tensor(0.0022)\n",
      "iteration 3722, train loss: 0.003166, validation loss: 0.00222\n",
      "tensor(0.0022)\n",
      "iteration 3723, train loss: 0.003209, validation loss: 0.002165\n",
      "tensor(0.0022)\n",
      "iteration 3724, train loss: 0.003158, validation loss: 0.002173\n",
      "tensor(0.0023)\n",
      "iteration 3725, train loss: 0.003206, validation loss: 0.002323\n",
      "tensor(0.0022)\n",
      "iteration 3726, train loss: 0.003273, validation loss: 0.002239\n",
      "tensor(0.0022)\n",
      "iteration 3727, train loss: 0.003215, validation loss: 0.002219\n",
      "tensor(0.0022)\n",
      "iteration 3728, train loss: 0.003257, validation loss: 0.002211\n",
      "tensor(0.0023)\n",
      "iteration 3729, train loss: 0.003233, validation loss: 0.002333\n",
      "tensor(0.0023)\n",
      "iteration 3730, train loss: 0.003229, validation loss: 0.002257\n",
      "tensor(0.0023)\n",
      "iteration 3731, train loss: 0.003263, validation loss: 0.002327\n",
      "tensor(0.0023)\n",
      "iteration 3732, train loss: 0.003325, validation loss: 0.002266\n",
      "tensor(0.0023)\n",
      "iteration 3733, train loss: 0.003204, validation loss: 0.002336\n",
      "tensor(0.0022)\n",
      "iteration 3734, train loss: 0.003286, validation loss: 0.002179\n",
      "tensor(0.0022)\n",
      "iteration 3735, train loss: 0.00321, validation loss: 0.002206\n",
      "tensor(0.0023)\n",
      "iteration 3736, train loss: 0.003324, validation loss: 0.00231\n",
      "tensor(0.0024)\n",
      "iteration 3737, train loss: 0.003179, validation loss: 0.002378\n",
      "tensor(0.0022)\n",
      "iteration 3738, train loss: 0.003233, validation loss: 0.002212\n",
      "tensor(0.0022)\n",
      "iteration 3739, train loss: 0.003223, validation loss: 0.002194\n",
      "tensor(0.0023)\n",
      "iteration 3740, train loss: 0.003325, validation loss: 0.002328\n",
      "tensor(0.0024)\n",
      "iteration 3741, train loss: 0.00326, validation loss: 0.002381\n",
      "tensor(0.0023)\n",
      "iteration 3742, train loss: 0.003292, validation loss: 0.002313\n",
      "tensor(0.0022)\n",
      "iteration 3743, train loss: 0.003223, validation loss: 0.002243\n",
      "tensor(0.0023)\n",
      "iteration 3744, train loss: 0.003211, validation loss: 0.002254\n",
      "tensor(0.0022)\n",
      "iteration 3745, train loss: 0.003213, validation loss: 0.002246\n",
      "tensor(0.0022)\n",
      "iteration 3746, train loss: 0.003293, validation loss: 0.002171\n",
      "tensor(0.0022)\n",
      "iteration 3747, train loss: 0.003221, validation loss: 0.002229\n",
      "tensor(0.0024)\n",
      "iteration 3748, train loss: 0.003262, validation loss: 0.00242\n",
      "tensor(0.0023)\n",
      "iteration 3749, train loss: 0.003208, validation loss: 0.002297\n",
      "tensor(0.0022)\n",
      "iteration 3750, train loss: 0.003261, validation loss: 0.002175\n",
      "tensor(0.0021)\n",
      "iteration 3751, train loss: 0.003094, validation loss: \u001b[92m0.002149\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3752, train loss: 0.003218, validation loss: 0.002172\n",
      "tensor(0.0024)\n",
      "iteration 3753, train loss: 0.003104, validation loss: 0.002446\n",
      "tensor(0.0023)\n",
      "iteration 3754, train loss: 0.003237, validation loss: 0.002301\n",
      "tensor(0.0022)\n",
      "iteration 3755, train loss: 0.003112, validation loss: 0.002172\n",
      "tensor(0.0022)\n",
      "iteration 3756, train loss: 0.003198, validation loss: 0.002165\n",
      "tensor(0.0023)\n",
      "iteration 3757, train loss: 0.003183, validation loss: 0.002309\n",
      "tensor(0.0022)\n",
      "iteration 3758, train loss: 0.003212, validation loss: 0.00224\n",
      "tensor(0.0023)\n",
      "iteration 3759, train loss: 0.003196, validation loss: 0.002278\n",
      "tensor(0.0022)\n",
      "iteration 3760, train loss: 0.00325, validation loss: 0.002225\n",
      "tensor(0.0023)\n",
      "iteration 3761, train loss: 0.003182, validation loss: 0.002334\n",
      "tensor(0.0022)\n",
      "iteration 3762, train loss: 0.003185, validation loss: 0.002187\n",
      "tensor(0.0022)\n",
      "iteration 3763, train loss: 0.003079, validation loss: 0.002216\n",
      "tensor(0.0022)\n",
      "iteration 3764, train loss: 0.003243, validation loss: 0.002188\n",
      "tensor(0.0023)\n",
      "iteration 3765, train loss: 0.003182, validation loss: 0.002298\n",
      "tensor(0.0023)\n",
      "iteration 3766, train loss: 0.003269, validation loss: 0.00229\n",
      "tensor(0.0022)\n",
      "iteration 3767, train loss: 0.003167, validation loss: 0.002182\n",
      "tensor(0.0022)\n",
      "iteration 3768, train loss: 0.003197, validation loss: 0.002213\n",
      "tensor(0.0022)\n",
      "iteration 3769, train loss: 0.00314, validation loss: 0.00222\n",
      "tensor(0.0022)\n",
      "iteration 3770, train loss: 0.003183, validation loss: 0.002164\n",
      "tensor(0.0022)\n",
      "iteration 3771, train loss: 0.003089, validation loss: 0.002162\n",
      "tensor(0.0022)\n",
      "iteration 3772, train loss: 0.00318, validation loss: 0.002186\n",
      "tensor(0.0022)\n",
      "iteration 3773, train loss: 0.0032, validation loss: 0.00218\n",
      "tensor(0.0022)\n",
      "iteration 3774, train loss: 0.003207, validation loss: 0.002167\n",
      "tensor(0.0021)\n",
      "iteration 3775, train loss: 0.003128, validation loss: \u001b[92m0.002142\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 3776, train loss: 0.003113, validation loss: 0.002143\n",
      "tensor(0.0022)\n",
      "iteration 3777, train loss: 0.003157, validation loss: 0.002227\n",
      "tensor(0.0023)\n",
      "iteration 3778, train loss: 0.003213, validation loss: 0.002289\n",
      "tensor(0.0023)\n",
      "iteration 3779, train loss: 0.003136, validation loss: 0.002298\n",
      "tensor(0.0022)\n",
      "iteration 3780, train loss: 0.003148, validation loss: 0.002184\n",
      "tensor(0.0021)\n",
      "iteration 3781, train loss: 0.003175, validation loss: 0.002143\n",
      "tensor(0.0022)\n",
      "iteration 3782, train loss: 0.003135, validation loss: 0.002234\n",
      "tensor(0.0022)\n",
      "iteration 3783, train loss: 0.003291, validation loss: 0.002163\n",
      "tensor(0.0023)\n",
      "iteration 3784, train loss: 0.003185, validation loss: 0.002252\n",
      "tensor(0.0023)\n",
      "iteration 3785, train loss: \u001b[92m0.00303\u001b[0m, validation loss: 0.002285\n",
      "tensor(0.0022)\n",
      "iteration 3786, train loss: 0.003091, validation loss: 0.002192\n",
      "tensor(0.0022)\n",
      "iteration 3787, train loss: 0.00318, validation loss: 0.002195\n",
      "tensor(0.0022)\n",
      "iteration 3788, train loss: 0.00306, validation loss: 0.002182\n",
      "tensor(0.0022)\n",
      "iteration 3789, train loss: 0.003212, validation loss: 0.002159\n",
      "tensor(0.0022)\n",
      "iteration 3790, train loss: 0.003095, validation loss: 0.00217\n",
      "tensor(0.0022)\n",
      "iteration 3791, train loss: 0.003162, validation loss: 0.002232\n",
      "tensor(0.0022)\n",
      "iteration 3792, train loss: 0.003283, validation loss: 0.002172\n",
      "tensor(0.0022)\n",
      "iteration 3793, train loss: 0.003134, validation loss: 0.002164\n",
      "tensor(0.0022)\n",
      "iteration 3794, train loss: 0.003295, validation loss: 0.002153\n",
      "tensor(0.0023)\n",
      "iteration 3795, train loss: 0.003118, validation loss: 0.002324\n",
      "tensor(0.0022)\n",
      "iteration 3796, train loss: 0.003218, validation loss: 0.002182\n",
      "tensor(0.0022)\n",
      "iteration 3797, train loss: 0.003173, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3798, train loss: 0.003207, validation loss: 0.002172\n",
      "tensor(0.0022)\n",
      "iteration 3799, train loss: 0.003204, validation loss: 0.00223\n",
      "tensor(0.0022)\n",
      "iteration 3800, train loss: 0.003156, validation loss: 0.002187\n",
      "tensor(0.0022)\n",
      "iteration 3801, train loss: 0.003175, validation loss: 0.002183\n",
      "tensor(0.0022)\n",
      "iteration 3802, train loss: 0.003197, validation loss: 0.002197\n",
      "tensor(0.0023)\n",
      "iteration 3803, train loss: 0.003144, validation loss: 0.002284\n",
      "tensor(0.0022)\n",
      "iteration 3804, train loss: 0.003233, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3805, train loss: 0.003067, validation loss: 0.002182\n",
      "tensor(0.0022)\n",
      "iteration 3806, train loss: 0.003161, validation loss: 0.002172\n",
      "tensor(0.0022)\n",
      "iteration 3807, train loss: 0.003204, validation loss: 0.002178\n",
      "tensor(0.0023)\n",
      "iteration 3808, train loss: 0.003154, validation loss: 0.002255\n",
      "tensor(0.0022)\n",
      "iteration 3809, train loss: 0.003219, validation loss: 0.002164\n",
      "tensor(0.0021)\n",
      "iteration 3810, train loss: 0.00313, validation loss: 0.002143\n",
      "tensor(0.0023)\n",
      "iteration 3811, train loss: 0.003201, validation loss: 0.00225\n",
      "tensor(0.0023)\n",
      "iteration 3812, train loss: 0.003164, validation loss: 0.002327\n",
      "tensor(0.0022)\n",
      "iteration 3813, train loss: 0.003181, validation loss: 0.002195\n",
      "tensor(0.0021)\n",
      "iteration 3814, train loss: 0.003221, validation loss: \u001b[92m0.002137\u001b[0m\n",
      "tensor(0.0023)\n",
      "iteration 3815, train loss: 0.003147, validation loss: 0.002256\n",
      "tensor(0.0023)\n",
      "iteration 3816, train loss: 0.003155, validation loss: 0.002263\n",
      "tensor(0.0022)\n",
      "iteration 3817, train loss: 0.003194, validation loss: 0.002186\n",
      "tensor(0.0022)\n",
      "iteration 3818, train loss: 0.003199, validation loss: 0.00216\n",
      "tensor(0.0022)\n",
      "iteration 3819, train loss: 0.003245, validation loss: 0.002208\n",
      "tensor(0.0023)\n",
      "iteration 3820, train loss: 0.003226, validation loss: 0.002337\n",
      "tensor(0.0022)\n",
      "iteration 3821, train loss: 0.003252, validation loss: 0.002165\n",
      "tensor(0.0022)\n",
      "iteration 3822, train loss: 0.003229, validation loss: 0.002166\n",
      "tensor(0.0022)\n",
      "iteration 3823, train loss: 0.003204, validation loss: 0.002185\n",
      "tensor(0.0023)\n",
      "iteration 3824, train loss: 0.003179, validation loss: 0.002253\n",
      "tensor(0.0021)\n",
      "iteration 3825, train loss: 0.003164, validation loss: 0.002144\n",
      "tensor(0.0022)\n",
      "iteration 3826, train loss: 0.003091, validation loss: 0.002201\n",
      "tensor(0.0022)\n",
      "iteration 3827, train loss: 0.003347, validation loss: 0.002212\n",
      "tensor(0.0024)\n",
      "iteration 3828, train loss: 0.003126, validation loss: 0.002444\n",
      "tensor(0.0022)\n",
      "iteration 3829, train loss: 0.003279, validation loss: 0.002237\n",
      "tensor(0.0023)\n",
      "iteration 3830, train loss: 0.003211, validation loss: 0.002253\n",
      "tensor(0.0022)\n",
      "iteration 3831, train loss: 0.003261, validation loss: 0.002199\n",
      "tensor(0.0024)\n",
      "iteration 3832, train loss: 0.003179, validation loss: 0.002373\n",
      "tensor(0.0022)\n",
      "iteration 3833, train loss: 0.003326, validation loss: 0.002171\n",
      "tensor(0.0023)\n",
      "iteration 3834, train loss: 0.003112, validation loss: 0.002273\n",
      "tensor(0.0023)\n",
      "iteration 3835, train loss: 0.003285, validation loss: 0.002283\n",
      "tensor(0.0024)\n",
      "iteration 3836, train loss: 0.003173, validation loss: 0.002362\n",
      "tensor(0.0022)\n",
      "iteration 3837, train loss: 0.003294, validation loss: 0.00216\n",
      "tensor(0.0022)\n",
      "iteration 3838, train loss: 0.003302, validation loss: 0.002162\n",
      "tensor(0.0022)\n",
      "iteration 3839, train loss: 0.003156, validation loss: 0.002248\n",
      "tensor(0.0023)\n",
      "iteration 3840, train loss: 0.003161, validation loss: 0.002268\n",
      "tensor(0.0022)\n",
      "iteration 3841, train loss: 0.003196, validation loss: 0.002186\n",
      "tensor(0.0021)\n",
      "iteration 3842, train loss: 0.003167, validation loss: 0.002143\n",
      "tensor(0.0022)\n",
      "iteration 3843, train loss: 0.003112, validation loss: 0.002168\n",
      "tensor(0.0023)\n",
      "iteration 3844, train loss: 0.003178, validation loss: 0.002263\n",
      "tensor(0.0022)\n",
      "iteration 3845, train loss: 0.003178, validation loss: 0.002195\n",
      "tensor(0.0022)\n",
      "iteration 3846, train loss: 0.003122, validation loss: 0.002169\n",
      "tensor(0.0021)\n",
      "iteration 3847, train loss: 0.00311, validation loss: \u001b[92m0.002122\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3848, train loss: 0.003191, validation loss: 0.002229\n",
      "tensor(0.0022)\n",
      "iteration 3849, train loss: 0.003174, validation loss: 0.002221\n",
      "tensor(0.0021)\n",
      "iteration 3850, train loss: 0.003148, validation loss: 0.002148\n",
      "tensor(0.0021)\n",
      "iteration 3851, train loss: 0.003123, validation loss: 0.002138\n",
      "tensor(0.0021)\n",
      "iteration 3852, train loss: 0.003218, validation loss: 0.002142\n",
      "tensor(0.0022)\n",
      "iteration 3853, train loss: 0.003137, validation loss: 0.002215\n",
      "tensor(0.0022)\n",
      "iteration 3854, train loss: 0.003133, validation loss: 0.002211\n",
      "tensor(0.0022)\n",
      "iteration 3855, train loss: 0.003182, validation loss: 0.002171\n",
      "tensor(0.0022)\n",
      "iteration 3856, train loss: 0.003222, validation loss: 0.002246\n",
      "tensor(0.0022)\n",
      "iteration 3857, train loss: 0.00324, validation loss: 0.00222\n",
      "tensor(0.0022)\n",
      "iteration 3858, train loss: 0.003125, validation loss: 0.002173\n",
      "tensor(0.0022)\n",
      "iteration 3859, train loss: 0.003167, validation loss: 0.002162\n",
      "tensor(0.0021)\n",
      "iteration 3860, train loss: 0.003203, validation loss: 0.002128\n",
      "tensor(0.0023)\n",
      "iteration 3861, train loss: 0.003105, validation loss: 0.00231\n",
      "tensor(0.0021)\n",
      "iteration 3862, train loss: 0.003272, validation loss: 0.002145\n",
      "tensor(0.0022)\n",
      "iteration 3863, train loss: 0.00313, validation loss: 0.002179\n",
      "tensor(0.0022)\n",
      "iteration 3864, train loss: 0.00318, validation loss: 0.002223\n",
      "tensor(0.0023)\n",
      "iteration 3865, train loss: 0.003081, validation loss: 0.002268\n",
      "tensor(0.0022)\n",
      "iteration 3866, train loss: 0.003142, validation loss: 0.002152\n",
      "tensor(0.0021)\n",
      "iteration 3867, train loss: 0.003129, validation loss: 0.002147\n",
      "tensor(0.0022)\n",
      "iteration 3868, train loss: 0.003168, validation loss: 0.002192\n",
      "tensor(0.0023)\n",
      "iteration 3869, train loss: 0.003176, validation loss: 0.002251\n",
      "tensor(0.0022)\n",
      "iteration 3870, train loss: 0.003167, validation loss: 0.002168\n",
      "tensor(0.0022)\n",
      "iteration 3871, train loss: 0.003163, validation loss: 0.002191\n",
      "tensor(0.0022)\n",
      "iteration 3872, train loss: 0.003147, validation loss: 0.002179\n",
      "tensor(0.0024)\n",
      "iteration 3873, train loss: 0.003084, validation loss: 0.002407\n",
      "tensor(0.0022)\n",
      "iteration 3874, train loss: 0.003334, validation loss: 0.002226\n",
      "tensor(0.0023)\n",
      "iteration 3875, train loss: 0.003152, validation loss: 0.002258\n",
      "tensor(0.0022)\n",
      "iteration 3876, train loss: 0.003205, validation loss: 0.002162\n",
      "tensor(0.0022)\n",
      "iteration 3877, train loss: 0.003124, validation loss: 0.002239\n",
      "tensor(0.0021)\n",
      "iteration 3878, train loss: 0.003289, validation loss: 0.002132\n",
      "tensor(0.0021)\n",
      "iteration 3879, train loss: 0.003129, validation loss: 0.00215\n",
      "tensor(0.0022)\n",
      "iteration 3880, train loss: 0.003055, validation loss: 0.002203\n",
      "tensor(0.0022)\n",
      "iteration 3881, train loss: 0.003262, validation loss: 0.002169\n",
      "tensor(0.0022)\n",
      "iteration 3882, train loss: 0.003053, validation loss: 0.002241\n",
      "tensor(0.0022)\n",
      "iteration 3883, train loss: 0.003267, validation loss: 0.002178\n",
      "tensor(0.0022)\n",
      "iteration 3884, train loss: 0.003107, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 3885, train loss: 0.003287, validation loss: 0.00217\n",
      "tensor(0.0022)\n",
      "iteration 3886, train loss: 0.003165, validation loss: 0.002188\n",
      "tensor(0.0022)\n",
      "iteration 3887, train loss: 0.003059, validation loss: 0.002181\n",
      "tensor(0.0022)\n",
      "iteration 3888, train loss: 0.003071, validation loss: 0.002177\n",
      "tensor(0.0022)\n",
      "iteration 3889, train loss: 0.003221, validation loss: 0.002191\n",
      "tensor(0.0022)\n",
      "iteration 3890, train loss: 0.003148, validation loss: 0.002188\n",
      "tensor(0.0022)\n",
      "iteration 3891, train loss: 0.003077, validation loss: 0.002152\n",
      "tensor(0.0021)\n",
      "iteration 3892, train loss: 0.003132, validation loss: \u001b[92m0.002117\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3893, train loss: 0.003146, validation loss: 0.002231\n",
      "tensor(0.0024)\n",
      "iteration 3894, train loss: 0.003144, validation loss: 0.002363\n",
      "tensor(0.0021)\n",
      "iteration 3895, train loss: 0.003185, validation loss: 0.002139\n",
      "tensor(0.0021)\n",
      "iteration 3896, train loss: 0.003094, validation loss: \u001b[92m0.002094\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 3897, train loss: 0.003231, validation loss: 0.002129\n",
      "tensor(0.0022)\n",
      "iteration 3898, train loss: 0.003112, validation loss: 0.002177\n",
      "tensor(0.0022)\n",
      "iteration 3899, train loss: 0.003217, validation loss: 0.002201\n",
      "tensor(0.0021)\n",
      "iteration 3900, train loss: 0.003031, validation loss: 0.002135\n",
      "tensor(0.0021)\n",
      "iteration 3901, train loss: 0.003146, validation loss: 0.002121\n",
      "tensor(0.0023)\n",
      "iteration 3902, train loss: 0.003174, validation loss: 0.002256\n",
      "tensor(0.0022)\n",
      "iteration 3903, train loss: 0.003304, validation loss: 0.002156\n",
      "tensor(0.0023)\n",
      "iteration 3904, train loss: 0.003141, validation loss: 0.002288\n",
      "tensor(0.0022)\n",
      "iteration 3905, train loss: 0.003339, validation loss: 0.002173\n",
      "tensor(0.0022)\n",
      "iteration 3906, train loss: 0.003042, validation loss: 0.002168\n",
      "tensor(0.0022)\n",
      "iteration 3907, train loss: 0.003212, validation loss: 0.002161\n",
      "tensor(0.0021)\n",
      "iteration 3908, train loss: 0.003162, validation loss: 0.002126\n",
      "tensor(0.0023)\n",
      "iteration 3909, train loss: 0.003206, validation loss: 0.002317\n",
      "tensor(0.0023)\n",
      "iteration 3910, train loss: 0.003186, validation loss: 0.0023\n",
      "tensor(0.0021)\n",
      "iteration 3911, train loss: 0.00315, validation loss: 0.002119\n",
      "tensor(0.0022)\n",
      "iteration 3912, train loss: 0.003112, validation loss: 0.00217\n",
      "tensor(0.0022)\n",
      "iteration 3913, train loss: 0.003204, validation loss: 0.002151\n",
      "tensor(0.0023)\n",
      "iteration 3914, train loss: 0.00309, validation loss: 0.002303\n",
      "tensor(0.0022)\n",
      "iteration 3915, train loss: 0.003222, validation loss: 0.002219\n",
      "tensor(0.0021)\n",
      "iteration 3916, train loss: 0.003212, validation loss: 0.002124\n",
      "tensor(0.0022)\n",
      "iteration 3917, train loss: 0.003131, validation loss: 0.002187\n",
      "tensor(0.0022)\n",
      "iteration 3918, train loss: 0.003141, validation loss: 0.002153\n",
      "tensor(0.0021)\n",
      "iteration 3919, train loss: 0.003145, validation loss: 0.002097\n",
      "tensor(0.0022)\n",
      "iteration 3920, train loss: 0.003117, validation loss: 0.002177\n",
      "tensor(0.0023)\n",
      "iteration 3921, train loss: 0.003147, validation loss: 0.002281\n",
      "tensor(0.0022)\n",
      "iteration 3922, train loss: 0.003173, validation loss: 0.002205\n",
      "tensor(0.0021)\n",
      "iteration 3923, train loss: 0.00313, validation loss: 0.002129\n",
      "tensor(0.0021)\n",
      "iteration 3924, train loss: \u001b[92m0.003027\u001b[0m, validation loss: 0.002108\n",
      "tensor(0.0022)\n",
      "iteration 3925, train loss: 0.003113, validation loss: 0.002162\n",
      "tensor(0.0022)\n",
      "iteration 3926, train loss: 0.00312, validation loss: 0.002221\n",
      "tensor(0.0022)\n",
      "iteration 3927, train loss: 0.003146, validation loss: 0.002157\n",
      "tensor(0.0021)\n",
      "iteration 3928, train loss: 0.00307, validation loss: 0.002108\n",
      "tensor(0.0021)\n",
      "iteration 3929, train loss: 0.003066, validation loss: 0.002107\n",
      "tensor(0.0021)\n",
      "iteration 3930, train loss: 0.003102, validation loss: 0.002096\n",
      "tensor(0.0021)\n",
      "iteration 3931, train loss: 0.003124, validation loss: 0.002146\n",
      "tensor(0.0022)\n",
      "iteration 3932, train loss: 0.003128, validation loss: 0.002186\n",
      "tensor(0.0022)\n",
      "iteration 3933, train loss: 0.003076, validation loss: 0.002165\n",
      "tensor(0.0022)\n",
      "iteration 3934, train loss: 0.003142, validation loss: 0.00219\n",
      "tensor(0.0021)\n",
      "iteration 3935, train loss: 0.003156, validation loss: 0.002109\n",
      "tensor(0.0021)\n",
      "iteration 3936, train loss: 0.003143, validation loss: 0.002122\n",
      "tensor(0.0022)\n",
      "iteration 3937, train loss: 0.003126, validation loss: 0.002194\n",
      "tensor(0.0023)\n",
      "iteration 3938, train loss: 0.003092, validation loss: 0.002275\n",
      "tensor(0.0021)\n",
      "iteration 3939, train loss: 0.003188, validation loss: \u001b[92m0.002086\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 3940, train loss: 0.00317, validation loss: 0.002093\n",
      "tensor(0.0021)\n",
      "iteration 3941, train loss: 0.003123, validation loss: 0.00212\n",
      "tensor(0.0023)\n",
      "iteration 3942, train loss: \u001b[92m0.002986\u001b[0m, validation loss: 0.002253\n",
      "tensor(0.0023)\n",
      "iteration 3943, train loss: 0.003143, validation loss: 0.002252\n",
      "tensor(0.0022)\n",
      "iteration 3944, train loss: 0.003115, validation loss: 0.00215\n",
      "tensor(0.0021)\n",
      "iteration 3945, train loss: 0.003198, validation loss: 0.00209\n",
      "tensor(0.0022)\n",
      "iteration 3946, train loss: 0.003053, validation loss: 0.002191\n",
      "tensor(0.0021)\n",
      "iteration 3947, train loss: 0.003173, validation loss: 0.002122\n",
      "tensor(0.0021)\n",
      "iteration 3948, train loss: 0.003106, validation loss: 0.002148\n",
      "tensor(0.0021)\n",
      "iteration 3949, train loss: 0.003228, validation loss: 0.002136\n",
      "tensor(0.0022)\n",
      "iteration 3950, train loss: 0.003079, validation loss: 0.002234\n",
      "tensor(0.0021)\n",
      "iteration 3951, train loss: 0.003112, validation loss: 0.002148\n",
      "tensor(0.0022)\n",
      "iteration 3952, train loss: 0.003085, validation loss: 0.002164\n",
      "tensor(0.0022)\n",
      "iteration 3953, train loss: 0.003329, validation loss: 0.002241\n",
      "tensor(0.0023)\n",
      "iteration 3954, train loss: 0.003107, validation loss: 0.002313\n",
      "tensor(0.0022)\n",
      "iteration 3955, train loss: 0.003157, validation loss: 0.002218\n",
      "tensor(0.0021)\n",
      "iteration 3956, train loss: 0.00313, validation loss: 0.002134\n",
      "tensor(0.0021)\n",
      "iteration 3957, train loss: 0.00321, validation loss: \u001b[92m0.002067\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3958, train loss: 0.0031, validation loss: 0.002237\n",
      "tensor(0.0022)\n",
      "iteration 3959, train loss: 0.00311, validation loss: 0.002173\n",
      "tensor(0.0021)\n",
      "iteration 3960, train loss: 0.00316, validation loss: 0.002129\n",
      "tensor(0.0021)\n",
      "iteration 3961, train loss: 0.003158, validation loss: 0.002127\n",
      "tensor(0.0023)\n",
      "iteration 3962, train loss: 0.003118, validation loss: 0.002276\n",
      "tensor(0.0021)\n",
      "iteration 3963, train loss: 0.003211, validation loss: 0.002143\n",
      "tensor(0.0022)\n",
      "iteration 3964, train loss: 0.003049, validation loss: 0.002165\n",
      "tensor(0.0021)\n",
      "iteration 3965, train loss: 0.003173, validation loss: 0.002117\n",
      "tensor(0.0022)\n",
      "iteration 3966, train loss: 0.003061, validation loss: 0.002194\n",
      "tensor(0.0021)\n",
      "iteration 3967, train loss: 0.003167, validation loss: 0.002119\n",
      "tensor(0.0021)\n",
      "iteration 3968, train loss: \u001b[92m0.002974\u001b[0m, validation loss: 0.002095\n",
      "tensor(0.0022)\n",
      "iteration 3969, train loss: 0.003098, validation loss: 0.002175\n",
      "tensor(0.0021)\n",
      "iteration 3970, train loss: 0.003081, validation loss: 0.002127\n",
      "tensor(0.0021)\n",
      "iteration 3971, train loss: 0.003127, validation loss: 0.002109\n",
      "tensor(0.0022)\n",
      "iteration 3972, train loss: 0.003158, validation loss: 0.002159\n",
      "tensor(0.0022)\n",
      "iteration 3973, train loss: 0.003222, validation loss: 0.002154\n",
      "tensor(0.0022)\n",
      "iteration 3974, train loss: 0.003007, validation loss: 0.002165\n",
      "tensor(0.0021)\n",
      "iteration 3975, train loss: 0.003196, validation loss: 0.00211\n",
      "tensor(0.0021)\n",
      "iteration 3976, train loss: 0.00307, validation loss: \u001b[92m0.00206\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 3977, train loss: 0.003187, validation loss: 0.002136\n",
      "tensor(0.0022)\n",
      "iteration 3978, train loss: 0.003095, validation loss: 0.002214\n",
      "tensor(0.0021)\n",
      "iteration 3979, train loss: 0.003144, validation loss: 0.002105\n",
      "tensor(0.0021)\n",
      "iteration 3980, train loss: 0.003154, validation loss: 0.002083\n",
      "tensor(0.0022)\n",
      "iteration 3981, train loss: 0.003152, validation loss: 0.002178\n",
      "tensor(0.0022)\n",
      "iteration 3982, train loss: 0.003094, validation loss: 0.00217\n",
      "tensor(0.0022)\n",
      "iteration 3983, train loss: 0.003079, validation loss: 0.002168\n",
      "tensor(0.0022)\n",
      "iteration 3984, train loss: 0.00306, validation loss: 0.002195\n",
      "tensor(0.0021)\n",
      "iteration 3985, train loss: 0.003149, validation loss: 0.002132\n",
      "tensor(0.0021)\n",
      "iteration 3986, train loss: 0.002994, validation loss: 0.002133\n",
      "tensor(0.0021)\n",
      "iteration 3987, train loss: 0.003094, validation loss: 0.002063\n",
      "tensor(0.0021)\n",
      "iteration 3988, train loss: 0.003027, validation loss: 0.002088\n",
      "tensor(0.0021)\n",
      "iteration 3989, train loss: 0.003068, validation loss: 0.002105\n",
      "tensor(0.0023)\n",
      "iteration 3990, train loss: 0.002996, validation loss: 0.002271\n",
      "tensor(0.0022)\n",
      "iteration 3991, train loss: 0.003045, validation loss: 0.002177\n",
      "tensor(0.0021)\n",
      "iteration 3992, train loss: 0.003028, validation loss: 0.002131\n",
      "tensor(0.0021)\n",
      "iteration 3993, train loss: 0.003104, validation loss: 0.00214\n",
      "tensor(0.0022)\n",
      "iteration 3994, train loss: 0.003056, validation loss: 0.002181\n",
      "tensor(0.0021)\n",
      "iteration 3995, train loss: 0.00307, validation loss: 0.002121\n",
      "tensor(0.0021)\n",
      "iteration 3996, train loss: 0.003132, validation loss: \u001b[92m0.002054\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 3997, train loss: 0.003096, validation loss: \u001b[92m0.00205\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 3998, train loss: 0.003161, validation loss: 0.002228\n",
      "tensor(0.0023)\n",
      "iteration 3999, train loss: 0.003117, validation loss: 0.002279\n",
      "tensor(0.0022)\n",
      "iteration 4000, train loss: 0.003181, validation loss: 0.002151\n",
      "tensor(0.0021)\n",
      "iteration 4001, train loss: 0.003115, validation loss: 0.002099\n",
      "tensor(0.0022)\n",
      "iteration 4002, train loss: 0.003086, validation loss: 0.00221\n",
      "tensor(0.0022)\n",
      "iteration 4003, train loss: 0.003066, validation loss: 0.002166\n",
      "tensor(0.0021)\n",
      "iteration 4004, train loss: 0.003049, validation loss: 0.002131\n",
      "tensor(0.0021)\n",
      "iteration 4005, train loss: 0.003081, validation loss: 0.002134\n",
      "tensor(0.0021)\n",
      "iteration 4006, train loss: 0.003095, validation loss: 0.002123\n",
      "tensor(0.0021)\n",
      "iteration 4007, train loss: 0.003039, validation loss: 0.002115\n",
      "tensor(0.0021)\n",
      "iteration 4008, train loss: 0.003093, validation loss: 0.00206\n",
      "tensor(0.0021)\n",
      "iteration 4009, train loss: 0.003187, validation loss: 0.002074\n",
      "tensor(0.0021)\n",
      "iteration 4010, train loss: 0.003109, validation loss: 0.002138\n",
      "tensor(0.0021)\n",
      "iteration 4011, train loss: 0.003016, validation loss: 0.002149\n",
      "tensor(0.0021)\n",
      "iteration 4012, train loss: 0.003086, validation loss: 0.002086\n",
      "tensor(0.0021)\n",
      "iteration 4013, train loss: 0.003009, validation loss: 0.002065\n",
      "tensor(0.0021)\n",
      "iteration 4014, train loss: \u001b[92m0.002956\u001b[0m, validation loss: 0.002066\n",
      "tensor(0.0021)\n",
      "iteration 4015, train loss: 0.00312, validation loss: 0.002129\n",
      "tensor(0.0021)\n",
      "iteration 4016, train loss: 0.003132, validation loss: 0.002125\n",
      "tensor(0.0021)\n",
      "iteration 4017, train loss: 0.003028, validation loss: 0.002129\n",
      "tensor(0.0021)\n",
      "iteration 4018, train loss: 0.002964, validation loss: 0.002126\n",
      "tensor(0.0021)\n",
      "iteration 4019, train loss: 0.002967, validation loss: 0.00214\n",
      "tensor(0.0021)\n",
      "iteration 4020, train loss: 0.003051, validation loss: 0.002077\n",
      "tensor(0.0021)\n",
      "iteration 4021, train loss: 0.003093, validation loss: 0.002091\n",
      "tensor(0.0022)\n",
      "iteration 4022, train loss: 0.00301, validation loss: 0.00218\n",
      "tensor(0.0022)\n",
      "iteration 4023, train loss: 0.003069, validation loss: 0.00223\n",
      "tensor(0.0021)\n",
      "iteration 4024, train loss: 0.003084, validation loss: 0.002085\n",
      "tensor(0.0021)\n",
      "iteration 4025, train loss: 0.003023, validation loss: 0.002058\n",
      "tensor(0.0021)\n",
      "iteration 4026, train loss: 0.003159, validation loss: 0.002052\n",
      "tensor(0.0021)\n",
      "iteration 4027, train loss: 0.003039, validation loss: 0.002133\n",
      "tensor(0.0022)\n",
      "iteration 4028, train loss: 0.003084, validation loss: 0.002202\n",
      "tensor(0.0022)\n",
      "iteration 4029, train loss: 0.003032, validation loss: 0.00216\n",
      "tensor(0.0021)\n",
      "iteration 4030, train loss: 0.003105, validation loss: 0.002062\n",
      "tensor(0.0021)\n",
      "iteration 4031, train loss: 0.003077, validation loss: 0.002089\n",
      "tensor(0.0021)\n",
      "iteration 4032, train loss: 0.003169, validation loss: 0.002102\n",
      "tensor(0.0022)\n",
      "iteration 4033, train loss: 0.003131, validation loss: 0.002162\n",
      "tensor(0.0022)\n",
      "iteration 4034, train loss: 0.003042, validation loss: 0.002157\n",
      "tensor(0.0021)\n",
      "iteration 4035, train loss: 0.003182, validation loss: 0.002127\n",
      "tensor(0.0021)\n",
      "iteration 4036, train loss: 0.00307, validation loss: 0.002067\n",
      "tensor(0.0021)\n",
      "iteration 4037, train loss: 0.003015, validation loss: 0.002054\n",
      "tensor(0.0021)\n",
      "iteration 4038, train loss: 0.003164, validation loss: 0.002125\n",
      "tensor(0.0022)\n",
      "iteration 4039, train loss: 0.00301, validation loss: 0.002207\n",
      "tensor(0.0022)\n",
      "iteration 4040, train loss: 0.003043, validation loss: 0.002181\n",
      "tensor(0.0021)\n",
      "iteration 4041, train loss: 0.003153, validation loss: 0.00207\n",
      "tensor(0.0020)\n",
      "iteration 4042, train loss: 0.003131, validation loss: \u001b[92m0.002047\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4043, train loss: 0.003151, validation loss: 0.002053\n",
      "tensor(0.0021)\n",
      "iteration 4044, train loss: 0.003081, validation loss: 0.00214\n",
      "tensor(0.0022)\n",
      "iteration 4045, train loss: 0.003051, validation loss: 0.002206\n",
      "tensor(0.0021)\n",
      "iteration 4046, train loss: 0.003122, validation loss: 0.002122\n",
      "tensor(0.0021)\n",
      "iteration 4047, train loss: 0.00301, validation loss: 0.002082\n",
      "tensor(0.0021)\n",
      "iteration 4048, train loss: 0.003017, validation loss: 0.002088\n",
      "tensor(0.0021)\n",
      "iteration 4049, train loss: 0.00307, validation loss: 0.002111\n",
      "tensor(0.0022)\n",
      "iteration 4050, train loss: 0.003084, validation loss: 0.002156\n",
      "tensor(0.0021)\n",
      "iteration 4051, train loss: 0.003111, validation loss: 0.002102\n",
      "tensor(0.0021)\n",
      "iteration 4052, train loss: 0.003064, validation loss: 0.00211\n",
      "tensor(0.0021)\n",
      "iteration 4053, train loss: 0.003041, validation loss: 0.002057\n",
      "tensor(0.0021)\n",
      "iteration 4054, train loss: 0.003074, validation loss: 0.002092\n",
      "tensor(0.0021)\n",
      "iteration 4055, train loss: 0.002976, validation loss: 0.002112\n",
      "tensor(0.0021)\n",
      "iteration 4056, train loss: 0.003077, validation loss: 0.002133\n",
      "tensor(0.0021)\n",
      "iteration 4057, train loss: 0.00306, validation loss: 0.002114\n",
      "tensor(0.0021)\n",
      "iteration 4058, train loss: 0.003004, validation loss: 0.002055\n",
      "tensor(0.0021)\n",
      "iteration 4059, train loss: 0.00301, validation loss: 0.00207\n",
      "tensor(0.0021)\n",
      "iteration 4060, train loss: 0.003022, validation loss: 0.002089\n",
      "tensor(0.0021)\n",
      "iteration 4061, train loss: 0.002959, validation loss: 0.002126\n",
      "tensor(0.0021)\n",
      "iteration 4062, train loss: 0.00308, validation loss: 0.002057\n",
      "tensor(0.0021)\n",
      "iteration 4063, train loss: 0.003031, validation loss: 0.002081\n",
      "tensor(0.0021)\n",
      "iteration 4064, train loss: 0.003072, validation loss: 0.002124\n",
      "tensor(0.0021)\n",
      "iteration 4065, train loss: 0.003068, validation loss: 0.002094\n",
      "tensor(0.0021)\n",
      "iteration 4066, train loss: 0.003052, validation loss: 0.002054\n",
      "tensor(0.0021)\n",
      "iteration 4067, train loss: 0.003055, validation loss: 0.002068\n",
      "tensor(0.0021)\n",
      "iteration 4068, train loss: 0.003036, validation loss: 0.002097\n",
      "tensor(0.0021)\n",
      "iteration 4069, train loss: 0.003061, validation loss: 0.002097\n",
      "tensor(0.0021)\n",
      "iteration 4070, train loss: 0.002957, validation loss: 0.002101\n",
      "tensor(0.0021)\n",
      "iteration 4071, train loss: 0.002996, validation loss: 0.002095\n",
      "tensor(0.0021)\n",
      "iteration 4072, train loss: 0.00302, validation loss: 0.0021\n",
      "tensor(0.0021)\n",
      "iteration 4073, train loss: 0.003041, validation loss: 0.002062\n",
      "tensor(0.0020)\n",
      "iteration 4074, train loss: 0.003052, validation loss: \u001b[92m0.00203\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4075, train loss: 0.003131, validation loss: 0.002089\n",
      "tensor(0.0021)\n",
      "iteration 4076, train loss: 0.002983, validation loss: 0.002136\n",
      "tensor(0.0021)\n",
      "iteration 4077, train loss: 0.003054, validation loss: 0.002136\n",
      "tensor(0.0021)\n",
      "iteration 4078, train loss: 0.003033, validation loss: 0.002134\n",
      "tensor(0.0020)\n",
      "iteration 4079, train loss: 0.003122, validation loss: 0.002038\n",
      "tensor(0.0021)\n",
      "iteration 4080, train loss: 0.003018, validation loss: 0.002114\n",
      "tensor(0.0021)\n",
      "iteration 4081, train loss: 0.003136, validation loss: 0.002056\n",
      "tensor(0.0022)\n",
      "iteration 4082, train loss: 0.003011, validation loss: 0.002194\n",
      "tensor(0.0023)\n",
      "iteration 4083, train loss: 0.003174, validation loss: 0.00229\n",
      "tensor(0.0021)\n",
      "iteration 4084, train loss: 0.003045, validation loss: 0.002067\n",
      "tensor(0.0021)\n",
      "iteration 4085, train loss: 0.003046, validation loss: 0.002055\n",
      "tensor(0.0020)\n",
      "iteration 4086, train loss: 0.003081, validation loss: 0.002043\n",
      "tensor(0.0022)\n",
      "iteration 4087, train loss: 0.003077, validation loss: 0.002235\n",
      "tensor(0.0023)\n",
      "iteration 4088, train loss: 0.003108, validation loss: 0.002285\n",
      "tensor(0.0021)\n",
      "iteration 4089, train loss: 0.003078, validation loss: 0.002098\n",
      "tensor(0.0020)\n",
      "iteration 4090, train loss: 0.00303, validation loss: 0.002042\n",
      "tensor(0.0021)\n",
      "iteration 4091, train loss: 0.003094, validation loss: 0.002065\n",
      "tensor(0.0021)\n",
      "iteration 4092, train loss: 0.003042, validation loss: 0.002097\n",
      "tensor(0.0021)\n",
      "iteration 4093, train loss: 0.003092, validation loss: 0.002122\n",
      "tensor(0.0021)\n",
      "iteration 4094, train loss: 0.003003, validation loss: 0.00209\n",
      "tensor(0.0021)\n",
      "iteration 4095, train loss: 0.003003, validation loss: 0.002082\n",
      "tensor(0.0021)\n",
      "iteration 4096, train loss: 0.002961, validation loss: 0.002063\n",
      "tensor(0.0020)\n",
      "iteration 4097, train loss: 0.00307, validation loss: 0.002033\n",
      "tensor(0.0021)\n",
      "iteration 4098, train loss: 0.003042, validation loss: 0.002074\n",
      "tensor(0.0021)\n",
      "iteration 4099, train loss: 0.002969, validation loss: 0.002091\n",
      "tensor(0.0021)\n",
      "iteration 4100, train loss: 0.003014, validation loss: 0.002096\n",
      "tensor(0.0020)\n",
      "iteration 4101, train loss: 0.003044, validation loss: 0.002036\n",
      "tensor(0.0020)\n",
      "iteration 4102, train loss: 0.003002, validation loss: 0.002045\n",
      "tensor(0.0021)\n",
      "iteration 4103, train loss: 0.003022, validation loss: 0.002083\n",
      "tensor(0.0021)\n",
      "iteration 4104, train loss: 0.003071, validation loss: 0.002078\n",
      "tensor(0.0021)\n",
      "iteration 4105, train loss: 0.002983, validation loss: 0.002126\n",
      "tensor(0.0021)\n",
      "iteration 4106, train loss: 0.003191, validation loss: 0.002089\n",
      "tensor(0.0022)\n",
      "iteration 4107, train loss: 0.003111, validation loss: 0.002156\n",
      "tensor(0.0022)\n",
      "iteration 4108, train loss: 0.003021, validation loss: 0.002199\n",
      "tensor(0.0020)\n",
      "iteration 4109, train loss: 0.003146, validation loss: 0.002036\n",
      "tensor(0.0020)\n",
      "iteration 4110, train loss: 0.003066, validation loss: \u001b[92m0.002023\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4111, train loss: 0.003131, validation loss: 0.002086\n",
      "tensor(0.0021)\n",
      "iteration 4112, train loss: 0.00297, validation loss: 0.002053\n",
      "tensor(0.0021)\n",
      "iteration 4113, train loss: 0.003046, validation loss: 0.00209\n",
      "tensor(0.0022)\n",
      "iteration 4114, train loss: 0.00308, validation loss: 0.002158\n",
      "tensor(0.0022)\n",
      "iteration 4115, train loss: 0.003001, validation loss: 0.002165\n",
      "tensor(0.0021)\n",
      "iteration 4116, train loss: 0.003135, validation loss: 0.00206\n",
      "tensor(0.0021)\n",
      "iteration 4117, train loss: 0.003008, validation loss: 0.00205\n",
      "tensor(0.0020)\n",
      "iteration 4118, train loss: 0.003046, validation loss: 0.002045\n",
      "tensor(0.0021)\n",
      "iteration 4119, train loss: 0.003018, validation loss: 0.002088\n",
      "tensor(0.0021)\n",
      "iteration 4120, train loss: 0.002981, validation loss: 0.002073\n",
      "tensor(0.0020)\n",
      "iteration 4121, train loss: 0.003042, validation loss: 0.002039\n",
      "tensor(0.0021)\n",
      "iteration 4122, train loss: 0.002973, validation loss: 0.00208\n",
      "tensor(0.0021)\n",
      "iteration 4123, train loss: 0.003045, validation loss: 0.002113\n",
      "tensor(0.0021)\n",
      "iteration 4124, train loss: 0.002979, validation loss: 0.002076\n",
      "tensor(0.0022)\n",
      "iteration 4125, train loss: 0.002995, validation loss: 0.002158\n",
      "tensor(0.0021)\n",
      "iteration 4126, train loss: 0.002997, validation loss: 0.002149\n",
      "tensor(0.0020)\n",
      "iteration 4127, train loss: 0.002968, validation loss: \u001b[92m0.002019\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4128, train loss: 0.003023, validation loss: 0.00205\n",
      "tensor(0.0020)\n",
      "iteration 4129, train loss: 0.003018, validation loss: 0.002049\n",
      "tensor(0.0021)\n",
      "iteration 4130, train loss: 0.002992, validation loss: 0.002105\n",
      "tensor(0.0021)\n",
      "iteration 4131, train loss: 0.003071, validation loss: 0.002143\n",
      "tensor(0.0021)\n",
      "iteration 4132, train loss: 0.003027, validation loss: 0.002133\n",
      "tensor(0.0021)\n",
      "iteration 4133, train loss: 0.003074, validation loss: 0.002069\n",
      "tensor(0.0021)\n",
      "iteration 4134, train loss: 0.003074, validation loss: 0.002065\n",
      "tensor(0.0020)\n",
      "iteration 4135, train loss: 0.003052, validation loss: 0.002037\n",
      "tensor(0.0021)\n",
      "iteration 4136, train loss: \u001b[92m0.002942\u001b[0m, validation loss: 0.002138\n",
      "tensor(0.0021)\n",
      "iteration 4137, train loss: 0.003069, validation loss: 0.002128\n",
      "tensor(0.0021)\n",
      "iteration 4138, train loss: 0.00299, validation loss: 0.002052\n",
      "tensor(0.0020)\n",
      "iteration 4139, train loss: 0.003079, validation loss: 0.002023\n",
      "tensor(0.0021)\n",
      "iteration 4140, train loss: 0.003003, validation loss: 0.002073\n",
      "tensor(0.0021)\n",
      "iteration 4141, train loss: 0.002952, validation loss: 0.002087\n",
      "tensor(0.0021)\n",
      "iteration 4142, train loss: 0.003044, validation loss: 0.002134\n",
      "tensor(0.0021)\n",
      "iteration 4143, train loss: 0.003171, validation loss: 0.002057\n",
      "tensor(0.0021)\n",
      "iteration 4144, train loss: 0.003043, validation loss: 0.002116\n",
      "tensor(0.0020)\n",
      "iteration 4145, train loss: 0.003059, validation loss: 0.002048\n",
      "tensor(0.0020)\n",
      "iteration 4146, train loss: 0.003098, validation loss: 0.002035\n",
      "tensor(0.0020)\n",
      "iteration 4147, train loss: 0.003055, validation loss: 0.002036\n",
      "tensor(0.0021)\n",
      "iteration 4148, train loss: 0.002992, validation loss: 0.002088\n",
      "tensor(0.0020)\n",
      "iteration 4149, train loss: 0.00302, validation loss: 0.002042\n",
      "tensor(0.0020)\n",
      "iteration 4150, train loss: 0.002972, validation loss: \u001b[92m0.002016\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4151, train loss: 0.003113, validation loss: 0.002072\n",
      "tensor(0.0021)\n",
      "iteration 4152, train loss: 0.003074, validation loss: 0.002137\n",
      "tensor(0.0021)\n",
      "iteration 4153, train loss: 0.002972, validation loss: 0.002115\n",
      "tensor(0.0021)\n",
      "iteration 4154, train loss: 0.003015, validation loss: 0.002109\n",
      "tensor(0.0022)\n",
      "iteration 4155, train loss: 0.002975, validation loss: 0.002159\n",
      "tensor(0.0020)\n",
      "iteration 4156, train loss: 0.003042, validation loss: 0.002041\n",
      "tensor(0.0020)\n",
      "iteration 4157, train loss: 0.003006, validation loss: 0.002034\n",
      "tensor(0.0022)\n",
      "iteration 4158, train loss: 0.003005, validation loss: 0.002177\n",
      "tensor(0.0021)\n",
      "iteration 4159, train loss: 0.003149, validation loss: 0.002085\n",
      "tensor(0.0020)\n",
      "iteration 4160, train loss: 0.002991, validation loss: 0.002031\n",
      "tensor(0.0020)\n",
      "iteration 4161, train loss: 0.003003, validation loss: 0.002041\n",
      "tensor(0.0021)\n",
      "iteration 4162, train loss: 0.003081, validation loss: 0.002121\n",
      "tensor(0.0021)\n",
      "iteration 4163, train loss: 0.003164, validation loss: 0.002106\n",
      "tensor(0.0021)\n",
      "iteration 4164, train loss: 0.003068, validation loss: 0.002136\n",
      "tensor(0.0021)\n",
      "iteration 4165, train loss: 0.003063, validation loss: 0.002133\n",
      "tensor(0.0021)\n",
      "iteration 4166, train loss: 0.002988, validation loss: 0.002064\n",
      "tensor(0.0020)\n",
      "iteration 4167, train loss: 0.003023, validation loss: 0.002029\n",
      "tensor(0.0020)\n",
      "iteration 4168, train loss: 0.003083, validation loss: 0.002045\n",
      "tensor(0.0021)\n",
      "iteration 4169, train loss: 0.003016, validation loss: 0.002097\n",
      "tensor(0.0021)\n",
      "iteration 4170, train loss: 0.002985, validation loss: 0.002149\n",
      "tensor(0.0021)\n",
      "iteration 4171, train loss: 0.003039, validation loss: 0.002067\n",
      "tensor(0.0020)\n",
      "iteration 4172, train loss: 0.00305, validation loss: 0.002018\n",
      "tensor(0.0021)\n",
      "iteration 4173, train loss: \u001b[92m0.002935\u001b[0m, validation loss: 0.00206\n",
      "tensor(0.0021)\n",
      "iteration 4174, train loss: 0.003008, validation loss: 0.002142\n",
      "tensor(0.0021)\n",
      "iteration 4175, train loss: 0.003035, validation loss: 0.002108\n",
      "tensor(0.0020)\n",
      "iteration 4176, train loss: 0.003074, validation loss: 0.002049\n",
      "tensor(0.0021)\n",
      "iteration 4177, train loss: 0.003012, validation loss: 0.00207\n",
      "tensor(0.0020)\n",
      "iteration 4178, train loss: 0.003069, validation loss: 0.002048\n",
      "tensor(0.0021)\n",
      "iteration 4179, train loss: 0.002984, validation loss: 0.002083\n",
      "tensor(0.0021)\n",
      "iteration 4180, train loss: 0.003071, validation loss: 0.002058\n",
      "tensor(0.0022)\n",
      "iteration 4181, train loss: 0.003063, validation loss: 0.002235\n",
      "tensor(0.0020)\n",
      "iteration 4182, train loss: 0.003109, validation loss: 0.00204\n",
      "tensor(0.0021)\n",
      "iteration 4183, train loss: 0.00308, validation loss: 0.002054\n",
      "tensor(0.0022)\n",
      "iteration 4184, train loss: 0.003058, validation loss: 0.002156\n",
      "tensor(0.0022)\n",
      "iteration 4185, train loss: 0.003086, validation loss: 0.002245\n",
      "tensor(0.0022)\n",
      "iteration 4186, train loss: 0.003046, validation loss: 0.002174\n",
      "tensor(0.0020)\n",
      "iteration 4187, train loss: 0.003231, validation loss: \u001b[92m0.001994\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 4188, train loss: 0.002997, validation loss: 0.002244\n",
      "tensor(0.0020)\n",
      "iteration 4189, train loss: 0.003209, validation loss: 0.00203\n",
      "tensor(0.0022)\n",
      "iteration 4190, train loss: 0.003056, validation loss: 0.00218\n",
      "tensor(0.0021)\n",
      "iteration 4191, train loss: 0.00317, validation loss: 0.002125\n",
      "tensor(0.0024)\n",
      "iteration 4192, train loss: 0.003045, validation loss: 0.002416\n",
      "tensor(0.0021)\n",
      "iteration 4193, train loss: 0.003303, validation loss: 0.002097\n",
      "tensor(0.0021)\n",
      "iteration 4194, train loss: 0.003023, validation loss: 0.00208\n",
      "tensor(0.0021)\n",
      "iteration 4195, train loss: 0.003079, validation loss: 0.002123\n",
      "tensor(0.0022)\n",
      "iteration 4196, train loss: 0.003064, validation loss: 0.002163\n",
      "tensor(0.0021)\n",
      "iteration 4197, train loss: 0.003039, validation loss: 0.002055\n",
      "tensor(0.0020)\n",
      "iteration 4198, train loss: 0.003014, validation loss: 0.002041\n",
      "tensor(0.0021)\n",
      "iteration 4199, train loss: 0.003095, validation loss: 0.002121\n",
      "tensor(0.0022)\n",
      "iteration 4200, train loss: 0.002984, validation loss: 0.002212\n",
      "tensor(0.0021)\n",
      "iteration 4201, train loss: 0.003092, validation loss: 0.002057\n",
      "tensor(0.0021)\n",
      "iteration 4202, train loss: 0.003085, validation loss: 0.002128\n",
      "tensor(0.0021)\n",
      "iteration 4203, train loss: 0.003093, validation loss: 0.002089\n",
      "tensor(0.0023)\n",
      "iteration 4204, train loss: 0.002961, validation loss: 0.002276\n",
      "tensor(0.0020)\n",
      "iteration 4205, train loss: 0.003197, validation loss: 0.002047\n",
      "tensor(0.0021)\n",
      "iteration 4206, train loss: 0.003029, validation loss: 0.002124\n",
      "tensor(0.0021)\n",
      "iteration 4207, train loss: 0.003054, validation loss: 0.002123\n",
      "tensor(0.0022)\n",
      "iteration 4208, train loss: 0.002991, validation loss: 0.002156\n",
      "tensor(0.0020)\n",
      "iteration 4209, train loss: 0.003082, validation loss: 0.00201\n",
      "tensor(0.0020)\n",
      "iteration 4210, train loss: 0.003084, validation loss: 0.002018\n",
      "tensor(0.0022)\n",
      "iteration 4211, train loss: 0.003134, validation loss: 0.002221\n",
      "tensor(0.0021)\n",
      "iteration 4212, train loss: 0.003127, validation loss: 0.002084\n",
      "tensor(0.0020)\n",
      "iteration 4213, train loss: \u001b[92m0.002931\u001b[0m, validation loss: 0.002032\n",
      "tensor(0.0020)\n",
      "iteration 4214, train loss: 0.003141, validation loss: 0.002031\n",
      "tensor(0.0022)\n",
      "iteration 4215, train loss: 0.003028, validation loss: 0.002155\n",
      "tensor(0.0021)\n",
      "iteration 4216, train loss: 0.003061, validation loss: 0.002143\n",
      "tensor(0.0021)\n",
      "iteration 4217, train loss: 0.003044, validation loss: 0.002057\n",
      "tensor(0.0021)\n",
      "iteration 4218, train loss: 0.003039, validation loss: 0.002063\n",
      "tensor(0.0021)\n",
      "iteration 4219, train loss: 0.003045, validation loss: 0.002074\n",
      "tensor(0.0020)\n",
      "iteration 4220, train loss: 0.002981, validation loss: \u001b[92m0.001991\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4221, train loss: 0.002996, validation loss: 0.002032\n",
      "tensor(0.0021)\n",
      "iteration 4222, train loss: 0.003038, validation loss: 0.002137\n",
      "tensor(0.0021)\n",
      "iteration 4223, train loss: 0.00306, validation loss: 0.002083\n",
      "tensor(0.0021)\n",
      "iteration 4224, train loss: 0.002974, validation loss: 0.002057\n",
      "tensor(0.0020)\n",
      "iteration 4225, train loss: 0.003011, validation loss: 0.002\n",
      "tensor(0.0021)\n",
      "iteration 4226, train loss: \u001b[92m0.002926\u001b[0m, validation loss: 0.002115\n",
      "tensor(0.0021)\n",
      "iteration 4227, train loss: 0.003043, validation loss: 0.002058\n",
      "tensor(0.0021)\n",
      "iteration 4228, train loss: 0.003086, validation loss: 0.002105\n",
      "tensor(0.0021)\n",
      "iteration 4229, train loss: 0.003116, validation loss: 0.00205\n",
      "tensor(0.0022)\n",
      "iteration 4230, train loss: 0.003004, validation loss: 0.00215\n",
      "tensor(0.0020)\n",
      "iteration 4231, train loss: 0.003111, validation loss: 0.002004\n",
      "tensor(0.0021)\n",
      "iteration 4232, train loss: \u001b[92m0.002892\u001b[0m, validation loss: 0.002079\n",
      "tensor(0.0021)\n",
      "iteration 4233, train loss: 0.003157, validation loss: 0.002061\n",
      "tensor(0.0021)\n",
      "iteration 4234, train loss: 0.003036, validation loss: 0.002123\n",
      "tensor(0.0020)\n",
      "iteration 4235, train loss: 0.003111, validation loss: 0.002033\n",
      "tensor(0.0021)\n",
      "iteration 4236, train loss: 0.003075, validation loss: 0.00208\n",
      "tensor(0.0021)\n",
      "iteration 4237, train loss: 0.002985, validation loss: 0.00214\n",
      "tensor(0.0021)\n",
      "iteration 4238, train loss: 0.003056, validation loss: 0.002095\n",
      "tensor(0.0020)\n",
      "iteration 4239, train loss: 0.003034, validation loss: 0.002012\n",
      "tensor(0.0020)\n",
      "iteration 4240, train loss: 0.003075, validation loss: 0.002034\n",
      "tensor(0.0021)\n",
      "iteration 4241, train loss: 0.003007, validation loss: 0.002109\n",
      "tensor(0.0021)\n",
      "iteration 4242, train loss: 0.003078, validation loss: 0.002077\n",
      "tensor(0.0020)\n",
      "iteration 4243, train loss: 0.002968, validation loss: 0.002022\n",
      "tensor(0.0020)\n",
      "iteration 4244, train loss: 0.003003, validation loss: 0.002011\n",
      "tensor(0.0020)\n",
      "iteration 4245, train loss: 0.003055, validation loss: 0.002038\n",
      "tensor(0.0021)\n",
      "iteration 4246, train loss: 0.003012, validation loss: 0.002061\n",
      "tensor(0.0020)\n",
      "iteration 4247, train loss: \u001b[92m0.002891\u001b[0m, validation loss: 0.002038\n",
      "tensor(0.0020)\n",
      "iteration 4248, train loss: 0.00308, validation loss: 0.00202\n",
      "tensor(0.0021)\n",
      "iteration 4249, train loss: 0.003005, validation loss: 0.002108\n",
      "tensor(0.0020)\n",
      "iteration 4250, train loss: 0.003013, validation loss: 0.002039\n",
      "tensor(0.0020)\n",
      "iteration 4251, train loss: 0.002953, validation loss: 0.002032\n",
      "tensor(0.0020)\n",
      "iteration 4252, train loss: 0.002931, validation loss: 0.002038\n",
      "tensor(0.0021)\n",
      "iteration 4253, train loss: 0.002953, validation loss: 0.002142\n",
      "tensor(0.0020)\n",
      "iteration 4254, train loss: 0.003129, validation loss: 0.002009\n",
      "tensor(0.0020)\n",
      "iteration 4255, train loss: 0.002915, validation loss: 0.002017\n",
      "tensor(0.0020)\n",
      "iteration 4256, train loss: 0.002975, validation loss: 0.002048\n",
      "tensor(0.0022)\n",
      "iteration 4257, train loss: 0.003014, validation loss: 0.002152\n",
      "tensor(0.0020)\n",
      "iteration 4258, train loss: 0.003102, validation loss: 0.002028\n",
      "tensor(0.0020)\n",
      "iteration 4259, train loss: 0.00307, validation loss: 0.002008\n",
      "tensor(0.0021)\n",
      "iteration 4260, train loss: 0.003058, validation loss: 0.002058\n",
      "tensor(0.0021)\n",
      "iteration 4261, train loss: 0.002963, validation loss: 0.002093\n",
      "tensor(0.0020)\n",
      "iteration 4262, train loss: 0.003058, validation loss: 0.002018\n",
      "tensor(0.0021)\n",
      "iteration 4263, train loss: 0.003065, validation loss: 0.002063\n",
      "tensor(0.0021)\n",
      "iteration 4264, train loss: 0.003013, validation loss: 0.002058\n",
      "tensor(0.0020)\n",
      "iteration 4265, train loss: 0.002983, validation loss: 0.002018\n",
      "tensor(0.0021)\n",
      "iteration 4266, train loss: 0.003083, validation loss: 0.002069\n",
      "tensor(0.0021)\n",
      "iteration 4267, train loss: 0.003032, validation loss: 0.002101\n",
      "tensor(0.0021)\n",
      "iteration 4268, train loss: 0.003114, validation loss: 0.002131\n",
      "tensor(0.0020)\n",
      "iteration 4269, train loss: 0.00302, validation loss: 0.001996\n",
      "tensor(0.0020)\n",
      "iteration 4270, train loss: 0.003068, validation loss: 0.002018\n",
      "tensor(0.0020)\n",
      "iteration 4271, train loss: 0.003033, validation loss: 0.002037\n",
      "tensor(0.0021)\n",
      "iteration 4272, train loss: 0.003122, validation loss: 0.00212\n",
      "tensor(0.0021)\n",
      "iteration 4273, train loss: 0.003047, validation loss: 0.002135\n",
      "tensor(0.0021)\n",
      "iteration 4274, train loss: 0.003092, validation loss: 0.002136\n",
      "tensor(0.0021)\n",
      "iteration 4275, train loss: 0.003064, validation loss: 0.002149\n",
      "tensor(0.0022)\n",
      "iteration 4276, train loss: 0.003081, validation loss: 0.002167\n",
      "tensor(0.0020)\n",
      "iteration 4277, train loss: 0.003149, validation loss: 0.002038\n",
      "tensor(0.0020)\n",
      "iteration 4278, train loss: 0.003054, validation loss: 0.002027\n",
      "tensor(0.0022)\n",
      "iteration 4279, train loss: 0.003001, validation loss: 0.002215\n",
      "tensor(0.0020)\n",
      "iteration 4280, train loss: 0.003072, validation loss: 0.002038\n",
      "tensor(0.0020)\n",
      "iteration 4281, train loss: 0.002981, validation loss: 0.00203\n",
      "tensor(0.0020)\n",
      "iteration 4282, train loss: 0.003143, validation loss: 0.002003\n",
      "tensor(0.0023)\n",
      "iteration 4283, train loss: 0.003026, validation loss: 0.002267\n",
      "tensor(0.0021)\n",
      "iteration 4284, train loss: 0.00313, validation loss: 0.002099\n",
      "tensor(0.0021)\n",
      "iteration 4285, train loss: 0.002982, validation loss: 0.002062\n",
      "tensor(0.0020)\n",
      "iteration 4286, train loss: 0.003163, validation loss: 0.002048\n",
      "tensor(0.0021)\n",
      "iteration 4287, train loss: 0.002968, validation loss: 0.002055\n",
      "tensor(0.0020)\n",
      "iteration 4288, train loss: 0.003036, validation loss: 0.001992\n",
      "tensor(0.0020)\n",
      "iteration 4289, train loss: 0.003028, validation loss: 0.002049\n",
      "tensor(0.0021)\n",
      "iteration 4290, train loss: 0.003029, validation loss: 0.002058\n",
      "tensor(0.0020)\n",
      "iteration 4291, train loss: 0.002981, validation loss: 0.002042\n",
      "tensor(0.0021)\n",
      "iteration 4292, train loss: 0.003022, validation loss: 0.002071\n",
      "tensor(0.0021)\n",
      "iteration 4293, train loss: 0.00297, validation loss: 0.00211\n",
      "tensor(0.0021)\n",
      "iteration 4294, train loss: 0.002963, validation loss: 0.002058\n",
      "tensor(0.0021)\n",
      "iteration 4295, train loss: 0.002975, validation loss: 0.002104\n",
      "tensor(0.0020)\n",
      "iteration 4296, train loss: 0.002976, validation loss: 0.002004\n",
      "tensor(0.0020)\n",
      "iteration 4297, train loss: 0.002972, validation loss: \u001b[92m0.001977\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4298, train loss: 0.002963, validation loss: 0.002005\n",
      "tensor(0.0021)\n",
      "iteration 4299, train loss: 0.00309, validation loss: 0.002052\n",
      "tensor(0.0020)\n",
      "iteration 4300, train loss: 0.003087, validation loss: 0.002013\n",
      "tensor(0.0020)\n",
      "iteration 4301, train loss: 0.002923, validation loss: 0.002025\n",
      "tensor(0.0021)\n",
      "iteration 4302, train loss: 0.003077, validation loss: 0.002098\n",
      "tensor(0.0020)\n",
      "iteration 4303, train loss: 0.002946, validation loss: 0.00205\n",
      "tensor(0.0020)\n",
      "iteration 4304, train loss: 0.002924, validation loss: 0.002006\n",
      "tensor(0.0020)\n",
      "iteration 4305, train loss: 0.002949, validation loss: 0.001997\n",
      "tensor(0.0020)\n",
      "iteration 4306, train loss: 0.003064, validation loss: 0.002036\n",
      "tensor(0.0020)\n",
      "iteration 4307, train loss: 0.002938, validation loss: 0.002013\n",
      "tensor(0.0020)\n",
      "iteration 4308, train loss: 0.002935, validation loss: 0.001979\n",
      "tensor(0.0020)\n",
      "iteration 4309, train loss: 0.003076, validation loss: 0.00199\n",
      "tensor(0.0021)\n",
      "iteration 4310, train loss: 0.002968, validation loss: 0.002055\n",
      "tensor(0.0021)\n",
      "iteration 4311, train loss: 0.003023, validation loss: 0.00205\n",
      "tensor(0.0021)\n",
      "iteration 4312, train loss: 0.002961, validation loss: 0.002076\n",
      "tensor(0.0021)\n",
      "iteration 4313, train loss: 0.003038, validation loss: 0.0021\n",
      "tensor(0.0021)\n",
      "iteration 4314, train loss: 0.003014, validation loss: 0.002148\n",
      "tensor(0.0020)\n",
      "iteration 4315, train loss: 0.003063, validation loss: 0.001994\n",
      "tensor(0.0020)\n",
      "iteration 4316, train loss: 0.003052, validation loss: 0.001986\n",
      "tensor(0.0021)\n",
      "iteration 4317, train loss: 0.002981, validation loss: 0.002084\n",
      "tensor(0.0021)\n",
      "iteration 4318, train loss: 0.002989, validation loss: 0.002144\n",
      "tensor(0.0020)\n",
      "iteration 4319, train loss: 0.003015, validation loss: 0.002014\n",
      "tensor(0.0020)\n",
      "iteration 4320, train loss: 0.002898, validation loss: 0.002007\n",
      "tensor(0.0020)\n",
      "iteration 4321, train loss: 0.002983, validation loss: 0.00205\n",
      "tensor(0.0021)\n",
      "iteration 4322, train loss: 0.00297, validation loss: 0.002072\n",
      "tensor(0.0020)\n",
      "iteration 4323, train loss: 0.003058, validation loss: 0.002008\n",
      "tensor(0.0020)\n",
      "iteration 4324, train loss: 0.003027, validation loss: 0.002024\n",
      "tensor(0.0021)\n",
      "iteration 4325, train loss: 0.00308, validation loss: 0.002059\n",
      "tensor(0.0020)\n",
      "iteration 4326, train loss: 0.002998, validation loss: 0.002002\n",
      "tensor(0.0020)\n",
      "iteration 4327, train loss: 0.00298, validation loss: 0.001989\n",
      "tensor(0.0020)\n",
      "iteration 4328, train loss: 0.002965, validation loss: 0.002044\n",
      "tensor(0.0021)\n",
      "iteration 4329, train loss: 0.002945, validation loss: 0.002069\n",
      "tensor(0.0020)\n",
      "iteration 4330, train loss: 0.00292, validation loss: 0.001998\n",
      "tensor(0.0020)\n",
      "iteration 4331, train loss: 0.002938, validation loss: 0.001989\n",
      "tensor(0.0020)\n",
      "iteration 4332, train loss: 0.002962, validation loss: 0.002023\n",
      "tensor(0.0020)\n",
      "iteration 4333, train loss: 0.002914, validation loss: 0.002034\n",
      "tensor(0.0020)\n",
      "iteration 4334, train loss: 0.002973, validation loss: 0.002009\n",
      "tensor(0.0020)\n",
      "iteration 4335, train loss: 0.003112, validation loss: \u001b[92m0.00197\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4336, train loss: 0.002904, validation loss: 0.002033\n",
      "tensor(0.0020)\n",
      "iteration 4337, train loss: 0.002981, validation loss: 0.002022\n",
      "tensor(0.0021)\n",
      "iteration 4338, train loss: 0.002972, validation loss: 0.002079\n",
      "tensor(0.0021)\n",
      "iteration 4339, train loss: 0.003027, validation loss: 0.002068\n",
      "tensor(0.0021)\n",
      "iteration 4340, train loss: 0.002904, validation loss: 0.002106\n",
      "tensor(0.0020)\n",
      "iteration 4341, train loss: 0.003005, validation loss: 0.001981\n",
      "tensor(0.0020)\n",
      "iteration 4342, train loss: 0.002929, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4343, train loss: 0.003083, validation loss: 0.002025\n",
      "tensor(0.0021)\n",
      "iteration 4344, train loss: 0.003018, validation loss: 0.002091\n",
      "tensor(0.0020)\n",
      "iteration 4345, train loss: \u001b[92m0.002886\u001b[0m, validation loss: 0.002021\n",
      "tensor(0.0020)\n",
      "iteration 4346, train loss: 0.002983, validation loss: 0.001978\n",
      "tensor(0.0020)\n",
      "iteration 4347, train loss: 0.003013, validation loss: 0.002023\n",
      "tensor(0.0021)\n",
      "iteration 4348, train loss: 0.00299, validation loss: 0.002149\n",
      "tensor(0.0020)\n",
      "iteration 4349, train loss: 0.003081, validation loss: 0.001993\n",
      "tensor(0.0020)\n",
      "iteration 4350, train loss: 0.003013, validation loss: 0.002028\n",
      "tensor(0.0021)\n",
      "iteration 4351, train loss: 0.002976, validation loss: 0.002099\n",
      "tensor(0.0021)\n",
      "iteration 4352, train loss: 0.003005, validation loss: 0.002132\n",
      "tensor(0.0020)\n",
      "iteration 4353, train loss: 0.003063, validation loss: 0.00198\n",
      "tensor(0.0020)\n",
      "iteration 4354, train loss: 0.003052, validation loss: 0.002021\n",
      "tensor(0.0021)\n",
      "iteration 4355, train loss: 0.002976, validation loss: 0.002119\n",
      "tensor(0.0021)\n",
      "iteration 4356, train loss: 0.00302, validation loss: 0.002122\n",
      "tensor(0.0021)\n",
      "iteration 4357, train loss: 0.002921, validation loss: 0.002109\n",
      "tensor(0.0020)\n",
      "iteration 4358, train loss: 0.003009, validation loss: 0.001989\n",
      "tensor(0.0020)\n",
      "iteration 4359, train loss: 0.002895, validation loss: 0.002031\n",
      "tensor(0.0021)\n",
      "iteration 4360, train loss: 0.002966, validation loss: 0.002098\n",
      "tensor(0.0021)\n",
      "iteration 4361, train loss: 0.003071, validation loss: 0.002079\n",
      "tensor(0.0021)\n",
      "iteration 4362, train loss: 0.002941, validation loss: 0.00209\n",
      "tensor(0.0020)\n",
      "iteration 4363, train loss: 0.00303, validation loss: 0.002031\n",
      "tensor(0.0020)\n",
      "iteration 4364, train loss: 0.002973, validation loss: 0.001985\n",
      "tensor(0.0020)\n",
      "iteration 4365, train loss: 0.002952, validation loss: \u001b[92m0.001962\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4366, train loss: 0.002972, validation loss: 0.001981\n",
      "tensor(0.0020)\n",
      "iteration 4367, train loss: 0.002892, validation loss: 0.002022\n",
      "tensor(0.0020)\n",
      "iteration 4368, train loss: 0.002958, validation loss: 0.00201\n",
      "tensor(0.0021)\n",
      "iteration 4369, train loss: 0.003019, validation loss: 0.002052\n",
      "tensor(0.0020)\n",
      "iteration 4370, train loss: 0.002934, validation loss: 0.002026\n",
      "tensor(0.0020)\n",
      "iteration 4371, train loss: 0.003011, validation loss: 0.00201\n",
      "tensor(0.0021)\n",
      "iteration 4372, train loss: 0.003039, validation loss: 0.002082\n",
      "tensor(0.0020)\n",
      "iteration 4373, train loss: 0.002981, validation loss: 0.002038\n",
      "tensor(0.0020)\n",
      "iteration 4374, train loss: 0.00299, validation loss: 0.001992\n",
      "tensor(0.0020)\n",
      "iteration 4375, train loss: 0.002959, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4376, train loss: \u001b[92m0.002873\u001b[0m, validation loss: 0.002005\n",
      "tensor(0.0021)\n",
      "iteration 4377, train loss: 0.003027, validation loss: 0.002077\n",
      "tensor(0.0020)\n",
      "iteration 4378, train loss: 0.002993, validation loss: 0.002023\n",
      "tensor(0.0020)\n",
      "iteration 4379, train loss: 0.002995, validation loss: \u001b[92m0.001956\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4380, train loss: 0.002971, validation loss: 0.002007\n",
      "tensor(0.0021)\n",
      "iteration 4381, train loss: 0.002961, validation loss: 0.002081\n",
      "tensor(0.0020)\n",
      "iteration 4382, train loss: 0.002974, validation loss: 0.002033\n",
      "tensor(0.0020)\n",
      "iteration 4383, train loss: 0.002973, validation loss: 0.002012\n",
      "tensor(0.0021)\n",
      "iteration 4384, train loss: 0.003071, validation loss: 0.002075\n",
      "tensor(0.0020)\n",
      "iteration 4385, train loss: 0.00303, validation loss: 0.002014\n",
      "tensor(0.0020)\n",
      "iteration 4386, train loss: 0.002962, validation loss: 0.001979\n",
      "tensor(0.0020)\n",
      "iteration 4387, train loss: 0.003017, validation loss: 0.001969\n",
      "tensor(0.0021)\n",
      "iteration 4388, train loss: 0.002926, validation loss: 0.002078\n",
      "tensor(0.0020)\n",
      "iteration 4389, train loss: 0.002983, validation loss: 0.002049\n",
      "tensor(0.0020)\n",
      "iteration 4390, train loss: 0.002932, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4391, train loss: 0.002918, validation loss: 0.002015\n",
      "tensor(0.0020)\n",
      "iteration 4392, train loss: 0.003006, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4393, train loss: 0.002952, validation loss: 0.001998\n",
      "tensor(0.0020)\n",
      "iteration 4394, train loss: 0.003038, validation loss: 0.001989\n",
      "tensor(0.0021)\n",
      "iteration 4395, train loss: 0.002988, validation loss: 0.002091\n",
      "tensor(0.0020)\n",
      "iteration 4396, train loss: 0.002997, validation loss: 0.001979\n",
      "tensor(0.0020)\n",
      "iteration 4397, train loss: 0.002928, validation loss: 0.001964\n",
      "tensor(0.0020)\n",
      "iteration 4398, train loss: 0.00294, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4399, train loss: 0.002967, validation loss: 0.00196\n",
      "tensor(0.0020)\n",
      "iteration 4400, train loss: 0.002988, validation loss: 0.002048\n",
      "tensor(0.0021)\n",
      "iteration 4401, train loss: 0.002977, validation loss: 0.002121\n",
      "tensor(0.0020)\n",
      "iteration 4402, train loss: 0.003023, validation loss: 0.002016\n",
      "tensor(0.0020)\n",
      "iteration 4403, train loss: 0.003018, validation loss: 0.002004\n",
      "tensor(0.0020)\n",
      "iteration 4404, train loss: 0.00302, validation loss: 0.002027\n",
      "tensor(0.0020)\n",
      "iteration 4405, train loss: 0.002955, validation loss: 0.002024\n",
      "tensor(0.0020)\n",
      "iteration 4406, train loss: 0.002983, validation loss: 0.001987\n",
      "tensor(0.0020)\n",
      "iteration 4407, train loss: 0.003034, validation loss: 0.002009\n",
      "tensor(0.0022)\n",
      "iteration 4408, train loss: 0.002991, validation loss: 0.002166\n",
      "tensor(0.0020)\n",
      "iteration 4409, train loss: 0.002966, validation loss: 0.002004\n",
      "tensor(0.0020)\n",
      "iteration 4410, train loss: 0.002925, validation loss: 0.002029\n",
      "tensor(0.0021)\n",
      "iteration 4411, train loss: 0.003065, validation loss: 0.002073\n",
      "tensor(0.0020)\n",
      "iteration 4412, train loss: 0.00294, validation loss: 0.002039\n",
      "tensor(0.0020)\n",
      "iteration 4413, train loss: 0.003011, validation loss: 0.001964\n",
      "tensor(0.0020)\n",
      "iteration 4414, train loss: 0.003025, validation loss: 0.001969\n",
      "tensor(0.0021)\n",
      "iteration 4415, train loss: \u001b[92m0.002855\u001b[0m, validation loss: 0.002062\n",
      "tensor(0.0021)\n",
      "iteration 4416, train loss: 0.003001, validation loss: 0.002079\n",
      "tensor(0.0020)\n",
      "iteration 4417, train loss: 0.003048, validation loss: 0.001967\n",
      "tensor(0.0020)\n",
      "iteration 4418, train loss: 0.002917, validation loss: \u001b[92m0.001953\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4419, train loss: 0.002956, validation loss: 0.001983\n",
      "tensor(0.0020)\n",
      "iteration 4420, train loss: 0.003008, validation loss: 0.00199\n",
      "tensor(0.0020)\n",
      "iteration 4421, train loss: 0.002956, validation loss: 0.002047\n",
      "tensor(0.0020)\n",
      "iteration 4422, train loss: 0.002984, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4423, train loss: 0.002922, validation loss: 0.001987\n",
      "tensor(0.0020)\n",
      "iteration 4424, train loss: 0.002907, validation loss: 0.002037\n",
      "tensor(0.0020)\n",
      "iteration 4425, train loss: 0.003007, validation loss: 0.00199\n",
      "tensor(0.0021)\n",
      "iteration 4426, train loss: 0.002909, validation loss: 0.002096\n",
      "tensor(0.0021)\n",
      "iteration 4427, train loss: 0.003037, validation loss: 0.002093\n",
      "tensor(0.0020)\n",
      "iteration 4428, train loss: 0.002981, validation loss: 0.001974\n",
      "tensor(0.0020)\n",
      "iteration 4429, train loss: 0.002968, validation loss: 0.001958\n",
      "tensor(0.0020)\n",
      "iteration 4430, train loss: 0.003005, validation loss: 0.001974\n",
      "tensor(0.0021)\n",
      "iteration 4431, train loss: 0.002976, validation loss: 0.002112\n",
      "tensor(0.0020)\n",
      "iteration 4432, train loss: 0.003051, validation loss: 0.001971\n",
      "tensor(0.0020)\n",
      "iteration 4433, train loss: 0.002862, validation loss: 0.00196\n",
      "tensor(0.0020)\n",
      "iteration 4434, train loss: 0.003005, validation loss: 0.001967\n",
      "tensor(0.0020)\n",
      "iteration 4435, train loss: 0.002939, validation loss: 0.002017\n",
      "tensor(0.0021)\n",
      "iteration 4436, train loss: 0.002883, validation loss: 0.002123\n",
      "tensor(0.0020)\n",
      "iteration 4437, train loss: 0.003006, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4438, train loss: 0.002963, validation loss: 0.001971\n",
      "tensor(0.0020)\n",
      "iteration 4439, train loss: 0.002918, validation loss: 0.001979\n",
      "tensor(0.0020)\n",
      "iteration 4440, train loss: 0.002938, validation loss: 0.002034\n",
      "tensor(0.0020)\n",
      "iteration 4441, train loss: 0.002962, validation loss: 0.002021\n",
      "tensor(0.0020)\n",
      "iteration 4442, train loss: 0.002881, validation loss: 0.001963\n",
      "tensor(0.0020)\n",
      "iteration 4443, train loss: 0.003055, validation loss: \u001b[92m0.001952\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4444, train loss: 0.003013, validation loss: 0.001981\n",
      "tensor(0.0020)\n",
      "iteration 4445, train loss: 0.002932, validation loss: 0.001977\n",
      "tensor(0.0020)\n",
      "iteration 4446, train loss: 0.00288, validation loss: 0.001982\n",
      "tensor(0.0020)\n",
      "iteration 4447, train loss: 0.002934, validation loss: 0.002035\n",
      "tensor(0.0020)\n",
      "iteration 4448, train loss: 0.002991, validation loss: 0.001956\n",
      "tensor(0.0019)\n",
      "iteration 4449, train loss: 0.002921, validation loss: \u001b[92m0.001936\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4450, train loss: 0.002929, validation loss: 0.001992\n",
      "tensor(0.0021)\n",
      "iteration 4451, train loss: \u001b[92m0.002847\u001b[0m, validation loss: 0.002064\n",
      "tensor(0.0020)\n",
      "iteration 4452, train loss: 0.00296, validation loss: 0.002048\n",
      "tensor(0.0020)\n",
      "iteration 4453, train loss: 0.0029, validation loss: 0.002047\n",
      "tensor(0.0020)\n",
      "iteration 4454, train loss: 0.002953, validation loss: 0.001964\n",
      "tensor(0.0021)\n",
      "iteration 4455, train loss: 0.00289, validation loss: 0.002074\n",
      "tensor(0.0020)\n",
      "iteration 4456, train loss: 0.002938, validation loss: 0.001994\n",
      "tensor(0.0020)\n",
      "iteration 4457, train loss: 0.002943, validation loss: 0.00201\n",
      "tensor(0.0021)\n",
      "iteration 4458, train loss: 0.003039, validation loss: 0.002124\n",
      "tensor(0.0020)\n",
      "iteration 4459, train loss: 0.002936, validation loss: 0.002006\n",
      "tensor(0.0019)\n",
      "iteration 4460, train loss: 0.002914, validation loss: 0.001937\n",
      "tensor(0.0020)\n",
      "iteration 4461, train loss: 0.00292, validation loss: 0.001959\n",
      "tensor(0.0021)\n",
      "iteration 4462, train loss: 0.00304, validation loss: 0.002083\n",
      "tensor(0.0020)\n",
      "iteration 4463, train loss: 0.003013, validation loss: 0.002015\n",
      "tensor(0.0019)\n",
      "iteration 4464, train loss: 0.003021, validation loss: 0.001948\n",
      "tensor(0.0020)\n",
      "iteration 4465, train loss: 0.002933, validation loss: 0.001984\n",
      "tensor(0.0021)\n",
      "iteration 4466, train loss: 0.002951, validation loss: 0.002125\n",
      "tensor(0.0020)\n",
      "iteration 4467, train loss: 0.002989, validation loss: 0.001979\n",
      "tensor(0.0020)\n",
      "iteration 4468, train loss: 0.003077, validation loss: 0.002032\n",
      "tensor(0.0020)\n",
      "iteration 4469, train loss: 0.002957, validation loss: 0.002001\n",
      "tensor(0.0020)\n",
      "iteration 4470, train loss: 0.002936, validation loss: 0.002017\n",
      "tensor(0.0020)\n",
      "iteration 4471, train loss: 0.002991, validation loss: 0.001996\n",
      "tensor(0.0020)\n",
      "iteration 4472, train loss: 0.002882, validation loss: 0.002026\n",
      "tensor(0.0019)\n",
      "iteration 4473, train loss: 0.002855, validation loss: 0.00195\n",
      "tensor(0.0020)\n",
      "iteration 4474, train loss: 0.002857, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4475, train loss: 0.002931, validation loss: 0.001991\n",
      "tensor(0.0020)\n",
      "iteration 4476, train loss: 0.002949, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4477, train loss: 0.002882, validation loss: 0.001962\n",
      "tensor(0.0020)\n",
      "iteration 4478, train loss: 0.002927, validation loss: 0.002005\n",
      "tensor(0.0020)\n",
      "iteration 4479, train loss: 0.002948, validation loss: 0.001974\n",
      "tensor(0.0019)\n",
      "iteration 4480, train loss: 0.003026, validation loss: 0.001937\n",
      "tensor(0.0020)\n",
      "iteration 4481, train loss: 0.002943, validation loss: 0.001957\n",
      "tensor(0.0020)\n",
      "iteration 4482, train loss: 0.002926, validation loss: 0.001985\n",
      "tensor(0.0019)\n",
      "iteration 4483, train loss: 0.002871, validation loss: 0.001937\n",
      "tensor(0.0020)\n",
      "iteration 4484, train loss: 0.002942, validation loss: 0.001959\n",
      "tensor(0.0020)\n",
      "iteration 4485, train loss: 0.002983, validation loss: 0.002034\n",
      "tensor(0.0020)\n",
      "iteration 4486, train loss: 0.003024, validation loss: 0.001992\n",
      "tensor(0.0020)\n",
      "iteration 4487, train loss: 0.002926, validation loss: 0.001961\n",
      "tensor(0.0020)\n",
      "iteration 4488, train loss: 0.002979, validation loss: 0.001954\n",
      "tensor(0.0021)\n",
      "iteration 4489, train loss: 0.002931, validation loss: 0.002077\n",
      "tensor(0.0021)\n",
      "iteration 4490, train loss: 0.00295, validation loss: 0.002078\n",
      "tensor(0.0020)\n",
      "iteration 4491, train loss: 0.002948, validation loss: 0.001962\n",
      "tensor(0.0019)\n",
      "iteration 4492, train loss: 0.002936, validation loss: 0.001948\n",
      "tensor(0.0023)\n",
      "iteration 4493, train loss: 0.00297, validation loss: 0.002254\n",
      "tensor(0.0020)\n",
      "iteration 4494, train loss: 0.003102, validation loss: 0.001969\n",
      "tensor(0.0020)\n",
      "iteration 4495, train loss: 0.002892, validation loss: 0.002044\n",
      "tensor(0.0019)\n",
      "iteration 4496, train loss: 0.00315, validation loss: 0.001942\n",
      "tensor(0.0023)\n",
      "iteration 4497, train loss: 0.002871, validation loss: 0.002261\n",
      "tensor(0.0020)\n",
      "iteration 4498, train loss: 0.00308, validation loss: 0.001979\n",
      "tensor(0.0021)\n",
      "iteration 4499, train loss: 0.002962, validation loss: 0.002056\n",
      "tensor(0.0020)\n",
      "iteration 4500, train loss: 0.003082, validation loss: 0.001993\n",
      "tensor(0.0022)\n",
      "iteration 4501, train loss: 0.002935, validation loss: 0.002169\n",
      "tensor(0.0020)\n",
      "iteration 4502, train loss: 0.002944, validation loss: 0.001973\n",
      "tensor(0.0020)\n",
      "iteration 4503, train loss: 0.002927, validation loss: 0.002016\n",
      "tensor(0.0020)\n",
      "iteration 4504, train loss: 0.003162, validation loss: 0.001984\n",
      "tensor(0.0022)\n",
      "iteration 4505, train loss: 0.003009, validation loss: 0.002244\n",
      "tensor(0.0020)\n",
      "iteration 4506, train loss: 0.003132, validation loss: 0.001952\n",
      "tensor(0.0020)\n",
      "iteration 4507, train loss: 0.003005, validation loss: 0.001997\n",
      "tensor(0.0021)\n",
      "iteration 4508, train loss: 0.003085, validation loss: 0.002079\n",
      "tensor(0.0021)\n",
      "iteration 4509, train loss: 0.002979, validation loss: 0.002076\n",
      "tensor(0.0019)\n",
      "iteration 4510, train loss: 0.002991, validation loss: 0.00195\n",
      "tensor(0.0019)\n",
      "iteration 4511, train loss: 0.002978, validation loss: 0.00195\n",
      "tensor(0.0021)\n",
      "iteration 4512, train loss: 0.00294, validation loss: 0.00209\n",
      "tensor(0.0020)\n",
      "iteration 4513, train loss: 0.003031, validation loss: 0.001998\n",
      "tensor(0.0020)\n",
      "iteration 4514, train loss: 0.002955, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4515, train loss: 0.00298, validation loss: 0.001986\n",
      "tensor(0.0020)\n",
      "iteration 4516, train loss: 0.002876, validation loss: 0.002013\n",
      "tensor(0.0019)\n",
      "iteration 4517, train loss: 0.002943, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 4518, train loss: \u001b[92m0.002843\u001b[0m, validation loss: \u001b[92m0.001929\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4519, train loss: 0.002958, validation loss: 0.001996\n",
      "tensor(0.0020)\n",
      "iteration 4520, train loss: 0.002866, validation loss: 0.002028\n",
      "tensor(0.0020)\n",
      "iteration 4521, train loss: 0.002998, validation loss: 0.001954\n",
      "tensor(0.0019)\n",
      "iteration 4522, train loss: 0.002883, validation loss: \u001b[92m0.001929\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4523, train loss: 0.002912, validation loss: 0.002005\n",
      "tensor(0.0020)\n",
      "iteration 4524, train loss: 0.002981, validation loss: 0.001969\n",
      "tensor(0.0020)\n",
      "iteration 4525, train loss: 0.002958, validation loss: 0.002\n",
      "tensor(0.0020)\n",
      "iteration 4526, train loss: 0.002983, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4527, train loss: 0.002855, validation loss: 0.001962\n",
      "tensor(0.0019)\n",
      "iteration 4528, train loss: 0.002889, validation loss: 0.001938\n",
      "tensor(0.0019)\n",
      "iteration 4529, train loss: 0.003029, validation loss: \u001b[92m0.001926\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4530, train loss: 0.002962, validation loss: 0.001958\n",
      "tensor(0.0020)\n",
      "iteration 4531, train loss: 0.002884, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4532, train loss: 0.002882, validation loss: 0.00201\n",
      "tensor(0.0020)\n",
      "iteration 4533, train loss: 0.002923, validation loss: 0.001994\n",
      "tensor(0.0020)\n",
      "iteration 4534, train loss: 0.002848, validation loss: 0.001971\n",
      "tensor(0.0020)\n",
      "iteration 4535, train loss: 0.003009, validation loss: 0.002036\n",
      "tensor(0.0020)\n",
      "iteration 4536, train loss: 0.002866, validation loss: 0.001952\n",
      "tensor(0.0020)\n",
      "iteration 4537, train loss: 0.003003, validation loss: 0.00201\n",
      "tensor(0.0020)\n",
      "iteration 4538, train loss: 0.002935, validation loss: 0.002039\n",
      "tensor(0.0020)\n",
      "iteration 4539, train loss: 0.003, validation loss: 0.001992\n",
      "tensor(0.0020)\n",
      "iteration 4540, train loss: 0.002987, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4541, train loss: 0.002998, validation loss: 0.00195\n",
      "tensor(0.0020)\n",
      "iteration 4542, train loss: 0.002934, validation loss: 0.001959\n",
      "tensor(0.0020)\n",
      "iteration 4543, train loss: 0.002943, validation loss: 0.001974\n",
      "tensor(0.0020)\n",
      "iteration 4544, train loss: 0.002881, validation loss: 0.001963\n",
      "tensor(0.0020)\n",
      "iteration 4545, train loss: 0.002999, validation loss: 0.00196\n",
      "tensor(0.0020)\n",
      "iteration 4546, train loss: 0.002959, validation loss: 0.002036\n",
      "tensor(0.0019)\n",
      "iteration 4547, train loss: 0.002901, validation loss: 0.001928\n",
      "tensor(0.0019)\n",
      "iteration 4548, train loss: 0.002871, validation loss: 0.00194\n",
      "tensor(0.0019)\n",
      "iteration 4549, train loss: 0.002937, validation loss: 0.001945\n",
      "tensor(0.0021)\n",
      "iteration 4550, train loss: 0.002896, validation loss: 0.002074\n",
      "tensor(0.0019)\n",
      "iteration 4551, train loss: 0.002945, validation loss: 0.001932\n",
      "tensor(0.0019)\n",
      "iteration 4552, train loss: 0.002912, validation loss: 0.001938\n",
      "tensor(0.0019)\n",
      "iteration 4553, train loss: 0.002928, validation loss: \u001b[92m0.001917\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4554, train loss: 0.002938, validation loss: 0.002119\n",
      "tensor(0.0020)\n",
      "iteration 4555, train loss: 0.002943, validation loss: 0.001964\n",
      "tensor(0.0020)\n",
      "iteration 4556, train loss: 0.002911, validation loss: 0.00199\n",
      "tensor(0.0020)\n",
      "iteration 4557, train loss: 0.002941, validation loss: 0.001961\n",
      "tensor(0.0021)\n",
      "iteration 4558, train loss: 0.002917, validation loss: 0.002069\n",
      "tensor(0.0020)\n",
      "iteration 4559, train loss: 0.002903, validation loss: 0.00203\n",
      "tensor(0.0020)\n",
      "iteration 4560, train loss: 0.002945, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4561, train loss: 0.002987, validation loss: 0.001956\n",
      "tensor(0.0020)\n",
      "iteration 4562, train loss: 0.002915, validation loss: 0.001996\n",
      "tensor(0.0019)\n",
      "iteration 4563, train loss: 0.00299, validation loss: \u001b[92m0.001911\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4564, train loss: 0.00291, validation loss: 0.001945\n",
      "tensor(0.0020)\n",
      "iteration 4565, train loss: \u001b[92m0.002834\u001b[0m, validation loss: 0.001978\n",
      "tensor(0.0020)\n",
      "iteration 4566, train loss: 0.002912, validation loss: 0.001967\n",
      "tensor(0.0020)\n",
      "iteration 4567, train loss: 0.002921, validation loss: 0.001972\n",
      "tensor(0.0020)\n",
      "iteration 4568, train loss: 0.002941, validation loss: 0.00201\n",
      "tensor(0.0020)\n",
      "iteration 4569, train loss: 0.002899, validation loss: 0.001973\n",
      "tensor(0.0019)\n",
      "iteration 4570, train loss: 0.00293, validation loss: 0.001931\n",
      "tensor(0.0021)\n",
      "iteration 4571, train loss: 0.002881, validation loss: 0.00206\n",
      "tensor(0.0019)\n",
      "iteration 4572, train loss: 0.003056, validation loss: 0.00192\n",
      "tensor(0.0019)\n",
      "iteration 4573, train loss: 0.002958, validation loss: 0.001927\n",
      "tensor(0.0023)\n",
      "iteration 4574, train loss: 0.002992, validation loss: 0.002268\n",
      "tensor(0.0020)\n",
      "iteration 4575, train loss: 0.003036, validation loss: 0.00204\n",
      "tensor(0.0021)\n",
      "iteration 4576, train loss: 0.002922, validation loss: 0.002072\n",
      "tensor(0.0019)\n",
      "iteration 4577, train loss: 0.003243, validation loss: 0.001915\n",
      "tensor(0.0023)\n",
      "iteration 4578, train loss: 0.002987, validation loss: 0.002347\n",
      "tensor(0.0020)\n",
      "iteration 4579, train loss: 0.00317, validation loss: 0.001966\n",
      "tensor(0.0021)\n",
      "iteration 4580, train loss: 0.002921, validation loss: 0.002069\n",
      "tensor(0.0021)\n",
      "iteration 4581, train loss: 0.003193, validation loss: 0.002071\n",
      "tensor(0.0022)\n",
      "iteration 4582, train loss: 0.00297, validation loss: 0.00217\n",
      "tensor(0.0020)\n",
      "iteration 4583, train loss: 0.003139, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4584, train loss: 0.002939, validation loss: 0.001986\n",
      "tensor(0.0021)\n",
      "iteration 4585, train loss: 0.003042, validation loss: 0.002141\n",
      "tensor(0.0021)\n",
      "iteration 4586, train loss: 0.002986, validation loss: 0.002071\n",
      "tensor(0.0020)\n",
      "iteration 4587, train loss: 0.002977, validation loss: 0.002001\n",
      "tensor(0.0019)\n",
      "iteration 4588, train loss: 0.003055, validation loss: 0.001914\n",
      "tensor(0.0022)\n",
      "iteration 4589, train loss: 0.002987, validation loss: 0.002153\n",
      "tensor(0.0021)\n",
      "iteration 4590, train loss: 0.003002, validation loss: 0.002094\n",
      "tensor(0.0020)\n",
      "iteration 4591, train loss: 0.003, validation loss: 0.001997\n",
      "tensor(0.0019)\n",
      "iteration 4592, train loss: 0.002991, validation loss: 0.001912\n",
      "tensor(0.0021)\n",
      "iteration 4593, train loss: 0.002959, validation loss: 0.002084\n",
      "tensor(0.0019)\n",
      "iteration 4594, train loss: 0.003048, validation loss: 0.001939\n",
      "tensor(0.0020)\n",
      "iteration 4595, train loss: 0.002923, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4596, train loss: 0.002971, validation loss: 0.001958\n",
      "tensor(0.0020)\n",
      "iteration 4597, train loss: 0.002882, validation loss: 0.002033\n",
      "tensor(0.0019)\n",
      "iteration 4598, train loss: 0.002979, validation loss: 0.001914\n",
      "tensor(0.0019)\n",
      "iteration 4599, train loss: 0.002998, validation loss: 0.001925\n",
      "tensor(0.0020)\n",
      "iteration 4600, train loss: 0.002883, validation loss: 0.002005\n",
      "tensor(0.0021)\n",
      "iteration 4601, train loss: 0.00293, validation loss: 0.002119\n",
      "tensor(0.0020)\n",
      "iteration 4602, train loss: 0.003027, validation loss: 0.001983\n",
      "tensor(0.0019)\n",
      "iteration 4603, train loss: 0.00294, validation loss: 0.001917\n",
      "tensor(0.0019)\n",
      "iteration 4604, train loss: 0.002882, validation loss: 0.001917\n",
      "tensor(0.0020)\n",
      "iteration 4605, train loss: 0.003002, validation loss: 0.001974\n",
      "tensor(0.0023)\n",
      "iteration 4606, train loss: 0.002836, validation loss: 0.002316\n",
      "tensor(0.0020)\n",
      "iteration 4607, train loss: 0.003166, validation loss: 0.00195\n",
      "tensor(0.0020)\n",
      "iteration 4608, train loss: 0.00291, validation loss: 0.001976\n",
      "tensor(0.0020)\n",
      "iteration 4609, train loss: 0.00304, validation loss: 0.001969\n",
      "tensor(0.0022)\n",
      "iteration 4610, train loss: 0.002864, validation loss: 0.002239\n",
      "tensor(0.0020)\n",
      "iteration 4611, train loss: 0.00305, validation loss: 0.001996\n",
      "tensor(0.0020)\n",
      "iteration 4612, train loss: 0.002935, validation loss: 0.001984\n",
      "tensor(0.0020)\n",
      "iteration 4613, train loss: 0.003083, validation loss: 0.002036\n",
      "tensor(0.0022)\n",
      "iteration 4614, train loss: 0.002885, validation loss: 0.002234\n",
      "tensor(0.0020)\n",
      "iteration 4615, train loss: 0.003126, validation loss: 0.001957\n",
      "tensor(0.0020)\n",
      "iteration 4616, train loss: 0.002942, validation loss: 0.002022\n",
      "tensor(0.0021)\n",
      "iteration 4617, train loss: 0.003114, validation loss: 0.002068\n",
      "tensor(0.0022)\n",
      "iteration 4618, train loss: 0.002957, validation loss: 0.002239\n",
      "tensor(0.0020)\n",
      "iteration 4619, train loss: 0.003194, validation loss: 0.002001\n",
      "tensor(0.0020)\n",
      "iteration 4620, train loss: 0.003039, validation loss: 0.001998\n",
      "tensor(0.0021)\n",
      "iteration 4621, train loss: 0.00299, validation loss: 0.002128\n",
      "tensor(0.0022)\n",
      "iteration 4622, train loss: 0.003016, validation loss: 0.002206\n",
      "tensor(0.0020)\n",
      "iteration 4623, train loss: 0.003039, validation loss: 0.001968\n",
      "tensor(0.0020)\n",
      "iteration 4624, train loss: 0.002925, validation loss: 0.002001\n",
      "tensor(0.0019)\n",
      "iteration 4625, train loss: 0.003012, validation loss: 0.001939\n",
      "tensor(0.0023)\n",
      "iteration 4626, train loss: 0.002925, validation loss: 0.00227\n",
      "tensor(0.0020)\n",
      "iteration 4627, train loss: 0.003199, validation loss: 0.001964\n",
      "tensor(0.0021)\n",
      "iteration 4628, train loss: 0.002914, validation loss: 0.002081\n",
      "tensor(0.0019)\n",
      "iteration 4629, train loss: 0.003084, validation loss: 0.00195\n",
      "tensor(0.0023)\n",
      "iteration 4630, train loss: 0.002942, validation loss: 0.002317\n",
      "tensor(0.0020)\n",
      "iteration 4631, train loss: 0.003172, validation loss: 0.001963\n",
      "tensor(0.0020)\n",
      "iteration 4632, train loss: 0.002875, validation loss: 0.002002\n",
      "tensor(0.0019)\n",
      "iteration 4633, train loss: 0.002926, validation loss: \u001b[92m0.001908\u001b[0m\n",
      "tensor(0.0022)\n",
      "iteration 4634, train loss: 0.002915, validation loss: 0.002191\n",
      "tensor(0.0019)\n",
      "iteration 4635, train loss: 0.003192, validation loss: \u001b[92m0.001904\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4636, train loss: 0.002878, validation loss: 0.001966\n",
      "tensor(0.0020)\n",
      "iteration 4637, train loss: 0.003013, validation loss: 0.00199\n",
      "tensor(0.0021)\n",
      "iteration 4638, train loss: 0.002952, validation loss: 0.002145\n",
      "tensor(0.0019)\n",
      "iteration 4639, train loss: 0.002994, validation loss: 0.001911\n",
      "tensor(0.0019)\n",
      "iteration 4640, train loss: 0.00293, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 4641, train loss: 0.002885, validation loss: \u001b[92m0.001898\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4642, train loss: 0.00289, validation loss: 0.002065\n",
      "tensor(0.0020)\n",
      "iteration 4643, train loss: 0.003058, validation loss: 0.001964\n",
      "tensor(0.0019)\n",
      "iteration 4644, train loss: 0.002854, validation loss: 0.001948\n",
      "tensor(0.0019)\n",
      "iteration 4645, train loss: 0.003002, validation loss: 0.001901\n",
      "tensor(0.0020)\n",
      "iteration 4646, train loss: 0.002922, validation loss: 0.002022\n",
      "tensor(0.0019)\n",
      "iteration 4647, train loss: 0.002883, validation loss: 0.001924\n",
      "tensor(0.0020)\n",
      "iteration 4648, train loss: 0.002878, validation loss: 0.001954\n",
      "tensor(0.0019)\n",
      "iteration 4649, train loss: 0.002921, validation loss: 0.001942\n",
      "tensor(0.0020)\n",
      "iteration 4650, train loss: 0.00296, validation loss: 0.00196\n",
      "tensor(0.0019)\n",
      "iteration 4651, train loss: 0.0029, validation loss: 0.001902\n",
      "tensor(0.0020)\n",
      "iteration 4652, train loss: 0.002848, validation loss: 0.00197\n",
      "tensor(0.0019)\n",
      "iteration 4653, train loss: 0.002997, validation loss: 0.00193\n",
      "tensor(0.0021)\n",
      "iteration 4654, train loss: 0.002863, validation loss: 0.002117\n",
      "tensor(0.0019)\n",
      "iteration 4655, train loss: 0.002979, validation loss: 0.001923\n",
      "tensor(0.0019)\n",
      "iteration 4656, train loss: 0.002899, validation loss: 0.001916\n",
      "tensor(0.0019)\n",
      "iteration 4657, train loss: 0.003012, validation loss: 0.001916\n",
      "tensor(0.0020)\n",
      "iteration 4658, train loss: \u001b[92m0.002797\u001b[0m, validation loss: 0.002005\n",
      "tensor(0.0019)\n",
      "iteration 4659, train loss: 0.00293, validation loss: 0.001949\n",
      "tensor(0.0019)\n",
      "iteration 4660, train loss: 0.002974, validation loss: 0.00192\n",
      "tensor(0.0019)\n",
      "iteration 4661, train loss: 0.002869, validation loss: 0.001941\n",
      "tensor(0.0020)\n",
      "iteration 4662, train loss: 0.002897, validation loss: 0.002013\n",
      "tensor(0.0020)\n",
      "iteration 4663, train loss: 0.002843, validation loss: 0.001971\n",
      "tensor(0.0019)\n",
      "iteration 4664, train loss: 0.00295, validation loss: \u001b[92m0.001896\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4665, train loss: 0.002942, validation loss: \u001b[92m0.001895\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4666, train loss: 0.003018, validation loss: 0.001993\n",
      "tensor(0.0020)\n",
      "iteration 4667, train loss: 0.00296, validation loss: 0.001976\n",
      "tensor(0.0020)\n",
      "iteration 4668, train loss: 0.002942, validation loss: 0.00196\n",
      "tensor(0.0019)\n",
      "iteration 4669, train loss: 0.00288, validation loss: 0.001911\n",
      "tensor(0.0020)\n",
      "iteration 4670, train loss: 0.002871, validation loss: 0.001967\n",
      "tensor(0.0021)\n",
      "iteration 4671, train loss: 0.002918, validation loss: 0.002064\n",
      "tensor(0.0020)\n",
      "iteration 4672, train loss: 0.002953, validation loss: 0.001982\n",
      "tensor(0.0020)\n",
      "iteration 4673, train loss: 0.002902, validation loss: 0.001962\n",
      "tensor(0.0020)\n",
      "iteration 4674, train loss: 0.00291, validation loss: 0.002034\n",
      "tensor(0.0020)\n",
      "iteration 4675, train loss: 0.002952, validation loss: 0.001968\n",
      "tensor(0.0019)\n",
      "iteration 4676, train loss: 0.002945, validation loss: 0.001899\n",
      "tensor(0.0019)\n",
      "iteration 4677, train loss: 0.002913, validation loss: 0.001925\n",
      "tensor(0.0019)\n",
      "iteration 4678, train loss: 0.002944, validation loss: 0.001944\n",
      "tensor(0.0020)\n",
      "iteration 4679, train loss: 0.002885, validation loss: 0.002017\n",
      "tensor(0.0019)\n",
      "iteration 4680, train loss: 0.002985, validation loss: 0.001948\n",
      "tensor(0.0019)\n",
      "iteration 4681, train loss: 0.002915, validation loss: 0.001926\n",
      "tensor(0.0019)\n",
      "iteration 4682, train loss: 0.002891, validation loss: 0.001949\n",
      "tensor(0.0019)\n",
      "iteration 4683, train loss: 0.002932, validation loss: 0.001947\n",
      "tensor(0.0019)\n",
      "iteration 4684, train loss: 0.002926, validation loss: 0.001902\n",
      "tensor(0.0019)\n",
      "iteration 4685, train loss: 0.002908, validation loss: 0.001936\n",
      "tensor(0.0019)\n",
      "iteration 4686, train loss: 0.002904, validation loss: 0.001944\n",
      "tensor(0.0020)\n",
      "iteration 4687, train loss: 0.00296, validation loss: 0.001964\n",
      "tensor(0.0019)\n",
      "iteration 4688, train loss: 0.002869, validation loss: 0.001941\n",
      "tensor(0.0019)\n",
      "iteration 4689, train loss: 0.002929, validation loss: 0.001927\n",
      "tensor(0.0020)\n",
      "iteration 4690, train loss: 0.002851, validation loss: 0.001995\n",
      "tensor(0.0020)\n",
      "iteration 4691, train loss: 0.002866, validation loss: 0.001983\n",
      "tensor(0.0019)\n",
      "iteration 4692, train loss: 0.002924, validation loss: 0.001927\n",
      "tensor(0.0019)\n",
      "iteration 4693, train loss: 0.002841, validation loss: 0.001923\n",
      "tensor(0.0019)\n",
      "iteration 4694, train loss: 0.002833, validation loss: 0.001931\n",
      "tensor(0.0020)\n",
      "iteration 4695, train loss: 0.002837, validation loss: 0.00198\n",
      "tensor(0.0020)\n",
      "iteration 4696, train loss: 0.002867, validation loss: 0.001961\n",
      "tensor(0.0019)\n",
      "iteration 4697, train loss: 0.002919, validation loss: 0.001931\n",
      "tensor(0.0020)\n",
      "iteration 4698, train loss: \u001b[92m0.002777\u001b[0m, validation loss: 0.001983\n",
      "tensor(0.0019)\n",
      "iteration 4699, train loss: 0.002901, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 4700, train loss: 0.002876, validation loss: 0.001935\n",
      "tensor(0.0019)\n",
      "iteration 4701, train loss: 0.002826, validation loss: 0.001915\n",
      "tensor(0.0019)\n",
      "iteration 4702, train loss: 0.002925, validation loss: 0.001916\n",
      "tensor(0.0019)\n",
      "iteration 4703, train loss: 0.002997, validation loss: 0.001947\n",
      "tensor(0.0019)\n",
      "iteration 4704, train loss: 0.002922, validation loss: 0.001938\n",
      "tensor(0.0020)\n",
      "iteration 4705, train loss: 0.002889, validation loss: 0.00201\n",
      "tensor(0.0020)\n",
      "iteration 4706, train loss: 0.002819, validation loss: 0.001971\n",
      "tensor(0.0019)\n",
      "iteration 4707, train loss: 0.002891, validation loss: 0.001909\n",
      "tensor(0.0019)\n",
      "iteration 4708, train loss: 0.002888, validation loss: 0.001922\n",
      "tensor(0.0019)\n",
      "iteration 4709, train loss: 0.00292, validation loss: 0.001937\n",
      "tensor(0.0020)\n",
      "iteration 4710, train loss: 0.002821, validation loss: 0.001955\n",
      "tensor(0.0019)\n",
      "iteration 4711, train loss: 0.002914, validation loss: 0.001932\n",
      "tensor(0.0020)\n",
      "iteration 4712, train loss: 0.002923, validation loss: 0.001955\n",
      "tensor(0.0019)\n",
      "iteration 4713, train loss: 0.002828, validation loss: 0.001928\n",
      "tensor(0.0019)\n",
      "iteration 4714, train loss: 0.002938, validation loss: 0.001896\n",
      "tensor(0.0019)\n",
      "iteration 4715, train loss: 0.002898, validation loss: 0.001933\n",
      "tensor(0.0019)\n",
      "iteration 4716, train loss: 0.002825, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 4717, train loss: 0.00286, validation loss: 0.001897\n",
      "tensor(0.0019)\n",
      "iteration 4718, train loss: 0.002862, validation loss: \u001b[92m0.001892\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4719, train loss: 0.002845, validation loss: 0.001926\n",
      "tensor(0.0019)\n",
      "iteration 4720, train loss: 0.002915, validation loss: 0.001948\n",
      "tensor(0.0019)\n",
      "iteration 4721, train loss: 0.00291, validation loss: 0.001942\n",
      "tensor(0.0019)\n",
      "iteration 4722, train loss: 0.002866, validation loss: 0.0019\n",
      "tensor(0.0019)\n",
      "iteration 4723, train loss: 0.002889, validation loss: 0.001896\n",
      "tensor(0.0020)\n",
      "iteration 4724, train loss: 0.002849, validation loss: 0.001972\n",
      "tensor(0.0020)\n",
      "iteration 4725, train loss: 0.002924, validation loss: 0.001976\n",
      "tensor(0.0020)\n",
      "iteration 4726, train loss: 0.002879, validation loss: 0.00195\n",
      "tensor(0.0019)\n",
      "iteration 4727, train loss: 0.002986, validation loss: 0.001899\n",
      "tensor(0.0019)\n",
      "iteration 4728, train loss: 0.002986, validation loss: 0.001925\n",
      "tensor(0.0021)\n",
      "iteration 4729, train loss: 0.002838, validation loss: 0.002061\n",
      "tensor(0.0021)\n",
      "iteration 4730, train loss: 0.002933, validation loss: 0.002053\n",
      "tensor(0.0019)\n",
      "iteration 4731, train loss: 0.002866, validation loss: 0.001902\n",
      "tensor(0.0019)\n",
      "iteration 4732, train loss: 0.002924, validation loss: 0.0019\n",
      "tensor(0.0019)\n",
      "iteration 4733, train loss: 0.002859, validation loss: 0.001915\n",
      "tensor(0.0020)\n",
      "iteration 4734, train loss: 0.002839, validation loss: 0.001977\n",
      "tensor(0.0020)\n",
      "iteration 4735, train loss: 0.002917, validation loss: 0.001959\n",
      "tensor(0.0019)\n",
      "iteration 4736, train loss: 0.002894, validation loss: 0.001903\n",
      "tensor(0.0019)\n",
      "iteration 4737, train loss: 0.002891, validation loss: 0.001936\n",
      "tensor(0.0019)\n",
      "iteration 4738, train loss: 0.002951, validation loss: 0.001942\n",
      "tensor(0.0020)\n",
      "iteration 4739, train loss: 0.002941, validation loss: 0.002031\n",
      "tensor(0.0020)\n",
      "iteration 4740, train loss: 0.002917, validation loss: 0.001996\n",
      "tensor(0.0020)\n",
      "iteration 4741, train loss: 0.00299, validation loss: 0.001965\n",
      "tensor(0.0021)\n",
      "iteration 4742, train loss: 0.002954, validation loss: 0.002072\n",
      "tensor(0.0019)\n",
      "iteration 4743, train loss: 0.002926, validation loss: 0.001894\n",
      "tensor(0.0019)\n",
      "iteration 4744, train loss: 0.002925, validation loss: 0.001938\n",
      "tensor(0.0019)\n",
      "iteration 4745, train loss: 0.002913, validation loss: 0.001927\n",
      "tensor(0.0020)\n",
      "iteration 4746, train loss: 0.002795, validation loss: 0.00196\n",
      "tensor(0.0019)\n",
      "iteration 4747, train loss: 0.002904, validation loss: 0.001895\n",
      "tensor(0.0019)\n",
      "iteration 4748, train loss: \u001b[92m0.002766\u001b[0m, validation loss: 0.001923\n",
      "tensor(0.0020)\n",
      "iteration 4749, train loss: 0.002877, validation loss: 0.001968\n",
      "tensor(0.0021)\n",
      "iteration 4750, train loss: 0.002959, validation loss: 0.00206\n",
      "tensor(0.0019)\n",
      "iteration 4751, train loss: 0.002885, validation loss: \u001b[92m0.001889\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4752, train loss: 0.002863, validation loss: 0.001923\n",
      "tensor(0.0019)\n",
      "iteration 4753, train loss: 0.002917, validation loss: 0.001948\n",
      "tensor(0.0020)\n",
      "iteration 4754, train loss: 0.002848, validation loss: 0.002006\n",
      "tensor(0.0019)\n",
      "iteration 4755, train loss: 0.003039, validation loss: 0.001904\n",
      "tensor(0.0019)\n",
      "iteration 4756, train loss: 0.00286, validation loss: 0.001923\n",
      "tensor(0.0020)\n",
      "iteration 4757, train loss: 0.002954, validation loss: 0.001963\n",
      "tensor(0.0020)\n",
      "iteration 4758, train loss: 0.002877, validation loss: 0.002021\n",
      "tensor(0.0019)\n",
      "iteration 4759, train loss: 0.002879, validation loss: 0.001932\n",
      "tensor(0.0019)\n",
      "iteration 4760, train loss: 0.00285, validation loss: 0.001914\n",
      "tensor(0.0019)\n",
      "iteration 4761, train loss: 0.002974, validation loss: \u001b[92m0.001886\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4762, train loss: 0.002844, validation loss: 0.002059\n",
      "tensor(0.0020)\n",
      "iteration 4763, train loss: 0.002851, validation loss: 0.001958\n",
      "tensor(0.0020)\n",
      "iteration 4764, train loss: 0.002866, validation loss: 0.001967\n",
      "tensor(0.0019)\n",
      "iteration 4765, train loss: 0.00306, validation loss: 0.001921\n",
      "tensor(0.0020)\n",
      "iteration 4766, train loss: 0.002854, validation loss: 0.001983\n",
      "tensor(0.0019)\n",
      "iteration 4767, train loss: 0.002908, validation loss: 0.001909\n",
      "tensor(0.0020)\n",
      "iteration 4768, train loss: 0.002838, validation loss: 0.002001\n",
      "tensor(0.0020)\n",
      "iteration 4769, train loss: 0.0029, validation loss: 0.001996\n",
      "tensor(0.0019)\n",
      "iteration 4770, train loss: 0.002906, validation loss: \u001b[92m0.001884\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4771, train loss: 0.002885, validation loss: 0.001932\n",
      "tensor(0.0019)\n",
      "iteration 4772, train loss: 0.002849, validation loss: 0.001929\n",
      "tensor(0.0020)\n",
      "iteration 4773, train loss: 0.002877, validation loss: 0.001972\n",
      "tensor(0.0020)\n",
      "iteration 4774, train loss: 0.002967, validation loss: 0.002036\n",
      "tensor(0.0020)\n",
      "iteration 4775, train loss: 0.002915, validation loss: 0.001968\n",
      "tensor(0.0019)\n",
      "iteration 4776, train loss: 0.002892, validation loss: \u001b[92m0.001877\u001b[0m\n",
      "tensor(0.0020)\n",
      "iteration 4777, train loss: 0.002876, validation loss: 0.001985\n",
      "tensor(0.0019)\n",
      "iteration 4778, train loss: 0.003042, validation loss: 0.001932\n",
      "tensor(0.0020)\n",
      "iteration 4779, train loss: 0.002845, validation loss: 0.002048\n",
      "tensor(0.0020)\n",
      "iteration 4780, train loss: 0.002895, validation loss: 0.001962\n",
      "tensor(0.0019)\n",
      "iteration 4781, train loss: 0.002908, validation loss: 0.001879\n",
      "tensor(0.0019)\n",
      "iteration 4782, train loss: 0.002843, validation loss: 0.001929\n",
      "tensor(0.0019)\n",
      "iteration 4783, train loss: 0.002949, validation loss: 0.001937\n",
      "tensor(0.0020)\n",
      "iteration 4784, train loss: 0.002906, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 4785, train loss: 0.002895, validation loss: 0.001974\n",
      "tensor(0.0019)\n",
      "iteration 4786, train loss: 0.002986, validation loss: 0.001918\n",
      "tensor(0.0019)\n",
      "iteration 4787, train loss: 0.002915, validation loss: 0.001937\n",
      "tensor(0.0019)\n",
      "iteration 4788, train loss: 0.002837, validation loss: \u001b[92m0.001873\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4789, train loss: 0.002847, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 4790, train loss: 0.002875, validation loss: 0.001917\n",
      "tensor(0.0020)\n",
      "iteration 4791, train loss: 0.002888, validation loss: 0.001985\n",
      "tensor(0.0019)\n",
      "iteration 4792, train loss: 0.002868, validation loss: 0.00191\n",
      "tensor(0.0019)\n",
      "iteration 4793, train loss: 0.002813, validation loss: 0.001919\n",
      "tensor(0.0019)\n",
      "iteration 4794, train loss: 0.002906, validation loss: 0.001949\n",
      "tensor(0.0019)\n",
      "iteration 4795, train loss: 0.002874, validation loss: 0.001927\n",
      "tensor(0.0019)\n",
      "iteration 4796, train loss: 0.00289, validation loss: 0.00191\n",
      "tensor(0.0019)\n",
      "iteration 4797, train loss: 0.002884, validation loss: 0.001892\n",
      "tensor(0.0020)\n",
      "iteration 4798, train loss: 0.002891, validation loss: 0.001962\n",
      "tensor(0.0019)\n",
      "iteration 4799, train loss: 0.002891, validation loss: 0.001894\n",
      "tensor(0.0019)\n",
      "iteration 4800, train loss: 0.002774, validation loss: 0.001912\n",
      "tensor(0.0020)\n",
      "iteration 4801, train loss: 0.002872, validation loss: 0.002001\n",
      "tensor(0.0019)\n",
      "iteration 4802, train loss: 0.002935, validation loss: 0.00189\n",
      "tensor(0.0019)\n",
      "iteration 4803, train loss: 0.002857, validation loss: 0.001903\n",
      "tensor(0.0019)\n",
      "iteration 4804, train loss: 0.002824, validation loss: 0.001911\n",
      "tensor(0.0019)\n",
      "iteration 4805, train loss: 0.002859, validation loss: 0.001902\n",
      "tensor(0.0019)\n",
      "iteration 4806, train loss: 0.002785, validation loss: 0.001937\n",
      "tensor(0.0019)\n",
      "iteration 4807, train loss: 0.002878, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 4808, train loss: 0.002866, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 4809, train loss: 0.002892, validation loss: \u001b[92m0.001871\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4810, train loss: 0.002824, validation loss: 0.001897\n",
      "tensor(0.0020)\n",
      "iteration 4811, train loss: 0.002894, validation loss: 0.001965\n",
      "tensor(0.0020)\n",
      "iteration 4812, train loss: 0.002816, validation loss: 0.002026\n",
      "tensor(0.0019)\n",
      "iteration 4813, train loss: 0.003004, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 4814, train loss: 0.002871, validation loss: 0.00191\n",
      "tensor(0.0019)\n",
      "iteration 4815, train loss: 0.002975, validation loss: 0.001923\n",
      "tensor(0.0020)\n",
      "iteration 4816, train loss: 0.002805, validation loss: 0.00203\n",
      "tensor(0.0020)\n",
      "iteration 4817, train loss: 0.002928, validation loss: 0.001975\n",
      "tensor(0.0019)\n",
      "iteration 4818, train loss: 0.002938, validation loss: 0.001896\n",
      "tensor(0.0019)\n",
      "iteration 4819, train loss: 0.002855, validation loss: 0.001924\n",
      "tensor(0.0019)\n",
      "iteration 4820, train loss: 0.002907, validation loss: 0.001922\n",
      "tensor(0.0019)\n",
      "iteration 4821, train loss: 0.002823, validation loss: 0.001918\n",
      "tensor(0.0019)\n",
      "iteration 4822, train loss: 0.002829, validation loss: 0.001923\n",
      "tensor(0.0020)\n",
      "iteration 4823, train loss: 0.002925, validation loss: 0.001965\n",
      "tensor(0.0019)\n",
      "iteration 4824, train loss: 0.00288, validation loss: 0.001933\n",
      "tensor(0.0019)\n",
      "iteration 4825, train loss: 0.002851, validation loss: 0.001882\n",
      "tensor(0.0019)\n",
      "iteration 4826, train loss: 0.002838, validation loss: 0.00191\n",
      "tensor(0.0020)\n",
      "iteration 4827, train loss: 0.002789, validation loss: 0.002008\n",
      "tensor(0.0019)\n",
      "iteration 4828, train loss: 0.002956, validation loss: 0.001899\n",
      "tensor(0.0020)\n",
      "iteration 4829, train loss: 0.002841, validation loss: 0.001953\n",
      "tensor(0.0020)\n",
      "iteration 4830, train loss: 0.003013, validation loss: 0.001971\n",
      "tensor(0.0020)\n",
      "iteration 4831, train loss: 0.002857, validation loss: 0.001999\n",
      "tensor(0.0020)\n",
      "iteration 4832, train loss: 0.002932, validation loss: 0.001993\n",
      "tensor(0.0019)\n",
      "iteration 4833, train loss: 0.00291, validation loss: 0.001894\n",
      "tensor(0.0019)\n",
      "iteration 4834, train loss: 0.002875, validation loss: 0.001922\n",
      "tensor(0.0019)\n",
      "iteration 4835, train loss: 0.002854, validation loss: 0.00191\n",
      "tensor(0.0019)\n",
      "iteration 4836, train loss: 0.002811, validation loss: 0.001908\n",
      "tensor(0.0019)\n",
      "iteration 4837, train loss: 0.002853, validation loss: 0.001947\n",
      "tensor(0.0019)\n",
      "iteration 4838, train loss: 0.002886, validation loss: 0.0019\n",
      "tensor(0.0020)\n",
      "iteration 4839, train loss: 0.002818, validation loss: 0.001999\n",
      "tensor(0.0019)\n",
      "iteration 4840, train loss: 0.002885, validation loss: 0.001886\n",
      "tensor(0.0019)\n",
      "iteration 4841, train loss: 0.00286, validation loss: 0.001898\n",
      "tensor(0.0019)\n",
      "iteration 4842, train loss: 0.002891, validation loss: 0.0019\n",
      "tensor(0.0019)\n",
      "iteration 4843, train loss: 0.002939, validation loss: 0.001929\n",
      "tensor(0.0019)\n",
      "iteration 4844, train loss: 0.002814, validation loss: 0.001914\n",
      "tensor(0.0019)\n",
      "iteration 4845, train loss: 0.002843, validation loss: 0.001918\n",
      "tensor(0.0019)\n",
      "iteration 4846, train loss: 0.002855, validation loss: 0.001907\n",
      "tensor(0.0019)\n",
      "iteration 4847, train loss: 0.002838, validation loss: 0.00194\n",
      "tensor(0.0020)\n",
      "iteration 4848, train loss: 0.002835, validation loss: 0.001966\n",
      "tensor(0.0019)\n",
      "iteration 4849, train loss: 0.002848, validation loss: 0.001897\n",
      "tensor(0.0019)\n",
      "iteration 4850, train loss: 0.002903, validation loss: 0.001917\n",
      "tensor(0.0020)\n",
      "iteration 4851, train loss: 0.002891, validation loss: 0.001983\n",
      "tensor(0.0019)\n",
      "iteration 4852, train loss: 0.002865, validation loss: 0.00193\n",
      "tensor(0.0019)\n",
      "iteration 4853, train loss: 0.002846, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 4854, train loss: 0.003, validation loss: 0.001872\n",
      "tensor(0.0020)\n",
      "iteration 4855, train loss: 0.002903, validation loss: 0.002007\n",
      "tensor(0.0019)\n",
      "iteration 4856, train loss: 0.002871, validation loss: 0.001894\n",
      "tensor(0.0019)\n",
      "iteration 4857, train loss: 0.002837, validation loss: 0.001894\n",
      "tensor(0.0019)\n",
      "iteration 4858, train loss: 0.002962, validation loss: 0.001906\n",
      "tensor(0.0020)\n",
      "iteration 4859, train loss: 0.002849, validation loss: 0.002038\n",
      "tensor(0.0019)\n",
      "iteration 4860, train loss: 0.002978, validation loss: 0.001923\n",
      "tensor(0.0020)\n",
      "iteration 4861, train loss: 0.00283, validation loss: 0.002009\n",
      "tensor(0.0019)\n",
      "iteration 4862, train loss: 0.002953, validation loss: 0.001903\n",
      "tensor(0.0021)\n",
      "iteration 4863, train loss: 0.002912, validation loss: 0.002104\n",
      "tensor(0.0019)\n",
      "iteration 4864, train loss: 0.003063, validation loss: 0.001892\n",
      "tensor(0.0020)\n",
      "iteration 4865, train loss: 0.002894, validation loss: 0.001972\n",
      "tensor(0.0020)\n",
      "iteration 4866, train loss: 0.003084, validation loss: 0.001989\n",
      "tensor(0.0021)\n",
      "iteration 4867, train loss: 0.002965, validation loss: 0.002074\n",
      "tensor(0.0019)\n",
      "iteration 4868, train loss: 0.0029, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 4869, train loss: 0.002931, validation loss: 0.001903\n",
      "tensor(0.0019)\n",
      "iteration 4870, train loss: 0.002863, validation loss: 0.001906\n",
      "tensor(0.0021)\n",
      "iteration 4871, train loss: 0.00291, validation loss: 0.002088\n",
      "tensor(0.0020)\n",
      "iteration 4872, train loss: 0.002888, validation loss: 0.001982\n",
      "tensor(0.0019)\n",
      "iteration 4873, train loss: 0.00291, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 4874, train loss: 0.002835, validation loss: 0.001893\n",
      "tensor(0.0020)\n",
      "iteration 4875, train loss: 0.002852, validation loss: 0.00196\n",
      "tensor(0.0020)\n",
      "iteration 4876, train loss: 0.00283, validation loss: 0.002003\n",
      "tensor(0.0020)\n",
      "iteration 4877, train loss: 0.002862, validation loss: 0.001967\n",
      "tensor(0.0019)\n",
      "iteration 4878, train loss: 0.002841, validation loss: 0.001928\n",
      "tensor(0.0019)\n",
      "iteration 4879, train loss: 0.002878, validation loss: 0.001919\n",
      "tensor(0.0019)\n",
      "iteration 4880, train loss: 0.002914, validation loss: 0.001897\n",
      "tensor(0.0019)\n",
      "iteration 4881, train loss: 0.002969, validation loss: 0.001887\n",
      "tensor(0.0020)\n",
      "iteration 4882, train loss: 0.002866, validation loss: 0.001981\n",
      "tensor(0.0020)\n",
      "iteration 4883, train loss: 0.002844, validation loss: 0.001961\n",
      "tensor(0.0019)\n",
      "iteration 4884, train loss: 0.002876, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 4885, train loss: 0.002889, validation loss: 0.001874\n",
      "tensor(0.0021)\n",
      "iteration 4886, train loss: 0.002856, validation loss: 0.002092\n",
      "tensor(0.0020)\n",
      "iteration 4887, train loss: 0.002919, validation loss: 0.002004\n",
      "tensor(0.0020)\n",
      "iteration 4888, train loss: 0.003023, validation loss: 0.001956\n",
      "tensor(0.0019)\n",
      "iteration 4889, train loss: 0.002985, validation loss: 0.001896\n",
      "tensor(0.0020)\n",
      "iteration 4890, train loss: 0.002948, validation loss: 0.002028\n",
      "tensor(0.0020)\n",
      "iteration 4891, train loss: 0.002985, validation loss: 0.001956\n",
      "tensor(0.0020)\n",
      "iteration 4892, train loss: 0.002879, validation loss: 0.002023\n",
      "tensor(0.0019)\n",
      "iteration 4893, train loss: 0.002955, validation loss: 0.00193\n",
      "tensor(0.0020)\n",
      "iteration 4894, train loss: 0.002842, validation loss: 0.00201\n",
      "tensor(0.0019)\n",
      "iteration 4895, train loss: 0.002966, validation loss: 0.001904\n",
      "tensor(0.0019)\n",
      "iteration 4896, train loss: 0.002789, validation loss: 0.001878\n",
      "tensor(0.0019)\n",
      "iteration 4897, train loss: 0.002916, validation loss: 0.00193\n",
      "tensor(0.0022)\n",
      "iteration 4898, train loss: 0.002932, validation loss: 0.00216\n",
      "tensor(0.0019)\n",
      "iteration 4899, train loss: 0.00305, validation loss: 0.001903\n",
      "tensor(0.0020)\n",
      "iteration 4900, train loss: \u001b[92m0.002729\u001b[0m, validation loss: 0.001979\n",
      "tensor(0.0019)\n",
      "iteration 4901, train loss: 0.003058, validation loss: \u001b[92m0.001865\u001b[0m\n",
      "tensor(0.0021)\n",
      "iteration 4902, train loss: 0.002867, validation loss: 0.002062\n",
      "tensor(0.0019)\n",
      "iteration 4903, train loss: 0.002896, validation loss: 0.001925\n",
      "tensor(0.0019)\n",
      "iteration 4904, train loss: 0.002781, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 4905, train loss: 0.002897, validation loss: 0.001883\n",
      "tensor(0.0019)\n",
      "iteration 4906, train loss: 0.002839, validation loss: 0.001921\n",
      "tensor(0.0019)\n",
      "iteration 4907, train loss: 0.002972, validation loss: 0.001919\n",
      "tensor(0.0020)\n",
      "iteration 4908, train loss: 0.002867, validation loss: 0.001994\n",
      "tensor(0.0019)\n",
      "iteration 4909, train loss: 0.00293, validation loss: 0.001948\n",
      "tensor(0.0019)\n",
      "iteration 4910, train loss: 0.002796, validation loss: 0.001932\n",
      "tensor(0.0019)\n",
      "iteration 4911, train loss: 0.002892, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 4912, train loss: 0.0029, validation loss: 0.001894\n",
      "tensor(0.0020)\n",
      "iteration 4913, train loss: 0.002934, validation loss: 0.001988\n",
      "tensor(0.0019)\n",
      "iteration 4914, train loss: 0.002883, validation loss: 0.001917\n",
      "tensor(0.0019)\n",
      "iteration 4915, train loss: 0.002834, validation loss: 0.001885\n",
      "tensor(0.0019)\n",
      "iteration 4916, train loss: 0.002955, validation loss: 0.001902\n",
      "tensor(0.0020)\n",
      "iteration 4917, train loss: 0.002948, validation loss: 0.001982\n",
      "tensor(0.0020)\n",
      "iteration 4918, train loss: 0.002825, validation loss: 0.00198\n",
      "tensor(0.0020)\n",
      "iteration 4919, train loss: 0.002818, validation loss: 0.001958\n",
      "tensor(0.0019)\n",
      "iteration 4920, train loss: 0.002836, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 4921, train loss: 0.002828, validation loss: \u001b[92m0.001855\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4922, train loss: 0.002886, validation loss: 0.001858\n",
      "tensor(0.0019)\n",
      "iteration 4923, train loss: 0.002797, validation loss: 0.001919\n",
      "tensor(0.0020)\n",
      "iteration 4924, train loss: 0.002763, validation loss: 0.001965\n",
      "tensor(0.0019)\n",
      "iteration 4925, train loss: 0.002866, validation loss: 0.001934\n",
      "tensor(0.0019)\n",
      "iteration 4926, train loss: 0.002882, validation loss: \u001b[92m0.001852\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4927, train loss: 0.002825, validation loss: 0.001878\n",
      "tensor(0.0019)\n",
      "iteration 4928, train loss: 0.002991, validation loss: 0.001903\n",
      "tensor(0.0019)\n",
      "iteration 4929, train loss: 0.002836, validation loss: 0.001909\n",
      "tensor(0.0019)\n",
      "iteration 4930, train loss: 0.00282, validation loss: 0.001915\n",
      "tensor(0.0019)\n",
      "iteration 4931, train loss: 0.002881, validation loss: 0.001863\n",
      "tensor(0.0019)\n",
      "iteration 4932, train loss: 0.002852, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 4933, train loss: 0.002901, validation loss: 0.001865\n",
      "tensor(0.0019)\n",
      "iteration 4934, train loss: 0.002769, validation loss: 0.001865\n",
      "tensor(0.0019)\n",
      "iteration 4935, train loss: 0.002796, validation loss: 0.001867\n",
      "tensor(0.0019)\n",
      "iteration 4936, train loss: 0.002785, validation loss: 0.001948\n",
      "tensor(0.0019)\n",
      "iteration 4937, train loss: 0.002958, validation loss: \u001b[92m0.001852\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4938, train loss: 0.002892, validation loss: 0.001855\n",
      "tensor(0.0018)\n",
      "iteration 4939, train loss: 0.002834, validation loss: \u001b[92m0.001847\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 4940, train loss: 0.002827, validation loss: \u001b[92m0.001845\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 4941, train loss: 0.002868, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 4942, train loss: 0.002881, validation loss: 0.001917\n",
      "tensor(0.0019)\n",
      "iteration 4943, train loss: 0.002836, validation loss: 0.001929\n",
      "tensor(0.0019)\n",
      "iteration 4944, train loss: 0.002858, validation loss: 0.001926\n",
      "tensor(0.0019)\n",
      "iteration 4945, train loss: 0.002854, validation loss: 0.001867\n",
      "tensor(0.0019)\n",
      "iteration 4946, train loss: 0.002785, validation loss: 0.00188\n",
      "tensor(0.0019)\n",
      "iteration 4947, train loss: 0.002819, validation loss: 0.001861\n",
      "tensor(0.0019)\n",
      "iteration 4948, train loss: 0.002881, validation loss: 0.001904\n",
      "tensor(0.0020)\n",
      "iteration 4949, train loss: 0.002791, validation loss: 0.002001\n",
      "tensor(0.0019)\n",
      "iteration 4950, train loss: 0.002844, validation loss: 0.001903\n",
      "tensor(0.0019)\n",
      "iteration 4951, train loss: 0.002828, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 4952, train loss: 0.002919, validation loss: 0.001879\n",
      "tensor(0.0019)\n",
      "iteration 4953, train loss: 0.002851, validation loss: 0.001862\n",
      "tensor(0.0019)\n",
      "iteration 4954, train loss: 0.00285, validation loss: 0.001927\n",
      "tensor(0.0019)\n",
      "iteration 4955, train loss: 0.00286, validation loss: 0.001943\n",
      "tensor(0.0020)\n",
      "iteration 4956, train loss: 0.002904, validation loss: 0.001998\n",
      "tensor(0.0019)\n",
      "iteration 4957, train loss: 0.002885, validation loss: 0.001944\n",
      "tensor(0.0019)\n",
      "iteration 4958, train loss: 0.002989, validation loss: 0.001934\n",
      "tensor(0.0020)\n",
      "iteration 4959, train loss: 0.002871, validation loss: 0.001977\n",
      "tensor(0.0018)\n",
      "iteration 4960, train loss: 0.002932, validation loss: 0.001849\n",
      "tensor(0.0019)\n",
      "iteration 4961, train loss: 0.002733, validation loss: 0.001877\n",
      "tensor(0.0019)\n",
      "iteration 4962, train loss: 0.002935, validation loss: 0.001933\n",
      "tensor(0.0019)\n",
      "iteration 4963, train loss: 0.002808, validation loss: 0.001947\n",
      "tensor(0.0019)\n",
      "iteration 4964, train loss: 0.002782, validation loss: 0.001891\n",
      "tensor(0.0019)\n",
      "iteration 4965, train loss: 0.002827, validation loss: 0.001901\n",
      "tensor(0.0019)\n",
      "iteration 4966, train loss: 0.002934, validation loss: 0.001873\n",
      "tensor(0.0020)\n",
      "iteration 4967, train loss: 0.002808, validation loss: 0.001989\n",
      "tensor(0.0019)\n",
      "iteration 4968, train loss: 0.002876, validation loss: 0.001929\n",
      "tensor(0.0019)\n",
      "iteration 4969, train loss: 0.002923, validation loss: 0.00189\n",
      "tensor(0.0020)\n",
      "iteration 4970, train loss: 0.002886, validation loss: 0.001997\n",
      "tensor(0.0019)\n",
      "iteration 4971, train loss: 0.002976, validation loss: 0.001895\n",
      "tensor(0.0019)\n",
      "iteration 4972, train loss: 0.002914, validation loss: 0.00191\n",
      "tensor(0.0020)\n",
      "iteration 4973, train loss: 0.002833, validation loss: 0.001959\n",
      "tensor(0.0019)\n",
      "iteration 4974, train loss: 0.002889, validation loss: 0.001866\n",
      "tensor(0.0020)\n",
      "iteration 4975, train loss: 0.002805, validation loss: 0.001951\n",
      "tensor(0.0019)\n",
      "iteration 4976, train loss: 0.002887, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 4977, train loss: 0.002873, validation loss: 0.001851\n",
      "tensor(0.0019)\n",
      "iteration 4978, train loss: 0.002832, validation loss: 0.001925\n",
      "tensor(0.0020)\n",
      "iteration 4979, train loss: 0.002883, validation loss: 0.002008\n",
      "tensor(0.0019)\n",
      "iteration 4980, train loss: 0.002889, validation loss: 0.001858\n",
      "tensor(0.0019)\n",
      "iteration 4981, train loss: 0.002785, validation loss: 0.001888\n",
      "tensor(0.0019)\n",
      "iteration 4982, train loss: 0.002977, validation loss: 0.001907\n",
      "tensor(0.0019)\n",
      "iteration 4983, train loss: 0.002836, validation loss: 0.001922\n",
      "tensor(0.0019)\n",
      "iteration 4984, train loss: 0.002808, validation loss: 0.001913\n",
      "tensor(0.0018)\n",
      "iteration 4985, train loss: 0.002872, validation loss: 0.001849\n",
      "tensor(0.0019)\n",
      "iteration 4986, train loss: 0.002815, validation loss: 0.001856\n",
      "tensor(0.0019)\n",
      "iteration 4987, train loss: 0.002823, validation loss: 0.00192\n",
      "tensor(0.0019)\n",
      "iteration 4988, train loss: 0.002869, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 4989, train loss: 0.002787, validation loss: 0.001907\n",
      "tensor(0.0019)\n",
      "iteration 4990, train loss: 0.002848, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 4991, train loss: 0.002824, validation loss: 0.001911\n",
      "tensor(0.0018)\n",
      "iteration 4992, train loss: 0.002873, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 4993, train loss: 0.002826, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 4994, train loss: 0.002828, validation loss: 0.001897\n",
      "tensor(0.0019)\n",
      "iteration 4995, train loss: 0.002878, validation loss: 0.001928\n",
      "tensor(0.0019)\n",
      "iteration 4996, train loss: 0.00303, validation loss: 0.001897\n",
      "tensor(0.0019)\n",
      "iteration 4997, train loss: 0.002872, validation loss: 0.001883\n",
      "tensor(0.0019)\n",
      "iteration 4998, train loss: 0.002813, validation loss: 0.001918\n",
      "tensor(0.0019)\n",
      "iteration 4999, train loss: 0.002945, validation loss: 0.001899\n",
      "tensor(0.0019)\n",
      "iteration 5000, train loss: 0.002853, validation loss: 0.001875\n",
      "tensor(0.0019)\n",
      "iteration 5001, train loss: 0.002853, validation loss: 0.001863\n",
      "tensor(0.0020)\n",
      "iteration 5002, train loss: 0.002903, validation loss: 0.001967\n",
      "tensor(0.0019)\n",
      "iteration 5003, train loss: 0.002797, validation loss: 0.00195\n",
      "tensor(0.0019)\n",
      "iteration 5004, train loss: 0.002792, validation loss: 0.001859\n",
      "tensor(0.0019)\n",
      "iteration 5005, train loss: 0.002806, validation loss: 0.001855\n",
      "tensor(0.0019)\n",
      "iteration 5006, train loss: 0.002892, validation loss: 0.001895\n",
      "tensor(0.0019)\n",
      "iteration 5007, train loss: 0.002822, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 5008, train loss: 0.002875, validation loss: 0.001873\n",
      "tensor(0.0018)\n",
      "iteration 5009, train loss: 0.002815, validation loss: \u001b[92m0.001834\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5010, train loss: 0.002898, validation loss: 0.001912\n",
      "tensor(0.0019)\n",
      "iteration 5011, train loss: 0.002898, validation loss: 0.001899\n",
      "tensor(0.0019)\n",
      "iteration 5012, train loss: 0.00281, validation loss: 0.001873\n",
      "tensor(0.0019)\n",
      "iteration 5013, train loss: 0.002853, validation loss: 0.00186\n",
      "tensor(0.0019)\n",
      "iteration 5014, train loss: 0.00281, validation loss: 0.001912\n",
      "tensor(0.0018)\n",
      "iteration 5015, train loss: 0.002851, validation loss: 0.001842\n",
      "tensor(0.0018)\n",
      "iteration 5016, train loss: 0.002843, validation loss: 0.001841\n",
      "tensor(0.0020)\n",
      "iteration 5017, train loss: 0.002774, validation loss: 0.00195\n",
      "tensor(0.0019)\n",
      "iteration 5018, train loss: 0.002851, validation loss: 0.001933\n",
      "tensor(0.0020)\n",
      "iteration 5019, train loss: 0.002927, validation loss: 0.001985\n",
      "tensor(0.0019)\n",
      "iteration 5020, train loss: 0.003098, validation loss: 0.001864\n",
      "tensor(0.0020)\n",
      "iteration 5021, train loss: 0.002868, validation loss: 0.002\n",
      "tensor(0.0019)\n",
      "iteration 5022, train loss: 0.002928, validation loss: 0.001882\n",
      "tensor(0.0019)\n",
      "iteration 5023, train loss: 0.002776, validation loss: 0.001919\n",
      "tensor(0.0019)\n",
      "iteration 5024, train loss: 0.00286, validation loss: 0.001889\n",
      "tensor(0.0020)\n",
      "iteration 5025, train loss: 0.002842, validation loss: 0.002006\n",
      "tensor(0.0018)\n",
      "iteration 5026, train loss: 0.003048, validation loss: 0.001844\n",
      "tensor(0.0019)\n",
      "iteration 5027, train loss: 0.002763, validation loss: 0.001882\n",
      "tensor(0.0018)\n",
      "iteration 5028, train loss: 0.002849, validation loss: 0.001841\n",
      "tensor(0.0019)\n",
      "iteration 5029, train loss: 0.002846, validation loss: 0.001945\n",
      "tensor(0.0019)\n",
      "iteration 5030, train loss: 0.002823, validation loss: 0.001936\n",
      "tensor(0.0019)\n",
      "iteration 5031, train loss: 0.002881, validation loss: 0.001855\n",
      "tensor(0.0019)\n",
      "iteration 5032, train loss: 0.002944, validation loss: 0.001863\n",
      "tensor(0.0020)\n",
      "iteration 5033, train loss: 0.002994, validation loss: 0.00196\n",
      "tensor(0.0020)\n",
      "iteration 5034, train loss: 0.002907, validation loss: 0.001975\n",
      "tensor(0.0020)\n",
      "iteration 5035, train loss: 0.002932, validation loss: 0.001963\n",
      "tensor(0.0018)\n",
      "iteration 5036, train loss: 0.002889, validation loss: \u001b[92m0.001833\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5037, train loss: \u001b[92m0.002719\u001b[0m, validation loss: 0.001904\n",
      "tensor(0.0018)\n",
      "iteration 5038, train loss: 0.002887, validation loss: 0.001838\n",
      "tensor(0.0019)\n",
      "iteration 5039, train loss: 0.002743, validation loss: 0.001869\n",
      "tensor(0.0019)\n",
      "iteration 5040, train loss: 0.002848, validation loss: 0.001895\n",
      "tensor(0.0019)\n",
      "iteration 5041, train loss: 0.002821, validation loss: 0.001905\n",
      "tensor(0.0019)\n",
      "iteration 5042, train loss: 0.00284, validation loss: 0.001863\n",
      "tensor(0.0018)\n",
      "iteration 5043, train loss: 0.00279, validation loss: 0.001837\n",
      "tensor(0.0018)\n",
      "iteration 5044, train loss: 0.002852, validation loss: 0.001843\n",
      "tensor(0.0019)\n",
      "iteration 5045, train loss: 0.002746, validation loss: 0.001863\n",
      "tensor(0.0018)\n",
      "iteration 5046, train loss: 0.002787, validation loss: 0.001834\n",
      "tensor(0.0018)\n",
      "iteration 5047, train loss: 0.002784, validation loss: 0.001846\n",
      "tensor(0.0018)\n",
      "iteration 5048, train loss: 0.002811, validation loss: \u001b[92m0.001829\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5049, train loss: 0.002782, validation loss: 0.001832\n",
      "tensor(0.0019)\n",
      "iteration 5050, train loss: 0.002799, validation loss: 0.001941\n",
      "tensor(0.0019)\n",
      "iteration 5051, train loss: 0.002901, validation loss: 0.001876\n",
      "tensor(0.0018)\n",
      "iteration 5052, train loss: 0.002808, validation loss: 0.001846\n",
      "tensor(0.0018)\n",
      "iteration 5053, train loss: 0.002854, validation loss: 0.001832\n",
      "tensor(0.0019)\n",
      "iteration 5054, train loss: 0.002792, validation loss: 0.001938\n",
      "tensor(0.0019)\n",
      "iteration 5055, train loss: 0.002767, validation loss: 0.001871\n",
      "tensor(0.0018)\n",
      "iteration 5056, train loss: 0.00276, validation loss: \u001b[92m0.001828\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5057, train loss: 0.002898, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 5058, train loss: 0.00276, validation loss: 0.001877\n",
      "tensor(0.0019)\n",
      "iteration 5059, train loss: 0.002817, validation loss: 0.001898\n",
      "tensor(0.0019)\n",
      "iteration 5060, train loss: 0.002757, validation loss: 0.001917\n",
      "tensor(0.0018)\n",
      "iteration 5061, train loss: 0.002871, validation loss: 0.001837\n",
      "tensor(0.0019)\n",
      "iteration 5062, train loss: 0.002903, validation loss: 0.001902\n",
      "tensor(0.0019)\n",
      "iteration 5063, train loss: 0.002774, validation loss: 0.001869\n",
      "tensor(0.0019)\n",
      "iteration 5064, train loss: 0.002753, validation loss: 0.0019\n",
      "tensor(0.0018)\n",
      "iteration 5065, train loss: 0.002819, validation loss: 0.001838\n",
      "tensor(0.0019)\n",
      "iteration 5066, train loss: 0.002749, validation loss: 0.001897\n",
      "tensor(0.0018)\n",
      "iteration 5067, train loss: 0.002865, validation loss: \u001b[92m0.001827\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5068, train loss: 0.002869, validation loss: 0.00184\n",
      "tensor(0.0019)\n",
      "iteration 5069, train loss: 0.002843, validation loss: 0.001949\n",
      "tensor(0.0019)\n",
      "iteration 5070, train loss: 0.00283, validation loss: 0.001904\n",
      "tensor(0.0019)\n",
      "iteration 5071, train loss: 0.002834, validation loss: 0.001885\n",
      "tensor(0.0019)\n",
      "iteration 5072, train loss: 0.002802, validation loss: 0.00188\n",
      "tensor(0.0020)\n",
      "iteration 5073, train loss: 0.002778, validation loss: 0.001976\n",
      "tensor(0.0020)\n",
      "iteration 5074, train loss: 0.002818, validation loss: 0.001977\n",
      "tensor(0.0018)\n",
      "iteration 5075, train loss: 0.002815, validation loss: 0.001848\n",
      "tensor(0.0019)\n",
      "iteration 5076, train loss: 0.002765, validation loss: 0.001866\n",
      "tensor(0.0019)\n",
      "iteration 5077, train loss: 0.002895, validation loss: 0.001924\n",
      "tensor(0.0019)\n",
      "iteration 5078, train loss: 0.002933, validation loss: 0.001861\n",
      "tensor(0.0019)\n",
      "iteration 5079, train loss: \u001b[92m0.002683\u001b[0m, validation loss: 0.001912\n",
      "tensor(0.0018)\n",
      "iteration 5080, train loss: 0.002905, validation loss: 0.001847\n",
      "tensor(0.0019)\n",
      "iteration 5081, train loss: 0.002777, validation loss: 0.001901\n",
      "tensor(0.0019)\n",
      "iteration 5082, train loss: 0.002822, validation loss: 0.001862\n",
      "tensor(0.0019)\n",
      "iteration 5083, train loss: 0.002825, validation loss: 0.001851\n",
      "tensor(0.0019)\n",
      "iteration 5084, train loss: 0.002838, validation loss: 0.00187\n",
      "tensor(0.0019)\n",
      "iteration 5085, train loss: 0.002828, validation loss: 0.001909\n",
      "tensor(0.0018)\n",
      "iteration 5086, train loss: 0.002796, validation loss: 0.001837\n",
      "tensor(0.0018)\n",
      "iteration 5087, train loss: 0.002863, validation loss: \u001b[92m0.001823\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5088, train loss: 0.002789, validation loss: 0.001835\n",
      "tensor(0.0018)\n",
      "iteration 5089, train loss: 0.002804, validation loss: 0.001833\n",
      "tensor(0.0019)\n",
      "iteration 5090, train loss: 0.002715, validation loss: 0.001859\n",
      "tensor(0.0018)\n",
      "iteration 5091, train loss: 0.00288, validation loss: 0.00184\n",
      "tensor(0.0018)\n",
      "iteration 5092, train loss: 0.002761, validation loss: 0.001842\n",
      "tensor(0.0019)\n",
      "iteration 5093, train loss: 0.002802, validation loss: 0.001858\n",
      "tensor(0.0019)\n",
      "iteration 5094, train loss: 0.002834, validation loss: 0.001891\n",
      "tensor(0.0019)\n",
      "iteration 5095, train loss: 0.00282, validation loss: 0.001859\n",
      "tensor(0.0019)\n",
      "iteration 5096, train loss: 0.002736, validation loss: 0.001852\n",
      "tensor(0.0018)\n",
      "iteration 5097, train loss: 0.002886, validation loss: 0.00183\n",
      "tensor(0.0019)\n",
      "iteration 5098, train loss: 0.002796, validation loss: 0.001869\n",
      "tensor(0.0019)\n",
      "iteration 5099, train loss: 0.002828, validation loss: 0.001875\n",
      "tensor(0.0019)\n",
      "iteration 5100, train loss: 0.002834, validation loss: 0.00186\n",
      "tensor(0.0019)\n",
      "iteration 5101, train loss: 0.002761, validation loss: 0.001851\n",
      "tensor(0.0018)\n",
      "iteration 5102, train loss: 0.002706, validation loss: 0.001836\n",
      "tensor(0.0019)\n",
      "iteration 5103, train loss: 0.002916, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 5104, train loss: 0.002729, validation loss: 0.001905\n",
      "tensor(0.0019)\n",
      "iteration 5105, train loss: 0.002859, validation loss: 0.001852\n",
      "tensor(0.0019)\n",
      "iteration 5106, train loss: 0.002783, validation loss: 0.001872\n",
      "tensor(0.0018)\n",
      "iteration 5107, train loss: 0.002794, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 5108, train loss: 0.002828, validation loss: 0.00185\n",
      "tensor(0.0018)\n",
      "iteration 5109, train loss: 0.002852, validation loss: 0.001839\n",
      "tensor(0.0019)\n",
      "iteration 5110, train loss: 0.002768, validation loss: 0.001851\n",
      "tensor(0.0019)\n",
      "iteration 5111, train loss: 0.00284, validation loss: 0.001888\n",
      "tensor(0.0018)\n",
      "iteration 5112, train loss: 0.002799, validation loss: 0.001848\n",
      "tensor(0.0018)\n",
      "iteration 5113, train loss: 0.002852, validation loss: 0.001846\n",
      "tensor(0.0018)\n",
      "iteration 5114, train loss: 0.002836, validation loss: 0.001841\n",
      "tensor(0.0019)\n",
      "iteration 5115, train loss: 0.002875, validation loss: 0.001883\n",
      "tensor(0.0019)\n",
      "iteration 5116, train loss: 0.002799, validation loss: 0.001858\n",
      "tensor(0.0019)\n",
      "iteration 5117, train loss: 0.002817, validation loss: 0.001858\n",
      "tensor(0.0019)\n",
      "iteration 5118, train loss: 0.002834, validation loss: 0.001882\n",
      "tensor(0.0018)\n",
      "iteration 5119, train loss: 0.002734, validation loss: 0.001831\n",
      "tensor(0.0018)\n",
      "iteration 5120, train loss: 0.002799, validation loss: \u001b[92m0.001814\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5121, train loss: 0.002843, validation loss: 0.001842\n",
      "tensor(0.0018)\n",
      "iteration 5122, train loss: 0.002771, validation loss: 0.001843\n",
      "tensor(0.0018)\n",
      "iteration 5123, train loss: 0.002743, validation loss: 0.001827\n",
      "tensor(0.0019)\n",
      "iteration 5124, train loss: 0.002817, validation loss: 0.001859\n",
      "tensor(0.0020)\n",
      "iteration 5125, train loss: 0.002759, validation loss: 0.001955\n",
      "tensor(0.0018)\n",
      "iteration 5126, train loss: 0.002863, validation loss: 0.00184\n",
      "tensor(0.0019)\n",
      "iteration 5127, train loss: 0.002835, validation loss: 0.001875\n",
      "tensor(0.0018)\n",
      "iteration 5128, train loss: 0.002919, validation loss: 0.001837\n",
      "tensor(0.0018)\n",
      "iteration 5129, train loss: 0.002775, validation loss: 0.00184\n",
      "tensor(0.0018)\n",
      "iteration 5130, train loss: 0.002842, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 5131, train loss: 0.002819, validation loss: 0.001876\n",
      "tensor(0.0019)\n",
      "iteration 5132, train loss: 0.002792, validation loss: 0.001885\n",
      "tensor(0.0018)\n",
      "iteration 5133, train loss: 0.002734, validation loss: 0.001815\n",
      "tensor(0.0018)\n",
      "iteration 5134, train loss: 0.002828, validation loss: 0.001816\n",
      "tensor(0.0018)\n",
      "iteration 5135, train loss: 0.002697, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 5136, train loss: 0.00277, validation loss: 0.001871\n",
      "tensor(0.0018)\n",
      "iteration 5137, train loss: 0.002835, validation loss: 0.001835\n",
      "tensor(0.0018)\n",
      "iteration 5138, train loss: 0.0028, validation loss: \u001b[92m0.001813\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5139, train loss: 0.002767, validation loss: 0.001825\n",
      "tensor(0.0019)\n",
      "iteration 5140, train loss: 0.002828, validation loss: 0.00186\n",
      "tensor(0.0018)\n",
      "iteration 5141, train loss: 0.002766, validation loss: 0.001849\n",
      "tensor(0.0019)\n",
      "iteration 5142, train loss: 0.002857, validation loss: 0.001852\n",
      "tensor(0.0019)\n",
      "iteration 5143, train loss: 0.002798, validation loss: 0.001864\n",
      "tensor(0.0018)\n",
      "iteration 5144, train loss: 0.002902, validation loss: \u001b[92m0.001813\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5145, train loss: 0.002781, validation loss: 0.001831\n",
      "tensor(0.0018)\n",
      "iteration 5146, train loss: 0.002807, validation loss: 0.00184\n",
      "tensor(0.0019)\n",
      "iteration 5147, train loss: 0.00271, validation loss: 0.001876\n",
      "tensor(0.0019)\n",
      "iteration 5148, train loss: 0.002745, validation loss: 0.001854\n",
      "tensor(0.0018)\n",
      "iteration 5149, train loss: 0.002805, validation loss: 0.001817\n",
      "tensor(0.0019)\n",
      "iteration 5150, train loss: 0.002816, validation loss: 0.001855\n",
      "tensor(0.0019)\n",
      "iteration 5151, train loss: 0.00275, validation loss: 0.001886\n",
      "tensor(0.0018)\n",
      "iteration 5152, train loss: 0.002849, validation loss: 0.001833\n",
      "tensor(0.0018)\n",
      "iteration 5153, train loss: 0.002722, validation loss: 0.001814\n",
      "tensor(0.0019)\n",
      "iteration 5154, train loss: 0.00277, validation loss: 0.001875\n",
      "tensor(0.0018)\n",
      "iteration 5155, train loss: 0.002804, validation loss: 0.001815\n",
      "tensor(0.0019)\n",
      "iteration 5156, train loss: 0.002697, validation loss: 0.001857\n",
      "tensor(0.0019)\n",
      "iteration 5157, train loss: 0.002821, validation loss: 0.00188\n",
      "tensor(0.0019)\n",
      "iteration 5158, train loss: 0.002766, validation loss: 0.001853\n",
      "tensor(0.0018)\n",
      "iteration 5159, train loss: 0.002756, validation loss: \u001b[92m0.00181\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5160, train loss: 0.002762, validation loss: 0.001821\n",
      "tensor(0.0019)\n",
      "iteration 5161, train loss: 0.002797, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 5162, train loss: 0.002702, validation loss: 0.001885\n",
      "tensor(0.0019)\n",
      "iteration 5163, train loss: 0.002759, validation loss: 0.001853\n",
      "tensor(0.0018)\n",
      "iteration 5164, train loss: 0.002775, validation loss: 0.001814\n",
      "tensor(0.0020)\n",
      "iteration 5165, train loss: 0.002792, validation loss: 0.00196\n",
      "tensor(0.0018)\n",
      "iteration 5166, train loss: 0.002938, validation loss: 0.001814\n",
      "tensor(0.0019)\n",
      "iteration 5167, train loss: 0.00269, validation loss: 0.001934\n",
      "tensor(0.0018)\n",
      "iteration 5168, train loss: 0.002892, validation loss: 0.001811\n",
      "tensor(0.0020)\n",
      "iteration 5169, train loss: 0.002746, validation loss: 0.001956\n",
      "tensor(0.0018)\n",
      "iteration 5170, train loss: 0.002852, validation loss: 0.001818\n",
      "tensor(0.0019)\n",
      "iteration 5171, train loss: 0.002772, validation loss: 0.001877\n",
      "tensor(0.0019)\n",
      "iteration 5172, train loss: 0.002842, validation loss: 0.001887\n",
      "tensor(0.0018)\n",
      "iteration 5173, train loss: 0.002763, validation loss: 0.001845\n",
      "tensor(0.0018)\n",
      "iteration 5174, train loss: 0.002767, validation loss: 0.001834\n",
      "tensor(0.0018)\n",
      "iteration 5175, train loss: 0.002848, validation loss: 0.001838\n",
      "tensor(0.0019)\n",
      "iteration 5176, train loss: 0.002776, validation loss: 0.001928\n",
      "tensor(0.0018)\n",
      "iteration 5177, train loss: 0.002817, validation loss: \u001b[92m0.001797\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5178, train loss: 0.002718, validation loss: 0.001857\n",
      "tensor(0.0018)\n",
      "iteration 5179, train loss: 0.002863, validation loss: 0.001818\n",
      "tensor(0.0019)\n",
      "iteration 5180, train loss: 0.002825, validation loss: 0.001886\n",
      "tensor(0.0019)\n",
      "iteration 5181, train loss: 0.002857, validation loss: 0.001887\n",
      "tensor(0.0019)\n",
      "iteration 5182, train loss: 0.002695, validation loss: 0.001876\n",
      "tensor(0.0019)\n",
      "iteration 5183, train loss: 0.002862, validation loss: 0.001942\n",
      "tensor(0.0018)\n",
      "iteration 5184, train loss: 0.002891, validation loss: 0.001818\n",
      "tensor(0.0018)\n",
      "iteration 5185, train loss: 0.002779, validation loss: 0.001848\n",
      "tensor(0.0018)\n",
      "iteration 5186, train loss: 0.002803, validation loss: 0.001816\n",
      "tensor(0.0018)\n",
      "iteration 5187, train loss: 0.002745, validation loss: 0.001841\n",
      "tensor(0.0018)\n",
      "iteration 5188, train loss: 0.002731, validation loss: 0.001828\n",
      "tensor(0.0018)\n",
      "iteration 5189, train loss: 0.002897, validation loss: 0.001819\n",
      "tensor(0.0019)\n",
      "iteration 5190, train loss: 0.002793, validation loss: 0.001872\n",
      "tensor(0.0019)\n",
      "iteration 5191, train loss: 0.00286, validation loss: 0.001947\n",
      "tensor(0.0018)\n",
      "iteration 5192, train loss: 0.002849, validation loss: 0.001825\n",
      "tensor(0.0018)\n",
      "iteration 5193, train loss: 0.002837, validation loss: 0.001819\n",
      "tensor(0.0018)\n",
      "iteration 5194, train loss: 0.002752, validation loss: \u001b[92m0.001792\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5195, train loss: 0.002766, validation loss: 0.001801\n",
      "tensor(0.0019)\n",
      "iteration 5196, train loss: 0.002837, validation loss: 0.00187\n",
      "tensor(0.0018)\n",
      "iteration 5197, train loss: 0.002814, validation loss: 0.001845\n",
      "tensor(0.0019)\n",
      "iteration 5198, train loss: 0.002734, validation loss: 0.001877\n",
      "tensor(0.0018)\n",
      "iteration 5199, train loss: 0.002815, validation loss: 0.001805\n",
      "tensor(0.0019)\n",
      "iteration 5200, train loss: 0.002791, validation loss: 0.001944\n",
      "tensor(0.0018)\n",
      "iteration 5201, train loss: 0.002818, validation loss: 0.00183\n",
      "tensor(0.0018)\n",
      "iteration 5202, train loss: 0.002852, validation loss: 0.001836\n",
      "tensor(0.0019)\n",
      "iteration 5203, train loss: 0.002761, validation loss: 0.001895\n",
      "tensor(0.0019)\n",
      "iteration 5204, train loss: 0.002848, validation loss: 0.001888\n",
      "tensor(0.0019)\n",
      "iteration 5205, train loss: 0.002871, validation loss: 0.001854\n",
      "tensor(0.0019)\n",
      "iteration 5206, train loss: 0.002775, validation loss: 0.001864\n",
      "tensor(0.0019)\n",
      "iteration 5207, train loss: 0.002971, validation loss: 0.00191\n",
      "tensor(0.0020)\n",
      "iteration 5208, train loss: 0.00288, validation loss: 0.001964\n",
      "tensor(0.0019)\n",
      "iteration 5209, train loss: 0.002859, validation loss: 0.00187\n",
      "tensor(0.0018)\n",
      "iteration 5210, train loss: 0.002845, validation loss: 0.001802\n",
      "tensor(0.0019)\n",
      "iteration 5211, train loss: 0.002815, validation loss: 0.001899\n",
      "tensor(0.0018)\n",
      "iteration 5212, train loss: 0.002852, validation loss: 0.001793\n",
      "tensor(0.0019)\n",
      "iteration 5213, train loss: 0.002811, validation loss: 0.001869\n",
      "tensor(0.0019)\n",
      "iteration 5214, train loss: 0.002812, validation loss: 0.001866\n",
      "tensor(0.0018)\n",
      "iteration 5215, train loss: 0.002829, validation loss: 0.001818\n",
      "tensor(0.0019)\n",
      "iteration 5216, train loss: 0.002684, validation loss: 0.001853\n",
      "tensor(0.0018)\n",
      "iteration 5217, train loss: 0.002728, validation loss: 0.001848\n",
      "tensor(0.0019)\n",
      "iteration 5218, train loss: 0.002826, validation loss: 0.001871\n",
      "tensor(0.0019)\n",
      "iteration 5219, train loss: 0.002913, validation loss: 0.001878\n",
      "tensor(0.0019)\n",
      "iteration 5220, train loss: 0.002759, validation loss: 0.001852\n",
      "tensor(0.0018)\n",
      "iteration 5221, train loss: 0.002728, validation loss: 0.001818\n",
      "tensor(0.0018)\n",
      "iteration 5222, train loss: 0.002755, validation loss: 0.001798\n",
      "tensor(0.0019)\n",
      "iteration 5223, train loss: 0.002832, validation loss: 0.00186\n",
      "tensor(0.0019)\n",
      "iteration 5224, train loss: 0.002825, validation loss: 0.001875\n",
      "tensor(0.0018)\n",
      "iteration 5225, train loss: 0.002733, validation loss: 0.001809\n",
      "tensor(0.0019)\n",
      "iteration 5226, train loss: 0.002771, validation loss: 0.001871\n",
      "tensor(0.0018)\n",
      "iteration 5227, train loss: 0.002883, validation loss: 0.001795\n",
      "tensor(0.0019)\n",
      "iteration 5228, train loss: 0.002773, validation loss: 0.001937\n",
      "tensor(0.0019)\n",
      "iteration 5229, train loss: 0.002711, validation loss: 0.001914\n",
      "tensor(0.0018)\n",
      "iteration 5230, train loss: 0.002825, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5231, train loss: 0.002772, validation loss: 0.001834\n",
      "tensor(0.0020)\n",
      "iteration 5232, train loss: 0.00275, validation loss: 0.002023\n",
      "tensor(0.0018)\n",
      "iteration 5233, train loss: 0.002973, validation loss: 0.001835\n",
      "tensor(0.0020)\n",
      "iteration 5234, train loss: 0.002733, validation loss: 0.002004\n",
      "tensor(0.0019)\n",
      "iteration 5235, train loss: 0.003008, validation loss: 0.001884\n",
      "tensor(0.0020)\n",
      "iteration 5236, train loss: 0.002841, validation loss: 0.001957\n",
      "tensor(0.0019)\n",
      "iteration 5237, train loss: 0.002859, validation loss: 0.001853\n",
      "tensor(0.0019)\n",
      "iteration 5238, train loss: 0.002731, validation loss: 0.001903\n",
      "tensor(0.0020)\n",
      "iteration 5239, train loss: 0.002964, validation loss: 0.002024\n",
      "tensor(0.0020)\n",
      "iteration 5240, train loss: 0.002917, validation loss: 0.001973\n",
      "tensor(0.0019)\n",
      "iteration 5241, train loss: 0.002822, validation loss: 0.001878\n",
      "tensor(0.0018)\n",
      "iteration 5242, train loss: 0.002939, validation loss: 0.001825\n",
      "tensor(0.0019)\n",
      "iteration 5243, train loss: 0.002767, validation loss: 0.001927\n",
      "tensor(0.0018)\n",
      "iteration 5244, train loss: 0.002877, validation loss: \u001b[92m0.001787\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5245, train loss: 0.002772, validation loss: 0.001827\n",
      "tensor(0.0018)\n",
      "iteration 5246, train loss: 0.002814, validation loss: 0.001829\n",
      "tensor(0.0020)\n",
      "iteration 5247, train loss: 0.002785, validation loss: 0.00195\n",
      "tensor(0.0018)\n",
      "iteration 5248, train loss: 0.002853, validation loss: 0.001842\n",
      "tensor(0.0018)\n",
      "iteration 5249, train loss: 0.002744, validation loss: 0.001846\n",
      "tensor(0.0019)\n",
      "iteration 5250, train loss: 0.002858, validation loss: 0.001926\n",
      "tensor(0.0019)\n",
      "iteration 5251, train loss: 0.002871, validation loss: 0.001934\n",
      "tensor(0.0018)\n",
      "iteration 5252, train loss: 0.002755, validation loss: 0.001823\n",
      "tensor(0.0018)\n",
      "iteration 5253, train loss: 0.002779, validation loss: 0.001837\n",
      "tensor(0.0018)\n",
      "iteration 5254, train loss: 0.002777, validation loss: 0.001846\n",
      "tensor(0.0018)\n",
      "iteration 5255, train loss: 0.002745, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5256, train loss: 0.002759, validation loss: \u001b[92m0.001785\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5257, train loss: 0.002775, validation loss: 0.001804\n",
      "tensor(0.0018)\n",
      "iteration 5258, train loss: 0.00275, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5259, train loss: 0.002781, validation loss: 0.001798\n",
      "tensor(0.0018)\n",
      "iteration 5260, train loss: 0.002765, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5261, train loss: 0.002777, validation loss: 0.001837\n",
      "tensor(0.0018)\n",
      "iteration 5262, train loss: 0.002776, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5263, train loss: 0.002787, validation loss: 0.001803\n",
      "tensor(0.0019)\n",
      "iteration 5264, train loss: 0.002841, validation loss: 0.001855\n",
      "tensor(0.0019)\n",
      "iteration 5265, train loss: 0.002828, validation loss: 0.001904\n",
      "tensor(0.0018)\n",
      "iteration 5266, train loss: 0.002798, validation loss: 0.001848\n",
      "tensor(0.0018)\n",
      "iteration 5267, train loss: 0.002767, validation loss: 0.001835\n",
      "tensor(0.0019)\n",
      "iteration 5268, train loss: 0.002826, validation loss: 0.001928\n",
      "tensor(0.0019)\n",
      "iteration 5269, train loss: 0.002893, validation loss: 0.001886\n",
      "tensor(0.0019)\n",
      "iteration 5270, train loss: 0.002831, validation loss: 0.001862\n",
      "tensor(0.0018)\n",
      "iteration 5271, train loss: 0.002883, validation loss: 0.001824\n",
      "tensor(0.0018)\n",
      "iteration 5272, train loss: 0.002777, validation loss: 0.001848\n",
      "tensor(0.0018)\n",
      "iteration 5273, train loss: 0.002831, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5274, train loss: \u001b[92m0.002661\u001b[0m, validation loss: 0.001805\n",
      "tensor(0.0018)\n",
      "iteration 5275, train loss: 0.00278, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5276, train loss: 0.002716, validation loss: 0.001796\n",
      "tensor(0.0018)\n",
      "iteration 5277, train loss: 0.00267, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5278, train loss: 0.002762, validation loss: 0.001795\n",
      "tensor(0.0019)\n",
      "iteration 5279, train loss: 0.002713, validation loss: 0.001859\n",
      "tensor(0.0018)\n",
      "iteration 5280, train loss: 0.002783, validation loss: 0.00179\n",
      "tensor(0.0018)\n",
      "iteration 5281, train loss: 0.002733, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5282, train loss: 0.002702, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5283, train loss: 0.00272, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5284, train loss: 0.002752, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5285, train loss: 0.002744, validation loss: 0.001801\n",
      "tensor(0.0018)\n",
      "iteration 5286, train loss: 0.002677, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5287, train loss: 0.00272, validation loss: 0.001803\n",
      "tensor(0.0018)\n",
      "iteration 5288, train loss: 0.002692, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5289, train loss: 0.002815, validation loss: 0.001813\n",
      "tensor(0.0018)\n",
      "iteration 5290, train loss: 0.002756, validation loss: 0.001792\n",
      "tensor(0.0018)\n",
      "iteration 5291, train loss: 0.002767, validation loss: 0.001808\n",
      "tensor(0.0018)\n",
      "iteration 5292, train loss: 0.002813, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5293, train loss: 0.002789, validation loss: 0.001797\n",
      "tensor(0.0018)\n",
      "iteration 5294, train loss: \u001b[92m0.002654\u001b[0m, validation loss: \u001b[92m0.001777\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5295, train loss: 0.002736, validation loss: 0.001786\n",
      "tensor(0.0019)\n",
      "iteration 5296, train loss: 0.002724, validation loss: 0.00187\n",
      "tensor(0.0019)\n",
      "iteration 5297, train loss: 0.002731, validation loss: 0.001853\n",
      "tensor(0.0018)\n",
      "iteration 5298, train loss: 0.002839, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5299, train loss: 0.002737, validation loss: 0.001782\n",
      "tensor(0.0018)\n",
      "iteration 5300, train loss: 0.002782, validation loss: 0.001834\n",
      "tensor(0.0018)\n",
      "iteration 5301, train loss: 0.002679, validation loss: 0.001818\n",
      "tensor(0.0018)\n",
      "iteration 5302, train loss: 0.002831, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5303, train loss: 0.002739, validation loss: 0.001797\n",
      "tensor(0.0018)\n",
      "iteration 5304, train loss: 0.002731, validation loss: 0.001841\n",
      "tensor(0.0018)\n",
      "iteration 5305, train loss: 0.002811, validation loss: 0.001799\n",
      "tensor(0.0018)\n",
      "iteration 5306, train loss: 0.002783, validation loss: 0.001831\n",
      "tensor(0.0018)\n",
      "iteration 5307, train loss: 0.002726, validation loss: 0.001834\n",
      "tensor(0.0018)\n",
      "iteration 5308, train loss: 0.002767, validation loss: 0.001822\n",
      "tensor(0.0018)\n",
      "iteration 5309, train loss: 0.002766, validation loss: 0.001784\n",
      "tensor(0.0018)\n",
      "iteration 5310, train loss: 0.002751, validation loss: 0.001782\n",
      "tensor(0.0019)\n",
      "iteration 5311, train loss: 0.002775, validation loss: 0.001874\n",
      "tensor(0.0019)\n",
      "iteration 5312, train loss: 0.002756, validation loss: 0.001916\n",
      "tensor(0.0018)\n",
      "iteration 5313, train loss: 0.002719, validation loss: 0.001831\n",
      "tensor(0.0018)\n",
      "iteration 5314, train loss: 0.002803, validation loss: 0.001815\n",
      "tensor(0.0018)\n",
      "iteration 5315, train loss: 0.002763, validation loss: 0.001797\n",
      "tensor(0.0018)\n",
      "iteration 5316, train loss: 0.002794, validation loss: 0.001843\n",
      "tensor(0.0018)\n",
      "iteration 5317, train loss: 0.002802, validation loss: 0.001808\n",
      "tensor(0.0018)\n",
      "iteration 5318, train loss: 0.002791, validation loss: 0.001818\n",
      "tensor(0.0020)\n",
      "iteration 5319, train loss: 0.002879, validation loss: 0.001972\n",
      "tensor(0.0018)\n",
      "iteration 5320, train loss: 0.002933, validation loss: 0.001787\n",
      "tensor(0.0018)\n",
      "iteration 5321, train loss: 0.002669, validation loss: 0.001831\n",
      "tensor(0.0018)\n",
      "iteration 5322, train loss: 0.002769, validation loss: 0.001805\n",
      "tensor(0.0019)\n",
      "iteration 5323, train loss: 0.002795, validation loss: 0.001853\n",
      "tensor(0.0018)\n",
      "iteration 5324, train loss: 0.002834, validation loss: 0.001813\n",
      "tensor(0.0018)\n",
      "iteration 5325, train loss: 0.002705, validation loss: 0.001813\n",
      "tensor(0.0018)\n",
      "iteration 5326, train loss: 0.002741, validation loss: 0.001829\n",
      "tensor(0.0018)\n",
      "iteration 5327, train loss: 0.002767, validation loss: 0.001835\n",
      "tensor(0.0018)\n",
      "iteration 5328, train loss: 0.002768, validation loss: 0.001784\n",
      "tensor(0.0018)\n",
      "iteration 5329, train loss: 0.002756, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5330, train loss: 0.002794, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5331, train loss: 0.002733, validation loss: 0.001815\n",
      "tensor(0.0018)\n",
      "iteration 5332, train loss: 0.002685, validation loss: 0.00182\n",
      "tensor(0.0018)\n",
      "iteration 5333, train loss: 0.002716, validation loss: 0.00184\n",
      "tensor(0.0018)\n",
      "iteration 5334, train loss: 0.00281, validation loss: 0.001817\n",
      "tensor(0.0019)\n",
      "iteration 5335, train loss: 0.002745, validation loss: 0.001947\n",
      "tensor(0.0018)\n",
      "iteration 5336, train loss: 0.002963, validation loss: 0.001827\n",
      "tensor(0.0019)\n",
      "iteration 5337, train loss: 0.002776, validation loss: 0.00189\n",
      "tensor(0.0019)\n",
      "iteration 5338, train loss: 0.002867, validation loss: 0.001936\n",
      "tensor(0.0018)\n",
      "iteration 5339, train loss: 0.002881, validation loss: 0.0018\n",
      "tensor(0.0018)\n",
      "iteration 5340, train loss: 0.002734, validation loss: 0.001841\n",
      "tensor(0.0018)\n",
      "iteration 5341, train loss: 0.002864, validation loss: 0.001808\n",
      "tensor(0.0019)\n",
      "iteration 5342, train loss: 0.002734, validation loss: 0.00195\n",
      "tensor(0.0019)\n",
      "iteration 5343, train loss: 0.002829, validation loss: 0.001875\n",
      "tensor(0.0018)\n",
      "iteration 5344, train loss: 0.002835, validation loss: 0.001811\n",
      "tensor(0.0019)\n",
      "iteration 5345, train loss: 0.002804, validation loss: 0.001893\n",
      "tensor(0.0018)\n",
      "iteration 5346, train loss: 0.002756, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5347, train loss: 0.002728, validation loss: 0.00182\n",
      "tensor(0.0018)\n",
      "iteration 5348, train loss: 0.002721, validation loss: 0.001807\n",
      "tensor(0.0019)\n",
      "iteration 5349, train loss: 0.002734, validation loss: 0.001873\n",
      "tensor(0.0018)\n",
      "iteration 5350, train loss: 0.002831, validation loss: 0.001779\n",
      "tensor(0.0018)\n",
      "iteration 5351, train loss: 0.002772, validation loss: 0.00181\n",
      "tensor(0.0019)\n",
      "iteration 5352, train loss: 0.002807, validation loss: 0.001919\n",
      "tensor(0.0019)\n",
      "iteration 5353, train loss: 0.00282, validation loss: 0.001859\n",
      "tensor(0.0018)\n",
      "iteration 5354, train loss: 0.002799, validation loss: 0.001817\n",
      "tensor(0.0018)\n",
      "iteration 5355, train loss: 0.002699, validation loss: 0.001785\n",
      "tensor(0.0019)\n",
      "iteration 5356, train loss: 0.00278, validation loss: 0.00185\n",
      "tensor(0.0018)\n",
      "iteration 5357, train loss: 0.00274, validation loss: \u001b[92m0.00177\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5358, train loss: 0.002806, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5359, train loss: 0.002838, validation loss: 0.001796\n",
      "tensor(0.0018)\n",
      "iteration 5360, train loss: 0.002748, validation loss: 0.001778\n",
      "tensor(0.0018)\n",
      "iteration 5361, train loss: 0.002695, validation loss: 0.001777\n",
      "tensor(0.0018)\n",
      "iteration 5362, train loss: 0.002734, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5363, train loss: 0.002735, validation loss: 0.001842\n",
      "tensor(0.0018)\n",
      "iteration 5364, train loss: 0.002743, validation loss: 0.001777\n",
      "tensor(0.0018)\n",
      "iteration 5365, train loss: 0.002722, validation loss: \u001b[92m0.00177\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5366, train loss: 0.002722, validation loss: 0.001854\n",
      "tensor(0.0018)\n",
      "iteration 5367, train loss: 0.002709, validation loss: 0.001822\n",
      "tensor(0.0018)\n",
      "iteration 5368, train loss: 0.002785, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5369, train loss: 0.002688, validation loss: 0.001796\n",
      "tensor(0.0019)\n",
      "iteration 5370, train loss: 0.002754, validation loss: 0.001852\n",
      "tensor(0.0019)\n",
      "iteration 5371, train loss: 0.002774, validation loss: 0.001876\n",
      "tensor(0.0018)\n",
      "iteration 5372, train loss: 0.002752, validation loss: 0.00184\n",
      "tensor(0.0018)\n",
      "iteration 5373, train loss: 0.002843, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5374, train loss: 0.002774, validation loss: 0.001847\n",
      "tensor(0.0019)\n",
      "iteration 5375, train loss: 0.002716, validation loss: 0.00185\n",
      "tensor(0.0018)\n",
      "iteration 5376, train loss: 0.002774, validation loss: 0.001793\n",
      "tensor(0.0018)\n",
      "iteration 5377, train loss: 0.002843, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5378, train loss: 0.002732, validation loss: 0.001809\n",
      "tensor(0.0018)\n",
      "iteration 5379, train loss: 0.00277, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5380, train loss: 0.002723, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5381, train loss: 0.002747, validation loss: 0.001802\n",
      "tensor(0.0018)\n",
      "iteration 5382, train loss: \u001b[92m0.002626\u001b[0m, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5383, train loss: 0.002752, validation loss: 0.001798\n",
      "tensor(0.0018)\n",
      "iteration 5384, train loss: 0.002736, validation loss: \u001b[92m0.001768\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5385, train loss: 0.002781, validation loss: 0.001776\n",
      "tensor(0.0018)\n",
      "iteration 5386, train loss: 0.002681, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5387, train loss: 0.002764, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5388, train loss: 0.002701, validation loss: 0.001787\n",
      "tensor(0.0018)\n",
      "iteration 5389, train loss: 0.002784, validation loss: 0.001812\n",
      "tensor(0.0019)\n",
      "iteration 5390, train loss: 0.002809, validation loss: 0.001935\n",
      "tensor(0.0018)\n",
      "iteration 5391, train loss: 0.002808, validation loss: 0.001802\n",
      "tensor(0.0018)\n",
      "iteration 5392, train loss: 0.002746, validation loss: 0.00185\n",
      "tensor(0.0018)\n",
      "iteration 5393, train loss: 0.002783, validation loss: 0.001824\n",
      "tensor(0.0018)\n",
      "iteration 5394, train loss: 0.002817, validation loss: 0.0018\n",
      "tensor(0.0019)\n",
      "iteration 5395, train loss: 0.002754, validation loss: 0.001868\n",
      "tensor(0.0018)\n",
      "iteration 5396, train loss: 0.002879, validation loss: 0.001775\n",
      "tensor(0.0019)\n",
      "iteration 5397, train loss: 0.002749, validation loss: 0.001889\n",
      "tensor(0.0018)\n",
      "iteration 5398, train loss: 0.002795, validation loss: 0.00181\n",
      "tensor(0.0018)\n",
      "iteration 5399, train loss: 0.002684, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5400, train loss: 0.00276, validation loss: 0.001821\n",
      "tensor(0.0019)\n",
      "iteration 5401, train loss: 0.00272, validation loss: 0.001862\n",
      "tensor(0.0018)\n",
      "iteration 5402, train loss: 0.002691, validation loss: 0.001825\n",
      "tensor(0.0018)\n",
      "iteration 5403, train loss: 0.002735, validation loss: 0.001828\n",
      "tensor(0.0018)\n",
      "iteration 5404, train loss: 0.00282, validation loss: 0.00177\n",
      "tensor(0.0019)\n",
      "iteration 5405, train loss: 0.002791, validation loss: 0.001902\n",
      "tensor(0.0018)\n",
      "iteration 5406, train loss: 0.002868, validation loss: 0.00179\n",
      "tensor(0.0018)\n",
      "iteration 5407, train loss: 0.002732, validation loss: 0.001827\n",
      "tensor(0.0018)\n",
      "iteration 5408, train loss: 0.002876, validation loss: 0.001812\n",
      "tensor(0.0018)\n",
      "iteration 5409, train loss: 0.002756, validation loss: 0.001828\n",
      "tensor(0.0018)\n",
      "iteration 5410, train loss: 0.002762, validation loss: 0.001811\n",
      "tensor(0.0018)\n",
      "iteration 5411, train loss: 0.002845, validation loss: 0.001802\n",
      "tensor(0.0019)\n",
      "iteration 5412, train loss: 0.002659, validation loss: 0.001872\n",
      "tensor(0.0018)\n",
      "iteration 5413, train loss: 0.002759, validation loss: \u001b[92m0.001756\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5414, train loss: 0.002703, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5415, train loss: 0.002797, validation loss: 0.001802\n",
      "tensor(0.0018)\n",
      "iteration 5416, train loss: 0.00279, validation loss: 0.001787\n",
      "tensor(0.0018)\n",
      "iteration 5417, train loss: 0.002692, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5418, train loss: 0.002818, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5419, train loss: 0.002702, validation loss: 0.00178\n",
      "tensor(0.0018)\n",
      "iteration 5420, train loss: 0.00275, validation loss: 0.001758\n",
      "tensor(0.0018)\n",
      "iteration 5421, train loss: 0.002707, validation loss: 0.001768\n",
      "tensor(0.0018)\n",
      "iteration 5422, train loss: 0.002758, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5423, train loss: 0.002709, validation loss: 0.001774\n",
      "tensor(0.0018)\n",
      "iteration 5424, train loss: 0.002708, validation loss: 0.00176\n",
      "tensor(0.0018)\n",
      "iteration 5425, train loss: 0.002693, validation loss: 0.001763\n",
      "tensor(0.0018)\n",
      "iteration 5426, train loss: 0.002751, validation loss: 0.001768\n",
      "tensor(0.0018)\n",
      "iteration 5427, train loss: 0.002695, validation loss: 0.001769\n",
      "tensor(0.0018)\n",
      "iteration 5428, train loss: 0.002732, validation loss: 0.001798\n",
      "tensor(0.0018)\n",
      "iteration 5429, train loss: 0.00271, validation loss: 0.00177\n",
      "tensor(0.0018)\n",
      "iteration 5430, train loss: 0.002712, validation loss: 0.001778\n",
      "tensor(0.0018)\n",
      "iteration 5431, train loss: 0.002702, validation loss: 0.001774\n",
      "tensor(0.0018)\n",
      "iteration 5432, train loss: 0.00271, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5433, train loss: 0.002777, validation loss: 0.001796\n",
      "tensor(0.0019)\n",
      "iteration 5434, train loss: 0.002665, validation loss: 0.001852\n",
      "tensor(0.0018)\n",
      "iteration 5435, train loss: 0.002703, validation loss: 0.001792\n",
      "tensor(0.0018)\n",
      "iteration 5436, train loss: 0.002758, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5437, train loss: 0.002754, validation loss: 0.00185\n",
      "tensor(0.0018)\n",
      "iteration 5438, train loss: 0.002731, validation loss: 0.001813\n",
      "tensor(0.0018)\n",
      "iteration 5439, train loss: 0.002747, validation loss: 0.001817\n",
      "tensor(0.0018)\n",
      "iteration 5440, train loss: 0.002759, validation loss: 0.001803\n",
      "tensor(0.0019)\n",
      "iteration 5441, train loss: 0.002726, validation loss: 0.001851\n",
      "tensor(0.0019)\n",
      "iteration 5442, train loss: 0.002773, validation loss: 0.00187\n",
      "tensor(0.0018)\n",
      "iteration 5443, train loss: 0.002821, validation loss: 0.001821\n",
      "tensor(0.0019)\n",
      "iteration 5444, train loss: 0.002775, validation loss: 0.001874\n",
      "tensor(0.0018)\n",
      "iteration 5445, train loss: 0.002768, validation loss: 0.001808\n",
      "tensor(0.0018)\n",
      "iteration 5446, train loss: 0.0027, validation loss: 0.001813\n",
      "tensor(0.0018)\n",
      "iteration 5447, train loss: 0.00283, validation loss: 0.001795\n",
      "tensor(0.0020)\n",
      "iteration 5448, train loss: 0.002748, validation loss: 0.001974\n",
      "tensor(0.0018)\n",
      "iteration 5449, train loss: 0.002921, validation loss: 0.001784\n",
      "tensor(0.0018)\n",
      "iteration 5450, train loss: 0.002789, validation loss: 0.001826\n",
      "tensor(0.0018)\n",
      "iteration 5451, train loss: 0.002792, validation loss: 0.001804\n",
      "tensor(0.0019)\n",
      "iteration 5452, train loss: 0.002808, validation loss: 0.001905\n",
      "tensor(0.0018)\n",
      "iteration 5453, train loss: 0.002736, validation loss: 0.001836\n",
      "tensor(0.0018)\n",
      "iteration 5454, train loss: 0.002802, validation loss: 0.001773\n",
      "tensor(0.0020)\n",
      "iteration 5455, train loss: 0.002791, validation loss: 0.001961\n",
      "tensor(0.0018)\n",
      "iteration 5456, train loss: 0.002983, validation loss: 0.001779\n",
      "tensor(0.0019)\n",
      "iteration 5457, train loss: 0.002816, validation loss: 0.001944\n",
      "tensor(0.0018)\n",
      "iteration 5458, train loss: 0.002905, validation loss: 0.001786\n",
      "tensor(0.0019)\n",
      "iteration 5459, train loss: 0.002772, validation loss: 0.001898\n",
      "tensor(0.0018)\n",
      "iteration 5460, train loss: 0.002769, validation loss: 0.001771\n",
      "tensor(0.0018)\n",
      "iteration 5461, train loss: 0.002648, validation loss: 0.001806\n",
      "tensor(0.0018)\n",
      "iteration 5462, train loss: 0.002813, validation loss: 0.001828\n",
      "tensor(0.0018)\n",
      "iteration 5463, train loss: 0.002789, validation loss: 0.001796\n",
      "tensor(0.0018)\n",
      "iteration 5464, train loss: 0.002678, validation loss: 0.001798\n",
      "tensor(0.0018)\n",
      "iteration 5465, train loss: 0.00281, validation loss: 0.001768\n",
      "tensor(0.0018)\n",
      "iteration 5466, train loss: 0.002711, validation loss: 0.001783\n",
      "tensor(0.0018)\n",
      "iteration 5467, train loss: 0.00269, validation loss: 0.00183\n",
      "tensor(0.0018)\n",
      "iteration 5468, train loss: 0.002768, validation loss: 0.001774\n",
      "tensor(0.0018)\n",
      "iteration 5469, train loss: 0.00266, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5470, train loss: 0.002755, validation loss: 0.001765\n",
      "tensor(0.0018)\n",
      "iteration 5471, train loss: 0.002701, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5472, train loss: 0.002828, validation loss: 0.001816\n",
      "tensor(0.0018)\n",
      "iteration 5473, train loss: 0.002762, validation loss: 0.001773\n",
      "tensor(0.0018)\n",
      "iteration 5474, train loss: 0.002668, validation loss: 0.001805\n",
      "tensor(0.0018)\n",
      "iteration 5475, train loss: 0.00272, validation loss: 0.001829\n",
      "tensor(0.0018)\n",
      "iteration 5476, train loss: 0.002725, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5477, train loss: 0.002802, validation loss: 0.001762\n",
      "tensor(0.0018)\n",
      "iteration 5478, train loss: 0.002687, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5479, train loss: 0.002751, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5480, train loss: 0.002736, validation loss: \u001b[92m0.001755\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5481, train loss: 0.002769, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5482, train loss: 0.002697, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5483, train loss: \u001b[92m0.0026\u001b[0m, validation loss: 0.001764\n",
      "tensor(0.0018)\n",
      "iteration 5484, train loss: 0.002702, validation loss: 0.001755\n",
      "tensor(0.0018)\n",
      "iteration 5485, train loss: 0.002728, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5486, train loss: 0.002721, validation loss: \u001b[92m0.001754\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5487, train loss: 0.002764, validation loss: \u001b[92m0.00175\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5488, train loss: 0.002712, validation loss: 0.001759\n",
      "tensor(0.0018)\n",
      "iteration 5489, train loss: 0.002693, validation loss: 0.001844\n",
      "tensor(0.0018)\n",
      "iteration 5490, train loss: 0.002764, validation loss: 0.001754\n",
      "tensor(0.0018)\n",
      "iteration 5491, train loss: 0.002683, validation loss: 0.001765\n",
      "tensor(0.0017)\n",
      "iteration 5492, train loss: 0.00269, validation loss: \u001b[92m0.001745\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5493, train loss: 0.002644, validation loss: 0.001798\n",
      "tensor(0.0017)\n",
      "iteration 5494, train loss: 0.002699, validation loss: \u001b[92m0.001741\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5495, train loss: 0.002658, validation loss: 0.001758\n",
      "tensor(0.0018)\n",
      "iteration 5496, train loss: 0.002691, validation loss: 0.001764\n",
      "tensor(0.0019)\n",
      "iteration 5497, train loss: 0.002682, validation loss: 0.001855\n",
      "tensor(0.0017)\n",
      "iteration 5498, train loss: 0.002802, validation loss: 0.001749\n",
      "tensor(0.0018)\n",
      "iteration 5499, train loss: 0.002714, validation loss: 0.001772\n",
      "tensor(0.0018)\n",
      "iteration 5500, train loss: 0.002692, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5501, train loss: 0.002779, validation loss: 0.001779\n",
      "tensor(0.0018)\n",
      "iteration 5502, train loss: 0.00274, validation loss: 0.001752\n",
      "tensor(0.0018)\n",
      "iteration 5503, train loss: 0.002733, validation loss: 0.00177\n",
      "tensor(0.0018)\n",
      "iteration 5504, train loss: 0.00275, validation loss: 0.001815\n",
      "tensor(0.0018)\n",
      "iteration 5505, train loss: 0.002704, validation loss: 0.001796\n",
      "tensor(0.0018)\n",
      "iteration 5506, train loss: 0.002778, validation loss: 0.001779\n",
      "tensor(0.0019)\n",
      "iteration 5507, train loss: 0.002697, validation loss: 0.001866\n",
      "tensor(0.0018)\n",
      "iteration 5508, train loss: 0.002742, validation loss: 0.001771\n",
      "tensor(0.0018)\n",
      "iteration 5509, train loss: 0.002688, validation loss: 0.001784\n",
      "tensor(0.0018)\n",
      "iteration 5510, train loss: 0.002691, validation loss: 0.001768\n",
      "tensor(0.0019)\n",
      "iteration 5511, train loss: 0.002694, validation loss: 0.001902\n",
      "tensor(0.0018)\n",
      "iteration 5512, train loss: 0.002849, validation loss: 0.001772\n",
      "tensor(0.0018)\n",
      "iteration 5513, train loss: 0.00272, validation loss: 0.001771\n",
      "tensor(0.0018)\n",
      "iteration 5514, train loss: 0.002782, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5515, train loss: 0.002761, validation loss: 0.001765\n",
      "tensor(0.0018)\n",
      "iteration 5516, train loss: 0.002762, validation loss: 0.001781\n",
      "tensor(0.0018)\n",
      "iteration 5517, train loss: 0.002799, validation loss: 0.001817\n",
      "tensor(0.0018)\n",
      "iteration 5518, train loss: 0.002813, validation loss: 0.001786\n",
      "tensor(0.0018)\n",
      "iteration 5519, train loss: 0.002843, validation loss: 0.001793\n",
      "tensor(0.0018)\n",
      "iteration 5520, train loss: 0.002779, validation loss: 0.001762\n",
      "tensor(0.0018)\n",
      "iteration 5521, train loss: 0.002736, validation loss: 0.001811\n",
      "tensor(0.0019)\n",
      "iteration 5522, train loss: 0.00269, validation loss: 0.0019\n",
      "tensor(0.0018)\n",
      "iteration 5523, train loss: 0.002878, validation loss: 0.001822\n",
      "tensor(0.0018)\n",
      "iteration 5524, train loss: 0.002705, validation loss: 0.00177\n",
      "tensor(0.0019)\n",
      "iteration 5525, train loss: 0.002782, validation loss: 0.001873\n",
      "tensor(0.0018)\n",
      "iteration 5526, train loss: 0.002779, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5527, train loss: 0.002662, validation loss: 0.001849\n",
      "tensor(0.0018)\n",
      "iteration 5528, train loss: 0.00274, validation loss: 0.001818\n",
      "tensor(0.0018)\n",
      "iteration 5529, train loss: 0.002727, validation loss: 0.001815\n",
      "tensor(0.0018)\n",
      "iteration 5530, train loss: 0.002729, validation loss: 0.001761\n",
      "tensor(0.0019)\n",
      "iteration 5531, train loss: 0.002676, validation loss: 0.001852\n",
      "tensor(0.0018)\n",
      "iteration 5532, train loss: 0.002942, validation loss: 0.001805\n",
      "tensor(0.0020)\n",
      "iteration 5533, train loss: \u001b[92m0.002595\u001b[0m, validation loss: 0.001957\n",
      "tensor(0.0018)\n",
      "iteration 5534, train loss: 0.002858, validation loss: 0.001842\n",
      "tensor(0.0018)\n",
      "iteration 5535, train loss: 0.002831, validation loss: 0.001814\n",
      "tensor(0.0020)\n",
      "iteration 5536, train loss: 0.002836, validation loss: 0.001958\n",
      "tensor(0.0017)\n",
      "iteration 5537, train loss: 0.00302, validation loss: 0.001742\n",
      "tensor(0.0018)\n",
      "iteration 5538, train loss: 0.002697, validation loss: 0.001822\n",
      "tensor(0.0018)\n",
      "iteration 5539, train loss: 0.002708, validation loss: 0.001757\n",
      "tensor(0.0019)\n",
      "iteration 5540, train loss: 0.002729, validation loss: 0.001918\n",
      "tensor(0.0018)\n",
      "iteration 5541, train loss: 0.002818, validation loss: 0.001764\n",
      "tensor(0.0017)\n",
      "iteration 5542, train loss: 0.002807, validation loss: 0.001743\n",
      "tensor(0.0018)\n",
      "iteration 5543, train loss: 0.002669, validation loss: 0.0018\n",
      "tensor(0.0018)\n",
      "iteration 5544, train loss: 0.002728, validation loss: 0.001773\n",
      "tensor(0.0018)\n",
      "iteration 5545, train loss: 0.002663, validation loss: 0.001755\n",
      "tensor(0.0018)\n",
      "iteration 5546, train loss: 0.002723, validation loss: 0.001761\n",
      "tensor(0.0017)\n",
      "iteration 5547, train loss: 0.002739, validation loss: 0.001742\n",
      "tensor(0.0017)\n",
      "iteration 5548, train loss: 0.002657, validation loss: \u001b[92m0.001737\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5549, train loss: 0.002662, validation loss: 0.001751\n",
      "tensor(0.0018)\n",
      "iteration 5550, train loss: 0.002725, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5551, train loss: 0.002747, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5552, train loss: 0.002737, validation loss: 0.001778\n",
      "tensor(0.0017)\n",
      "iteration 5553, train loss: 0.002663, validation loss: 0.00174\n",
      "tensor(0.0018)\n",
      "iteration 5554, train loss: 0.002653, validation loss: 0.001825\n",
      "tensor(0.0018)\n",
      "iteration 5555, train loss: 0.002791, validation loss: 0.001767\n",
      "tensor(0.0017)\n",
      "iteration 5556, train loss: 0.002783, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 5557, train loss: 0.002716, validation loss: 0.001745\n",
      "tensor(0.0018)\n",
      "iteration 5558, train loss: 0.002676, validation loss: 0.001777\n",
      "tensor(0.0018)\n",
      "iteration 5559, train loss: 0.002742, validation loss: 0.001751\n",
      "tensor(0.0017)\n",
      "iteration 5560, train loss: 0.002685, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 5561, train loss: 0.002685, validation loss: 0.001743\n",
      "tensor(0.0017)\n",
      "iteration 5562, train loss: 0.002682, validation loss: 0.001737\n",
      "tensor(0.0018)\n",
      "iteration 5563, train loss: 0.002655, validation loss: 0.001765\n",
      "tensor(0.0017)\n",
      "iteration 5564, train loss: 0.0027, validation loss: 0.001737\n",
      "tensor(0.0018)\n",
      "iteration 5565, train loss: 0.002721, validation loss: 0.001797\n",
      "tensor(0.0018)\n",
      "iteration 5566, train loss: 0.002755, validation loss: 0.001772\n",
      "tensor(0.0018)\n",
      "iteration 5567, train loss: 0.002652, validation loss: 0.001763\n",
      "tensor(0.0018)\n",
      "iteration 5568, train loss: 0.002702, validation loss: 0.001776\n",
      "tensor(0.0018)\n",
      "iteration 5569, train loss: 0.002738, validation loss: 0.001777\n",
      "tensor(0.0018)\n",
      "iteration 5570, train loss: 0.002698, validation loss: 0.001795\n",
      "tensor(0.0018)\n",
      "iteration 5571, train loss: 0.00268, validation loss: 0.001767\n",
      "tensor(0.0017)\n",
      "iteration 5572, train loss: 0.002702, validation loss: 0.001747\n",
      "tensor(0.0018)\n",
      "iteration 5573, train loss: 0.002682, validation loss: 0.001773\n",
      "tensor(0.0018)\n",
      "iteration 5574, train loss: 0.00276, validation loss: 0.001762\n",
      "tensor(0.0018)\n",
      "iteration 5575, train loss: 0.00275, validation loss: 0.001805\n",
      "tensor(0.0018)\n",
      "iteration 5576, train loss: 0.002668, validation loss: 0.001774\n",
      "tensor(0.0018)\n",
      "iteration 5577, train loss: 0.002788, validation loss: 0.001812\n",
      "tensor(0.0018)\n",
      "iteration 5578, train loss: 0.002744, validation loss: 0.001767\n",
      "tensor(0.0018)\n",
      "iteration 5579, train loss: 0.00268, validation loss: 0.001785\n",
      "tensor(0.0018)\n",
      "iteration 5580, train loss: 0.002821, validation loss: 0.001762\n",
      "tensor(0.0018)\n",
      "iteration 5581, train loss: 0.002697, validation loss: 0.001788\n",
      "tensor(0.0017)\n",
      "iteration 5582, train loss: 0.002701, validation loss: 0.001738\n",
      "tensor(0.0018)\n",
      "iteration 5583, train loss: 0.002654, validation loss: 0.001753\n",
      "tensor(0.0018)\n",
      "iteration 5584, train loss: 0.002714, validation loss: 0.001848\n",
      "tensor(0.0018)\n",
      "iteration 5585, train loss: 0.002783, validation loss: 0.001819\n",
      "tensor(0.0018)\n",
      "iteration 5586, train loss: 0.002745, validation loss: 0.001785\n",
      "tensor(0.0017)\n",
      "iteration 5587, train loss: 0.00275, validation loss: 0.001744\n",
      "tensor(0.0018)\n",
      "iteration 5588, train loss: 0.002649, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5589, train loss: 0.002744, validation loss: 0.001774\n",
      "tensor(0.0018)\n",
      "iteration 5590, train loss: 0.002684, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5591, train loss: 0.002668, validation loss: 0.00179\n",
      "tensor(0.0017)\n",
      "iteration 5592, train loss: 0.002796, validation loss: 0.001746\n",
      "tensor(0.0018)\n",
      "iteration 5593, train loss: 0.002733, validation loss: 0.001805\n",
      "tensor(0.0018)\n",
      "iteration 5594, train loss: 0.002847, validation loss: 0.001782\n",
      "tensor(0.0019)\n",
      "iteration 5595, train loss: 0.002671, validation loss: 0.001886\n",
      "tensor(0.0018)\n",
      "iteration 5596, train loss: 0.00275, validation loss: 0.001783\n",
      "tensor(0.0017)\n",
      "iteration 5597, train loss: 0.002841, validation loss: \u001b[92m0.00173\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5598, train loss: 0.002686, validation loss: 0.00187\n",
      "tensor(0.0017)\n",
      "iteration 5599, train loss: 0.002777, validation loss: \u001b[92m0.001729\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5600, train loss: 0.002632, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5601, train loss: 0.002769, validation loss: 0.001773\n",
      "tensor(0.0018)\n",
      "iteration 5602, train loss: 0.002782, validation loss: 0.001775\n",
      "tensor(0.0018)\n",
      "iteration 5603, train loss: 0.002722, validation loss: 0.001834\n",
      "tensor(0.0017)\n",
      "iteration 5604, train loss: 0.002832, validation loss: 0.001747\n",
      "tensor(0.0018)\n",
      "iteration 5605, train loss: 0.002695, validation loss: 0.001811\n",
      "tensor(0.0018)\n",
      "iteration 5606, train loss: 0.002769, validation loss: 0.001788\n",
      "tensor(0.0018)\n",
      "iteration 5607, train loss: 0.002698, validation loss: 0.001761\n",
      "tensor(0.0018)\n",
      "iteration 5608, train loss: 0.002684, validation loss: 0.001775\n",
      "tensor(0.0018)\n",
      "iteration 5609, train loss: 0.002751, validation loss: 0.001782\n",
      "tensor(0.0019)\n",
      "iteration 5610, train loss: 0.00269, validation loss: 0.00191\n",
      "tensor(0.0018)\n",
      "iteration 5611, train loss: 0.002782, validation loss: 0.001776\n",
      "tensor(0.0018)\n",
      "iteration 5612, train loss: 0.002767, validation loss: 0.001814\n",
      "tensor(0.0018)\n",
      "iteration 5613, train loss: 0.002832, validation loss: 0.001791\n",
      "tensor(0.0018)\n",
      "iteration 5614, train loss: 0.002778, validation loss: 0.001801\n",
      "tensor(0.0017)\n",
      "iteration 5615, train loss: 0.002769, validation loss: 0.001745\n",
      "tensor(0.0018)\n",
      "iteration 5616, train loss: \u001b[92m0.002579\u001b[0m, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 5617, train loss: 0.002654, validation loss: 0.001792\n",
      "tensor(0.0019)\n",
      "iteration 5618, train loss: 0.002657, validation loss: 0.001851\n",
      "tensor(0.0018)\n",
      "iteration 5619, train loss: 0.002756, validation loss: 0.001754\n",
      "tensor(0.0018)\n",
      "iteration 5620, train loss: 0.002659, validation loss: 0.001772\n",
      "tensor(0.0018)\n",
      "iteration 5621, train loss: 0.002736, validation loss: 0.001779\n",
      "tensor(0.0018)\n",
      "iteration 5622, train loss: 0.002638, validation loss: 0.001776\n",
      "tensor(0.0018)\n",
      "iteration 5623, train loss: 0.002697, validation loss: 0.001785\n",
      "tensor(0.0018)\n",
      "iteration 5624, train loss: 0.002741, validation loss: 0.001755\n",
      "tensor(0.0020)\n",
      "iteration 5625, train loss: 0.002877, validation loss: 0.00204\n",
      "tensor(0.0018)\n",
      "iteration 5626, train loss: 0.00292, validation loss: 0.001785\n",
      "tensor(0.0020)\n",
      "iteration 5627, train loss: 0.002707, validation loss: 0.001974\n",
      "tensor(0.0017)\n",
      "iteration 5628, train loss: 0.002994, validation loss: 0.001737\n",
      "tensor(0.0021)\n",
      "iteration 5629, train loss: 0.002714, validation loss: 0.00208\n",
      "tensor(0.0018)\n",
      "iteration 5630, train loss: 0.002988, validation loss: 0.001764\n",
      "tensor(0.0018)\n",
      "iteration 5631, train loss: 0.002713, validation loss: 0.001817\n",
      "tensor(0.0018)\n",
      "iteration 5632, train loss: 0.002773, validation loss: 0.001847\n",
      "tensor(0.0018)\n",
      "iteration 5633, train loss: 0.002755, validation loss: 0.001792\n",
      "tensor(0.0018)\n",
      "iteration 5634, train loss: 0.002673, validation loss: 0.001762\n",
      "tensor(0.0017)\n",
      "iteration 5635, train loss: 0.002688, validation loss: 0.001749\n",
      "tensor(0.0018)\n",
      "iteration 5636, train loss: 0.002712, validation loss: 0.001811\n",
      "tensor(0.0018)\n",
      "iteration 5637, train loss: 0.002766, validation loss: 0.001753\n",
      "tensor(0.0018)\n",
      "iteration 5638, train loss: 0.002689, validation loss: 0.001766\n",
      "tensor(0.0017)\n",
      "iteration 5639, train loss: 0.002775, validation loss: 0.001735\n",
      "tensor(0.0018)\n",
      "iteration 5640, train loss: 0.002679, validation loss: 0.001805\n",
      "tensor(0.0017)\n",
      "iteration 5641, train loss: 0.002687, validation loss: \u001b[92m0.001727\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5642, train loss: 0.002663, validation loss: 0.001732\n",
      "tensor(0.0018)\n",
      "iteration 5643, train loss: 0.002749, validation loss: 0.001776\n",
      "tensor(0.0017)\n",
      "iteration 5644, train loss: 0.002695, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 5645, train loss: 0.002672, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 5646, train loss: 0.002652, validation loss: 0.00175\n",
      "tensor(0.0017)\n",
      "iteration 5647, train loss: 0.00268, validation loss: 0.001742\n",
      "tensor(0.0018)\n",
      "iteration 5648, train loss: 0.002721, validation loss: 0.001757\n",
      "tensor(0.0018)\n",
      "iteration 5649, train loss: 0.002747, validation loss: 0.001761\n",
      "tensor(0.0018)\n",
      "iteration 5650, train loss: 0.002696, validation loss: 0.001768\n",
      "tensor(0.0018)\n",
      "iteration 5651, train loss: 0.002798, validation loss: 0.001793\n",
      "tensor(0.0017)\n",
      "iteration 5652, train loss: 0.002752, validation loss: 0.001738\n",
      "tensor(0.0018)\n",
      "iteration 5653, train loss: 0.002636, validation loss: 0.001828\n",
      "tensor(0.0019)\n",
      "iteration 5654, train loss: 0.002746, validation loss: 0.001864\n",
      "tensor(0.0018)\n",
      "iteration 5655, train loss: 0.002785, validation loss: 0.001771\n",
      "tensor(0.0018)\n",
      "iteration 5656, train loss: 0.002655, validation loss: 0.001828\n",
      "tensor(0.0017)\n",
      "iteration 5657, train loss: 0.002817, validation loss: 0.001732\n",
      "tensor(0.0018)\n",
      "iteration 5658, train loss: 0.002668, validation loss: 0.001831\n",
      "tensor(0.0017)\n",
      "iteration 5659, train loss: 0.002797, validation loss: 0.001743\n",
      "tensor(0.0018)\n",
      "iteration 5660, train loss: 0.002725, validation loss: 0.001752\n",
      "tensor(0.0018)\n",
      "iteration 5661, train loss: 0.002743, validation loss: 0.001755\n",
      "tensor(0.0017)\n",
      "iteration 5662, train loss: 0.002732, validation loss: \u001b[92m0.001719\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5663, train loss: 0.002631, validation loss: 0.001731\n",
      "tensor(0.0018)\n",
      "iteration 5664, train loss: 0.002764, validation loss: 0.001817\n",
      "tensor(0.0018)\n",
      "iteration 5665, train loss: 0.002683, validation loss: 0.00177\n",
      "tensor(0.0018)\n",
      "iteration 5666, train loss: 0.002644, validation loss: 0.001757\n",
      "tensor(0.0017)\n",
      "iteration 5667, train loss: 0.00265, validation loss: 0.001737\n",
      "tensor(0.0018)\n",
      "iteration 5668, train loss: 0.002662, validation loss: 0.001785\n",
      "tensor(0.0017)\n",
      "iteration 5669, train loss: 0.002746, validation loss: 0.001723\n",
      "tensor(0.0017)\n",
      "iteration 5670, train loss: 0.00264, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 5671, train loss: 0.002679, validation loss: 0.001748\n",
      "tensor(0.0017)\n",
      "iteration 5672, train loss: 0.00268, validation loss: \u001b[92m0.001714\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5673, train loss: 0.00268, validation loss: 0.001735\n",
      "tensor(0.0017)\n",
      "iteration 5674, train loss: 0.002725, validation loss: 0.001727\n",
      "tensor(0.0017)\n",
      "iteration 5675, train loss: 0.002757, validation loss: 0.001734\n",
      "tensor(0.0017)\n",
      "iteration 5676, train loss: 0.002738, validation loss: 0.001742\n",
      "tensor(0.0018)\n",
      "iteration 5677, train loss: 0.002657, validation loss: 0.001783\n",
      "tensor(0.0017)\n",
      "iteration 5678, train loss: 0.002629, validation loss: 0.001739\n",
      "tensor(0.0018)\n",
      "iteration 5679, train loss: 0.002641, validation loss: 0.001753\n",
      "tensor(0.0018)\n",
      "iteration 5680, train loss: 0.002744, validation loss: 0.001764\n",
      "tensor(0.0018)\n",
      "iteration 5681, train loss: 0.002725, validation loss: 0.001794\n",
      "tensor(0.0017)\n",
      "iteration 5682, train loss: 0.002766, validation loss: 0.001736\n",
      "tensor(0.0017)\n",
      "iteration 5683, train loss: 0.002724, validation loss: 0.001736\n",
      "tensor(0.0018)\n",
      "iteration 5684, train loss: 0.002703, validation loss: 0.00177\n",
      "tensor(0.0019)\n",
      "iteration 5685, train loss: 0.002638, validation loss: 0.001867\n",
      "tensor(0.0018)\n",
      "iteration 5686, train loss: 0.002724, validation loss: 0.001759\n",
      "tensor(0.0017)\n",
      "iteration 5687, train loss: 0.002719, validation loss: 0.001729\n",
      "tensor(0.0018)\n",
      "iteration 5688, train loss: 0.00267, validation loss: 0.001756\n",
      "tensor(0.0018)\n",
      "iteration 5689, train loss: 0.00274, validation loss: 0.001763\n",
      "tensor(0.0018)\n",
      "iteration 5690, train loss: 0.002641, validation loss: 0.001766\n",
      "tensor(0.0017)\n",
      "iteration 5691, train loss: 0.002653, validation loss: 0.001718\n",
      "tensor(0.0017)\n",
      "iteration 5692, train loss: 0.002615, validation loss: 0.001737\n",
      "tensor(0.0018)\n",
      "iteration 5693, train loss: 0.002765, validation loss: 0.001787\n",
      "tensor(0.0019)\n",
      "iteration 5694, train loss: 0.002641, validation loss: 0.001888\n",
      "tensor(0.0019)\n",
      "iteration 5695, train loss: 0.002887, validation loss: 0.001863\n",
      "tensor(0.0018)\n",
      "iteration 5696, train loss: 0.002817, validation loss: 0.001768\n",
      "tensor(0.0019)\n",
      "iteration 5697, train loss: 0.002777, validation loss: 0.001928\n",
      "tensor(0.0017)\n",
      "iteration 5698, train loss: 0.002893, validation loss: 0.001735\n",
      "tensor(0.0018)\n",
      "iteration 5699, train loss: 0.002682, validation loss: 0.001815\n",
      "tensor(0.0017)\n",
      "iteration 5700, train loss: 0.002847, validation loss: 0.00172\n",
      "tensor(0.0020)\n",
      "iteration 5701, train loss: 0.002701, validation loss: 0.001958\n",
      "tensor(0.0017)\n",
      "iteration 5702, train loss: 0.002784, validation loss: 0.001742\n",
      "tensor(0.0018)\n",
      "iteration 5703, train loss: 0.002659, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 5704, train loss: 0.002752, validation loss: 0.001726\n",
      "tensor(0.0018)\n",
      "iteration 5705, train loss: 0.002743, validation loss: 0.001769\n",
      "tensor(0.0017)\n",
      "iteration 5706, train loss: 0.002673, validation loss: 0.001725\n",
      "tensor(0.0017)\n",
      "iteration 5707, train loss: 0.002642, validation loss: 0.00174\n",
      "tensor(0.0018)\n",
      "iteration 5708, train loss: 0.002689, validation loss: 0.001758\n",
      "tensor(0.0018)\n",
      "iteration 5709, train loss: 0.002637, validation loss: 0.001752\n",
      "tensor(0.0017)\n",
      "iteration 5710, train loss: 0.00273, validation loss: 0.001748\n",
      "tensor(0.0017)\n",
      "iteration 5711, train loss: 0.00272, validation loss: 0.001727\n",
      "tensor(0.0018)\n",
      "iteration 5712, train loss: 0.002633, validation loss: 0.001839\n",
      "tensor(0.0018)\n",
      "iteration 5713, train loss: 0.00274, validation loss: 0.001812\n",
      "tensor(0.0017)\n",
      "iteration 5714, train loss: 0.002727, validation loss: 0.001732\n",
      "tensor(0.0017)\n",
      "iteration 5715, train loss: 0.002711, validation loss: 0.00173\n",
      "tensor(0.0018)\n",
      "iteration 5716, train loss: 0.002678, validation loss: 0.001758\n",
      "tensor(0.0017)\n",
      "iteration 5717, train loss: 0.002684, validation loss: 0.001719\n",
      "tensor(0.0017)\n",
      "iteration 5718, train loss: 0.002625, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 5719, train loss: 0.002697, validation loss: 0.001714\n",
      "tensor(0.0018)\n",
      "iteration 5720, train loss: 0.002639, validation loss: 0.001776\n",
      "tensor(0.0017)\n",
      "iteration 5721, train loss: 0.00274, validation loss: 0.001739\n",
      "tensor(0.0018)\n",
      "iteration 5722, train loss: 0.00273, validation loss: 0.001756\n",
      "tensor(0.0018)\n",
      "iteration 5723, train loss: 0.002715, validation loss: 0.001812\n",
      "tensor(0.0017)\n",
      "iteration 5724, train loss: 0.002696, validation loss: 0.00173\n",
      "tensor(0.0018)\n",
      "iteration 5725, train loss: 0.002616, validation loss: 0.001796\n",
      "tensor(0.0017)\n",
      "iteration 5726, train loss: 0.002792, validation loss: 0.001714\n",
      "tensor(0.0018)\n",
      "iteration 5727, train loss: 0.002649, validation loss: 0.001797\n",
      "tensor(0.0017)\n",
      "iteration 5728, train loss: 0.002691, validation loss: 0.001728\n",
      "tensor(0.0018)\n",
      "iteration 5729, train loss: 0.002645, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 5730, train loss: 0.002771, validation loss: 0.001718\n",
      "tensor(0.0018)\n",
      "iteration 5731, train loss: 0.002659, validation loss: 0.001764\n",
      "tensor(0.0017)\n",
      "iteration 5732, train loss: 0.002675, validation loss: 0.001721\n",
      "tensor(0.0017)\n",
      "iteration 5733, train loss: 0.002686, validation loss: 0.001721\n",
      "tensor(0.0017)\n",
      "iteration 5734, train loss: 0.002588, validation loss: 0.001745\n",
      "tensor(0.0017)\n",
      "iteration 5735, train loss: 0.002649, validation loss: 0.001749\n",
      "tensor(0.0017)\n",
      "iteration 5736, train loss: 0.002629, validation loss: 0.001745\n",
      "tensor(0.0017)\n",
      "iteration 5737, train loss: 0.002597, validation loss: 0.001726\n",
      "tensor(0.0017)\n",
      "iteration 5738, train loss: 0.002667, validation loss: \u001b[92m0.001706\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5739, train loss: 0.002604, validation loss: 0.001708\n",
      "tensor(0.0017)\n",
      "iteration 5740, train loss: 0.002592, validation loss: 0.001714\n",
      "tensor(0.0017)\n",
      "iteration 5741, train loss: 0.002611, validation loss: 0.001735\n",
      "tensor(0.0018)\n",
      "iteration 5742, train loss: 0.002681, validation loss: 0.001814\n",
      "tensor(0.0017)\n",
      "iteration 5743, train loss: 0.002751, validation loss: 0.001719\n",
      "tensor(0.0017)\n",
      "iteration 5744, train loss: 0.002666, validation loss: 0.00173\n",
      "tensor(0.0018)\n",
      "iteration 5745, train loss: 0.002743, validation loss: 0.001807\n",
      "tensor(0.0017)\n",
      "iteration 5746, train loss: 0.00281, validation loss: \u001b[92m0.001703\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5747, train loss: 0.002644, validation loss: 0.001768\n",
      "tensor(0.0017)\n",
      "iteration 5748, train loss: 0.002676, validation loss: \u001b[92m0.001698\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5749, train loss: 0.00266, validation loss: 0.001871\n",
      "tensor(0.0017)\n",
      "iteration 5750, train loss: 0.002722, validation loss: 0.001733\n",
      "tensor(0.0018)\n",
      "iteration 5751, train loss: 0.002673, validation loss: 0.001816\n",
      "tensor(0.0017)\n",
      "iteration 5752, train loss: 0.002852, validation loss: 0.00173\n",
      "tensor(0.0018)\n",
      "iteration 5753, train loss: 0.00262, validation loss: 0.001846\n",
      "tensor(0.0017)\n",
      "iteration 5754, train loss: 0.002788, validation loss: 0.001741\n",
      "tensor(0.0017)\n",
      "iteration 5755, train loss: 0.002664, validation loss: 0.001739\n",
      "tensor(0.0018)\n",
      "iteration 5756, train loss: 0.002768, validation loss: 0.001785\n",
      "tensor(0.0018)\n",
      "iteration 5757, train loss: 0.002705, validation loss: 0.001782\n",
      "tensor(0.0018)\n",
      "iteration 5758, train loss: 0.002667, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 5759, train loss: 0.002705, validation loss: 0.001709\n",
      "tensor(0.0018)\n",
      "iteration 5760, train loss: 0.002621, validation loss: 0.001784\n",
      "tensor(0.0017)\n",
      "iteration 5761, train loss: 0.002689, validation loss: 0.001746\n",
      "tensor(0.0018)\n",
      "iteration 5762, train loss: 0.002636, validation loss: 0.001751\n",
      "tensor(0.0017)\n",
      "iteration 5763, train loss: 0.002768, validation loss: \u001b[92m0.001698\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5764, train loss: 0.002657, validation loss: 0.001782\n",
      "tensor(0.0017)\n",
      "iteration 5765, train loss: 0.002782, validation loss: \u001b[92m0.001696\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5766, train loss: 0.002732, validation loss: \u001b[92m0.001694\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5767, train loss: 0.002633, validation loss: 0.001791\n",
      "tensor(0.0017)\n",
      "iteration 5768, train loss: 0.00265, validation loss: 0.001718\n",
      "tensor(0.0017)\n",
      "iteration 5769, train loss: 0.002636, validation loss: 0.001709\n",
      "tensor(0.0017)\n",
      "iteration 5770, train loss: 0.002653, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 5771, train loss: 0.002732, validation loss: 0.001741\n",
      "tensor(0.0017)\n",
      "iteration 5772, train loss: 0.0027, validation loss: 0.001706\n",
      "tensor(0.0017)\n",
      "iteration 5773, train loss: 0.002623, validation loss: 0.001718\n",
      "tensor(0.0017)\n",
      "iteration 5774, train loss: 0.002696, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 5775, train loss: 0.00268, validation loss: 0.001696\n",
      "tensor(0.0019)\n",
      "iteration 5776, train loss: 0.002658, validation loss: 0.001902\n",
      "tensor(0.0017)\n",
      "iteration 5777, train loss: 0.002905, validation loss: 0.001702\n",
      "tensor(0.0018)\n",
      "iteration 5778, train loss: 0.002602, validation loss: 0.001845\n",
      "tensor(0.0017)\n",
      "iteration 5779, train loss: 0.002786, validation loss: 0.001722\n",
      "tensor(0.0019)\n",
      "iteration 5780, train loss: 0.002676, validation loss: 0.00186\n",
      "tensor(0.0017)\n",
      "iteration 5781, train loss: 0.002799, validation loss: 0.00172\n",
      "tensor(0.0018)\n",
      "iteration 5782, train loss: 0.002695, validation loss: 0.001764\n",
      "tensor(0.0018)\n",
      "iteration 5783, train loss: 0.002699, validation loss: 0.001819\n",
      "tensor(0.0018)\n",
      "iteration 5784, train loss: 0.002679, validation loss: 0.001754\n",
      "tensor(0.0017)\n",
      "iteration 5785, train loss: 0.002603, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 5786, train loss: 0.002647, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 5787, train loss: 0.002734, validation loss: 0.001732\n",
      "tensor(0.0017)\n",
      "iteration 5788, train loss: 0.002595, validation loss: 0.001722\n",
      "tensor(0.0017)\n",
      "iteration 5789, train loss: 0.002671, validation loss: 0.001695\n",
      "tensor(0.0017)\n",
      "iteration 5790, train loss: 0.002746, validation loss: 0.001699\n",
      "tensor(0.0017)\n",
      "iteration 5791, train loss: 0.002615, validation loss: 0.001711\n",
      "tensor(0.0017)\n",
      "iteration 5792, train loss: 0.002602, validation loss: 0.001722\n",
      "tensor(0.0017)\n",
      "iteration 5793, train loss: \u001b[92m0.002549\u001b[0m, validation loss: 0.001706\n",
      "tensor(0.0017)\n",
      "iteration 5794, train loss: 0.002714, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 5795, train loss: 0.002707, validation loss: 0.001722\n",
      "tensor(0.0017)\n",
      "iteration 5796, train loss: 0.002645, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 5797, train loss: 0.002643, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 5798, train loss: 0.00258, validation loss: \u001b[92m0.001693\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5799, train loss: 0.002632, validation loss: 0.001708\n",
      "tensor(0.0017)\n",
      "iteration 5800, train loss: 0.002669, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 5801, train loss: 0.002708, validation loss: \u001b[92m0.001691\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5802, train loss: 0.002672, validation loss: 0.001735\n",
      "tensor(0.0017)\n",
      "iteration 5803, train loss: 0.002687, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 5804, train loss: 0.002667, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 5805, train loss: \u001b[92m0.002536\u001b[0m, validation loss: 0.001748\n",
      "tensor(0.0017)\n",
      "iteration 5806, train loss: 0.002636, validation loss: 0.001709\n",
      "tensor(0.0017)\n",
      "iteration 5807, train loss: 0.002656, validation loss: 0.001732\n",
      "tensor(0.0017)\n",
      "iteration 5808, train loss: 0.002583, validation loss: \u001b[92m0.00169\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5809, train loss: 0.002613, validation loss: 0.001714\n",
      "tensor(0.0018)\n",
      "iteration 5810, train loss: 0.002709, validation loss: 0.001754\n",
      "tensor(0.0017)\n",
      "iteration 5811, train loss: 0.002696, validation loss: 0.001702\n",
      "tensor(0.0018)\n",
      "iteration 5812, train loss: 0.002609, validation loss: 0.001756\n",
      "tensor(0.0017)\n",
      "iteration 5813, train loss: 0.00271, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 5814, train loss: 0.002666, validation loss: 0.001726\n",
      "tensor(0.0017)\n",
      "iteration 5815, train loss: 0.00269, validation loss: 0.001698\n",
      "tensor(0.0017)\n",
      "iteration 5816, train loss: 0.002717, validation loss: 0.001709\n",
      "tensor(0.0018)\n",
      "iteration 5817, train loss: 0.002601, validation loss: 0.001821\n",
      "tensor(0.0017)\n",
      "iteration 5818, train loss: 0.002742, validation loss: 0.001704\n",
      "tensor(0.0018)\n",
      "iteration 5819, train loss: 0.002649, validation loss: 0.001771\n",
      "tensor(0.0017)\n",
      "iteration 5820, train loss: 0.002777, validation loss: 0.001719\n",
      "tensor(0.0018)\n",
      "iteration 5821, train loss: 0.002669, validation loss: 0.00182\n",
      "tensor(0.0018)\n",
      "iteration 5822, train loss: 0.002726, validation loss: 0.001769\n",
      "tensor(0.0017)\n",
      "iteration 5823, train loss: 0.002664, validation loss: 0.001692\n",
      "tensor(0.0017)\n",
      "iteration 5824, train loss: 0.002665, validation loss: 0.001731\n",
      "tensor(0.0018)\n",
      "iteration 5825, train loss: 0.002692, validation loss: 0.001761\n",
      "tensor(0.0018)\n",
      "iteration 5826, train loss: 0.002728, validation loss: 0.001757\n",
      "tensor(0.0017)\n",
      "iteration 5827, train loss: 0.002758, validation loss: 0.001715\n",
      "tensor(0.0018)\n",
      "iteration 5828, train loss: 0.002638, validation loss: 0.001761\n",
      "tensor(0.0017)\n",
      "iteration 5829, train loss: 0.00268, validation loss: 0.001747\n",
      "tensor(0.0018)\n",
      "iteration 5830, train loss: 0.002691, validation loss: 0.001827\n",
      "tensor(0.0017)\n",
      "iteration 5831, train loss: 0.002817, validation loss: 0.001724\n",
      "tensor(0.0020)\n",
      "iteration 5832, train loss: 0.002659, validation loss: 0.001988\n",
      "tensor(0.0018)\n",
      "iteration 5833, train loss: 0.00286, validation loss: 0.001798\n",
      "tensor(0.0018)\n",
      "iteration 5834, train loss: 0.002776, validation loss: 0.001764\n",
      "tensor(0.0019)\n",
      "iteration 5835, train loss: 0.002769, validation loss: 0.001887\n",
      "tensor(0.0018)\n",
      "iteration 5836, train loss: 0.002793, validation loss: 0.001816\n",
      "tensor(0.0018)\n",
      "iteration 5837, train loss: 0.002679, validation loss: 0.001754\n",
      "tensor(0.0017)\n",
      "iteration 5838, train loss: 0.002696, validation loss: 0.001726\n",
      "tensor(0.0018)\n",
      "iteration 5839, train loss: 0.002612, validation loss: 0.001822\n",
      "tensor(0.0018)\n",
      "iteration 5840, train loss: 0.002734, validation loss: 0.001794\n",
      "tensor(0.0017)\n",
      "iteration 5841, train loss: 0.002673, validation loss: 0.001704\n",
      "tensor(0.0018)\n",
      "iteration 5842, train loss: 0.002586, validation loss: 0.001821\n",
      "tensor(0.0017)\n",
      "iteration 5843, train loss: 0.002786, validation loss: 0.001743\n",
      "tensor(0.0017)\n",
      "iteration 5844, train loss: 0.002762, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 5845, train loss: 0.00274, validation loss: 0.001723\n",
      "tensor(0.0017)\n",
      "iteration 5846, train loss: 0.002657, validation loss: 0.001714\n",
      "tensor(0.0018)\n",
      "iteration 5847, train loss: 0.00267, validation loss: 0.001832\n",
      "tensor(0.0017)\n",
      "iteration 5848, train loss: 0.002734, validation loss: \u001b[92m0.001689\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5849, train loss: 0.002659, validation loss: 0.001786\n",
      "tensor(0.0017)\n",
      "iteration 5850, train loss: 0.002693, validation loss: 0.001698\n",
      "tensor(0.0018)\n",
      "iteration 5851, train loss: 0.002682, validation loss: 0.001775\n",
      "tensor(0.0017)\n",
      "iteration 5852, train loss: 0.002688, validation loss: 0.001734\n",
      "tensor(0.0017)\n",
      "iteration 5853, train loss: 0.002611, validation loss: 0.001707\n",
      "tensor(0.0017)\n",
      "iteration 5854, train loss: 0.002682, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 5855, train loss: 0.002715, validation loss: 0.001702\n",
      "tensor(0.0017)\n",
      "iteration 5856, train loss: 0.002767, validation loss: 0.001703\n",
      "tensor(0.0017)\n",
      "iteration 5857, train loss: 0.002645, validation loss: \u001b[92m0.001684\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5858, train loss: 0.002659, validation loss: 0.001724\n",
      "tensor(0.0017)\n",
      "iteration 5859, train loss: 0.002599, validation loss: 0.001746\n",
      "tensor(0.0017)\n",
      "iteration 5860, train loss: 0.002688, validation loss: 0.001693\n",
      "tensor(0.0018)\n",
      "iteration 5861, train loss: 0.002651, validation loss: 0.001768\n",
      "tensor(0.0017)\n",
      "iteration 5862, train loss: 0.002719, validation loss: 0.001726\n",
      "tensor(0.0019)\n",
      "iteration 5863, train loss: 0.002681, validation loss: 0.001866\n",
      "tensor(0.0017)\n",
      "iteration 5864, train loss: 0.00277, validation loss: 0.001743\n",
      "tensor(0.0018)\n",
      "iteration 5865, train loss: 0.002719, validation loss: 0.001774\n",
      "tensor(0.0017)\n",
      "iteration 5866, train loss: 0.002794, validation loss: 0.001715\n",
      "tensor(0.0018)\n",
      "iteration 5867, train loss: 0.002665, validation loss: 0.001819\n",
      "tensor(0.0017)\n",
      "iteration 5868, train loss: 0.00273, validation loss: 0.001692\n",
      "tensor(0.0017)\n",
      "iteration 5869, train loss: 0.002676, validation loss: 0.001724\n",
      "tensor(0.0017)\n",
      "iteration 5870, train loss: 0.0027, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 5871, train loss: 0.002634, validation loss: 0.001719\n",
      "tensor(0.0017)\n",
      "iteration 5872, train loss: 0.002658, validation loss: 0.001698\n",
      "tensor(0.0017)\n",
      "iteration 5873, train loss: 0.002655, validation loss: 0.001695\n",
      "tensor(0.0018)\n",
      "iteration 5874, train loss: 0.002635, validation loss: 0.001757\n",
      "tensor(0.0018)\n",
      "iteration 5875, train loss: 0.002696, validation loss: 0.001795\n",
      "tensor(0.0017)\n",
      "iteration 5876, train loss: 0.002683, validation loss: 0.001716\n",
      "tensor(0.0017)\n",
      "iteration 5877, train loss: 0.002646, validation loss: 0.001705\n",
      "tensor(0.0018)\n",
      "iteration 5878, train loss: 0.002607, validation loss: 0.001814\n",
      "tensor(0.0017)\n",
      "iteration 5879, train loss: 0.002703, validation loss: 0.001745\n",
      "tensor(0.0017)\n",
      "iteration 5880, train loss: 0.002609, validation loss: 0.001717\n",
      "tensor(0.0017)\n",
      "iteration 5881, train loss: 0.002724, validation loss: 0.001716\n",
      "tensor(0.0018)\n",
      "iteration 5882, train loss: 0.002705, validation loss: 0.001818\n",
      "tensor(0.0017)\n",
      "iteration 5883, train loss: 0.002765, validation loss: 0.00172\n",
      "tensor(0.0018)\n",
      "iteration 5884, train loss: 0.002702, validation loss: 0.001754\n",
      "tensor(0.0018)\n",
      "iteration 5885, train loss: 0.002691, validation loss: 0.001758\n",
      "tensor(0.0017)\n",
      "iteration 5886, train loss: 0.00273, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 5887, train loss: 0.002637, validation loss: \u001b[92m0.001674\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5888, train loss: 0.00262, validation loss: 0.001699\n",
      "tensor(0.0017)\n",
      "iteration 5889, train loss: 0.002629, validation loss: 0.001703\n",
      "tensor(0.0017)\n",
      "iteration 5890, train loss: 0.002647, validation loss: 0.001705\n",
      "tensor(0.0017)\n",
      "iteration 5891, train loss: 0.002617, validation loss: 0.001749\n",
      "tensor(0.0017)\n",
      "iteration 5892, train loss: 0.002689, validation loss: 0.001678\n",
      "tensor(0.0017)\n",
      "iteration 5893, train loss: 0.002685, validation loss: 0.001706\n",
      "tensor(0.0017)\n",
      "iteration 5894, train loss: 0.002663, validation loss: 0.001719\n",
      "tensor(0.0018)\n",
      "iteration 5895, train loss: 0.002684, validation loss: 0.001827\n",
      "tensor(0.0017)\n",
      "iteration 5896, train loss: 0.002748, validation loss: 0.001678\n",
      "tensor(0.0019)\n",
      "iteration 5897, train loss: 0.002625, validation loss: 0.00185\n",
      "tensor(0.0017)\n",
      "iteration 5898, train loss: 0.002694, validation loss: 0.001677\n",
      "tensor(0.0017)\n",
      "iteration 5899, train loss: 0.002576, validation loss: 0.001741\n",
      "tensor(0.0017)\n",
      "iteration 5900, train loss: 0.002672, validation loss: 0.001727\n",
      "tensor(0.0019)\n",
      "iteration 5901, train loss: 0.002627, validation loss: 0.001895\n",
      "tensor(0.0017)\n",
      "iteration 5902, train loss: 0.00272, validation loss: 0.001682\n",
      "tensor(0.0018)\n",
      "iteration 5903, train loss: 0.002589, validation loss: 0.001776\n",
      "tensor(0.0018)\n",
      "iteration 5904, train loss: 0.002766, validation loss: 0.001755\n",
      "tensor(0.0018)\n",
      "iteration 5905, train loss: 0.00268, validation loss: 0.001799\n",
      "tensor(0.0018)\n",
      "iteration 5906, train loss: 0.002705, validation loss: 0.001776\n",
      "tensor(0.0017)\n",
      "iteration 5907, train loss: 0.002816, validation loss: \u001b[92m0.001667\u001b[0m\n",
      "tensor(0.0019)\n",
      "iteration 5908, train loss: 0.002708, validation loss: 0.001924\n",
      "tensor(0.0017)\n",
      "iteration 5909, train loss: 0.002886, validation loss: \u001b[92m0.001666\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5910, train loss: 0.002626, validation loss: 0.001789\n",
      "tensor(0.0017)\n",
      "iteration 5911, train loss: 0.002893, validation loss: 0.001747\n",
      "tensor(0.0019)\n",
      "iteration 5912, train loss: 0.002638, validation loss: 0.001854\n",
      "tensor(0.0017)\n",
      "iteration 5913, train loss: 0.002765, validation loss: 0.001734\n",
      "tensor(0.0018)\n",
      "iteration 5914, train loss: 0.002733, validation loss: 0.001789\n",
      "tensor(0.0018)\n",
      "iteration 5915, train loss: 0.002774, validation loss: 0.001793\n",
      "tensor(0.0019)\n",
      "iteration 5916, train loss: 0.002681, validation loss: 0.001856\n",
      "tensor(0.0018)\n",
      "iteration 5917, train loss: 0.002738, validation loss: 0.001751\n",
      "tensor(0.0017)\n",
      "iteration 5918, train loss: 0.002664, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 5919, train loss: 0.002735, validation loss: 0.001735\n",
      "tensor(0.0017)\n",
      "iteration 5920, train loss: 0.002669, validation loss: 0.001703\n",
      "tensor(0.0017)\n",
      "iteration 5921, train loss: 0.002627, validation loss: 0.001733\n",
      "tensor(0.0018)\n",
      "iteration 5922, train loss: 0.002682, validation loss: 0.001785\n",
      "tensor(0.0018)\n",
      "iteration 5923, train loss: 0.002697, validation loss: 0.001761\n",
      "tensor(0.0018)\n",
      "iteration 5924, train loss: 0.002652, validation loss: 0.001777\n",
      "tensor(0.0017)\n",
      "iteration 5925, train loss: 0.002821, validation loss: \u001b[92m0.001662\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 5926, train loss: 0.00266, validation loss: 0.001775\n",
      "tensor(0.0017)\n",
      "iteration 5927, train loss: 0.002781, validation loss: 0.001714\n",
      "tensor(0.0017)\n",
      "iteration 5928, train loss: 0.002671, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 5929, train loss: 0.002669, validation loss: 0.001704\n",
      "tensor(0.0017)\n",
      "iteration 5930, train loss: 0.002687, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 5931, train loss: 0.002654, validation loss: 0.00172\n",
      "tensor(0.0017)\n",
      "iteration 5932, train loss: 0.002654, validation loss: 0.001688\n",
      "tensor(0.0017)\n",
      "iteration 5933, train loss: 0.002556, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 5934, train loss: 0.00275, validation loss: 0.001695\n",
      "tensor(0.0017)\n",
      "iteration 5935, train loss: 0.002668, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 5936, train loss: 0.002639, validation loss: 0.00168\n",
      "tensor(0.0017)\n",
      "iteration 5937, train loss: 0.002552, validation loss: 0.001716\n",
      "tensor(0.0017)\n",
      "iteration 5938, train loss: 0.002596, validation loss: 0.001717\n",
      "tensor(0.0017)\n",
      "iteration 5939, train loss: 0.002621, validation loss: 0.001675\n",
      "tensor(0.0017)\n",
      "iteration 5940, train loss: 0.002631, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 5941, train loss: 0.002668, validation loss: 0.001681\n",
      "tensor(0.0017)\n",
      "iteration 5942, train loss: 0.002657, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 5943, train loss: 0.002595, validation loss: 0.001692\n",
      "tensor(0.0017)\n",
      "iteration 5944, train loss: 0.002598, validation loss: 0.001663\n",
      "tensor(0.0017)\n",
      "iteration 5945, train loss: 0.002679, validation loss: 0.001701\n",
      "tensor(0.0017)\n",
      "iteration 5946, train loss: 0.002571, validation loss: 0.001702\n",
      "tensor(0.0017)\n",
      "iteration 5947, train loss: 0.002674, validation loss: 0.001685\n",
      "tensor(0.0017)\n",
      "iteration 5948, train loss: 0.002611, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 5949, train loss: 0.002637, validation loss: 0.001677\n",
      "tensor(0.0018)\n",
      "iteration 5950, train loss: 0.002586, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 5951, train loss: 0.002795, validation loss: 0.001723\n",
      "tensor(0.0018)\n",
      "iteration 5952, train loss: 0.00282, validation loss: 0.001767\n",
      "tensor(0.0017)\n",
      "iteration 5953, train loss: 0.002698, validation loss: 0.001705\n",
      "tensor(0.0018)\n",
      "iteration 5954, train loss: 0.002626, validation loss: 0.001782\n",
      "tensor(0.0017)\n",
      "iteration 5955, train loss: 0.002689, validation loss: 0.001722\n",
      "tensor(0.0017)\n",
      "iteration 5956, train loss: 0.002647, validation loss: 0.001716\n",
      "tensor(0.0017)\n",
      "iteration 5957, train loss: 0.002679, validation loss: 0.001707\n",
      "tensor(0.0018)\n",
      "iteration 5958, train loss: 0.002621, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 5959, train loss: 0.002741, validation loss: 0.001706\n",
      "tensor(0.0017)\n",
      "iteration 5960, train loss: 0.002694, validation loss: 0.001685\n",
      "tensor(0.0018)\n",
      "iteration 5961, train loss: 0.002608, validation loss: 0.001767\n",
      "tensor(0.0017)\n",
      "iteration 5962, train loss: 0.00269, validation loss: 0.001713\n",
      "tensor(0.0017)\n",
      "iteration 5963, train loss: 0.002759, validation loss: 0.001685\n",
      "tensor(0.0018)\n",
      "iteration 5964, train loss: 0.002654, validation loss: 0.001804\n",
      "tensor(0.0017)\n",
      "iteration 5965, train loss: 0.002704, validation loss: 0.001738\n",
      "tensor(0.0017)\n",
      "iteration 5966, train loss: 0.002593, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 5967, train loss: 0.002702, validation loss: 0.001706\n",
      "tensor(0.0017)\n",
      "iteration 5968, train loss: 0.002645, validation loss: 0.001725\n",
      "tensor(0.0017)\n",
      "iteration 5969, train loss: 0.002593, validation loss: 0.001699\n",
      "tensor(0.0017)\n",
      "iteration 5970, train loss: 0.002636, validation loss: 0.001724\n",
      "tensor(0.0017)\n",
      "iteration 5971, train loss: 0.002691, validation loss: 0.001664\n",
      "tensor(0.0018)\n",
      "iteration 5972, train loss: 0.002647, validation loss: 0.001756\n",
      "tensor(0.0017)\n",
      "iteration 5973, train loss: 0.002758, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 5974, train loss: 0.002587, validation loss: 0.001677\n",
      "tensor(0.0018)\n",
      "iteration 5975, train loss: 0.002704, validation loss: 0.001756\n",
      "tensor(0.0017)\n",
      "iteration 5976, train loss: 0.002737, validation loss: 0.00168\n",
      "tensor(0.0017)\n",
      "iteration 5977, train loss: 0.002577, validation loss: 0.001678\n",
      "tensor(0.0017)\n",
      "iteration 5978, train loss: 0.002646, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 5979, train loss: 0.002665, validation loss: 0.001699\n",
      "tensor(0.0018)\n",
      "iteration 5980, train loss: 0.002646, validation loss: 0.001772\n",
      "tensor(0.0017)\n",
      "iteration 5981, train loss: 0.002755, validation loss: 0.001673\n",
      "tensor(0.0017)\n",
      "iteration 5982, train loss: 0.002545, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 5983, train loss: 0.002625, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 5984, train loss: 0.002645, validation loss: 0.001691\n",
      "tensor(0.0017)\n",
      "iteration 5985, train loss: 0.002586, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 5986, train loss: 0.002549, validation loss: 0.001708\n",
      "tensor(0.0017)\n",
      "iteration 5987, train loss: 0.002689, validation loss: 0.001677\n",
      "tensor(0.0017)\n",
      "iteration 5988, train loss: 0.002687, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 5989, train loss: \u001b[92m0.002519\u001b[0m, validation loss: 0.001707\n",
      "tensor(0.0017)\n",
      "iteration 5990, train loss: 0.002786, validation loss: 0.001683\n",
      "tensor(0.0017)\n",
      "iteration 5991, train loss: 0.002646, validation loss: 0.001686\n",
      "tensor(0.0017)\n",
      "iteration 5992, train loss: 0.002565, validation loss: 0.00167\n",
      "tensor(0.0017)\n",
      "iteration 5993, train loss: 0.002679, validation loss: 0.001666\n",
      "tensor(0.0017)\n",
      "iteration 5994, train loss: 0.002565, validation loss: \u001b[92m0.001657\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 5995, train loss: 0.002568, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 5996, train loss: 0.002589, validation loss: 0.001662\n",
      "tensor(0.0017)\n",
      "iteration 5997, train loss: 0.00269, validation loss: 0.001666\n",
      "tensor(0.0017)\n",
      "iteration 5998, train loss: 0.002663, validation loss: 0.001738\n",
      "tensor(0.0017)\n",
      "iteration 5999, train loss: 0.002627, validation loss: 0.001704\n",
      "tensor(0.0017)\n",
      "iteration 6000, train loss: 0.002658, validation loss: 0.001689\n",
      "tensor(0.0018)\n",
      "iteration 6001, train loss: 0.002602, validation loss: 0.001766\n",
      "tensor(0.0017)\n",
      "iteration 6002, train loss: 0.002736, validation loss: 0.001716\n",
      "tensor(0.0017)\n",
      "iteration 6003, train loss: 0.002617, validation loss: 0.001731\n",
      "tensor(0.0017)\n",
      "iteration 6004, train loss: 0.002686, validation loss: 0.00169\n",
      "tensor(0.0017)\n",
      "iteration 6005, train loss: 0.00264, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 6006, train loss: 0.002625, validation loss: 0.00173\n",
      "tensor(0.0017)\n",
      "iteration 6007, train loss: 0.002679, validation loss: 0.001679\n",
      "tensor(0.0017)\n",
      "iteration 6008, train loss: 0.002583, validation loss: 0.001683\n",
      "tensor(0.0017)\n",
      "iteration 6009, train loss: 0.002546, validation loss: 0.001745\n",
      "tensor(0.0017)\n",
      "iteration 6010, train loss: 0.002557, validation loss: \u001b[92m0.001653\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6011, train loss: 0.002672, validation loss: 0.001694\n",
      "tensor(0.0017)\n",
      "iteration 6012, train loss: 0.002671, validation loss: 0.001689\n",
      "tensor(0.0018)\n",
      "iteration 6013, train loss: 0.002615, validation loss: 0.001752\n",
      "tensor(0.0017)\n",
      "iteration 6014, train loss: 0.002643, validation loss: 0.001679\n",
      "tensor(0.0017)\n",
      "iteration 6015, train loss: 0.002689, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 6016, train loss: 0.002697, validation loss: 0.001728\n",
      "tensor(0.0018)\n",
      "iteration 6017, train loss: 0.002564, validation loss: 0.001752\n",
      "tensor(0.0017)\n",
      "iteration 6018, train loss: 0.002722, validation loss: 0.001686\n",
      "tensor(0.0017)\n",
      "iteration 6019, train loss: 0.002625, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 6020, train loss: 0.002665, validation loss: 0.00174\n",
      "tensor(0.0017)\n",
      "iteration 6021, train loss: 0.002668, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 6022, train loss: 0.002667, validation loss: 0.001703\n",
      "tensor(0.0017)\n",
      "iteration 6023, train loss: 0.002655, validation loss: 0.001746\n",
      "tensor(0.0017)\n",
      "iteration 6024, train loss: 0.002676, validation loss: 0.001681\n",
      "tensor(0.0017)\n",
      "iteration 6025, train loss: 0.002671, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 6026, train loss: 0.002647, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6027, train loss: 0.002583, validation loss: 0.001697\n",
      "tensor(0.0018)\n",
      "iteration 6028, train loss: 0.002529, validation loss: 0.00177\n",
      "tensor(0.0017)\n",
      "iteration 6029, train loss: 0.002635, validation loss: 0.001665\n",
      "tensor(0.0017)\n",
      "iteration 6030, train loss: 0.002595, validation loss: 0.001684\n",
      "tensor(0.0017)\n",
      "iteration 6031, train loss: 0.002605, validation loss: 0.001684\n",
      "tensor(0.0017)\n",
      "iteration 6032, train loss: 0.00259, validation loss: 0.001685\n",
      "tensor(0.0017)\n",
      "iteration 6033, train loss: 0.002602, validation loss: 0.001688\n",
      "tensor(0.0017)\n",
      "iteration 6034, train loss: 0.002632, validation loss: 0.001671\n",
      "tensor(0.0017)\n",
      "iteration 6035, train loss: 0.002696, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6036, train loss: 0.002672, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 6037, train loss: 0.002573, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 6038, train loss: 0.002742, validation loss: 0.00172\n",
      "tensor(0.0017)\n",
      "iteration 6039, train loss: 0.002586, validation loss: 0.001723\n",
      "tensor(0.0017)\n",
      "iteration 6040, train loss: 0.002769, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 6041, train loss: 0.002693, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 6042, train loss: 0.002722, validation loss: 0.001678\n",
      "tensor(0.0018)\n",
      "iteration 6043, train loss: 0.002718, validation loss: 0.001751\n",
      "tensor(0.0018)\n",
      "iteration 6044, train loss: 0.002744, validation loss: 0.001776\n",
      "tensor(0.0017)\n",
      "iteration 6045, train loss: 0.002694, validation loss: 0.001685\n",
      "tensor(0.0017)\n",
      "iteration 6046, train loss: 0.002651, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 6047, train loss: 0.002655, validation loss: 0.001677\n",
      "tensor(0.0018)\n",
      "iteration 6048, train loss: 0.002668, validation loss: 0.001849\n",
      "tensor(0.0017)\n",
      "iteration 6049, train loss: 0.00281, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 6050, train loss: 0.00267, validation loss: 0.00168\n",
      "tensor(0.0017)\n",
      "iteration 6051, train loss: 0.002679, validation loss: 0.001702\n",
      "tensor(0.0017)\n",
      "iteration 6052, train loss: 0.002606, validation loss: 0.001691\n",
      "tensor(0.0017)\n",
      "iteration 6053, train loss: 0.002636, validation loss: 0.00174\n",
      "tensor(0.0017)\n",
      "iteration 6054, train loss: 0.002575, validation loss: 0.001685\n",
      "tensor(0.0018)\n",
      "iteration 6055, train loss: 0.002657, validation loss: 0.001809\n",
      "tensor(0.0017)\n",
      "iteration 6056, train loss: 0.002789, validation loss: 0.001683\n",
      "tensor(0.0017)\n",
      "iteration 6057, train loss: 0.002588, validation loss: 0.001702\n",
      "tensor(0.0018)\n",
      "iteration 6058, train loss: 0.002624, validation loss: 0.001803\n",
      "tensor(0.0017)\n",
      "iteration 6059, train loss: 0.002665, validation loss: 0.001702\n",
      "tensor(0.0017)\n",
      "iteration 6060, train loss: 0.002697, validation loss: 0.00169\n",
      "tensor(0.0017)\n",
      "iteration 6061, train loss: 0.002671, validation loss: 0.001677\n",
      "tensor(0.0018)\n",
      "iteration 6062, train loss: 0.002695, validation loss: 0.001792\n",
      "tensor(0.0017)\n",
      "iteration 6063, train loss: 0.002761, validation loss: 0.001705\n",
      "tensor(0.0017)\n",
      "iteration 6064, train loss: 0.00267, validation loss: 0.001727\n",
      "tensor(0.0017)\n",
      "iteration 6065, train loss: 0.002614, validation loss: 0.001683\n",
      "tensor(0.0017)\n",
      "iteration 6066, train loss: 0.002596, validation loss: 0.001719\n",
      "tensor(0.0017)\n",
      "iteration 6067, train loss: 0.002621, validation loss: 0.001681\n",
      "tensor(0.0017)\n",
      "iteration 6068, train loss: 0.002602, validation loss: 0.00167\n",
      "tensor(0.0017)\n",
      "iteration 6069, train loss: 0.002658, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 6070, train loss: 0.002572, validation loss: 0.001737\n",
      "tensor(0.0017)\n",
      "iteration 6071, train loss: 0.00274, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 6072, train loss: 0.002662, validation loss: 0.001668\n",
      "tensor(0.0017)\n",
      "iteration 6073, train loss: 0.002646, validation loss: 0.001733\n",
      "tensor(0.0017)\n",
      "iteration 6074, train loss: 0.002633, validation loss: 0.001722\n",
      "tensor(0.0017)\n",
      "iteration 6075, train loss: 0.002598, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 6076, train loss: 0.002779, validation loss: 0.001684\n",
      "tensor(0.0018)\n",
      "iteration 6077, train loss: 0.002591, validation loss: 0.001789\n",
      "tensor(0.0017)\n",
      "iteration 6078, train loss: 0.002676, validation loss: 0.001694\n",
      "tensor(0.0017)\n",
      "iteration 6079, train loss: 0.002728, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 6080, train loss: 0.002639, validation loss: \u001b[92m0.001652\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6081, train loss: 0.002563, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 6082, train loss: 0.002581, validation loss: 0.001675\n",
      "tensor(0.0017)\n",
      "iteration 6083, train loss: 0.00264, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6084, train loss: 0.002629, validation loss: \u001b[92m0.001651\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 6085, train loss: 0.002697, validation loss: 0.001767\n",
      "tensor(0.0017)\n",
      "iteration 6086, train loss: 0.002637, validation loss: 0.001711\n",
      "tensor(0.0017)\n",
      "iteration 6087, train loss: 0.002652, validation loss: 0.001723\n",
      "tensor(0.0017)\n",
      "iteration 6088, train loss: 0.00273, validation loss: 0.001673\n",
      "tensor(0.0017)\n",
      "iteration 6089, train loss: 0.002603, validation loss: 0.001735\n",
      "tensor(0.0017)\n",
      "iteration 6090, train loss: 0.002582, validation loss: 0.001663\n",
      "tensor(0.0017)\n",
      "iteration 6091, train loss: 0.002594, validation loss: 0.00167\n",
      "tensor(0.0017)\n",
      "iteration 6092, train loss: 0.002628, validation loss: 0.001719\n",
      "tensor(0.0017)\n",
      "iteration 6093, train loss: 0.002587, validation loss: 0.001732\n",
      "tensor(0.0017)\n",
      "iteration 6094, train loss: 0.002522, validation loss: 0.001684\n",
      "tensor(0.0017)\n",
      "iteration 6095, train loss: 0.002571, validation loss: 0.001739\n",
      "tensor(0.0018)\n",
      "iteration 6096, train loss: 0.002642, validation loss: 0.001757\n",
      "tensor(0.0017)\n",
      "iteration 6097, train loss: 0.002649, validation loss: 0.001737\n",
      "tensor(0.0017)\n",
      "iteration 6098, train loss: 0.002669, validation loss: 0.001703\n",
      "tensor(0.0017)\n",
      "iteration 6099, train loss: 0.002681, validation loss: \u001b[92m0.00165\u001b[0m\n",
      "tensor(0.0018)\n",
      "iteration 6100, train loss: 0.002616, validation loss: 0.001769\n",
      "tensor(0.0017)\n",
      "iteration 6101, train loss: 0.002717, validation loss: 0.001666\n",
      "tensor(0.0017)\n",
      "iteration 6102, train loss: 0.002596, validation loss: 0.001683\n",
      "tensor(0.0017)\n",
      "iteration 6103, train loss: 0.002705, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 6104, train loss: 0.002699, validation loss: 0.001675\n",
      "tensor(0.0017)\n",
      "iteration 6105, train loss: 0.002598, validation loss: 0.001657\n",
      "tensor(0.0017)\n",
      "iteration 6106, train loss: 0.002616, validation loss: 0.001661\n",
      "tensor(0.0017)\n",
      "iteration 6107, train loss: 0.002621, validation loss: 0.001655\n",
      "tensor(0.0017)\n",
      "iteration 6108, train loss: 0.002584, validation loss: 0.001708\n",
      "tensor(0.0017)\n",
      "iteration 6109, train loss: 0.002724, validation loss: 0.001668\n",
      "tensor(0.0017)\n",
      "iteration 6110, train loss: 0.002653, validation loss: 0.001655\n",
      "tensor(0.0017)\n",
      "iteration 6111, train loss: 0.002567, validation loss: 0.001716\n",
      "tensor(0.0017)\n",
      "iteration 6112, train loss: 0.002601, validation loss: 0.001675\n",
      "tensor(0.0017)\n",
      "iteration 6113, train loss: 0.002617, validation loss: 0.001708\n",
      "tensor(0.0017)\n",
      "iteration 6114, train loss: 0.002695, validation loss: 0.001657\n",
      "tensor(0.0018)\n",
      "iteration 6115, train loss: 0.002563, validation loss: 0.001796\n",
      "tensor(0.0016)\n",
      "iteration 6116, train loss: 0.002642, validation loss: \u001b[92m0.001648\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6117, train loss: \u001b[92m0.002497\u001b[0m, validation loss: 0.001701\n",
      "tensor(0.0017)\n",
      "iteration 6118, train loss: 0.002703, validation loss: 0.001682\n",
      "tensor(0.0017)\n",
      "iteration 6119, train loss: 0.002675, validation loss: 0.001749\n",
      "tensor(0.0017)\n",
      "iteration 6120, train loss: 0.002591, validation loss: 0.001684\n",
      "tensor(0.0017)\n",
      "iteration 6121, train loss: 0.002636, validation loss: 0.001712\n",
      "tensor(0.0017)\n",
      "iteration 6122, train loss: 0.002667, validation loss: 0.001715\n",
      "tensor(0.0018)\n",
      "iteration 6123, train loss: 0.002589, validation loss: 0.001819\n",
      "tensor(0.0017)\n",
      "iteration 6124, train loss: 0.002571, validation loss: 0.001671\n",
      "tensor(0.0018)\n",
      "iteration 6125, train loss: 0.002681, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 6126, train loss: 0.002782, validation loss: 0.001698\n",
      "tensor(0.0019)\n",
      "iteration 6127, train loss: 0.002611, validation loss: 0.001852\n",
      "tensor(0.0017)\n",
      "iteration 6128, train loss: 0.002843, validation loss: 0.001682\n",
      "tensor(0.0017)\n",
      "iteration 6129, train loss: 0.002655, validation loss: 0.001664\n",
      "tensor(0.0018)\n",
      "iteration 6130, train loss: 0.002607, validation loss: 0.00176\n",
      "tensor(0.0017)\n",
      "iteration 6131, train loss: 0.002727, validation loss: 0.001655\n",
      "tensor(0.0017)\n",
      "iteration 6132, train loss: 0.002628, validation loss: 0.001668\n",
      "tensor(0.0016)\n",
      "iteration 6133, train loss: 0.002599, validation loss: \u001b[92m0.001646\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6134, train loss: 0.002582, validation loss: 0.001726\n",
      "tensor(0.0017)\n",
      "iteration 6135, train loss: 0.002633, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6136, train loss: 0.002573, validation loss: 0.001673\n",
      "tensor(0.0017)\n",
      "iteration 6137, train loss: 0.002542, validation loss: 0.001651\n",
      "tensor(0.0017)\n",
      "iteration 6138, train loss: 0.002571, validation loss: 0.00167\n",
      "tensor(0.0016)\n",
      "iteration 6139, train loss: 0.002657, validation loss: 0.001649\n",
      "tensor(0.0016)\n",
      "iteration 6140, train loss: 0.002648, validation loss: \u001b[92m0.00164\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6141, train loss: 0.002527, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6142, train loss: 0.0026, validation loss: 0.001652\n",
      "tensor(0.0017)\n",
      "iteration 6143, train loss: 0.002613, validation loss: 0.00172\n",
      "tensor(0.0017)\n",
      "iteration 6144, train loss: 0.002688, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 6145, train loss: 0.002635, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 6146, train loss: 0.002567, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6147, train loss: 0.002609, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 6148, train loss: 0.002662, validation loss: 0.001688\n",
      "tensor(0.0017)\n",
      "iteration 6149, train loss: 0.002614, validation loss: 0.001663\n",
      "tensor(0.0017)\n",
      "iteration 6150, train loss: 0.002612, validation loss: 0.001654\n",
      "tensor(0.0017)\n",
      "iteration 6151, train loss: 0.002628, validation loss: 0.001658\n",
      "tensor(0.0017)\n",
      "iteration 6152, train loss: 0.002624, validation loss: 0.001695\n",
      "tensor(0.0017)\n",
      "iteration 6153, train loss: 0.002675, validation loss: 0.001676\n",
      "tensor(0.0017)\n",
      "iteration 6154, train loss: 0.00254, validation loss: 0.00167\n",
      "tensor(0.0017)\n",
      "iteration 6155, train loss: 0.002628, validation loss: 0.001687\n",
      "tensor(0.0018)\n",
      "iteration 6156, train loss: 0.002648, validation loss: 0.001764\n",
      "tensor(0.0017)\n",
      "iteration 6157, train loss: 0.002686, validation loss: 0.001742\n",
      "tensor(0.0018)\n",
      "iteration 6158, train loss: 0.002675, validation loss: 0.001819\n",
      "tensor(0.0017)\n",
      "iteration 6159, train loss: 0.002786, validation loss: 0.001687\n",
      "tensor(0.0021)\n",
      "iteration 6160, train loss: 0.002646, validation loss: 0.00209\n",
      "tensor(0.0017)\n",
      "iteration 6161, train loss: 0.002968, validation loss: 0.00166\n",
      "tensor(0.0018)\n",
      "iteration 6162, train loss: 0.002663, validation loss: 0.001797\n",
      "tensor(0.0018)\n",
      "iteration 6163, train loss: 0.002846, validation loss: 0.001751\n",
      "tensor(0.0017)\n",
      "iteration 6164, train loss: 0.002705, validation loss: 0.001705\n",
      "tensor(0.0017)\n",
      "iteration 6165, train loss: 0.002675, validation loss: 0.001739\n",
      "tensor(0.0017)\n",
      "iteration 6166, train loss: 0.002795, validation loss: 0.001672\n",
      "tensor(0.0018)\n",
      "iteration 6167, train loss: 0.0026, validation loss: 0.001823\n",
      "tensor(0.0017)\n",
      "iteration 6168, train loss: 0.002719, validation loss: 0.001732\n",
      "tensor(0.0017)\n",
      "iteration 6169, train loss: 0.002638, validation loss: 0.001708\n",
      "tensor(0.0016)\n",
      "iteration 6170, train loss: 0.002623, validation loss: 0.00164\n",
      "tensor(0.0017)\n",
      "iteration 6171, train loss: 0.002529, validation loss: 0.001724\n",
      "tensor(0.0016)\n",
      "iteration 6172, train loss: 0.002606, validation loss: \u001b[92m0.001636\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6173, train loss: 0.002624, validation loss: 0.001647\n",
      "tensor(0.0016)\n",
      "iteration 6174, train loss: 0.002576, validation loss: \u001b[92m0.001629\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6175, train loss: 0.002676, validation loss: 0.001724\n",
      "tensor(0.0017)\n",
      "iteration 6176, train loss: 0.002597, validation loss: 0.001713\n",
      "tensor(0.0017)\n",
      "iteration 6177, train loss: 0.002674, validation loss: 0.001658\n",
      "tensor(0.0017)\n",
      "iteration 6178, train loss: 0.002616, validation loss: 0.001705\n",
      "tensor(0.0017)\n",
      "iteration 6179, train loss: 0.002685, validation loss: 0.001717\n",
      "tensor(0.0017)\n",
      "iteration 6180, train loss: 0.002635, validation loss: 0.001705\n",
      "tensor(0.0017)\n",
      "iteration 6181, train loss: 0.002593, validation loss: 0.001683\n",
      "tensor(0.0016)\n",
      "iteration 6182, train loss: 0.002662, validation loss: \u001b[92m0.001628\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6183, train loss: 0.002557, validation loss: 0.001749\n",
      "tensor(0.0017)\n",
      "iteration 6184, train loss: 0.002691, validation loss: 0.001656\n",
      "tensor(0.0017)\n",
      "iteration 6185, train loss: 0.002561, validation loss: 0.001741\n",
      "tensor(0.0017)\n",
      "iteration 6186, train loss: 0.002563, validation loss: 0.001691\n",
      "tensor(0.0017)\n",
      "iteration 6187, train loss: 0.002621, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 6188, train loss: 0.002528, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 6189, train loss: 0.002666, validation loss: 0.00165\n",
      "tensor(0.0017)\n",
      "iteration 6190, train loss: 0.002672, validation loss: 0.001734\n",
      "tensor(0.0017)\n",
      "iteration 6191, train loss: 0.002723, validation loss: 0.001676\n",
      "tensor(0.0017)\n",
      "iteration 6192, train loss: 0.002652, validation loss: 0.001682\n",
      "tensor(0.0016)\n",
      "iteration 6193, train loss: 0.002706, validation loss: 0.001644\n",
      "tensor(0.0017)\n",
      "iteration 6194, train loss: 0.002597, validation loss: 0.001717\n",
      "tensor(0.0017)\n",
      "iteration 6195, train loss: 0.00267, validation loss: 0.001704\n",
      "tensor(0.0017)\n",
      "iteration 6196, train loss: 0.002566, validation loss: 0.001655\n",
      "tensor(0.0016)\n",
      "iteration 6197, train loss: 0.002519, validation loss: 0.001639\n",
      "tensor(0.0017)\n",
      "iteration 6198, train loss: \u001b[92m0.002461\u001b[0m, validation loss: 0.001678\n",
      "tensor(0.0016)\n",
      "iteration 6199, train loss: 0.002551, validation loss: 0.001636\n",
      "tensor(0.0017)\n",
      "iteration 6200, train loss: \u001b[92m0.002432\u001b[0m, validation loss: 0.001653\n",
      "tensor(0.0016)\n",
      "iteration 6201, train loss: 0.002559, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6202, train loss: 0.002573, validation loss: 0.00165\n",
      "tensor(0.0016)\n",
      "iteration 6203, train loss: 0.002624, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6204, train loss: 0.002521, validation loss: 0.001636\n",
      "tensor(0.0017)\n",
      "iteration 6205, train loss: 0.002593, validation loss: 0.001718\n",
      "tensor(0.0016)\n",
      "iteration 6206, train loss: 0.002589, validation loss: 0.00165\n",
      "tensor(0.0017)\n",
      "iteration 6207, train loss: 0.002552, validation loss: 0.001651\n",
      "tensor(0.0017)\n",
      "iteration 6208, train loss: 0.002559, validation loss: 0.001664\n",
      "tensor(0.0017)\n",
      "iteration 6209, train loss: 0.002594, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6210, train loss: 0.002627, validation loss: 0.00174\n",
      "tensor(0.0016)\n",
      "iteration 6211, train loss: 0.002563, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 6212, train loss: 0.002536, validation loss: 0.001648\n",
      "tensor(0.0017)\n",
      "iteration 6213, train loss: 0.002614, validation loss: 0.001656\n",
      "tensor(0.0017)\n",
      "iteration 6214, train loss: 0.002544, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 6215, train loss: 0.002666, validation loss: 0.00165\n",
      "tensor(0.0017)\n",
      "iteration 6216, train loss: 0.002552, validation loss: 0.00169\n",
      "tensor(0.0018)\n",
      "iteration 6217, train loss: 0.00258, validation loss: 0.001777\n",
      "tensor(0.0016)\n",
      "iteration 6218, train loss: 0.002649, validation loss: 0.001633\n",
      "tensor(0.0017)\n",
      "iteration 6219, train loss: 0.002584, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 6220, train loss: 0.002704, validation loss: 0.001661\n",
      "tensor(0.0018)\n",
      "iteration 6221, train loss: 0.00256, validation loss: 0.001782\n",
      "tensor(0.0017)\n",
      "iteration 6222, train loss: 0.00268, validation loss: 0.001699\n",
      "tensor(0.0016)\n",
      "iteration 6223, train loss: 0.002636, validation loss: 0.001642\n",
      "tensor(0.0018)\n",
      "iteration 6224, train loss: 0.002622, validation loss: 0.001779\n",
      "tensor(0.0017)\n",
      "iteration 6225, train loss: 0.002736, validation loss: 0.001705\n",
      "tensor(0.0018)\n",
      "iteration 6226, train loss: 0.00256, validation loss: 0.001763\n",
      "tensor(0.0017)\n",
      "iteration 6227, train loss: 0.00277, validation loss: 0.001691\n",
      "tensor(0.0018)\n",
      "iteration 6228, train loss: 0.002654, validation loss: 0.00179\n",
      "tensor(0.0017)\n",
      "iteration 6229, train loss: 0.002705, validation loss: 0.001709\n",
      "tensor(0.0016)\n",
      "iteration 6230, train loss: 0.00269, validation loss: 0.001647\n",
      "tensor(0.0018)\n",
      "iteration 6231, train loss: 0.002612, validation loss: 0.001794\n",
      "tensor(0.0018)\n",
      "iteration 6232, train loss: 0.002675, validation loss: 0.00184\n",
      "tensor(0.0017)\n",
      "iteration 6233, train loss: 0.002826, validation loss: 0.001716\n",
      "tensor(0.0018)\n",
      "iteration 6234, train loss: 0.002694, validation loss: 0.00177\n",
      "tensor(0.0017)\n",
      "iteration 6235, train loss: 0.00274, validation loss: 0.001718\n",
      "tensor(0.0017)\n",
      "iteration 6236, train loss: 0.002644, validation loss: 0.001691\n",
      "tensor(0.0017)\n",
      "iteration 6237, train loss: 0.002758, validation loss: 0.001711\n",
      "tensor(0.0018)\n",
      "iteration 6238, train loss: 0.002644, validation loss: 0.001756\n",
      "tensor(0.0017)\n",
      "iteration 6239, train loss: 0.002685, validation loss: 0.001678\n",
      "tensor(0.0016)\n",
      "iteration 6240, train loss: 0.002591, validation loss: 0.001649\n",
      "tensor(0.0017)\n",
      "iteration 6241, train loss: 0.00257, validation loss: 0.00168\n",
      "tensor(0.0017)\n",
      "iteration 6242, train loss: 0.00265, validation loss: 0.001657\n",
      "tensor(0.0017)\n",
      "iteration 6243, train loss: 0.002565, validation loss: 0.001663\n",
      "tensor(0.0017)\n",
      "iteration 6244, train loss: 0.002516, validation loss: 0.001685\n",
      "tensor(0.0017)\n",
      "iteration 6245, train loss: 0.002581, validation loss: 0.001693\n",
      "tensor(0.0017)\n",
      "iteration 6246, train loss: 0.002575, validation loss: 0.001692\n",
      "tensor(0.0017)\n",
      "iteration 6247, train loss: 0.002587, validation loss: 0.001694\n",
      "tensor(0.0017)\n",
      "iteration 6248, train loss: 0.002729, validation loss: 0.001671\n",
      "tensor(0.0018)\n",
      "iteration 6249, train loss: 0.002658, validation loss: 0.001762\n",
      "tensor(0.0017)\n",
      "iteration 6250, train loss: 0.002719, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6251, train loss: 0.002504, validation loss: 0.001747\n",
      "tensor(0.0017)\n",
      "iteration 6252, train loss: 0.00265, validation loss: 0.001729\n",
      "tensor(0.0017)\n",
      "iteration 6253, train loss: 0.002615, validation loss: 0.00172\n",
      "tensor(0.0017)\n",
      "iteration 6254, train loss: 0.002556, validation loss: 0.001671\n",
      "tensor(0.0017)\n",
      "iteration 6255, train loss: 0.002559, validation loss: 0.00165\n",
      "tensor(0.0018)\n",
      "iteration 6256, train loss: 0.002565, validation loss: 0.001755\n",
      "tensor(0.0016)\n",
      "iteration 6257, train loss: 0.002705, validation loss: \u001b[92m0.001624\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6258, train loss: 0.002535, validation loss: 0.001741\n",
      "tensor(0.0017)\n",
      "iteration 6259, train loss: 0.002637, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 6260, train loss: 0.002653, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 6261, train loss: 0.002603, validation loss: 0.001709\n",
      "tensor(0.0017)\n",
      "iteration 6262, train loss: 0.002535, validation loss: 0.001683\n",
      "tensor(0.0018)\n",
      "iteration 6263, train loss: 0.002688, validation loss: 0.001754\n",
      "tensor(0.0017)\n",
      "iteration 6264, train loss: 0.002668, validation loss: 0.001725\n",
      "tensor(0.0017)\n",
      "iteration 6265, train loss: 0.002605, validation loss: 0.001657\n",
      "tensor(0.0017)\n",
      "iteration 6266, train loss: 0.002678, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 6267, train loss: 0.002595, validation loss: 0.001734\n",
      "tensor(0.0017)\n",
      "iteration 6268, train loss: 0.00263, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6269, train loss: 0.002517, validation loss: 0.001657\n",
      "tensor(0.0016)\n",
      "iteration 6270, train loss: 0.002596, validation loss: \u001b[92m0.001612\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6271, train loss: 0.002584, validation loss: 0.001695\n",
      "tensor(0.0016)\n",
      "iteration 6272, train loss: 0.002697, validation loss: 0.001626\n",
      "tensor(0.0017)\n",
      "iteration 6273, train loss: 0.002525, validation loss: 0.001676\n",
      "tensor(0.0016)\n",
      "iteration 6274, train loss: 0.002564, validation loss: 0.00163\n",
      "tensor(0.0017)\n",
      "iteration 6275, train loss: 0.002524, validation loss: 0.001697\n",
      "tensor(0.0016)\n",
      "iteration 6276, train loss: 0.002527, validation loss: 0.001643\n",
      "tensor(0.0016)\n",
      "iteration 6277, train loss: 0.002547, validation loss: 0.001645\n",
      "tensor(0.0017)\n",
      "iteration 6278, train loss: 0.002484, validation loss: 0.001661\n",
      "tensor(0.0017)\n",
      "iteration 6279, train loss: 0.00258, validation loss: 0.001665\n",
      "tensor(0.0016)\n",
      "iteration 6280, train loss: 0.002599, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 6281, train loss: 0.002541, validation loss: 0.001632\n",
      "tensor(0.0017)\n",
      "iteration 6282, train loss: 0.002542, validation loss: 0.001658\n",
      "tensor(0.0016)\n",
      "iteration 6283, train loss: 0.00255, validation loss: 0.001643\n",
      "tensor(0.0016)\n",
      "iteration 6284, train loss: 0.002528, validation loss: \u001b[92m0.001606\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6285, train loss: 0.002546, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6286, train loss: 0.002488, validation loss: 0.001617\n",
      "tensor(0.0016)\n",
      "iteration 6287, train loss: 0.002542, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6288, train loss: 0.002554, validation loss: 0.001615\n",
      "tensor(0.0016)\n",
      "iteration 6289, train loss: 0.002565, validation loss: 0.001636\n",
      "tensor(0.0017)\n",
      "iteration 6290, train loss: 0.002539, validation loss: 0.001703\n",
      "tensor(0.0016)\n",
      "iteration 6291, train loss: 0.002512, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6292, train loss: 0.002545, validation loss: 0.001637\n",
      "tensor(0.0017)\n",
      "iteration 6293, train loss: 0.002657, validation loss: 0.001728\n",
      "tensor(0.0016)\n",
      "iteration 6294, train loss: 0.00272, validation loss: 0.001637\n",
      "tensor(0.0018)\n",
      "iteration 6295, train loss: 0.002591, validation loss: 0.001774\n",
      "tensor(0.0016)\n",
      "iteration 6296, train loss: 0.002657, validation loss: 0.001634\n",
      "tensor(0.0017)\n",
      "iteration 6297, train loss: 0.00255, validation loss: 0.001746\n",
      "tensor(0.0016)\n",
      "iteration 6298, train loss: 0.00268, validation loss: 0.001621\n",
      "tensor(0.0018)\n",
      "iteration 6299, train loss: 0.002612, validation loss: 0.001798\n",
      "tensor(0.0017)\n",
      "iteration 6300, train loss: 0.002889, validation loss: 0.001744\n",
      "tensor(0.0017)\n",
      "iteration 6301, train loss: 0.002591, validation loss: 0.001735\n",
      "tensor(0.0019)\n",
      "iteration 6302, train loss: 0.00265, validation loss: 0.001851\n",
      "tensor(0.0017)\n",
      "iteration 6303, train loss: 0.002803, validation loss: 0.00167\n",
      "tensor(0.0020)\n",
      "iteration 6304, train loss: 0.00266, validation loss: 0.001975\n",
      "tensor(0.0016)\n",
      "iteration 6305, train loss: 0.002775, validation loss: 0.001617\n",
      "tensor(0.0019)\n",
      "iteration 6306, train loss: 0.002558, validation loss: 0.001854\n",
      "tensor(0.0017)\n",
      "iteration 6307, train loss: 0.002846, validation loss: 0.001659\n",
      "tensor(0.0020)\n",
      "iteration 6308, train loss: 0.002549, validation loss: 0.001959\n",
      "tensor(0.0017)\n",
      "iteration 6309, train loss: 0.002707, validation loss: 0.001679\n",
      "tensor(0.0018)\n",
      "iteration 6310, train loss: 0.00258, validation loss: 0.001844\n",
      "tensor(0.0017)\n",
      "iteration 6311, train loss: 0.002756, validation loss: 0.001707\n",
      "tensor(0.0018)\n",
      "iteration 6312, train loss: 0.00265, validation loss: 0.001798\n",
      "tensor(0.0017)\n",
      "iteration 6313, train loss: 0.002849, validation loss: 0.001713\n",
      "tensor(0.0018)\n",
      "iteration 6314, train loss: 0.002748, validation loss: 0.001833\n",
      "tensor(0.0017)\n",
      "iteration 6315, train loss: 0.002827, validation loss: 0.001735\n",
      "tensor(0.0019)\n",
      "iteration 6316, train loss: 0.002575, validation loss: 0.001883\n",
      "tensor(0.0017)\n",
      "iteration 6317, train loss: 0.002796, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6318, train loss: 0.002646, validation loss: 0.001717\n",
      "tensor(0.0017)\n",
      "iteration 6319, train loss: 0.002655, validation loss: 0.001684\n",
      "tensor(0.0018)\n",
      "iteration 6320, train loss: 0.002564, validation loss: 0.001764\n",
      "tensor(0.0017)\n",
      "iteration 6321, train loss: 0.002698, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 6322, train loss: 0.002619, validation loss: 0.001652\n",
      "tensor(0.0018)\n",
      "iteration 6323, train loss: 0.002587, validation loss: 0.001779\n",
      "tensor(0.0017)\n",
      "iteration 6324, train loss: 0.002594, validation loss: 0.001693\n",
      "tensor(0.0016)\n",
      "iteration 6325, train loss: 0.002611, validation loss: 0.001644\n",
      "tensor(0.0017)\n",
      "iteration 6326, train loss: 0.002582, validation loss: 0.001664\n",
      "tensor(0.0017)\n",
      "iteration 6327, train loss: 0.002667, validation loss: 0.001668\n",
      "tensor(0.0016)\n",
      "iteration 6328, train loss: 0.002639, validation loss: 0.001644\n",
      "tensor(0.0017)\n",
      "iteration 6329, train loss: 0.0026, validation loss: 0.001653\n",
      "tensor(0.0016)\n",
      "iteration 6330, train loss: 0.002607, validation loss: \u001b[92m0.001601\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6331, train loss: 0.002486, validation loss: 0.001739\n",
      "tensor(0.0016)\n",
      "iteration 6332, train loss: 0.002693, validation loss: \u001b[92m0.001599\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6333, train loss: 0.002548, validation loss: 0.001653\n",
      "tensor(0.0016)\n",
      "iteration 6334, train loss: 0.002598, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 6335, train loss: 0.002582, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6336, train loss: 0.002487, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 6337, train loss: 0.002535, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6338, train loss: 0.002521, validation loss: 0.00162\n",
      "tensor(0.0017)\n",
      "iteration 6339, train loss: 0.0025, validation loss: 0.001668\n",
      "tensor(0.0016)\n",
      "iteration 6340, train loss: 0.002597, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6341, train loss: 0.002668, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6342, train loss: 0.002645, validation loss: 0.001612\n",
      "tensor(0.0017)\n",
      "iteration 6343, train loss: 0.002585, validation loss: 0.001658\n",
      "tensor(0.0016)\n",
      "iteration 6344, train loss: 0.002639, validation loss: 0.001615\n",
      "tensor(0.0017)\n",
      "iteration 6345, train loss: 0.00256, validation loss: 0.001656\n",
      "tensor(0.0017)\n",
      "iteration 6346, train loss: 0.002594, validation loss: 0.001656\n",
      "tensor(0.0017)\n",
      "iteration 6347, train loss: 0.002611, validation loss: 0.00168\n",
      "tensor(0.0016)\n",
      "iteration 6348, train loss: 0.002592, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6349, train loss: 0.002635, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6350, train loss: 0.002545, validation loss: 0.001618\n",
      "tensor(0.0017)\n",
      "iteration 6351, train loss: 0.002513, validation loss: 0.001686\n",
      "tensor(0.0016)\n",
      "iteration 6352, train loss: 0.002535, validation loss: 0.001605\n",
      "tensor(0.0016)\n",
      "iteration 6353, train loss: 0.002531, validation loss: 0.00164\n",
      "tensor(0.0016)\n",
      "iteration 6354, train loss: 0.002541, validation loss: 0.001639\n",
      "tensor(0.0017)\n",
      "iteration 6355, train loss: 0.002631, validation loss: 0.001715\n",
      "tensor(0.0017)\n",
      "iteration 6356, train loss: 0.002678, validation loss: 0.001692\n",
      "tensor(0.0016)\n",
      "iteration 6357, train loss: 0.002589, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6358, train loss: 0.002617, validation loss: 0.001635\n",
      "tensor(0.0017)\n",
      "iteration 6359, train loss: 0.002536, validation loss: 0.001709\n",
      "tensor(0.0016)\n",
      "iteration 6360, train loss: 0.002616, validation loss: 0.001606\n",
      "tensor(0.0017)\n",
      "iteration 6361, train loss: 0.002558, validation loss: 0.001677\n",
      "tensor(0.0016)\n",
      "iteration 6362, train loss: 0.002643, validation loss: 0.001631\n",
      "tensor(0.0017)\n",
      "iteration 6363, train loss: 0.002557, validation loss: 0.001688\n",
      "tensor(0.0016)\n",
      "iteration 6364, train loss: 0.002562, validation loss: 0.001629\n",
      "tensor(0.0017)\n",
      "iteration 6365, train loss: 0.002595, validation loss: 0.001696\n",
      "tensor(0.0016)\n",
      "iteration 6366, train loss: 0.002615, validation loss: 0.001636\n",
      "tensor(0.0018)\n",
      "iteration 6367, train loss: 0.002488, validation loss: 0.001757\n",
      "tensor(0.0016)\n",
      "iteration 6368, train loss: 0.002668, validation loss: 0.001613\n",
      "tensor(0.0017)\n",
      "iteration 6369, train loss: 0.002518, validation loss: 0.001652\n",
      "tensor(0.0016)\n",
      "iteration 6370, train loss: 0.00265, validation loss: 0.001645\n",
      "tensor(0.0017)\n",
      "iteration 6371, train loss: 0.002545, validation loss: 0.001749\n",
      "tensor(0.0016)\n",
      "iteration 6372, train loss: 0.002569, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6373, train loss: 0.002596, validation loss: 0.001635\n",
      "tensor(0.0017)\n",
      "iteration 6374, train loss: 0.002576, validation loss: 0.001668\n",
      "tensor(0.0017)\n",
      "iteration 6375, train loss: 0.002633, validation loss: 0.001657\n",
      "tensor(0.0016)\n",
      "iteration 6376, train loss: 0.002574, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 6377, train loss: 0.002652, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 6378, train loss: 0.002598, validation loss: 0.001645\n",
      "tensor(0.0016)\n",
      "iteration 6379, train loss: 0.002581, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 6380, train loss: 0.002561, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6381, train loss: 0.002696, validation loss: 0.001634\n",
      "tensor(0.0017)\n",
      "iteration 6382, train loss: 0.002546, validation loss: 0.001663\n",
      "tensor(0.0016)\n",
      "iteration 6383, train loss: 0.002481, validation loss: 0.001633\n",
      "tensor(0.0016)\n",
      "iteration 6384, train loss: 0.002449, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6385, train loss: 0.002486, validation loss: 0.001629\n",
      "tensor(0.0016)\n",
      "iteration 6386, train loss: 0.002526, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6387, train loss: 0.002566, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 6388, train loss: 0.002598, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6389, train loss: 0.002596, validation loss: 0.00161\n",
      "tensor(0.0017)\n",
      "iteration 6390, train loss: 0.002515, validation loss: 0.001667\n",
      "tensor(0.0017)\n",
      "iteration 6391, train loss: 0.002597, validation loss: 0.001656\n",
      "tensor(0.0016)\n",
      "iteration 6392, train loss: 0.002588, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6393, train loss: 0.002589, validation loss: 0.001629\n",
      "tensor(0.0017)\n",
      "iteration 6394, train loss: 0.002554, validation loss: 0.001655\n",
      "tensor(0.0016)\n",
      "iteration 6395, train loss: 0.002674, validation loss: 0.00162\n",
      "tensor(0.0017)\n",
      "iteration 6396, train loss: 0.002609, validation loss: 0.001678\n",
      "tensor(0.0016)\n",
      "iteration 6397, train loss: 0.002639, validation loss: 0.001613\n",
      "tensor(0.0017)\n",
      "iteration 6398, train loss: 0.002462, validation loss: 0.001726\n",
      "tensor(0.0016)\n",
      "iteration 6399, train loss: 0.002547, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6400, train loss: 0.002568, validation loss: 0.001644\n",
      "tensor(0.0017)\n",
      "iteration 6401, train loss: 0.002627, validation loss: 0.00171\n",
      "tensor(0.0017)\n",
      "iteration 6402, train loss: 0.002534, validation loss: 0.001687\n",
      "tensor(0.0017)\n",
      "iteration 6403, train loss: 0.00262, validation loss: 0.001655\n",
      "tensor(0.0016)\n",
      "iteration 6404, train loss: 0.002647, validation loss: 0.001603\n",
      "tensor(0.0017)\n",
      "iteration 6405, train loss: 0.002511, validation loss: 0.001749\n",
      "tensor(0.0016)\n",
      "iteration 6406, train loss: 0.002715, validation loss: 0.001626\n",
      "tensor(0.0017)\n",
      "iteration 6407, train loss: 0.002657, validation loss: 0.001687\n",
      "tensor(0.0016)\n",
      "iteration 6408, train loss: 0.002606, validation loss: 0.001606\n",
      "tensor(0.0018)\n",
      "iteration 6409, train loss: 0.002494, validation loss: 0.001806\n",
      "tensor(0.0016)\n",
      "iteration 6410, train loss: 0.002698, validation loss: 0.001636\n",
      "tensor(0.0017)\n",
      "iteration 6411, train loss: 0.002516, validation loss: 0.001707\n",
      "tensor(0.0016)\n",
      "iteration 6412, train loss: 0.002698, validation loss: 0.00162\n",
      "tensor(0.0018)\n",
      "iteration 6413, train loss: 0.002498, validation loss: 0.001752\n",
      "tensor(0.0016)\n",
      "iteration 6414, train loss: 0.00269, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6415, train loss: 0.002505, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6416, train loss: 0.00253, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 6417, train loss: 0.002614, validation loss: 0.001629\n",
      "tensor(0.0016)\n",
      "iteration 6418, train loss: 0.002544, validation loss: \u001b[92m0.001596\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6419, train loss: 0.002509, validation loss: 0.001604\n",
      "tensor(0.0016)\n",
      "iteration 6420, train loss: 0.002539, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6421, train loss: 0.00266, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6422, train loss: 0.00253, validation loss: 0.001602\n",
      "tensor(0.0016)\n",
      "iteration 6423, train loss: 0.002563, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6424, train loss: 0.002537, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6425, train loss: 0.002509, validation loss: 0.001647\n",
      "tensor(0.0016)\n",
      "iteration 6426, train loss: 0.002525, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6427, train loss: 0.002581, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6428, train loss: 0.00261, validation loss: 0.001613\n",
      "tensor(0.0016)\n",
      "iteration 6429, train loss: 0.002485, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 6430, train loss: 0.002613, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6431, train loss: 0.002523, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6432, train loss: 0.002528, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6433, train loss: 0.002511, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6434, train loss: 0.002535, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 6435, train loss: 0.002525, validation loss: 0.001615\n",
      "tensor(0.0016)\n",
      "iteration 6436, train loss: 0.002478, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 6437, train loss: 0.00256, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6438, train loss: 0.002569, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 6439, train loss: 0.002556, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6440, train loss: 0.002614, validation loss: 0.001612\n",
      "tensor(0.0017)\n",
      "iteration 6441, train loss: 0.002536, validation loss: 0.001652\n",
      "tensor(0.0016)\n",
      "iteration 6442, train loss: 0.002539, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6443, train loss: 0.002594, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6444, train loss: 0.002552, validation loss: 0.001618\n",
      "tensor(0.0017)\n",
      "iteration 6445, train loss: 0.002565, validation loss: 0.001679\n",
      "tensor(0.0016)\n",
      "iteration 6446, train loss: 0.002597, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6447, train loss: 0.002563, validation loss: 0.001628\n",
      "tensor(0.0016)\n",
      "iteration 6448, train loss: 0.002531, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 6449, train loss: 0.002508, validation loss: 0.001637\n",
      "tensor(0.0016)\n",
      "iteration 6450, train loss: 0.002513, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6451, train loss: 0.002571, validation loss: 0.001629\n",
      "tensor(0.0016)\n",
      "iteration 6452, train loss: 0.002632, validation loss: 0.001608\n",
      "tensor(0.0017)\n",
      "iteration 6453, train loss: 0.00254, validation loss: 0.001672\n",
      "tensor(0.0016)\n",
      "iteration 6454, train loss: 0.002603, validation loss: 0.001602\n",
      "tensor(0.0017)\n",
      "iteration 6455, train loss: 0.002474, validation loss: 0.001661\n",
      "tensor(0.0016)\n",
      "iteration 6456, train loss: 0.002692, validation loss: 0.001638\n",
      "tensor(0.0017)\n",
      "iteration 6457, train loss: 0.002568, validation loss: 0.00165\n",
      "tensor(0.0017)\n",
      "iteration 6458, train loss: 0.00254, validation loss: 0.001697\n",
      "tensor(0.0016)\n",
      "iteration 6459, train loss: 0.002607, validation loss: 0.001633\n",
      "tensor(0.0016)\n",
      "iteration 6460, train loss: 0.002499, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6461, train loss: 0.002514, validation loss: 0.001646\n",
      "tensor(0.0016)\n",
      "iteration 6462, train loss: 0.002593, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6463, train loss: 0.002594, validation loss: 0.001646\n",
      "tensor(0.0016)\n",
      "iteration 6464, train loss: 0.002588, validation loss: 0.001642\n",
      "tensor(0.0017)\n",
      "iteration 6465, train loss: 0.002553, validation loss: 0.001691\n",
      "tensor(0.0017)\n",
      "iteration 6466, train loss: 0.002603, validation loss: 0.001689\n",
      "tensor(0.0016)\n",
      "iteration 6467, train loss: 0.00264, validation loss: 0.001625\n",
      "tensor(0.0017)\n",
      "iteration 6468, train loss: 0.002523, validation loss: 0.001657\n",
      "tensor(0.0016)\n",
      "iteration 6469, train loss: 0.002593, validation loss: 0.001634\n",
      "tensor(0.0017)\n",
      "iteration 6470, train loss: 0.002529, validation loss: 0.00167\n",
      "tensor(0.0016)\n",
      "iteration 6471, train loss: 0.00258, validation loss: \u001b[92m0.001592\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6472, train loss: 0.002523, validation loss: 0.001646\n",
      "tensor(0.0017)\n",
      "iteration 6473, train loss: 0.002634, validation loss: 0.00168\n",
      "tensor(0.0017)\n",
      "iteration 6474, train loss: 0.002634, validation loss: 0.001689\n",
      "tensor(0.0017)\n",
      "iteration 6475, train loss: 0.002658, validation loss: 0.001674\n",
      "tensor(0.0018)\n",
      "iteration 6476, train loss: 0.002611, validation loss: 0.00176\n",
      "tensor(0.0016)\n",
      "iteration 6477, train loss: 0.002609, validation loss: 0.001634\n",
      "tensor(0.0016)\n",
      "iteration 6478, train loss: 0.002576, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6479, train loss: 0.002632, validation loss: 0.001633\n",
      "tensor(0.0017)\n",
      "iteration 6480, train loss: 0.00263, validation loss: 0.001657\n",
      "tensor(0.0017)\n",
      "iteration 6481, train loss: 0.002609, validation loss: 0.001668\n",
      "tensor(0.0016)\n",
      "iteration 6482, train loss: 0.002565, validation loss: 0.001611\n",
      "tensor(0.0017)\n",
      "iteration 6483, train loss: 0.002564, validation loss: 0.001667\n",
      "tensor(0.0016)\n",
      "iteration 6484, train loss: 0.00264, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6485, train loss: 0.002495, validation loss: 0.001634\n",
      "tensor(0.0016)\n",
      "iteration 6486, train loss: 0.002588, validation loss: 0.001602\n",
      "tensor(0.0017)\n",
      "iteration 6487, train loss: 0.002436, validation loss: 0.001686\n",
      "tensor(0.0016)\n",
      "iteration 6488, train loss: 0.002559, validation loss: 0.00161\n",
      "tensor(0.0017)\n",
      "iteration 6489, train loss: 0.00253, validation loss: 0.001654\n",
      "tensor(0.0017)\n",
      "iteration 6490, train loss: 0.002599, validation loss: 0.001664\n",
      "tensor(0.0017)\n",
      "iteration 6491, train loss: 0.002546, validation loss: 0.001651\n",
      "tensor(0.0017)\n",
      "iteration 6492, train loss: 0.002556, validation loss: 0.001659\n",
      "tensor(0.0016)\n",
      "iteration 6493, train loss: 0.002523, validation loss: 0.001616\n",
      "tensor(0.0018)\n",
      "iteration 6494, train loss: 0.002577, validation loss: 0.001815\n",
      "tensor(0.0016)\n",
      "iteration 6495, train loss: 0.002713, validation loss: 0.001595\n",
      "tensor(0.0017)\n",
      "iteration 6496, train loss: 0.002476, validation loss: 0.001697\n",
      "tensor(0.0017)\n",
      "iteration 6497, train loss: 0.002684, validation loss: 0.001655\n",
      "tensor(0.0017)\n",
      "iteration 6498, train loss: 0.002608, validation loss: 0.001667\n",
      "tensor(0.0016)\n",
      "iteration 6499, train loss: 0.002521, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6500, train loss: 0.002563, validation loss: 0.001617\n",
      "tensor(0.0017)\n",
      "iteration 6501, train loss: 0.002532, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 6502, train loss: 0.002566, validation loss: 0.001695\n",
      "tensor(0.0018)\n",
      "iteration 6503, train loss: 0.00261, validation loss: 0.001767\n",
      "tensor(0.0016)\n",
      "iteration 6504, train loss: 0.002765, validation loss: 0.001606\n",
      "tensor(0.0018)\n",
      "iteration 6505, train loss: 0.002493, validation loss: 0.001825\n",
      "tensor(0.0016)\n",
      "iteration 6506, train loss: 0.002756, validation loss: 0.001621\n",
      "tensor(0.0017)\n",
      "iteration 6507, train loss: 0.002592, validation loss: 0.001746\n",
      "tensor(0.0018)\n",
      "iteration 6508, train loss: 0.002592, validation loss: 0.001752\n",
      "tensor(0.0017)\n",
      "iteration 6509, train loss: 0.002651, validation loss: 0.001729\n",
      "tensor(0.0017)\n",
      "iteration 6510, train loss: 0.002641, validation loss: 0.001694\n",
      "tensor(0.0016)\n",
      "iteration 6511, train loss: 0.002656, validation loss: 0.001625\n",
      "tensor(0.0017)\n",
      "iteration 6512, train loss: 0.002592, validation loss: 0.001697\n",
      "tensor(0.0016)\n",
      "iteration 6513, train loss: 0.002614, validation loss: 0.001605\n",
      "tensor(0.0016)\n",
      "iteration 6514, train loss: 0.002516, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6515, train loss: 0.00256, validation loss: \u001b[92m0.001591\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6516, train loss: 0.002494, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6517, train loss: 0.002573, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6518, train loss: 0.002584, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6519, train loss: 0.002527, validation loss: 0.001627\n",
      "tensor(0.0017)\n",
      "iteration 6520, train loss: 0.002463, validation loss: 0.001668\n",
      "tensor(0.0016)\n",
      "iteration 6521, train loss: 0.002529, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6522, train loss: 0.002567, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6523, train loss: 0.002544, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 6524, train loss: 0.002568, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6525, train loss: 0.002549, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6526, train loss: 0.002527, validation loss: 0.001642\n",
      "tensor(0.0017)\n",
      "iteration 6527, train loss: 0.002535, validation loss: 0.001744\n",
      "tensor(0.0016)\n",
      "iteration 6528, train loss: 0.002658, validation loss: 0.001649\n",
      "tensor(0.0016)\n",
      "iteration 6529, train loss: 0.002571, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6530, train loss: 0.002596, validation loss: \u001b[92m0.001589\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6531, train loss: 0.002518, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6532, train loss: 0.002508, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6533, train loss: 0.002545, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6534, train loss: 0.002691, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 6535, train loss: 0.002617, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6536, train loss: 0.002512, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6537, train loss: 0.002468, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6538, train loss: 0.002491, validation loss: 0.001597\n",
      "tensor(0.0016)\n",
      "iteration 6539, train loss: 0.002465, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6540, train loss: 0.002548, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6541, train loss: 0.00249, validation loss: 0.001625\n",
      "tensor(0.0016)\n",
      "iteration 6542, train loss: 0.002512, validation loss: 0.001627\n",
      "tensor(0.0016)\n",
      "iteration 6543, train loss: 0.002557, validation loss: 0.001601\n",
      "tensor(0.0017)\n",
      "iteration 6544, train loss: 0.002515, validation loss: 0.001662\n",
      "tensor(0.0016)\n",
      "iteration 6545, train loss: 0.002673, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6546, train loss: 0.002466, validation loss: 0.001637\n",
      "tensor(0.0016)\n",
      "iteration 6547, train loss: 0.002591, validation loss: 0.001613\n",
      "tensor(0.0018)\n",
      "iteration 6548, train loss: 0.002545, validation loss: 0.001763\n",
      "tensor(0.0016)\n",
      "iteration 6549, train loss: 0.00259, validation loss: 0.001639\n",
      "tensor(0.0017)\n",
      "iteration 6550, train loss: 0.00261, validation loss: 0.001654\n",
      "tensor(0.0016)\n",
      "iteration 6551, train loss: 0.002574, validation loss: 0.001648\n",
      "tensor(0.0017)\n",
      "iteration 6552, train loss: 0.002469, validation loss: 0.001678\n",
      "tensor(0.0017)\n",
      "iteration 6553, train loss: 0.002599, validation loss: 0.00167\n",
      "tensor(0.0016)\n",
      "iteration 6554, train loss: 0.002595, validation loss: 0.001647\n",
      "tensor(0.0017)\n",
      "iteration 6555, train loss: 0.002684, validation loss: 0.001734\n",
      "tensor(0.0016)\n",
      "iteration 6556, train loss: 0.00276, validation loss: 0.001607\n",
      "tensor(0.0017)\n",
      "iteration 6557, train loss: 0.002544, validation loss: 0.001707\n",
      "tensor(0.0016)\n",
      "iteration 6558, train loss: 0.00268, validation loss: 0.001604\n",
      "tensor(0.0018)\n",
      "iteration 6559, train loss: 0.002524, validation loss: 0.001782\n",
      "tensor(0.0016)\n",
      "iteration 6560, train loss: 0.002664, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6561, train loss: 0.002615, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 6562, train loss: 0.002555, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6563, train loss: 0.002498, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6564, train loss: 0.002452, validation loss: 0.001594\n",
      "tensor(0.0016)\n",
      "iteration 6565, train loss: 0.00256, validation loss: \u001b[92m0.001587\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6566, train loss: 0.002499, validation loss: \u001b[92m0.001587\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6567, train loss: 0.002606, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 6568, train loss: 0.002486, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6569, train loss: 0.002499, validation loss: \u001b[92m0.001583\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6570, train loss: 0.002505, validation loss: 0.001657\n",
      "tensor(0.0016)\n",
      "iteration 6571, train loss: 0.00252, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 6572, train loss: 0.002543, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6573, train loss: 0.002578, validation loss: 0.001637\n",
      "tensor(0.0016)\n",
      "iteration 6574, train loss: 0.002582, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 6575, train loss: 0.002487, validation loss: 0.001634\n",
      "tensor(0.0017)\n",
      "iteration 6576, train loss: 0.00256, validation loss: 0.001656\n",
      "tensor(0.0017)\n",
      "iteration 6577, train loss: 0.002579, validation loss: 0.001664\n",
      "tensor(0.0016)\n",
      "iteration 6578, train loss: 0.002479, validation loss: 0.001615\n",
      "tensor(0.0016)\n",
      "iteration 6579, train loss: 0.002546, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6580, train loss: 0.00267, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6581, train loss: 0.00254, validation loss: 0.001589\n",
      "tensor(0.0016)\n",
      "iteration 6582, train loss: 0.002473, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6583, train loss: 0.002487, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6584, train loss: 0.002525, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6585, train loss: 0.002553, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6586, train loss: 0.002549, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6587, train loss: 0.002448, validation loss: 0.001593\n",
      "tensor(0.0017)\n",
      "iteration 6588, train loss: 0.002473, validation loss: 0.001671\n",
      "tensor(0.0017)\n",
      "iteration 6589, train loss: 0.002528, validation loss: 0.001677\n",
      "tensor(0.0016)\n",
      "iteration 6590, train loss: 0.0027, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6591, train loss: 0.002547, validation loss: 0.001608\n",
      "tensor(0.0017)\n",
      "iteration 6592, train loss: 0.002635, validation loss: 0.001671\n",
      "tensor(0.0016)\n",
      "iteration 6593, train loss: 0.002595, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6594, train loss: 0.002535, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6595, train loss: 0.00263, validation loss: 0.001627\n",
      "tensor(0.0016)\n",
      "iteration 6596, train loss: 0.002534, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 6597, train loss: 0.002565, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6598, train loss: 0.00258, validation loss: 0.001614\n",
      "tensor(0.0017)\n",
      "iteration 6599, train loss: 0.002459, validation loss: 0.00167\n",
      "tensor(0.0016)\n",
      "iteration 6600, train loss: 0.002536, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6601, train loss: 0.002543, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6602, train loss: 0.002549, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 6603, train loss: 0.002508, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6604, train loss: 0.002575, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6605, train loss: 0.002578, validation loss: 0.001628\n",
      "tensor(0.0016)\n",
      "iteration 6606, train loss: 0.002475, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6607, train loss: 0.002491, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 6608, train loss: \u001b[92m0.002419\u001b[0m, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6609, train loss: 0.002594, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6610, train loss: 0.002488, validation loss: 0.001631\n",
      "tensor(0.0016)\n",
      "iteration 6611, train loss: 0.002476, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 6612, train loss: 0.002487, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 6613, train loss: 0.002506, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6614, train loss: 0.002456, validation loss: 0.001594\n",
      "tensor(0.0017)\n",
      "iteration 6615, train loss: 0.00248, validation loss: 0.001661\n",
      "tensor(0.0016)\n",
      "iteration 6616, train loss: 0.002522, validation loss: \u001b[92m0.001579\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6617, train loss: 0.002487, validation loss: 0.001675\n",
      "tensor(0.0016)\n",
      "iteration 6618, train loss: 0.002672, validation loss: \u001b[92m0.001575\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6619, train loss: 0.002512, validation loss: 0.0017\n",
      "tensor(0.0016)\n",
      "iteration 6620, train loss: 0.002622, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6621, train loss: 0.002585, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6622, train loss: 0.002578, validation loss: 0.00165\n",
      "tensor(0.0016)\n",
      "iteration 6623, train loss: 0.002603, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 6624, train loss: 0.002499, validation loss: 0.001631\n",
      "tensor(0.0016)\n",
      "iteration 6625, train loss: 0.002533, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6626, train loss: 0.00252, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6627, train loss: 0.002556, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6628, train loss: 0.002539, validation loss: 0.001602\n",
      "tensor(0.0016)\n",
      "iteration 6629, train loss: 0.002563, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6630, train loss: 0.002506, validation loss: 0.001616\n",
      "tensor(0.0017)\n",
      "iteration 6631, train loss: 0.002469, validation loss: 0.001657\n",
      "tensor(0.0016)\n",
      "iteration 6632, train loss: 0.00256, validation loss: 0.00165\n",
      "tensor(0.0016)\n",
      "iteration 6633, train loss: 0.002536, validation loss: 0.001606\n",
      "tensor(0.0017)\n",
      "iteration 6634, train loss: 0.002558, validation loss: 0.001663\n",
      "tensor(0.0017)\n",
      "iteration 6635, train loss: 0.002612, validation loss: 0.001662\n",
      "tensor(0.0017)\n",
      "iteration 6636, train loss: 0.00258, validation loss: 0.001723\n",
      "tensor(0.0016)\n",
      "iteration 6637, train loss: 0.002591, validation loss: 0.001631\n",
      "tensor(0.0017)\n",
      "iteration 6638, train loss: 0.002533, validation loss: 0.001737\n",
      "tensor(0.0016)\n",
      "iteration 6639, train loss: 0.002625, validation loss: 0.001609\n",
      "tensor(0.0017)\n",
      "iteration 6640, train loss: 0.002586, validation loss: 0.001709\n",
      "tensor(0.0017)\n",
      "iteration 6641, train loss: 0.002742, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 6642, train loss: 0.002591, validation loss: 0.001675\n",
      "tensor(0.0016)\n",
      "iteration 6643, train loss: 0.002547, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6644, train loss: 0.002675, validation loss: 0.001623\n",
      "tensor(0.0017)\n",
      "iteration 6645, train loss: 0.002572, validation loss: 0.00166\n",
      "tensor(0.0017)\n",
      "iteration 6646, train loss: 0.002659, validation loss: 0.001665\n",
      "tensor(0.0017)\n",
      "iteration 6647, train loss: 0.002564, validation loss: 0.001713\n",
      "tensor(0.0017)\n",
      "iteration 6648, train loss: 0.002623, validation loss: 0.001674\n",
      "tensor(0.0017)\n",
      "iteration 6649, train loss: 0.002523, validation loss: 0.001662\n",
      "tensor(0.0017)\n",
      "iteration 6650, train loss: 0.002599, validation loss: 0.001689\n",
      "tensor(0.0016)\n",
      "iteration 6651, train loss: 0.0026, validation loss: 0.001619\n",
      "tensor(0.0017)\n",
      "iteration 6652, train loss: 0.00255, validation loss: 0.001665\n",
      "tensor(0.0016)\n",
      "iteration 6653, train loss: 0.002506, validation loss: 0.001586\n",
      "tensor(0.0016)\n",
      "iteration 6654, train loss: 0.002524, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 6655, train loss: 0.002508, validation loss: 0.001603\n",
      "tensor(0.0017)\n",
      "iteration 6656, train loss: 0.002631, validation loss: 0.001678\n",
      "tensor(0.0017)\n",
      "iteration 6657, train loss: 0.002634, validation loss: 0.001707\n",
      "tensor(0.0016)\n",
      "iteration 6658, train loss: 0.002636, validation loss: 0.001581\n",
      "tensor(0.0017)\n",
      "iteration 6659, train loss: 0.002497, validation loss: 0.001662\n",
      "tensor(0.0016)\n",
      "iteration 6660, train loss: 0.002669, validation loss: 0.001579\n",
      "tensor(0.0016)\n",
      "iteration 6661, train loss: 0.002552, validation loss: 0.001629\n",
      "tensor(0.0017)\n",
      "iteration 6662, train loss: 0.002572, validation loss: 0.001658\n",
      "tensor(0.0016)\n",
      "iteration 6663, train loss: 0.002529, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6664, train loss: 0.00249, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 6665, train loss: 0.002506, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6666, train loss: 0.00254, validation loss: 0.001643\n",
      "tensor(0.0017)\n",
      "iteration 6667, train loss: 0.002514, validation loss: 0.001716\n",
      "tensor(0.0016)\n",
      "iteration 6668, train loss: 0.002622, validation loss: 0.001591\n",
      "tensor(0.0016)\n",
      "iteration 6669, train loss: 0.002523, validation loss: 0.001642\n",
      "tensor(0.0016)\n",
      "iteration 6670, train loss: 0.002616, validation loss: 0.001644\n",
      "tensor(0.0017)\n",
      "iteration 6671, train loss: 0.002493, validation loss: 0.001664\n",
      "tensor(0.0016)\n",
      "iteration 6672, train loss: 0.002539, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 6673, train loss: 0.0026, validation loss: \u001b[92m0.001572\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6674, train loss: 0.002486, validation loss: 0.001645\n",
      "tensor(0.0016)\n",
      "iteration 6675, train loss: 0.00257, validation loss: \u001b[92m0.001564\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6676, train loss: 0.002501, validation loss: 0.001613\n",
      "tensor(0.0017)\n",
      "iteration 6677, train loss: 0.002527, validation loss: 0.001676\n",
      "tensor(0.0016)\n",
      "iteration 6678, train loss: 0.002668, validation loss: 0.001579\n",
      "tensor(0.0016)\n",
      "iteration 6679, train loss: 0.002531, validation loss: 0.001613\n",
      "tensor(0.0016)\n",
      "iteration 6680, train loss: 0.002601, validation loss: 0.001604\n",
      "tensor(0.0016)\n",
      "iteration 6681, train loss: 0.002456, validation loss: 0.001647\n",
      "tensor(0.0016)\n",
      "iteration 6682, train loss: 0.002554, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6683, train loss: 0.002505, validation loss: 0.00158\n",
      "tensor(0.0016)\n",
      "iteration 6684, train loss: 0.002484, validation loss: 0.001607\n",
      "tensor(0.0017)\n",
      "iteration 6685, train loss: 0.002485, validation loss: 0.001651\n",
      "tensor(0.0016)\n",
      "iteration 6686, train loss: 0.002582, validation loss: 0.001624\n",
      "tensor(0.0017)\n",
      "iteration 6687, train loss: 0.002556, validation loss: 0.001683\n",
      "tensor(0.0016)\n",
      "iteration 6688, train loss: 0.00259, validation loss: 0.001627\n",
      "tensor(0.0016)\n",
      "iteration 6689, train loss: 0.002615, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 6690, train loss: 0.002552, validation loss: 0.001645\n",
      "tensor(0.0016)\n",
      "iteration 6691, train loss: 0.002646, validation loss: 0.001583\n",
      "tensor(0.0018)\n",
      "iteration 6692, train loss: 0.002556, validation loss: 0.001751\n",
      "tensor(0.0016)\n",
      "iteration 6693, train loss: 0.002668, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6694, train loss: 0.002602, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6695, train loss: 0.002565, validation loss: 0.00165\n",
      "tensor(0.0016)\n",
      "iteration 6696, train loss: 0.002544, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 6697, train loss: 0.002464, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6698, train loss: 0.002563, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 6699, train loss: 0.002558, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 6700, train loss: 0.002478, validation loss: 0.001617\n",
      "tensor(0.0016)\n",
      "iteration 6701, train loss: 0.002629, validation loss: 0.001579\n",
      "tensor(0.0017)\n",
      "iteration 6702, train loss: 0.002511, validation loss: 0.001744\n",
      "tensor(0.0016)\n",
      "iteration 6703, train loss: 0.002641, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6704, train loss: 0.002522, validation loss: 0.001604\n",
      "tensor(0.0017)\n",
      "iteration 6705, train loss: 0.002567, validation loss: 0.001693\n",
      "tensor(0.0016)\n",
      "iteration 6706, train loss: 0.002567, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6707, train loss: 0.002513, validation loss: 0.001627\n",
      "tensor(0.0016)\n",
      "iteration 6708, train loss: 0.002587, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6709, train loss: 0.002525, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6710, train loss: 0.002482, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6711, train loss: 0.002489, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6712, train loss: 0.002496, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6713, train loss: 0.00251, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 6714, train loss: 0.002458, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 6715, train loss: 0.002517, validation loss: \u001b[92m0.00156\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6716, train loss: 0.00255, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6717, train loss: 0.002544, validation loss: 0.001586\n",
      "tensor(0.0016)\n",
      "iteration 6718, train loss: 0.002582, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6719, train loss: 0.002476, validation loss: 0.001581\n",
      "tensor(0.0017)\n",
      "iteration 6720, train loss: 0.002501, validation loss: 0.001653\n",
      "tensor(0.0016)\n",
      "iteration 6721, train loss: 0.002541, validation loss: 0.001615\n",
      "tensor(0.0016)\n",
      "iteration 6722, train loss: 0.002581, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6723, train loss: 0.002478, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 6724, train loss: 0.002632, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6725, train loss: 0.002451, validation loss: 0.001646\n",
      "tensor(0.0017)\n",
      "iteration 6726, train loss: 0.00254, validation loss: 0.00166\n",
      "tensor(0.0016)\n",
      "iteration 6727, train loss: 0.00254, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 6728, train loss: 0.002509, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 6729, train loss: 0.002541, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 6730, train loss: 0.002457, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6731, train loss: 0.002607, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6732, train loss: 0.002509, validation loss: 0.001565\n",
      "tensor(0.0016)\n",
      "iteration 6733, train loss: 0.002517, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 6734, train loss: 0.00256, validation loss: 0.001595\n",
      "tensor(0.0016)\n",
      "iteration 6735, train loss: 0.002539, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6736, train loss: 0.002543, validation loss: 0.001614\n",
      "tensor(0.0017)\n",
      "iteration 6737, train loss: 0.002488, validation loss: 0.00166\n",
      "tensor(0.0016)\n",
      "iteration 6738, train loss: 0.002607, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6739, train loss: 0.002481, validation loss: 0.001645\n",
      "tensor(0.0017)\n",
      "iteration 6740, train loss: 0.002625, validation loss: 0.001728\n",
      "tensor(0.0016)\n",
      "iteration 6741, train loss: 0.002605, validation loss: 0.001626\n",
      "tensor(0.0017)\n",
      "iteration 6742, train loss: 0.002515, validation loss: 0.001664\n",
      "tensor(0.0016)\n",
      "iteration 6743, train loss: 0.002581, validation loss: 0.001616\n",
      "tensor(0.0017)\n",
      "iteration 6744, train loss: 0.002495, validation loss: 0.001702\n",
      "tensor(0.0016)\n",
      "iteration 6745, train loss: 0.002547, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 6746, train loss: 0.002522, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6747, train loss: 0.002537, validation loss: 0.001595\n",
      "tensor(0.0016)\n",
      "iteration 6748, train loss: 0.002618, validation loss: 0.001634\n",
      "tensor(0.0016)\n",
      "iteration 6749, train loss: 0.002623, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 6750, train loss: 0.002496, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6751, train loss: 0.002462, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6752, train loss: 0.002487, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6753, train loss: 0.002537, validation loss: 0.00164\n",
      "tensor(0.0016)\n",
      "iteration 6754, train loss: 0.002535, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 6755, train loss: 0.00242, validation loss: 0.001617\n",
      "tensor(0.0016)\n",
      "iteration 6756, train loss: 0.00258, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6757, train loss: 0.002498, validation loss: 0.001579\n",
      "tensor(0.0017)\n",
      "iteration 6758, train loss: 0.002491, validation loss: 0.001677\n",
      "tensor(0.0016)\n",
      "iteration 6759, train loss: 0.002553, validation loss: 0.00158\n",
      "tensor(0.0016)\n",
      "iteration 6760, train loss: 0.002537, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6761, train loss: 0.002601, validation loss: 0.001604\n",
      "tensor(0.0016)\n",
      "iteration 6762, train loss: 0.002473, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6763, train loss: 0.00247, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6764, train loss: 0.002519, validation loss: 0.001579\n",
      "tensor(0.0016)\n",
      "iteration 6765, train loss: 0.002521, validation loss: 0.001605\n",
      "tensor(0.0016)\n",
      "iteration 6766, train loss: 0.002498, validation loss: 0.001575\n",
      "tensor(0.0016)\n",
      "iteration 6767, train loss: 0.002479, validation loss: 0.001617\n",
      "tensor(0.0016)\n",
      "iteration 6768, train loss: 0.002574, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6769, train loss: 0.002462, validation loss: 0.001582\n",
      "tensor(0.0016)\n",
      "iteration 6770, train loss: 0.002493, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 6771, train loss: 0.002554, validation loss: 0.001575\n",
      "tensor(0.0016)\n",
      "iteration 6772, train loss: 0.002545, validation loss: 0.001637\n",
      "tensor(0.0016)\n",
      "iteration 6773, train loss: 0.002566, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6774, train loss: 0.002552, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6775, train loss: 0.002467, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6776, train loss: 0.002571, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 6777, train loss: 0.002511, validation loss: 0.001641\n",
      "tensor(0.0016)\n",
      "iteration 6778, train loss: 0.002563, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6779, train loss: 0.002559, validation loss: 0.001604\n",
      "tensor(0.0016)\n",
      "iteration 6780, train loss: 0.002614, validation loss: 0.001579\n",
      "tensor(0.0016)\n",
      "iteration 6781, train loss: \u001b[92m0.002411\u001b[0m, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6782, train loss: 0.002605, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 6783, train loss: 0.002514, validation loss: 0.001615\n",
      "tensor(0.0016)\n",
      "iteration 6784, train loss: 0.002736, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6785, train loss: 0.002552, validation loss: 0.00161\n",
      "tensor(0.0017)\n",
      "iteration 6786, train loss: 0.00252, validation loss: 0.001667\n",
      "tensor(0.0016)\n",
      "iteration 6787, train loss: 0.002538, validation loss: 0.001621\n",
      "tensor(0.0018)\n",
      "iteration 6788, train loss: 0.002569, validation loss: 0.001762\n",
      "tensor(0.0017)\n",
      "iteration 6789, train loss: 0.002588, validation loss: 0.00167\n",
      "tensor(0.0016)\n",
      "iteration 6790, train loss: 0.002475, validation loss: 0.001575\n",
      "tensor(0.0016)\n",
      "iteration 6791, train loss: 0.00247, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6792, train loss: 0.002605, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 6793, train loss: 0.00253, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6794, train loss: 0.002556, validation loss: 0.001612\n",
      "tensor(0.0017)\n",
      "iteration 6795, train loss: 0.002539, validation loss: 0.001654\n",
      "tensor(0.0016)\n",
      "iteration 6796, train loss: 0.002523, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6797, train loss: 0.002502, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 6798, train loss: 0.002564, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 6799, train loss: 0.002551, validation loss: 0.001607\n",
      "tensor(0.0016)\n",
      "iteration 6800, train loss: 0.00244, validation loss: 0.001643\n",
      "tensor(0.0016)\n",
      "iteration 6801, train loss: 0.002498, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 6802, train loss: 0.002494, validation loss: 0.00159\n",
      "tensor(0.0017)\n",
      "iteration 6803, train loss: 0.002617, validation loss: 0.001673\n",
      "tensor(0.0017)\n",
      "iteration 6804, train loss: 0.002463, validation loss: 0.001721\n",
      "tensor(0.0016)\n",
      "iteration 6805, train loss: 0.002542, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6806, train loss: 0.002523, validation loss: 0.001624\n",
      "tensor(0.0017)\n",
      "iteration 6807, train loss: 0.002719, validation loss: 0.001745\n",
      "tensor(0.0016)\n",
      "iteration 6808, train loss: 0.002681, validation loss: 0.001601\n",
      "tensor(0.0017)\n",
      "iteration 6809, train loss: 0.002543, validation loss: 0.001722\n",
      "tensor(0.0016)\n",
      "iteration 6810, train loss: 0.002619, validation loss: 0.001587\n",
      "tensor(0.0017)\n",
      "iteration 6811, train loss: 0.002499, validation loss: 0.001731\n",
      "tensor(0.0017)\n",
      "iteration 6812, train loss: 0.002613, validation loss: 0.001651\n",
      "tensor(0.0017)\n",
      "iteration 6813, train loss: 0.002546, validation loss: 0.001669\n",
      "tensor(0.0018)\n",
      "iteration 6814, train loss: 0.002593, validation loss: 0.001797\n",
      "tensor(0.0017)\n",
      "iteration 6815, train loss: 0.002611, validation loss: 0.001727\n",
      "tensor(0.0016)\n",
      "iteration 6816, train loss: 0.002563, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6817, train loss: 0.002615, validation loss: 0.001597\n",
      "tensor(0.0017)\n",
      "iteration 6818, train loss: 0.002564, validation loss: 0.001712\n",
      "tensor(0.0016)\n",
      "iteration 6819, train loss: 0.002612, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6820, train loss: 0.002494, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 6821, train loss: 0.00245, validation loss: 0.001572\n",
      "tensor(0.0017)\n",
      "iteration 6822, train loss: 0.002488, validation loss: 0.001687\n",
      "tensor(0.0016)\n",
      "iteration 6823, train loss: 0.002697, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6824, train loss: 0.002471, validation loss: \u001b[92m0.001559\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6825, train loss: 0.002509, validation loss: 0.001631\n",
      "tensor(0.0016)\n",
      "iteration 6826, train loss: 0.0025, validation loss: 0.001572\n",
      "tensor(0.0016)\n",
      "iteration 6827, train loss: 0.002452, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6828, train loss: 0.002425, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 6829, train loss: 0.002494, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 6830, train loss: 0.002436, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 6831, train loss: 0.002534, validation loss: 0.001574\n",
      "tensor(0.0016)\n",
      "iteration 6832, train loss: 0.002507, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 6833, train loss: 0.002435, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 6834, train loss: 0.002529, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6835, train loss: 0.002556, validation loss: 0.001561\n",
      "tensor(0.0017)\n",
      "iteration 6836, train loss: 0.002562, validation loss: 0.001655\n",
      "tensor(0.0016)\n",
      "iteration 6837, train loss: 0.002591, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 6838, train loss: 0.002508, validation loss: 0.001586\n",
      "tensor(0.0016)\n",
      "iteration 6839, train loss: 0.002484, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6840, train loss: \u001b[92m0.002397\u001b[0m, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 6841, train loss: 0.002552, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 6842, train loss: 0.002497, validation loss: \u001b[92m0.001553\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6843, train loss: 0.002436, validation loss: 0.001624\n",
      "tensor(0.0016)\n",
      "iteration 6844, train loss: 0.002525, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 6845, train loss: 0.00255, validation loss: 0.001628\n",
      "tensor(0.0017)\n",
      "iteration 6846, train loss: 0.00259, validation loss: 0.001662\n",
      "tensor(0.0017)\n",
      "iteration 6847, train loss: 0.00254, validation loss: 0.001656\n",
      "tensor(0.0016)\n",
      "iteration 6848, train loss: 0.002489, validation loss: 0.001597\n",
      "tensor(0.0016)\n",
      "iteration 6849, train loss: 0.002485, validation loss: 0.001579\n",
      "tensor(0.0016)\n",
      "iteration 6850, train loss: 0.002452, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 6851, train loss: 0.002519, validation loss: 0.001616\n",
      "tensor(0.0016)\n",
      "iteration 6852, train loss: 0.002611, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 6853, train loss: 0.002554, validation loss: 0.001557\n",
      "tensor(0.0016)\n",
      "iteration 6854, train loss: 0.002514, validation loss: 0.00158\n",
      "tensor(0.0016)\n",
      "iteration 6855, train loss: 0.002482, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6856, train loss: 0.00251, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6857, train loss: 0.002539, validation loss: 0.001631\n",
      "tensor(0.0016)\n",
      "iteration 6858, train loss: 0.002482, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 6859, train loss: 0.002538, validation loss: 0.001645\n",
      "tensor(0.0016)\n",
      "iteration 6860, train loss: 0.002627, validation loss: \u001b[92m0.001552\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6861, train loss: 0.002437, validation loss: 0.001658\n",
      "tensor(0.0016)\n",
      "iteration 6862, train loss: 0.002612, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 6863, train loss: 0.002498, validation loss: 0.001586\n",
      "tensor(0.0016)\n",
      "iteration 6864, train loss: 0.00253, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6865, train loss: 0.002572, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 6866, train loss: 0.002515, validation loss: 0.001597\n",
      "tensor(0.0017)\n",
      "iteration 6867, train loss: 0.002486, validation loss: 0.001653\n",
      "tensor(0.0017)\n",
      "iteration 6868, train loss: 0.002606, validation loss: 0.001655\n",
      "tensor(0.0016)\n",
      "iteration 6869, train loss: 0.002586, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6870, train loss: 0.002531, validation loss: 0.001605\n",
      "tensor(0.0016)\n",
      "iteration 6871, train loss: 0.002507, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6872, train loss: 0.002452, validation loss: 0.001594\n",
      "tensor(0.0016)\n",
      "iteration 6873, train loss: 0.002479, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6874, train loss: 0.002541, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6875, train loss: 0.002494, validation loss: 0.001582\n",
      "tensor(0.0016)\n",
      "iteration 6876, train loss: 0.002515, validation loss: 0.001589\n",
      "tensor(0.0016)\n",
      "iteration 6877, train loss: 0.002539, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 6878, train loss: 0.002556, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 6879, train loss: 0.002441, validation loss: 0.001562\n",
      "tensor(0.0016)\n",
      "iteration 6880, train loss: 0.002503, validation loss: 0.00158\n",
      "tensor(0.0016)\n",
      "iteration 6881, train loss: 0.002424, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6882, train loss: 0.002441, validation loss: \u001b[92m0.00155\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 6883, train loss: 0.002492, validation loss: \u001b[92m0.001547\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 6884, train loss: 0.002459, validation loss: 0.001585\n",
      "tensor(0.0016)\n",
      "iteration 6885, train loss: 0.002435, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 6886, train loss: 0.002398, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6887, train loss: 0.002459, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6888, train loss: 0.002467, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 6889, train loss: 0.00261, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 6890, train loss: 0.002547, validation loss: 0.001597\n",
      "tensor(0.0016)\n",
      "iteration 6891, train loss: 0.002461, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6892, train loss: 0.002482, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 6893, train loss: \u001b[92m0.002382\u001b[0m, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 6894, train loss: 0.002539, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 6895, train loss: 0.002465, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 6896, train loss: 0.00245, validation loss: 0.001551\n",
      "tensor(0.0016)\n",
      "iteration 6897, train loss: 0.00246, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 6898, train loss: 0.002429, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 6899, train loss: 0.002479, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 6900, train loss: 0.002505, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 6901, train loss: 0.002435, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 6902, train loss: 0.002477, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 6903, train loss: 0.002512, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 6904, train loss: 0.0025, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6905, train loss: 0.002467, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 6906, train loss: 0.002519, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 6907, train loss: 0.002418, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 6908, train loss: 0.002555, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 6909, train loss: 0.002466, validation loss: 0.001585\n",
      "tensor(0.0016)\n",
      "iteration 6910, train loss: 0.002479, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 6911, train loss: 0.002468, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 6912, train loss: 0.002523, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6913, train loss: 0.002501, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6914, train loss: 0.00247, validation loss: 0.00163\n",
      "tensor(0.0016)\n",
      "iteration 6915, train loss: 0.002424, validation loss: 0.001597\n",
      "tensor(0.0016)\n",
      "iteration 6916, train loss: 0.002537, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 6917, train loss: 0.002454, validation loss: 0.001584\n",
      "tensor(0.0016)\n",
      "iteration 6918, train loss: 0.002568, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6919, train loss: 0.002482, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 6920, train loss: 0.002474, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 6921, train loss: 0.00244, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 6922, train loss: 0.002505, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 6923, train loss: 0.002573, validation loss: 0.001556\n",
      "tensor(0.0016)\n",
      "iteration 6924, train loss: 0.002497, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 6925, train loss: 0.002554, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6926, train loss: 0.002496, validation loss: 0.001585\n",
      "tensor(0.0016)\n",
      "iteration 6927, train loss: 0.002489, validation loss: 0.001586\n",
      "tensor(0.0015)\n",
      "iteration 6928, train loss: 0.002494, validation loss: 0.001548\n",
      "tensor(0.0016)\n",
      "iteration 6929, train loss: 0.002494, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 6930, train loss: 0.002523, validation loss: 0.001616\n",
      "tensor(0.0015)\n",
      "iteration 6931, train loss: 0.00262, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 6932, train loss: 0.002551, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 6933, train loss: 0.002438, validation loss: 0.001576\n",
      "tensor(0.0017)\n",
      "iteration 6934, train loss: 0.00249, validation loss: 0.001685\n",
      "tensor(0.0016)\n",
      "iteration 6935, train loss: 0.002627, validation loss: 0.001617\n",
      "tensor(0.0017)\n",
      "iteration 6936, train loss: 0.002528, validation loss: 0.001663\n",
      "tensor(0.0016)\n",
      "iteration 6937, train loss: 0.002575, validation loss: 0.001555\n",
      "tensor(0.0017)\n",
      "iteration 6938, train loss: 0.002497, validation loss: 0.001679\n",
      "tensor(0.0016)\n",
      "iteration 6939, train loss: 0.00261, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6940, train loss: 0.002559, validation loss: 0.001649\n",
      "tensor(0.0016)\n",
      "iteration 6941, train loss: 0.002585, validation loss: 0.001594\n",
      "tensor(0.0016)\n",
      "iteration 6942, train loss: 0.002472, validation loss: 0.001595\n",
      "tensor(0.0016)\n",
      "iteration 6943, train loss: 0.002506, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 6944, train loss: 0.002466, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 6945, train loss: 0.002607, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 6946, train loss: 0.002505, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6947, train loss: 0.002505, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 6948, train loss: 0.002458, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6949, train loss: 0.002431, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 6950, train loss: 0.0025, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6951, train loss: 0.002528, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 6952, train loss: 0.002458, validation loss: 0.001606\n",
      "tensor(0.0016)\n",
      "iteration 6953, train loss: 0.002478, validation loss: 0.001632\n",
      "tensor(0.0016)\n",
      "iteration 6954, train loss: 0.002467, validation loss: 0.001589\n",
      "tensor(0.0016)\n",
      "iteration 6955, train loss: 0.002428, validation loss: 0.001595\n",
      "tensor(0.0018)\n",
      "iteration 6956, train loss: 0.002622, validation loss: 0.001777\n",
      "tensor(0.0016)\n",
      "iteration 6957, train loss: 0.00277, validation loss: 0.001565\n",
      "tensor(0.0016)\n",
      "iteration 6958, train loss: 0.002562, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 6959, train loss: 0.002557, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6960, train loss: 0.002415, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6961, train loss: 0.002592, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 6962, train loss: 0.002391, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 6963, train loss: 0.0025, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 6964, train loss: 0.002477, validation loss: 0.001633\n",
      "tensor(0.0016)\n",
      "iteration 6965, train loss: 0.002495, validation loss: 0.001604\n",
      "tensor(0.0016)\n",
      "iteration 6966, train loss: 0.002447, validation loss: 0.001607\n",
      "tensor(0.0017)\n",
      "iteration 6967, train loss: 0.002653, validation loss: 0.001677\n",
      "tensor(0.0016)\n",
      "iteration 6968, train loss: 0.002637, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 6969, train loss: 0.002456, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 6970, train loss: 0.002559, validation loss: 0.001584\n",
      "tensor(0.0017)\n",
      "iteration 6971, train loss: 0.002518, validation loss: 0.001688\n",
      "tensor(0.0016)\n",
      "iteration 6972, train loss: 0.002697, validation loss: 0.001597\n",
      "tensor(0.0016)\n",
      "iteration 6973, train loss: 0.002515, validation loss: 0.001563\n",
      "tensor(0.0018)\n",
      "iteration 6974, train loss: 0.002546, validation loss: 0.001838\n",
      "tensor(0.0016)\n",
      "iteration 6975, train loss: 0.002762, validation loss: 0.001581\n",
      "tensor(0.0018)\n",
      "iteration 6976, train loss: 0.002518, validation loss: 0.001759\n",
      "tensor(0.0016)\n",
      "iteration 6977, train loss: 0.002701, validation loss: 0.00156\n",
      "tensor(0.0017)\n",
      "iteration 6978, train loss: 0.002432, validation loss: 0.001697\n",
      "tensor(0.0016)\n",
      "iteration 6979, train loss: 0.002625, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 6980, train loss: 0.00252, validation loss: 0.001626\n",
      "tensor(0.0016)\n",
      "iteration 6981, train loss: 0.002528, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 6982, train loss: 0.002441, validation loss: 0.001605\n",
      "tensor(0.0016)\n",
      "iteration 6983, train loss: 0.002497, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6984, train loss: 0.002436, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 6985, train loss: 0.002524, validation loss: 0.001634\n",
      "tensor(0.0016)\n",
      "iteration 6986, train loss: 0.00257, validation loss: 0.001551\n",
      "tensor(0.0016)\n",
      "iteration 6987, train loss: 0.002445, validation loss: 0.001622\n",
      "tensor(0.0015)\n",
      "iteration 6988, train loss: 0.002591, validation loss: \u001b[92m0.00154\u001b[0m\n",
      "tensor(0.0017)\n",
      "iteration 6989, train loss: 0.002451, validation loss: 0.001743\n",
      "tensor(0.0016)\n",
      "iteration 6990, train loss: 0.002587, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 6991, train loss: 0.002529, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 6992, train loss: 0.00253, validation loss: 0.001635\n",
      "tensor(0.0016)\n",
      "iteration 6993, train loss: 0.002492, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 6994, train loss: 0.002554, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 6995, train loss: 0.002495, validation loss: 0.001601\n",
      "tensor(0.0017)\n",
      "iteration 6996, train loss: 0.002513, validation loss: 0.001719\n",
      "tensor(0.0016)\n",
      "iteration 6997, train loss: 0.002643, validation loss: 0.001555\n",
      "tensor(0.0017)\n",
      "iteration 6998, train loss: 0.002454, validation loss: 0.001702\n",
      "tensor(0.0015)\n",
      "iteration 6999, train loss: 0.002635, validation loss: 0.001548\n",
      "tensor(0.0017)\n",
      "iteration 7000, train loss: 0.002487, validation loss: 0.001717\n",
      "tensor(0.0016)\n",
      "iteration 7001, train loss: 0.002711, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 7002, train loss: 0.002669, validation loss: 0.001582\n",
      "tensor(0.0016)\n",
      "iteration 7003, train loss: 0.002542, validation loss: 0.001646\n",
      "tensor(0.0016)\n",
      "iteration 7004, train loss: 0.002575, validation loss: 0.001582\n",
      "tensor(0.0016)\n",
      "iteration 7005, train loss: 0.002493, validation loss: 0.001618\n",
      "tensor(0.0015)\n",
      "iteration 7006, train loss: 0.002635, validation loss: 0.001547\n",
      "tensor(0.0017)\n",
      "iteration 7007, train loss: 0.002449, validation loss: 0.001671\n",
      "tensor(0.0016)\n",
      "iteration 7008, train loss: 0.002637, validation loss: 0.001584\n",
      "tensor(0.0016)\n",
      "iteration 7009, train loss: 0.002631, validation loss: 0.001598\n",
      "tensor(0.0017)\n",
      "iteration 7010, train loss: 0.00244, validation loss: 0.00168\n",
      "tensor(0.0016)\n",
      "iteration 7011, train loss: 0.002567, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 7012, train loss: 0.002501, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 7013, train loss: 0.00247, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 7014, train loss: 0.002494, validation loss: 0.001584\n",
      "tensor(0.0016)\n",
      "iteration 7015, train loss: 0.002507, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7016, train loss: 0.002554, validation loss: 0.001544\n",
      "tensor(0.0016)\n",
      "iteration 7017, train loss: 0.00249, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 7018, train loss: 0.002528, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 7019, train loss: 0.002521, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7020, train loss: 0.00248, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7021, train loss: 0.002426, validation loss: 0.001565\n",
      "tensor(0.0016)\n",
      "iteration 7022, train loss: 0.002494, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 7023, train loss: 0.002463, validation loss: 0.001594\n",
      "tensor(0.0016)\n",
      "iteration 7024, train loss: 0.002451, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 7025, train loss: 0.002525, validation loss: 0.001582\n",
      "tensor(0.0017)\n",
      "iteration 7026, train loss: 0.002443, validation loss: 0.001687\n",
      "tensor(0.0016)\n",
      "iteration 7027, train loss: 0.002536, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 7028, train loss: 0.002518, validation loss: 0.001641\n",
      "tensor(0.0016)\n",
      "iteration 7029, train loss: 0.002523, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 7030, train loss: 0.002448, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 7031, train loss: 0.002485, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 7032, train loss: 0.002633, validation loss: 0.001621\n",
      "tensor(0.0017)\n",
      "iteration 7033, train loss: 0.002603, validation loss: 0.0017\n",
      "tensor(0.0017)\n",
      "iteration 7034, train loss: 0.00259, validation loss: 0.001655\n",
      "tensor(0.0017)\n",
      "iteration 7035, train loss: 0.002604, validation loss: 0.001664\n",
      "tensor(0.0016)\n",
      "iteration 7036, train loss: 0.002621, validation loss: 0.00161\n",
      "tensor(0.0017)\n",
      "iteration 7037, train loss: 0.002438, validation loss: 0.001689\n",
      "tensor(0.0016)\n",
      "iteration 7038, train loss: 0.002602, validation loss: 0.001581\n",
      "tensor(0.0016)\n",
      "iteration 7039, train loss: 0.002637, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 7040, train loss: 0.002563, validation loss: 0.001613\n",
      "tensor(0.0016)\n",
      "iteration 7041, train loss: 0.002578, validation loss: 0.001579\n",
      "tensor(0.0017)\n",
      "iteration 7042, train loss: 0.002547, validation loss: 0.001656\n",
      "tensor(0.0016)\n",
      "iteration 7043, train loss: 0.002562, validation loss: 0.001568\n",
      "tensor(0.0018)\n",
      "iteration 7044, train loss: 0.002471, validation loss: 0.001782\n",
      "tensor(0.0015)\n",
      "iteration 7045, train loss: 0.002612, validation loss: 0.001542\n",
      "tensor(0.0017)\n",
      "iteration 7046, train loss: 0.002433, validation loss: 0.001675\n",
      "tensor(0.0016)\n",
      "iteration 7047, train loss: 0.002651, validation loss: 0.001587\n",
      "tensor(0.0017)\n",
      "iteration 7048, train loss: 0.002528, validation loss: 0.001715\n",
      "tensor(0.0016)\n",
      "iteration 7049, train loss: 0.002577, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 7050, train loss: 0.002489, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 7051, train loss: 0.002542, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 7052, train loss: 0.002535, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7053, train loss: 0.002452, validation loss: 0.001577\n",
      "tensor(0.0016)\n",
      "iteration 7054, train loss: 0.002533, validation loss: 0.001584\n",
      "tensor(0.0015)\n",
      "iteration 7055, train loss: 0.002459, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7056, train loss: 0.002495, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 7057, train loss: 0.002533, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 7058, train loss: 0.0025, validation loss: 0.001646\n",
      "tensor(0.0016)\n",
      "iteration 7059, train loss: 0.002472, validation loss: 0.001601\n",
      "tensor(0.0015)\n",
      "iteration 7060, train loss: 0.002488, validation loss: 0.001544\n",
      "tensor(0.0016)\n",
      "iteration 7061, train loss: 0.002445, validation loss: 0.00158\n",
      "tensor(0.0015)\n",
      "iteration 7062, train loss: 0.002494, validation loss: 0.001548\n",
      "tensor(0.0016)\n",
      "iteration 7063, train loss: 0.002571, validation loss: 0.001611\n",
      "tensor(0.0017)\n",
      "iteration 7064, train loss: 0.002522, validation loss: 0.001659\n",
      "tensor(0.0016)\n",
      "iteration 7065, train loss: 0.00264, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 7066, train loss: 0.002594, validation loss: 0.001643\n",
      "tensor(0.0017)\n",
      "iteration 7067, train loss: 0.002521, validation loss: 0.001665\n",
      "tensor(0.0016)\n",
      "iteration 7068, train loss: 0.002467, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 7069, train loss: 0.002552, validation loss: 0.001636\n",
      "tensor(0.0016)\n",
      "iteration 7070, train loss: 0.002544, validation loss: 0.001572\n",
      "tensor(0.0016)\n",
      "iteration 7071, train loss: 0.002587, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 7072, train loss: 0.002549, validation loss: 0.001555\n",
      "tensor(0.0017)\n",
      "iteration 7073, train loss: 0.002525, validation loss: 0.001682\n",
      "tensor(0.0016)\n",
      "iteration 7074, train loss: 0.00266, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 7075, train loss: 0.002492, validation loss: 0.001557\n",
      "tensor(0.0016)\n",
      "iteration 7076, train loss: 0.002517, validation loss: 0.001574\n",
      "tensor(0.0016)\n",
      "iteration 7077, train loss: 0.002455, validation loss: 0.001556\n",
      "tensor(0.0016)\n",
      "iteration 7078, train loss: 0.002429, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 7079, train loss: 0.002458, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 7080, train loss: 0.002514, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 7081, train loss: 0.002391, validation loss: 0.001634\n",
      "tensor(0.0016)\n",
      "iteration 7082, train loss: 0.00255, validation loss: 0.001577\n",
      "tensor(0.0017)\n",
      "iteration 7083, train loss: 0.002527, validation loss: 0.001683\n",
      "tensor(0.0016)\n",
      "iteration 7084, train loss: 0.002464, validation loss: 0.001632\n",
      "tensor(0.0017)\n",
      "iteration 7085, train loss: 0.002594, validation loss: 0.001744\n",
      "tensor(0.0016)\n",
      "iteration 7086, train loss: 0.0027, validation loss: 0.001576\n",
      "tensor(0.0017)\n",
      "iteration 7087, train loss: 0.002518, validation loss: 0.00166\n",
      "tensor(0.0016)\n",
      "iteration 7088, train loss: 0.002529, validation loss: 0.001606\n",
      "tensor(0.0017)\n",
      "iteration 7089, train loss: 0.002511, validation loss: 0.001674\n",
      "tensor(0.0016)\n",
      "iteration 7090, train loss: 0.002459, validation loss: 0.001637\n",
      "tensor(0.0016)\n",
      "iteration 7091, train loss: 0.002607, validation loss: 0.001602\n",
      "tensor(0.0018)\n",
      "iteration 7092, train loss: 0.002583, validation loss: 0.00175\n",
      "tensor(0.0016)\n",
      "iteration 7093, train loss: 0.002587, validation loss: 0.001595\n",
      "tensor(0.0017)\n",
      "iteration 7094, train loss: 0.002479, validation loss: 0.001697\n",
      "tensor(0.0016)\n",
      "iteration 7095, train loss: 0.002745, validation loss: 0.001605\n",
      "tensor(0.0018)\n",
      "iteration 7096, train loss: 0.002474, validation loss: 0.001752\n",
      "tensor(0.0016)\n",
      "iteration 7097, train loss: 0.002633, validation loss: 0.001642\n",
      "tensor(0.0016)\n",
      "iteration 7098, train loss: 0.002595, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 7099, train loss: 0.002416, validation loss: 0.001617\n",
      "tensor(0.0016)\n",
      "iteration 7100, train loss: 0.002517, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 7101, train loss: 0.002551, validation loss: 0.001642\n",
      "tensor(0.0016)\n",
      "iteration 7102, train loss: 0.00255, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 7103, train loss: 0.002567, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 7104, train loss: 0.002556, validation loss: 0.001632\n",
      "tensor(0.0015)\n",
      "iteration 7105, train loss: 0.002569, validation loss: \u001b[92m0.001534\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7106, train loss: 0.002458, validation loss: 0.001574\n",
      "tensor(0.0015)\n",
      "iteration 7107, train loss: 0.002501, validation loss: \u001b[92m0.001534\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7108, train loss: 0.002478, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7109, train loss: 0.002492, validation loss: 0.001548\n",
      "tensor(0.0016)\n",
      "iteration 7110, train loss: 0.002528, validation loss: 0.001602\n",
      "tensor(0.0016)\n",
      "iteration 7111, train loss: 0.002549, validation loss: 0.001603\n",
      "tensor(0.0017)\n",
      "iteration 7112, train loss: 0.002539, validation loss: 0.001686\n",
      "tensor(0.0016)\n",
      "iteration 7113, train loss: 0.002583, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 7114, train loss: 0.002499, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 7115, train loss: 0.002479, validation loss: 0.001595\n",
      "tensor(0.0017)\n",
      "iteration 7116, train loss: 0.002544, validation loss: 0.001669\n",
      "tensor(0.0017)\n",
      "iteration 7117, train loss: 0.002603, validation loss: 0.001715\n",
      "tensor(0.0016)\n",
      "iteration 7118, train loss: 0.002665, validation loss: 0.001589\n",
      "tensor(0.0018)\n",
      "iteration 7119, train loss: 0.002568, validation loss: 0.001812\n",
      "tensor(0.0016)\n",
      "iteration 7120, train loss: 0.002672, validation loss: 0.001579\n",
      "tensor(0.0017)\n",
      "iteration 7121, train loss: 0.002484, validation loss: 0.001686\n",
      "tensor(0.0016)\n",
      "iteration 7122, train loss: 0.00256, validation loss: 0.001556\n",
      "tensor(0.0018)\n",
      "iteration 7123, train loss: 0.002481, validation loss: 0.001801\n",
      "tensor(0.0016)\n",
      "iteration 7124, train loss: 0.002763, validation loss: 0.001554\n",
      "tensor(0.0017)\n",
      "iteration 7125, train loss: 0.002477, validation loss: 0.001706\n",
      "tensor(0.0016)\n",
      "iteration 7126, train loss: 0.002646, validation loss: 0.001585\n",
      "tensor(0.0017)\n",
      "iteration 7127, train loss: 0.002515, validation loss: 0.001687\n",
      "tensor(0.0016)\n",
      "iteration 7128, train loss: 0.00259, validation loss: 0.001589\n",
      "tensor(0.0017)\n",
      "iteration 7129, train loss: 0.002538, validation loss: 0.001667\n",
      "tensor(0.0018)\n",
      "iteration 7130, train loss: 0.002606, validation loss: 0.00176\n",
      "tensor(0.0016)\n",
      "iteration 7131, train loss: 0.002573, validation loss: 0.001624\n",
      "tensor(0.0017)\n",
      "iteration 7132, train loss: 0.002528, validation loss: 0.001688\n",
      "tensor(0.0016)\n",
      "iteration 7133, train loss: 0.002745, validation loss: 0.00157\n",
      "tensor(0.0018)\n",
      "iteration 7134, train loss: 0.002491, validation loss: 0.001765\n",
      "tensor(0.0016)\n",
      "iteration 7135, train loss: 0.002752, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 7136, train loss: 0.002486, validation loss: 0.001638\n",
      "tensor(0.0016)\n",
      "iteration 7137, train loss: 0.002569, validation loss: 0.001553\n",
      "tensor(0.0017)\n",
      "iteration 7138, train loss: 0.002412, validation loss: 0.001665\n",
      "tensor(0.0016)\n",
      "iteration 7139, train loss: 0.002501, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 7140, train loss: 0.002402, validation loss: 0.001562\n",
      "tensor(0.0016)\n",
      "iteration 7141, train loss: 0.002444, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 7142, train loss: 0.002507, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7143, train loss: 0.00256, validation loss: \u001b[92m0.001528\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7144, train loss: 0.002503, validation loss: 0.001535\n",
      "tensor(0.0016)\n",
      "iteration 7145, train loss: 0.002472, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 7146, train loss: 0.002457, validation loss: 0.001628\n",
      "tensor(0.0016)\n",
      "iteration 7147, train loss: 0.002459, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 7148, train loss: 0.002566, validation loss: 0.001575\n",
      "tensor(0.0016)\n",
      "iteration 7149, train loss: 0.002438, validation loss: 0.001575\n",
      "tensor(0.0015)\n",
      "iteration 7150, train loss: 0.002461, validation loss: \u001b[92m0.001525\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7151, train loss: 0.002415, validation loss: 0.001591\n",
      "tensor(0.0016)\n",
      "iteration 7152, train loss: 0.002467, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 7153, train loss: 0.002481, validation loss: 0.001577\n",
      "tensor(0.0017)\n",
      "iteration 7154, train loss: 0.002474, validation loss: 0.001665\n",
      "tensor(0.0016)\n",
      "iteration 7155, train loss: 0.002542, validation loss: 0.001558\n",
      "tensor(0.0017)\n",
      "iteration 7156, train loss: 0.002455, validation loss: 0.001729\n",
      "tensor(0.0016)\n",
      "iteration 7157, train loss: 0.002623, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 7158, train loss: 0.00246, validation loss: 0.00163\n",
      "tensor(0.0016)\n",
      "iteration 7159, train loss: 0.002508, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 7160, train loss: 0.002515, validation loss: 0.001555\n",
      "tensor(0.0016)\n",
      "iteration 7161, train loss: 0.002555, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 7162, train loss: 0.002503, validation loss: 0.001583\n",
      "tensor(0.0017)\n",
      "iteration 7163, train loss: 0.002464, validation loss: 0.001695\n",
      "tensor(0.0015)\n",
      "iteration 7164, train loss: 0.002574, validation loss: \u001b[92m0.001521\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7165, train loss: 0.002404, validation loss: 0.001538\n",
      "tensor(0.0015)\n",
      "iteration 7166, train loss: 0.002503, validation loss: 0.001534\n",
      "tensor(0.0016)\n",
      "iteration 7167, train loss: 0.002407, validation loss: 0.001594\n",
      "tensor(0.0015)\n",
      "iteration 7168, train loss: 0.002477, validation loss: 0.001548\n",
      "tensor(0.0016)\n",
      "iteration 7169, train loss: 0.002447, validation loss: 0.001562\n",
      "tensor(0.0016)\n",
      "iteration 7170, train loss: 0.002556, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 7171, train loss: 0.002569, validation loss: 0.001627\n",
      "tensor(0.0015)\n",
      "iteration 7172, train loss: 0.002496, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7173, train loss: 0.002462, validation loss: 0.00158\n",
      "tensor(0.0016)\n",
      "iteration 7174, train loss: 0.002518, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7175, train loss: 0.002474, validation loss: 0.001573\n",
      "tensor(0.0015)\n",
      "iteration 7176, train loss: 0.002489, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7177, train loss: 0.002457, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 7178, train loss: 0.002511, validation loss: 0.001555\n",
      "tensor(0.0016)\n",
      "iteration 7179, train loss: 0.002433, validation loss: 0.001584\n",
      "tensor(0.0015)\n",
      "iteration 7180, train loss: 0.002465, validation loss: 0.001545\n",
      "tensor(0.0015)\n",
      "iteration 7181, train loss: 0.002503, validation loss: 0.00155\n",
      "tensor(0.0015)\n",
      "iteration 7182, train loss: 0.002478, validation loss: 0.001531\n",
      "tensor(0.0017)\n",
      "iteration 7183, train loss: 0.002407, validation loss: 0.001653\n",
      "tensor(0.0015)\n",
      "iteration 7184, train loss: 0.002588, validation loss: 0.001546\n",
      "tensor(0.0016)\n",
      "iteration 7185, train loss: 0.002513, validation loss: 0.001552\n",
      "tensor(0.0016)\n",
      "iteration 7186, train loss: 0.002475, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 7187, train loss: 0.002435, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 7188, train loss: 0.002484, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 7189, train loss: 0.002399, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7190, train loss: 0.00245, validation loss: 0.001553\n",
      "tensor(0.0015)\n",
      "iteration 7191, train loss: 0.002507, validation loss: 0.001542\n",
      "tensor(0.0016)\n",
      "iteration 7192, train loss: \u001b[92m0.002367\u001b[0m, validation loss: 0.001574\n",
      "tensor(0.0016)\n",
      "iteration 7193, train loss: 0.002481, validation loss: 0.001575\n",
      "tensor(0.0015)\n",
      "iteration 7194, train loss: 0.002458, validation loss: 0.00154\n",
      "tensor(0.0016)\n",
      "iteration 7195, train loss: 0.002439, validation loss: 0.001562\n",
      "tensor(0.0015)\n",
      "iteration 7196, train loss: 0.002464, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7197, train loss: 0.002524, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7198, train loss: 0.002412, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7199, train loss: 0.002543, validation loss: 0.001549\n",
      "tensor(0.0015)\n",
      "iteration 7200, train loss: 0.002444, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7201, train loss: 0.00243, validation loss: 0.001603\n",
      "tensor(0.0016)\n",
      "iteration 7202, train loss: 0.00244, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 7203, train loss: 0.002466, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7204, train loss: 0.002485, validation loss: 0.001595\n",
      "tensor(0.0016)\n",
      "iteration 7205, train loss: 0.002442, validation loss: 0.001568\n",
      "tensor(0.0015)\n",
      "iteration 7206, train loss: 0.002513, validation loss: 0.001538\n",
      "tensor(0.0016)\n",
      "iteration 7207, train loss: 0.002398, validation loss: 0.001552\n",
      "tensor(0.0015)\n",
      "iteration 7208, train loss: 0.00243, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7209, train loss: 0.002468, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 7210, train loss: 0.002471, validation loss: 0.001615\n",
      "tensor(0.0015)\n",
      "iteration 7211, train loss: 0.002459, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7212, train loss: 0.002528, validation loss: 0.001575\n",
      "tensor(0.0015)\n",
      "iteration 7213, train loss: 0.002472, validation loss: 0.001542\n",
      "tensor(0.0016)\n",
      "iteration 7214, train loss: 0.002417, validation loss: 0.001575\n",
      "tensor(0.0015)\n",
      "iteration 7215, train loss: 0.002392, validation loss: 0.001548\n",
      "tensor(0.0015)\n",
      "iteration 7216, train loss: 0.002461, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7217, train loss: 0.002505, validation loss: 0.001583\n",
      "tensor(0.0015)\n",
      "iteration 7218, train loss: 0.002435, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7219, train loss: 0.002441, validation loss: 0.001527\n",
      "tensor(0.0015)\n",
      "iteration 7220, train loss: 0.002412, validation loss: 0.001535\n",
      "tensor(0.0015)\n",
      "iteration 7221, train loss: 0.002455, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7222, train loss: 0.002431, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 7223, train loss: 0.002468, validation loss: 0.001606\n",
      "tensor(0.0015)\n",
      "iteration 7224, train loss: 0.002616, validation loss: 0.001526\n",
      "tensor(0.0016)\n",
      "iteration 7225, train loss: 0.002417, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 7226, train loss: 0.002475, validation loss: 0.00161\n",
      "tensor(0.0017)\n",
      "iteration 7227, train loss: 0.00248, validation loss: 0.001671\n",
      "tensor(0.0015)\n",
      "iteration 7228, train loss: 0.002548, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7229, train loss: 0.002454, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7230, train loss: 0.002468, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7231, train loss: 0.002491, validation loss: 0.001533\n",
      "tensor(0.0016)\n",
      "iteration 7232, train loss: 0.002487, validation loss: 0.001612\n",
      "tensor(0.0015)\n",
      "iteration 7233, train loss: 0.002468, validation loss: 0.001546\n",
      "tensor(0.0016)\n",
      "iteration 7234, train loss: 0.002411, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7235, train loss: 0.002454, validation loss: 0.001535\n",
      "tensor(0.0016)\n",
      "iteration 7236, train loss: 0.002384, validation loss: 0.001552\n",
      "tensor(0.0015)\n",
      "iteration 7237, train loss: 0.002487, validation loss: 0.001547\n",
      "tensor(0.0015)\n",
      "iteration 7238, train loss: 0.002405, validation loss: 0.001534\n",
      "tensor(0.0015)\n",
      "iteration 7239, train loss: 0.002458, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7240, train loss: 0.002432, validation loss: 0.001538\n",
      "tensor(0.0015)\n",
      "iteration 7241, train loss: 0.002462, validation loss: 0.001531\n",
      "tensor(0.0016)\n",
      "iteration 7242, train loss: 0.002429, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 7243, train loss: 0.002453, validation loss: 0.001644\n",
      "tensor(0.0016)\n",
      "iteration 7244, train loss: 0.002489, validation loss: 0.00159\n",
      "tensor(0.0016)\n",
      "iteration 7245, train loss: 0.002428, validation loss: 0.00156\n",
      "tensor(0.0015)\n",
      "iteration 7246, train loss: 0.002513, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7247, train loss: 0.002407, validation loss: 0.001594\n",
      "tensor(0.0015)\n",
      "iteration 7248, train loss: 0.002508, validation loss: \u001b[92m0.001519\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7249, train loss: \u001b[92m0.002314\u001b[0m, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7250, train loss: 0.002504, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 7251, train loss: 0.00237, validation loss: 0.001576\n",
      "tensor(0.0016)\n",
      "iteration 7252, train loss: 0.002419, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7253, train loss: 0.002454, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7254, train loss: 0.002423, validation loss: 0.001544\n",
      "tensor(0.0015)\n",
      "iteration 7255, train loss: 0.002418, validation loss: 0.001536\n",
      "tensor(0.0016)\n",
      "iteration 7256, train loss: 0.002488, validation loss: 0.00155\n",
      "tensor(0.0015)\n",
      "iteration 7257, train loss: 0.00245, validation loss: 0.001524\n",
      "tensor(0.0016)\n",
      "iteration 7258, train loss: 0.002383, validation loss: 0.001559\n",
      "tensor(0.0015)\n",
      "iteration 7259, train loss: 0.002484, validation loss: \u001b[92m0.001518\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7260, train loss: 0.002379, validation loss: 0.001585\n",
      "tensor(0.0016)\n",
      "iteration 7261, train loss: 0.002536, validation loss: 0.001645\n",
      "tensor(0.0017)\n",
      "iteration 7262, train loss: 0.00248, validation loss: 0.001666\n",
      "tensor(0.0016)\n",
      "iteration 7263, train loss: 0.002391, validation loss: 0.001567\n",
      "tensor(0.0015)\n",
      "iteration 7264, train loss: 0.002479, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7265, train loss: 0.002435, validation loss: 0.001636\n",
      "tensor(0.0015)\n",
      "iteration 7266, train loss: 0.002528, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7267, train loss: 0.002444, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7268, train loss: 0.002474, validation loss: 0.001595\n",
      "tensor(0.0015)\n",
      "iteration 7269, train loss: 0.002487, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7270, train loss: 0.002545, validation loss: 0.001569\n",
      "tensor(0.0015)\n",
      "iteration 7271, train loss: 0.002523, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7272, train loss: 0.002453, validation loss: 0.001547\n",
      "tensor(0.0015)\n",
      "iteration 7273, train loss: 0.002472, validation loss: 0.001547\n",
      "tensor(0.0016)\n",
      "iteration 7274, train loss: 0.002527, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 7275, train loss: 0.00251, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 7276, train loss: 0.002514, validation loss: 0.001585\n",
      "tensor(0.0015)\n",
      "iteration 7277, train loss: 0.002512, validation loss: 0.001542\n",
      "tensor(0.0015)\n",
      "iteration 7278, train loss: 0.002482, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7279, train loss: 0.0025, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 7280, train loss: 0.002547, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7281, train loss: 0.00248, validation loss: 0.001533\n",
      "tensor(0.0015)\n",
      "iteration 7282, train loss: 0.00243, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7283, train loss: 0.002474, validation loss: 0.001556\n",
      "tensor(0.0016)\n",
      "iteration 7284, train loss: 0.002457, validation loss: 0.001585\n",
      "tensor(0.0017)\n",
      "iteration 7285, train loss: 0.002418, validation loss: 0.001683\n",
      "tensor(0.0016)\n",
      "iteration 7286, train loss: 0.002518, validation loss: 0.001609\n",
      "tensor(0.0016)\n",
      "iteration 7287, train loss: 0.002453, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 7288, train loss: 0.002416, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 7289, train loss: 0.002494, validation loss: 0.001556\n",
      "tensor(0.0016)\n",
      "iteration 7290, train loss: 0.002528, validation loss: 0.001568\n",
      "tensor(0.0016)\n",
      "iteration 7291, train loss: 0.002567, validation loss: 0.001573\n",
      "tensor(0.0015)\n",
      "iteration 7292, train loss: 0.002545, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7293, train loss: 0.002373, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7294, train loss: 0.002451, validation loss: \u001b[92m0.001518\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7295, train loss: 0.002372, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7296, train loss: 0.002384, validation loss: 0.001545\n",
      "tensor(0.0015)\n",
      "iteration 7297, train loss: 0.002414, validation loss: 0.001548\n",
      "tensor(0.0015)\n",
      "iteration 7298, train loss: 0.002424, validation loss: 0.001531\n",
      "tensor(0.0016)\n",
      "iteration 7299, train loss: 0.002487, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7300, train loss: 0.002498, validation loss: 0.001527\n",
      "tensor(0.0015)\n",
      "iteration 7301, train loss: 0.002429, validation loss: 0.001541\n",
      "tensor(0.0016)\n",
      "iteration 7302, train loss: 0.002556, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 7303, train loss: 0.002526, validation loss: 0.001586\n",
      "tensor(0.0015)\n",
      "iteration 7304, train loss: 0.002488, validation loss: 0.001527\n",
      "tensor(0.0016)\n",
      "iteration 7305, train loss: 0.002442, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7306, train loss: 0.002422, validation loss: 0.001542\n",
      "tensor(0.0016)\n",
      "iteration 7307, train loss: 0.002452, validation loss: 0.001556\n",
      "tensor(0.0015)\n",
      "iteration 7308, train loss: 0.002421, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7309, train loss: 0.002391, validation loss: 0.001544\n",
      "tensor(0.0015)\n",
      "iteration 7310, train loss: 0.002435, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7311, train loss: 0.002416, validation loss: 0.001583\n",
      "tensor(0.0015)\n",
      "iteration 7312, train loss: 0.002488, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7313, train loss: 0.002385, validation loss: \u001b[92m0.001518\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7314, train loss: 0.002462, validation loss: \u001b[92m0.001513\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7315, train loss: 0.00245, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7316, train loss: 0.002496, validation loss: 0.001551\n",
      "tensor(0.0015)\n",
      "iteration 7317, train loss: 0.002468, validation loss: 0.001518\n",
      "tensor(0.0017)\n",
      "iteration 7318, train loss: 0.002425, validation loss: 0.001698\n",
      "tensor(0.0016)\n",
      "iteration 7319, train loss: 0.002541, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7320, train loss: 0.002419, validation loss: 0.001619\n",
      "tensor(0.0016)\n",
      "iteration 7321, train loss: 0.002534, validation loss: 0.001584\n",
      "tensor(0.0016)\n",
      "iteration 7322, train loss: 0.002395, validation loss: 0.001618\n",
      "tensor(0.0016)\n",
      "iteration 7323, train loss: 0.002483, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7324, train loss: 0.002605, validation loss: 0.001514\n",
      "tensor(0.0017)\n",
      "iteration 7325, train loss: 0.002503, validation loss: 0.001651\n",
      "tensor(0.0015)\n",
      "iteration 7326, train loss: 0.002528, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7327, train loss: 0.002379, validation loss: 0.001575\n",
      "tensor(0.0016)\n",
      "iteration 7328, train loss: 0.002511, validation loss: 0.001594\n",
      "tensor(0.0016)\n",
      "iteration 7329, train loss: 0.002482, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 7330, train loss: 0.002425, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7331, train loss: 0.0024, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 7332, train loss: 0.002553, validation loss: 0.001614\n",
      "tensor(0.0015)\n",
      "iteration 7333, train loss: 0.002515, validation loss: 0.00155\n",
      "tensor(0.0016)\n",
      "iteration 7334, train loss: 0.00244, validation loss: 0.001618\n",
      "tensor(0.0015)\n",
      "iteration 7335, train loss: 0.002501, validation loss: 0.001535\n",
      "tensor(0.0017)\n",
      "iteration 7336, train loss: 0.002426, validation loss: 0.001679\n",
      "tensor(0.0015)\n",
      "iteration 7337, train loss: 0.002555, validation loss: 0.001519\n",
      "tensor(0.0016)\n",
      "iteration 7338, train loss: 0.002479, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 7339, train loss: 0.002576, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7340, train loss: 0.002413, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7341, train loss: 0.002466, validation loss: 0.001531\n",
      "tensor(0.0016)\n",
      "iteration 7342, train loss: 0.002494, validation loss: 0.001565\n",
      "tensor(0.0016)\n",
      "iteration 7343, train loss: 0.002507, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 7344, train loss: 0.002478, validation loss: 0.001599\n",
      "tensor(0.0016)\n",
      "iteration 7345, train loss: 0.002506, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7346, train loss: 0.002408, validation loss: 0.001534\n",
      "tensor(0.0016)\n",
      "iteration 7347, train loss: 0.002457, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 7348, train loss: 0.002539, validation loss: 0.001608\n",
      "tensor(0.0015)\n",
      "iteration 7349, train loss: 0.00246, validation loss: 0.001546\n",
      "tensor(0.0015)\n",
      "iteration 7350, train loss: 0.002443, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7351, train loss: 0.00243, validation loss: 0.001542\n",
      "tensor(0.0016)\n",
      "iteration 7352, train loss: 0.002485, validation loss: 0.001551\n",
      "tensor(0.0015)\n",
      "iteration 7353, train loss: 0.002414, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7354, train loss: 0.002428, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7355, train loss: 0.002379, validation loss: 0.001601\n",
      "tensor(0.0016)\n",
      "iteration 7356, train loss: 0.002505, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7357, train loss: 0.002415, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7358, train loss: 0.002554, validation loss: 0.001519\n",
      "tensor(0.0016)\n",
      "iteration 7359, train loss: 0.002373, validation loss: 0.001561\n",
      "tensor(0.0016)\n",
      "iteration 7360, train loss: 0.002451, validation loss: 0.001552\n",
      "tensor(0.0016)\n",
      "iteration 7361, train loss: 0.002472, validation loss: 0.001563\n",
      "tensor(0.0015)\n",
      "iteration 7362, train loss: 0.002441, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7363, train loss: 0.002492, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7364, train loss: 0.002407, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7365, train loss: 0.002396, validation loss: 0.001534\n",
      "tensor(0.0015)\n",
      "iteration 7366, train loss: 0.002402, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7367, train loss: 0.002449, validation loss: 0.00153\n",
      "tensor(0.0016)\n",
      "iteration 7368, train loss: 0.002386, validation loss: 0.001631\n",
      "tensor(0.0015)\n",
      "iteration 7369, train loss: 0.002493, validation loss: 0.001534\n",
      "tensor(0.0015)\n",
      "iteration 7370, train loss: 0.002432, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7371, train loss: 0.002448, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7372, train loss: 0.002421, validation loss: 0.001519\n",
      "tensor(0.0015)\n",
      "iteration 7373, train loss: 0.002444, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7374, train loss: 0.002461, validation loss: 0.001515\n",
      "tensor(0.0015)\n",
      "iteration 7375, train loss: 0.002473, validation loss: \u001b[92m0.001512\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7376, train loss: 0.002423, validation loss: 0.001564\n",
      "tensor(0.0016)\n",
      "iteration 7377, train loss: 0.002402, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 7378, train loss: 0.002512, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7379, train loss: 0.002412, validation loss: 0.001551\n",
      "tensor(0.0015)\n",
      "iteration 7380, train loss: 0.002471, validation loss: 0.001516\n",
      "tensor(0.0016)\n",
      "iteration 7381, train loss: 0.002464, validation loss: 0.001573\n",
      "tensor(0.0015)\n",
      "iteration 7382, train loss: 0.002425, validation loss: 0.001516\n",
      "tensor(0.0015)\n",
      "iteration 7383, train loss: 0.002444, validation loss: 0.001534\n",
      "tensor(0.0015)\n",
      "iteration 7384, train loss: 0.002425, validation loss: \u001b[92m0.001507\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7385, train loss: 0.002443, validation loss: 0.001518\n",
      "tensor(0.0015)\n",
      "iteration 7386, train loss: 0.002454, validation loss: 0.001525\n",
      "tensor(0.0016)\n",
      "iteration 7387, train loss: 0.002447, validation loss: 0.001571\n",
      "tensor(0.0015)\n",
      "iteration 7388, train loss: 0.002525, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7389, train loss: 0.002424, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7390, train loss: 0.002429, validation loss: 0.00156\n",
      "tensor(0.0015)\n",
      "iteration 7391, train loss: 0.002484, validation loss: 0.001512\n",
      "tensor(0.0015)\n",
      "iteration 7392, train loss: 0.002382, validation loss: 0.001522\n",
      "tensor(0.0016)\n",
      "iteration 7393, train loss: 0.002458, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 7394, train loss: 0.002431, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 7395, train loss: 0.002494, validation loss: 0.001574\n",
      "tensor(0.0016)\n",
      "iteration 7396, train loss: 0.002458, validation loss: 0.001573\n",
      "tensor(0.0016)\n",
      "iteration 7397, train loss: 0.002492, validation loss: 0.001616\n",
      "tensor(0.0015)\n",
      "iteration 7398, train loss: 0.00248, validation loss: 0.001533\n",
      "tensor(0.0015)\n",
      "iteration 7399, train loss: 0.002373, validation loss: 0.001514\n",
      "tensor(0.0016)\n",
      "iteration 7400, train loss: 0.002405, validation loss: 0.001634\n",
      "tensor(0.0015)\n",
      "iteration 7401, train loss: 0.002615, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7402, train loss: 0.002482, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7403, train loss: 0.002427, validation loss: 0.001595\n",
      "tensor(0.0016)\n",
      "iteration 7404, train loss: 0.002432, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 7405, train loss: 0.002409, validation loss: 0.001597\n",
      "tensor(0.0015)\n",
      "iteration 7406, train loss: 0.002448, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7407, train loss: 0.002403, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7408, train loss: 0.002462, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7409, train loss: 0.002507, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7410, train loss: 0.002479, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7411, train loss: 0.002371, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7412, train loss: 0.002486, validation loss: 0.001544\n",
      "tensor(0.0016)\n",
      "iteration 7413, train loss: 0.002433, validation loss: 0.001551\n",
      "tensor(0.0015)\n",
      "iteration 7414, train loss: 0.002458, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7415, train loss: 0.002446, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7416, train loss: 0.002372, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7417, train loss: 0.002402, validation loss: 0.001542\n",
      "tensor(0.0015)\n",
      "iteration 7418, train loss: 0.002427, validation loss: \u001b[92m0.001505\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7419, train loss: 0.002489, validation loss: 0.00151\n",
      "tensor(0.0015)\n",
      "iteration 7420, train loss: 0.002404, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7421, train loss: 0.002373, validation loss: 0.001511\n",
      "tensor(0.0016)\n",
      "iteration 7422, train loss: 0.002435, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7423, train loss: 0.002425, validation loss: 0.001514\n",
      "tensor(0.0015)\n",
      "iteration 7424, train loss: 0.002434, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7425, train loss: 0.002389, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7426, train loss: 0.002484, validation loss: 0.001517\n",
      "tensor(0.0016)\n",
      "iteration 7427, train loss: 0.002411, validation loss: 0.00156\n",
      "tensor(0.0015)\n",
      "iteration 7428, train loss: 0.002498, validation loss: 0.00151\n",
      "tensor(0.0015)\n",
      "iteration 7429, train loss: 0.002376, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7430, train loss: 0.002457, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7431, train loss: 0.002517, validation loss: 0.001519\n",
      "tensor(0.0016)\n",
      "iteration 7432, train loss: 0.002371, validation loss: 0.001581\n",
      "tensor(0.0015)\n",
      "iteration 7433, train loss: 0.002593, validation loss: 0.001536\n",
      "tensor(0.0016)\n",
      "iteration 7434, train loss: 0.002415, validation loss: 0.001641\n",
      "tensor(0.0016)\n",
      "iteration 7435, train loss: 0.002586, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 7436, train loss: 0.002489, validation loss: 0.001556\n",
      "tensor(0.0017)\n",
      "iteration 7437, train loss: 0.002428, validation loss: 0.001662\n",
      "tensor(0.0015)\n",
      "iteration 7438, train loss: 0.002569, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7439, train loss: 0.0025, validation loss: 0.001568\n",
      "tensor(0.0015)\n",
      "iteration 7440, train loss: 0.00254, validation loss: 0.00151\n",
      "tensor(0.0016)\n",
      "iteration 7441, train loss: 0.002498, validation loss: 0.001558\n",
      "tensor(0.0015)\n",
      "iteration 7442, train loss: 0.002396, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7443, train loss: 0.002481, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7444, train loss: 0.002467, validation loss: 0.001517\n",
      "tensor(0.0015)\n",
      "iteration 7445, train loss: 0.002443, validation loss: 0.001528\n",
      "tensor(0.0016)\n",
      "iteration 7446, train loss: 0.002416, validation loss: 0.00155\n",
      "tensor(0.0015)\n",
      "iteration 7447, train loss: 0.002532, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7448, train loss: 0.002491, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7449, train loss: 0.002468, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7450, train loss: 0.002507, validation loss: 0.001514\n",
      "tensor(0.0016)\n",
      "iteration 7451, train loss: 0.002413, validation loss: 0.001571\n",
      "tensor(0.0016)\n",
      "iteration 7452, train loss: 0.002451, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7453, train loss: \u001b[92m0.002309\u001b[0m, validation loss: 0.001525\n",
      "tensor(0.0016)\n",
      "iteration 7454, train loss: 0.002397, validation loss: 0.001563\n",
      "tensor(0.0015)\n",
      "iteration 7455, train loss: 0.002451, validation loss: 0.001548\n",
      "tensor(0.0016)\n",
      "iteration 7456, train loss: 0.002456, validation loss: 0.0016\n",
      "tensor(0.0015)\n",
      "iteration 7457, train loss: 0.002521, validation loss: 0.001537\n",
      "tensor(0.0016)\n",
      "iteration 7458, train loss: 0.002537, validation loss: 0.001608\n",
      "tensor(0.0016)\n",
      "iteration 7459, train loss: 0.0025, validation loss: 0.00156\n",
      "tensor(0.0015)\n",
      "iteration 7460, train loss: 0.002475, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7461, train loss: 0.002448, validation loss: 0.00155\n",
      "tensor(0.0016)\n",
      "iteration 7462, train loss: 0.002514, validation loss: 0.001632\n",
      "tensor(0.0015)\n",
      "iteration 7463, train loss: 0.002459, validation loss: 0.00151\n",
      "tensor(0.0016)\n",
      "iteration 7464, train loss: 0.002385, validation loss: 0.001619\n",
      "tensor(0.0015)\n",
      "iteration 7465, train loss: 0.002532, validation loss: 0.001508\n",
      "tensor(0.0016)\n",
      "iteration 7466, train loss: 0.002381, validation loss: 0.001621\n",
      "tensor(0.0016)\n",
      "iteration 7467, train loss: 0.002536, validation loss: 0.001562\n",
      "tensor(0.0016)\n",
      "iteration 7468, train loss: 0.002491, validation loss: 0.001558\n",
      "tensor(0.0017)\n",
      "iteration 7469, train loss: 0.002491, validation loss: 0.001677\n",
      "tensor(0.0016)\n",
      "iteration 7470, train loss: 0.002507, validation loss: 0.001566\n",
      "tensor(0.0015)\n",
      "iteration 7471, train loss: 0.002431, validation loss: 0.001532\n",
      "tensor(0.0015)\n",
      "iteration 7472, train loss: 0.002534, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7473, train loss: 0.002473, validation loss: 0.00159\n",
      "tensor(0.0015)\n",
      "iteration 7474, train loss: 0.002471, validation loss: 0.001547\n",
      "tensor(0.0015)\n",
      "iteration 7475, train loss: 0.002516, validation loss: 0.001517\n",
      "tensor(0.0016)\n",
      "iteration 7476, train loss: 0.002363, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 7477, train loss: 0.002452, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 7478, train loss: 0.002411, validation loss: 0.001608\n",
      "tensor(0.0015)\n",
      "iteration 7479, train loss: 0.002552, validation loss: 0.001541\n",
      "tensor(0.0016)\n",
      "iteration 7480, train loss: 0.002388, validation loss: 0.001635\n",
      "tensor(0.0015)\n",
      "iteration 7481, train loss: 0.002515, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7482, train loss: 0.002455, validation loss: 0.001538\n",
      "tensor(0.0016)\n",
      "iteration 7483, train loss: 0.002407, validation loss: 0.0016\n",
      "tensor(0.0015)\n",
      "iteration 7484, train loss: 0.002524, validation loss: 0.001516\n",
      "tensor(0.0016)\n",
      "iteration 7485, train loss: 0.002399, validation loss: 0.001563\n",
      "tensor(0.0016)\n",
      "iteration 7486, train loss: 0.002543, validation loss: 0.001556\n",
      "tensor(0.0016)\n",
      "iteration 7487, train loss: 0.002546, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 7488, train loss: 0.002459, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7489, train loss: 0.002421, validation loss: 0.00152\n",
      "tensor(0.0015)\n",
      "iteration 7490, train loss: 0.002509, validation loss: 0.001513\n",
      "tensor(0.0015)\n",
      "iteration 7491, train loss: 0.002495, validation loss: 0.00153\n",
      "tensor(0.0015)\n",
      "iteration 7492, train loss: 0.002449, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7493, train loss: 0.002408, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7494, train loss: 0.002433, validation loss: 0.001586\n",
      "tensor(0.0016)\n",
      "iteration 7495, train loss: 0.002467, validation loss: 0.001623\n",
      "tensor(0.0015)\n",
      "iteration 7496, train loss: 0.002523, validation loss: 0.001518\n",
      "tensor(0.0016)\n",
      "iteration 7497, train loss: 0.002426, validation loss: 0.001555\n",
      "tensor(0.0016)\n",
      "iteration 7498, train loss: 0.002553, validation loss: 0.001565\n",
      "tensor(0.0016)\n",
      "iteration 7499, train loss: 0.002423, validation loss: 0.001612\n",
      "tensor(0.0016)\n",
      "iteration 7500, train loss: 0.002491, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 7501, train loss: 0.002455, validation loss: 0.00156\n",
      "tensor(0.0018)\n",
      "iteration 7502, train loss: 0.002473, validation loss: 0.001784\n",
      "tensor(0.0016)\n",
      "iteration 7503, train loss: 0.002575, validation loss: 0.001582\n",
      "tensor(0.0016)\n",
      "iteration 7504, train loss: 0.002466, validation loss: 0.001623\n",
      "tensor(0.0016)\n",
      "iteration 7505, train loss: 0.002567, validation loss: 0.001579\n",
      "tensor(0.0015)\n",
      "iteration 7506, train loss: 0.002487, validation loss: 0.001538\n",
      "tensor(0.0015)\n",
      "iteration 7507, train loss: 0.002443, validation loss: 0.001524\n",
      "tensor(0.0016)\n",
      "iteration 7508, train loss: 0.002409, validation loss: 0.001568\n",
      "tensor(0.0015)\n",
      "iteration 7509, train loss: 0.002407, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7510, train loss: 0.002353, validation loss: 0.001509\n",
      "tensor(0.0015)\n",
      "iteration 7511, train loss: 0.002378, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7512, train loss: 0.002456, validation loss: 0.001511\n",
      "tensor(0.0017)\n",
      "iteration 7513, train loss: 0.002354, validation loss: 0.001662\n",
      "tensor(0.0015)\n",
      "iteration 7514, train loss: 0.002494, validation loss: 0.001517\n",
      "tensor(0.0015)\n",
      "iteration 7515, train loss: 0.002321, validation loss: 0.001537\n",
      "tensor(0.0016)\n",
      "iteration 7516, train loss: 0.002488, validation loss: 0.001559\n",
      "tensor(0.0016)\n",
      "iteration 7517, train loss: 0.002477, validation loss: 0.001562\n",
      "tensor(0.0015)\n",
      "iteration 7518, train loss: 0.002443, validation loss: 0.001527\n",
      "tensor(0.0016)\n",
      "iteration 7519, train loss: 0.002351, validation loss: 0.001605\n",
      "tensor(0.0015)\n",
      "iteration 7520, train loss: 0.002559, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7521, train loss: 0.002525, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7522, train loss: 0.002433, validation loss: 0.001533\n",
      "tensor(0.0015)\n",
      "iteration 7523, train loss: 0.002517, validation loss: 0.00152\n",
      "tensor(0.0017)\n",
      "iteration 7524, train loss: 0.002418, validation loss: 0.001658\n",
      "tensor(0.0015)\n",
      "iteration 7525, train loss: 0.002527, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7526, train loss: 0.00249, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7527, train loss: 0.002456, validation loss: 0.001574\n",
      "tensor(0.0015)\n",
      "iteration 7528, train loss: 0.002477, validation loss: 0.001511\n",
      "tensor(0.0015)\n",
      "iteration 7529, train loss: 0.002399, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7530, train loss: 0.002381, validation loss: 0.001506\n",
      "tensor(0.0016)\n",
      "iteration 7531, train loss: 0.002332, validation loss: 0.001574\n",
      "tensor(0.0015)\n",
      "iteration 7532, train loss: 0.002411, validation loss: \u001b[92m0.001503\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7533, train loss: 0.002434, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7534, train loss: 0.002344, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7535, train loss: 0.002369, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7536, train loss: 0.002355, validation loss: 0.001547\n",
      "tensor(0.0016)\n",
      "iteration 7537, train loss: 0.002503, validation loss: 0.001558\n",
      "tensor(0.0015)\n",
      "iteration 7538, train loss: 0.002539, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7539, train loss: 0.00252, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7540, train loss: 0.002469, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7541, train loss: 0.002524, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7542, train loss: 0.002369, validation loss: 0.001533\n",
      "tensor(0.0016)\n",
      "iteration 7543, train loss: 0.00237, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 7544, train loss: 0.002519, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 7545, train loss: 0.002466, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7546, train loss: 0.002418, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7547, train loss: 0.002492, validation loss: 0.001547\n",
      "tensor(0.0015)\n",
      "iteration 7548, train loss: 0.002425, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7549, train loss: 0.002319, validation loss: \u001b[92m0.001488\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7550, train loss: 0.002405, validation loss: 0.001516\n",
      "tensor(0.0015)\n",
      "iteration 7551, train loss: 0.002413, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7552, train loss: 0.002428, validation loss: 0.001546\n",
      "tensor(0.0015)\n",
      "iteration 7553, train loss: 0.002459, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7554, train loss: 0.002486, validation loss: 0.001598\n",
      "tensor(0.0015)\n",
      "iteration 7555, train loss: 0.002541, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7556, train loss: 0.002314, validation loss: 0.001533\n",
      "tensor(0.0015)\n",
      "iteration 7557, train loss: 0.002451, validation loss: 0.001518\n",
      "tensor(0.0016)\n",
      "iteration 7558, train loss: 0.002434, validation loss: 0.00165\n",
      "tensor(0.0016)\n",
      "iteration 7559, train loss: 0.002525, validation loss: 0.001583\n",
      "tensor(0.0015)\n",
      "iteration 7560, train loss: 0.002507, validation loss: 0.001547\n",
      "tensor(0.0015)\n",
      "iteration 7561, train loss: 0.002431, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7562, train loss: 0.002451, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7563, train loss: 0.002483, validation loss: 0.001506\n",
      "tensor(0.0016)\n",
      "iteration 7564, train loss: 0.002323, validation loss: 0.001577\n",
      "tensor(0.0015)\n",
      "iteration 7565, train loss: 0.002421, validation loss: 0.001535\n",
      "tensor(0.0016)\n",
      "iteration 7566, train loss: 0.002441, validation loss: 0.00157\n",
      "tensor(0.0015)\n",
      "iteration 7567, train loss: 0.002492, validation loss: 0.001501\n",
      "tensor(0.0015)\n",
      "iteration 7568, train loss: 0.002364, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7569, train loss: 0.002413, validation loss: 0.001513\n",
      "tensor(0.0015)\n",
      "iteration 7570, train loss: 0.002378, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7571, train loss: 0.002411, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7572, train loss: 0.002451, validation loss: 0.001524\n",
      "tensor(0.0016)\n",
      "iteration 7573, train loss: 0.002432, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 7574, train loss: 0.002502, validation loss: 0.001592\n",
      "tensor(0.0016)\n",
      "iteration 7575, train loss: 0.002448, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7576, train loss: 0.002431, validation loss: 0.00155\n",
      "tensor(0.0015)\n",
      "iteration 7577, train loss: 0.002421, validation loss: 0.001512\n",
      "tensor(0.0017)\n",
      "iteration 7578, train loss: 0.002488, validation loss: 0.001721\n",
      "tensor(0.0015)\n",
      "iteration 7579, train loss: 0.002639, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7580, train loss: 0.002405, validation loss: 0.001551\n",
      "tensor(0.0016)\n",
      "iteration 7581, train loss: 0.00245, validation loss: 0.001639\n",
      "tensor(0.0016)\n",
      "iteration 7582, train loss: 0.002448, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 7583, train loss: 0.002459, validation loss: 0.001598\n",
      "tensor(0.0016)\n",
      "iteration 7584, train loss: 0.002452, validation loss: 0.00155\n",
      "tensor(0.0016)\n",
      "iteration 7585, train loss: 0.002424, validation loss: 0.001609\n",
      "tensor(0.0015)\n",
      "iteration 7586, train loss: 0.002533, validation loss: 0.001515\n",
      "tensor(0.0015)\n",
      "iteration 7587, train loss: 0.002482, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7588, train loss: 0.002431, validation loss: 0.001517\n",
      "tensor(0.0015)\n",
      "iteration 7589, train loss: 0.002463, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7590, train loss: 0.002408, validation loss: 0.001519\n",
      "tensor(0.0016)\n",
      "iteration 7591, train loss: 0.002412, validation loss: 0.001568\n",
      "tensor(0.0015)\n",
      "iteration 7592, train loss: 0.002397, validation loss: 0.001509\n",
      "tensor(0.0015)\n",
      "iteration 7593, train loss: 0.002399, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7594, train loss: 0.002486, validation loss: 0.001498\n",
      "tensor(0.0015)\n",
      "iteration 7595, train loss: 0.002444, validation loss: 0.001518\n",
      "tensor(0.0015)\n",
      "iteration 7596, train loss: 0.002472, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7597, train loss: 0.002468, validation loss: 0.001544\n",
      "tensor(0.0016)\n",
      "iteration 7598, train loss: 0.002428, validation loss: 0.001571\n",
      "tensor(0.0015)\n",
      "iteration 7599, train loss: 0.00244, validation loss: 0.001543\n",
      "tensor(0.0016)\n",
      "iteration 7600, train loss: 0.002395, validation loss: 0.001553\n",
      "tensor(0.0015)\n",
      "iteration 7601, train loss: 0.002452, validation loss: 0.001493\n",
      "tensor(0.0016)\n",
      "iteration 7602, train loss: 0.002458, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7603, train loss: 0.00258, validation loss: 0.001516\n",
      "tensor(0.0016)\n",
      "iteration 7604, train loss: 0.002387, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7605, train loss: 0.002437, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7606, train loss: 0.002344, validation loss: 0.001598\n",
      "tensor(0.0015)\n",
      "iteration 7607, train loss: 0.00247, validation loss: 0.001513\n",
      "tensor(0.0015)\n",
      "iteration 7608, train loss: 0.002418, validation loss: 0.001502\n",
      "tensor(0.0016)\n",
      "iteration 7609, train loss: 0.002404, validation loss: 0.001621\n",
      "tensor(0.0015)\n",
      "iteration 7610, train loss: 0.002578, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7611, train loss: 0.002351, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7612, train loss: 0.002521, validation loss: 0.001542\n",
      "tensor(0.0015)\n",
      "iteration 7613, train loss: 0.002501, validation loss: 0.001507\n",
      "tensor(0.0016)\n",
      "iteration 7614, train loss: 0.002367, validation loss: 0.001609\n",
      "tensor(0.0015)\n",
      "iteration 7615, train loss: 0.00252, validation loss: 0.001525\n",
      "tensor(0.0017)\n",
      "iteration 7616, train loss: 0.002395, validation loss: 0.00165\n",
      "tensor(0.0015)\n",
      "iteration 7617, train loss: 0.002505, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7618, train loss: 0.002432, validation loss: 0.001497\n",
      "tensor(0.0016)\n",
      "iteration 7619, train loss: 0.002415, validation loss: 0.001622\n",
      "tensor(0.0016)\n",
      "iteration 7620, train loss: 0.002429, validation loss: 0.001604\n",
      "tensor(0.0015)\n",
      "iteration 7621, train loss: 0.002492, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7622, train loss: 0.002436, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7623, train loss: 0.00248, validation loss: 0.001538\n",
      "tensor(0.0016)\n",
      "iteration 7624, train loss: 0.002387, validation loss: 0.001561\n",
      "tensor(0.0015)\n",
      "iteration 7625, train loss: 0.002378, validation loss: 0.001529\n",
      "tensor(0.0016)\n",
      "iteration 7626, train loss: 0.002549, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7627, train loss: 0.002498, validation loss: 0.001512\n",
      "tensor(0.0015)\n",
      "iteration 7628, train loss: 0.002508, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7629, train loss: 0.002544, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7630, train loss: 0.002418, validation loss: 0.001626\n",
      "tensor(0.0017)\n",
      "iteration 7631, train loss: 0.002481, validation loss: 0.001719\n",
      "tensor(0.0015)\n",
      "iteration 7632, train loss: 0.002433, validation loss: 0.001549\n",
      "tensor(0.0016)\n",
      "iteration 7633, train loss: 0.002366, validation loss: 0.001646\n",
      "tensor(0.0016)\n",
      "iteration 7634, train loss: 0.002591, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7635, train loss: 0.002493, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 7636, train loss: 0.002516, validation loss: 0.00155\n",
      "tensor(0.0016)\n",
      "iteration 7637, train loss: 0.002477, validation loss: 0.001557\n",
      "tensor(0.0016)\n",
      "iteration 7638, train loss: 0.002481, validation loss: 0.001625\n",
      "tensor(0.0016)\n",
      "iteration 7639, train loss: 0.002567, validation loss: 0.001605\n",
      "tensor(0.0017)\n",
      "iteration 7640, train loss: 0.002473, validation loss: 0.001651\n",
      "tensor(0.0015)\n",
      "iteration 7641, train loss: 0.002548, validation loss: 0.001517\n",
      "tensor(0.0017)\n",
      "iteration 7642, train loss: 0.002382, validation loss: 0.001705\n",
      "tensor(0.0015)\n",
      "iteration 7643, train loss: 0.002669, validation loss: 0.001506\n",
      "tensor(0.0016)\n",
      "iteration 7644, train loss: 0.002314, validation loss: 0.001625\n",
      "tensor(0.0016)\n",
      "iteration 7645, train loss: 0.002492, validation loss: 0.001611\n",
      "tensor(0.0016)\n",
      "iteration 7646, train loss: 0.002521, validation loss: 0.001566\n",
      "tensor(0.0016)\n",
      "iteration 7647, train loss: 0.00251, validation loss: 0.001569\n",
      "tensor(0.0015)\n",
      "iteration 7648, train loss: 0.002428, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7649, train loss: 0.002382, validation loss: 0.00154\n",
      "tensor(0.0016)\n",
      "iteration 7650, train loss: 0.002447, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7651, train loss: 0.002492, validation loss: 0.001509\n",
      "tensor(0.0015)\n",
      "iteration 7652, train loss: 0.002328, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7653, train loss: 0.002502, validation loss: 0.001512\n",
      "tensor(0.0017)\n",
      "iteration 7654, train loss: 0.002335, validation loss: 0.001711\n",
      "tensor(0.0015)\n",
      "iteration 7655, train loss: 0.002639, validation loss: 0.001536\n",
      "tensor(0.0016)\n",
      "iteration 7656, train loss: 0.002408, validation loss: 0.001583\n",
      "tensor(0.0016)\n",
      "iteration 7657, train loss: 0.002445, validation loss: 0.001644\n",
      "tensor(0.0015)\n",
      "iteration 7658, train loss: 0.002551, validation loss: 0.001541\n",
      "tensor(0.0016)\n",
      "iteration 7659, train loss: 0.002456, validation loss: 0.001563\n",
      "tensor(0.0015)\n",
      "iteration 7660, train loss: 0.002495, validation loss: 0.001502\n",
      "tensor(0.0017)\n",
      "iteration 7661, train loss: 0.002379, validation loss: 0.001746\n",
      "tensor(0.0015)\n",
      "iteration 7662, train loss: 0.002618, validation loss: 0.001495\n",
      "tensor(0.0016)\n",
      "iteration 7663, train loss: 0.002398, validation loss: 0.001611\n",
      "tensor(0.0015)\n",
      "iteration 7664, train loss: 0.002577, validation loss: 0.001506\n",
      "tensor(0.0016)\n",
      "iteration 7665, train loss: 0.002411, validation loss: 0.00155\n",
      "tensor(0.0016)\n",
      "iteration 7666, train loss: 0.002476, validation loss: 0.001567\n",
      "tensor(0.0016)\n",
      "iteration 7667, train loss: 0.002459, validation loss: 0.001569\n",
      "tensor(0.0016)\n",
      "iteration 7668, train loss: 0.002524, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7669, train loss: 0.002523, validation loss: 0.001513\n",
      "tensor(0.0016)\n",
      "iteration 7670, train loss: 0.002435, validation loss: 0.0016\n",
      "tensor(0.0016)\n",
      "iteration 7671, train loss: 0.002379, validation loss: 0.001641\n",
      "tensor(0.0016)\n",
      "iteration 7672, train loss: 0.002464, validation loss: 0.00157\n",
      "tensor(0.0016)\n",
      "iteration 7673, train loss: 0.002535, validation loss: 0.001573\n",
      "tensor(0.0015)\n",
      "iteration 7674, train loss: 0.002439, validation loss: 0.001499\n",
      "tensor(0.0015)\n",
      "iteration 7675, train loss: 0.002318, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7676, train loss: 0.002489, validation loss: 0.001526\n",
      "tensor(0.0015)\n",
      "iteration 7677, train loss: 0.002355, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7678, train loss: 0.002502, validation loss: \u001b[92m0.001484\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7679, train loss: 0.002384, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7680, train loss: 0.002461, validation loss: 0.00152\n",
      "tensor(0.0015)\n",
      "iteration 7681, train loss: 0.002373, validation loss: 0.001544\n",
      "tensor(0.0015)\n",
      "iteration 7682, train loss: 0.002427, validation loss: 0.001489\n",
      "tensor(0.0015)\n",
      "iteration 7683, train loss: 0.002392, validation loss: 0.001548\n",
      "tensor(0.0015)\n",
      "iteration 7684, train loss: 0.002448, validation loss: 0.001512\n",
      "tensor(0.0015)\n",
      "iteration 7685, train loss: 0.002435, validation loss: 0.001516\n",
      "tensor(0.0016)\n",
      "iteration 7686, train loss: 0.002419, validation loss: 0.00156\n",
      "tensor(0.0016)\n",
      "iteration 7687, train loss: 0.002505, validation loss: 0.001587\n",
      "tensor(0.0016)\n",
      "iteration 7688, train loss: 0.002419, validation loss: 0.001584\n",
      "tensor(0.0015)\n",
      "iteration 7689, train loss: 0.002446, validation loss: 0.001531\n",
      "tensor(0.0016)\n",
      "iteration 7690, train loss: 0.002417, validation loss: 0.001614\n",
      "tensor(0.0015)\n",
      "iteration 7691, train loss: 0.002559, validation loss: 0.001485\n",
      "tensor(0.0015)\n",
      "iteration 7692, train loss: 0.002395, validation loss: 0.001549\n",
      "tensor(0.0015)\n",
      "iteration 7693, train loss: 0.00246, validation loss: 0.001517\n",
      "tensor(0.0016)\n",
      "iteration 7694, train loss: 0.002492, validation loss: 0.00156\n",
      "tensor(0.0015)\n",
      "iteration 7695, train loss: 0.002471, validation loss: 0.001536\n",
      "tensor(0.0015)\n",
      "iteration 7696, train loss: 0.002433, validation loss: 0.001526\n",
      "tensor(0.0016)\n",
      "iteration 7697, train loss: 0.00239, validation loss: 0.001591\n",
      "tensor(0.0015)\n",
      "iteration 7698, train loss: 0.002469, validation loss: 0.001502\n",
      "tensor(0.0015)\n",
      "iteration 7699, train loss: 0.002327, validation loss: 0.001503\n",
      "tensor(0.0015)\n",
      "iteration 7700, train loss: 0.002465, validation loss: 0.001487\n",
      "tensor(0.0015)\n",
      "iteration 7701, train loss: 0.002427, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7702, train loss: 0.00242, validation loss: 0.001504\n",
      "tensor(0.0015)\n",
      "iteration 7703, train loss: 0.002365, validation loss: 0.001527\n",
      "tensor(0.0016)\n",
      "iteration 7704, train loss: 0.00239, validation loss: 0.001628\n",
      "tensor(0.0015)\n",
      "iteration 7705, train loss: 0.002507, validation loss: 0.001535\n",
      "tensor(0.0015)\n",
      "iteration 7706, train loss: 0.002383, validation loss: 0.001515\n",
      "tensor(0.0015)\n",
      "iteration 7707, train loss: 0.002489, validation loss: 0.001519\n",
      "tensor(0.0015)\n",
      "iteration 7708, train loss: 0.002431, validation loss: 0.00154\n",
      "tensor(0.0015)\n",
      "iteration 7709, train loss: 0.002453, validation loss: 0.001546\n",
      "tensor(0.0015)\n",
      "iteration 7710, train loss: 0.002451, validation loss: 0.001487\n",
      "tensor(0.0015)\n",
      "iteration 7711, train loss: 0.002382, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7712, train loss: 0.002463, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7713, train loss: 0.002425, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7714, train loss: 0.002414, validation loss: 0.001508\n",
      "tensor(0.0015)\n",
      "iteration 7715, train loss: 0.002412, validation loss: 0.001498\n",
      "tensor(0.0015)\n",
      "iteration 7716, train loss: 0.00241, validation loss: 0.001494\n",
      "tensor(0.0015)\n",
      "iteration 7717, train loss: 0.00241, validation loss: 0.001502\n",
      "tensor(0.0015)\n",
      "iteration 7718, train loss: 0.00241, validation loss: 0.001527\n",
      "tensor(0.0015)\n",
      "iteration 7719, train loss: 0.002484, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7720, train loss: \u001b[92m0.002284\u001b[0m, validation loss: 0.001542\n",
      "tensor(0.0015)\n",
      "iteration 7721, train loss: 0.002434, validation loss: 0.001514\n",
      "tensor(0.0016)\n",
      "iteration 7722, train loss: 0.002393, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7723, train loss: 0.002533, validation loss: \u001b[92m0.001482\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7724, train loss: 0.002393, validation loss: 0.001545\n",
      "tensor(0.0016)\n",
      "iteration 7725, train loss: 0.002438, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7726, train loss: 0.002486, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7727, train loss: 0.002461, validation loss: 0.001514\n",
      "tensor(0.0015)\n",
      "iteration 7728, train loss: 0.00244, validation loss: 0.001503\n",
      "tensor(0.0015)\n",
      "iteration 7729, train loss: 0.002462, validation loss: 0.001544\n",
      "tensor(0.0016)\n",
      "iteration 7730, train loss: 0.002439, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7731, train loss: 0.002414, validation loss: 0.00151\n",
      "tensor(0.0015)\n",
      "iteration 7732, train loss: 0.002365, validation loss: 0.001488\n",
      "tensor(0.0015)\n",
      "iteration 7733, train loss: 0.002447, validation loss: 0.001539\n",
      "tensor(0.0015)\n",
      "iteration 7734, train loss: 0.002365, validation loss: 0.001524\n",
      "tensor(0.0016)\n",
      "iteration 7735, train loss: 0.002349, validation loss: 0.00157\n",
      "tensor(0.0015)\n",
      "iteration 7736, train loss: 0.002464, validation loss: 0.001502\n",
      "tensor(0.0015)\n",
      "iteration 7737, train loss: 0.002478, validation loss: 0.001542\n",
      "tensor(0.0015)\n",
      "iteration 7738, train loss: 0.00245, validation loss: 0.001521\n",
      "tensor(0.0015)\n",
      "iteration 7739, train loss: 0.002425, validation loss: 0.001527\n",
      "tensor(0.0016)\n",
      "iteration 7740, train loss: 0.002374, validation loss: 0.001623\n",
      "tensor(0.0015)\n",
      "iteration 7741, train loss: 0.002599, validation loss: 0.001493\n",
      "tensor(0.0016)\n",
      "iteration 7742, train loss: 0.002418, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7743, train loss: 0.002568, validation loss: 0.001541\n",
      "tensor(0.0016)\n",
      "iteration 7744, train loss: 0.002494, validation loss: 0.001614\n",
      "tensor(0.0016)\n",
      "iteration 7745, train loss: 0.002501, validation loss: 0.001578\n",
      "tensor(0.0015)\n",
      "iteration 7746, train loss: 0.002456, validation loss: 0.001524\n",
      "tensor(0.0017)\n",
      "iteration 7747, train loss: 0.002439, validation loss: 0.001665\n",
      "tensor(0.0015)\n",
      "iteration 7748, train loss: 0.002522, validation loss: 0.001521\n",
      "tensor(0.0017)\n",
      "iteration 7749, train loss: 0.002356, validation loss: 0.001718\n",
      "tensor(0.0015)\n",
      "iteration 7750, train loss: 0.002632, validation loss: 0.001532\n",
      "tensor(0.0017)\n",
      "iteration 7751, train loss: 0.002448, validation loss: 0.001709\n",
      "tensor(0.0015)\n",
      "iteration 7752, train loss: 0.002578, validation loss: 0.00149\n",
      "tensor(0.0015)\n",
      "iteration 7753, train loss: 0.002442, validation loss: 0.001516\n",
      "tensor(0.0015)\n",
      "iteration 7754, train loss: 0.002455, validation loss: 0.001489\n",
      "tensor(0.0015)\n",
      "iteration 7755, train loss: 0.002341, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7756, train loss: 0.002397, validation loss: 0.001489\n",
      "tensor(0.0015)\n",
      "iteration 7757, train loss: 0.002448, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7758, train loss: 0.002453, validation loss: 0.001515\n",
      "tensor(0.0015)\n",
      "iteration 7759, train loss: 0.002373, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7760, train loss: 0.002425, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7761, train loss: 0.002387, validation loss: \u001b[92m0.001481\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7762, train loss: 0.002429, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7763, train loss: 0.00245, validation loss: 0.001502\n",
      "tensor(0.0016)\n",
      "iteration 7764, train loss: 0.002382, validation loss: 0.001575\n",
      "tensor(0.0015)\n",
      "iteration 7765, train loss: 0.002417, validation loss: 0.001545\n",
      "tensor(0.0015)\n",
      "iteration 7766, train loss: 0.002348, validation loss: 0.001527\n",
      "tensor(0.0015)\n",
      "iteration 7767, train loss: 0.002413, validation loss: \u001b[92m0.001479\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7768, train loss: 0.002404, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7769, train loss: 0.0024, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7770, train loss: 0.002364, validation loss: 0.001549\n",
      "tensor(0.0015)\n",
      "iteration 7771, train loss: 0.002412, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7772, train loss: 0.002388, validation loss: 0.001511\n",
      "tensor(0.0016)\n",
      "iteration 7773, train loss: 0.002466, validation loss: 0.001552\n",
      "tensor(0.0015)\n",
      "iteration 7774, train loss: 0.002491, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7775, train loss: 0.00243, validation loss: 0.001501\n",
      "tensor(0.0015)\n",
      "iteration 7776, train loss: 0.002381, validation loss: 0.001527\n",
      "tensor(0.0015)\n",
      "iteration 7777, train loss: 0.002474, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7778, train loss: 0.002419, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7779, train loss: 0.002355, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7780, train loss: 0.002379, validation loss: 0.00148\n",
      "tensor(0.0016)\n",
      "iteration 7781, train loss: 0.002455, validation loss: 0.001615\n",
      "tensor(0.0015)\n",
      "iteration 7782, train loss: 0.002455, validation loss: 0.001481\n",
      "tensor(0.0015)\n",
      "iteration 7783, train loss: 0.002374, validation loss: 0.001532\n",
      "tensor(0.0015)\n",
      "iteration 7784, train loss: 0.002478, validation loss: 0.001497\n",
      "tensor(0.0016)\n",
      "iteration 7785, train loss: 0.002492, validation loss: 0.001627\n",
      "tensor(0.0015)\n",
      "iteration 7786, train loss: 0.002636, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7787, train loss: 0.002454, validation loss: 0.001539\n",
      "tensor(0.0017)\n",
      "iteration 7788, train loss: 0.002442, validation loss: 0.001656\n",
      "tensor(0.0015)\n",
      "iteration 7789, train loss: 0.002559, validation loss: 0.001498\n",
      "tensor(0.0017)\n",
      "iteration 7790, train loss: 0.00247, validation loss: 0.001661\n",
      "tensor(0.0015)\n",
      "iteration 7791, train loss: 0.002548, validation loss: 0.001501\n",
      "tensor(0.0017)\n",
      "iteration 7792, train loss: 0.002409, validation loss: 0.001664\n",
      "tensor(0.0016)\n",
      "iteration 7793, train loss: 0.002486, validation loss: 0.001557\n",
      "tensor(0.0016)\n",
      "iteration 7794, train loss: 0.002472, validation loss: 0.001588\n",
      "tensor(0.0016)\n",
      "iteration 7795, train loss: 0.002488, validation loss: 0.001629\n",
      "tensor(0.0015)\n",
      "iteration 7796, train loss: 0.002461, validation loss: 0.00151\n",
      "tensor(0.0016)\n",
      "iteration 7797, train loss: 0.002356, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7798, train loss: 0.002581, validation loss: 0.001545\n",
      "tensor(0.0015)\n",
      "iteration 7799, train loss: 0.002442, validation loss: 0.001546\n",
      "tensor(0.0015)\n",
      "iteration 7800, train loss: 0.002422, validation loss: 0.001509\n",
      "tensor(0.0015)\n",
      "iteration 7801, train loss: 0.002344, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7802, train loss: 0.002423, validation loss: 0.001485\n",
      "tensor(0.0016)\n",
      "iteration 7803, train loss: 0.002345, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7804, train loss: 0.002498, validation loss: 0.001499\n",
      "tensor(0.0016)\n",
      "iteration 7805, train loss: 0.002391, validation loss: 0.001594\n",
      "tensor(0.0015)\n",
      "iteration 7806, train loss: 0.002559, validation loss: 0.001499\n",
      "tensor(0.0017)\n",
      "iteration 7807, train loss: 0.002415, validation loss: 0.001685\n",
      "tensor(0.0016)\n",
      "iteration 7808, train loss: 0.002582, validation loss: 0.001558\n",
      "tensor(0.0016)\n",
      "iteration 7809, train loss: 0.002457, validation loss: 0.00162\n",
      "tensor(0.0016)\n",
      "iteration 7810, train loss: 0.002587, validation loss: 0.001625\n",
      "tensor(0.0016)\n",
      "iteration 7811, train loss: 0.002491, validation loss: 0.001588\n",
      "tensor(0.0017)\n",
      "iteration 7812, train loss: 0.002438, validation loss: 0.001668\n",
      "tensor(0.0015)\n",
      "iteration 7813, train loss: 0.002613, validation loss: 0.001528\n",
      "tensor(0.0017)\n",
      "iteration 7814, train loss: 0.002504, validation loss: 0.001662\n",
      "tensor(0.0015)\n",
      "iteration 7815, train loss: 0.002617, validation loss: 0.001488\n",
      "tensor(0.0016)\n",
      "iteration 7816, train loss: 0.002343, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7817, train loss: 0.002464, validation loss: 0.001493\n",
      "tensor(0.0016)\n",
      "iteration 7818, train loss: 0.002408, validation loss: 0.001642\n",
      "tensor(0.0015)\n",
      "iteration 7819, train loss: 0.002431, validation loss: 0.001494\n",
      "tensor(0.0016)\n",
      "iteration 7820, train loss: 0.002399, validation loss: 0.001556\n",
      "tensor(0.0015)\n",
      "iteration 7821, train loss: 0.002375, validation loss: 0.001481\n",
      "tensor(0.0016)\n",
      "iteration 7822, train loss: 0.002376, validation loss: 0.001641\n",
      "tensor(0.0015)\n",
      "iteration 7823, train loss: 0.00248, validation loss: 0.001512\n",
      "tensor(0.0015)\n",
      "iteration 7824, train loss: 0.002414, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7825, train loss: 0.00249, validation loss: 0.00151\n",
      "tensor(0.0015)\n",
      "iteration 7826, train loss: 0.002423, validation loss: 0.001533\n",
      "tensor(0.0015)\n",
      "iteration 7827, train loss: 0.002477, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7828, train loss: 0.002484, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7829, train loss: 0.002448, validation loss: 0.001517\n",
      "tensor(0.0015)\n",
      "iteration 7830, train loss: 0.002414, validation loss: 0.001513\n",
      "tensor(0.0015)\n",
      "iteration 7831, train loss: 0.002411, validation loss: 0.001501\n",
      "tensor(0.0015)\n",
      "iteration 7832, train loss: 0.00241, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7833, train loss: 0.00244, validation loss: 0.001597\n",
      "tensor(0.0015)\n",
      "iteration 7834, train loss: 0.002475, validation loss: 0.0015\n",
      "tensor(0.0016)\n",
      "iteration 7835, train loss: 0.002411, validation loss: 0.001617\n",
      "tensor(0.0015)\n",
      "iteration 7836, train loss: 0.002483, validation loss: 0.001492\n",
      "tensor(0.0016)\n",
      "iteration 7837, train loss: 0.002452, validation loss: 0.001613\n",
      "tensor(0.0015)\n",
      "iteration 7838, train loss: 0.002431, validation loss: 0.001479\n",
      "tensor(0.0016)\n",
      "iteration 7839, train loss: 0.002387, validation loss: 0.001556\n",
      "tensor(0.0015)\n",
      "iteration 7840, train loss: 0.002524, validation loss: \u001b[92m0.001474\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7841, train loss: 0.002441, validation loss: 0.00161\n",
      "tensor(0.0016)\n",
      "iteration 7842, train loss: 0.002487, validation loss: 0.00155\n",
      "tensor(0.0015)\n",
      "iteration 7843, train loss: 0.002392, validation loss: 0.001532\n",
      "tensor(0.0016)\n",
      "iteration 7844, train loss: 0.002489, validation loss: 0.001593\n",
      "tensor(0.0016)\n",
      "iteration 7845, train loss: 0.002449, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7846, train loss: 0.002416, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7847, train loss: 0.002441, validation loss: 0.001511\n",
      "tensor(0.0015)\n",
      "iteration 7848, train loss: 0.00248, validation loss: 0.001511\n",
      "tensor(0.0016)\n",
      "iteration 7849, train loss: 0.00237, validation loss: 0.001554\n",
      "tensor(0.0015)\n",
      "iteration 7850, train loss: 0.002434, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7851, train loss: 0.002515, validation loss: 0.001523\n",
      "tensor(0.0015)\n",
      "iteration 7852, train loss: 0.00236, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7853, train loss: 0.002413, validation loss: 0.001517\n",
      "tensor(0.0015)\n",
      "iteration 7854, train loss: 0.002358, validation loss: 0.001483\n",
      "tensor(0.0015)\n",
      "iteration 7855, train loss: 0.002353, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7856, train loss: 0.002347, validation loss: 0.001494\n",
      "tensor(0.0015)\n",
      "iteration 7857, train loss: 0.002368, validation loss: 0.001494\n",
      "tensor(0.0015)\n",
      "iteration 7858, train loss: 0.002403, validation loss: 0.001482\n",
      "tensor(0.0015)\n",
      "iteration 7859, train loss: 0.002368, validation loss: 0.001485\n",
      "tensor(0.0015)\n",
      "iteration 7860, train loss: 0.002314, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7861, train loss: 0.002425, validation loss: 0.001482\n",
      "tensor(0.0015)\n",
      "iteration 7862, train loss: 0.00243, validation loss: 0.001475\n",
      "tensor(0.0015)\n",
      "iteration 7863, train loss: 0.002407, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7864, train loss: 0.002381, validation loss: 0.001478\n",
      "tensor(0.0015)\n",
      "iteration 7865, train loss: 0.002366, validation loss: \u001b[92m0.001473\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7866, train loss: 0.002373, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7867, train loss: 0.002332, validation loss: 0.001503\n",
      "tensor(0.0015)\n",
      "iteration 7868, train loss: 0.002357, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7869, train loss: 0.002412, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7870, train loss: 0.002469, validation loss: 0.001485\n",
      "tensor(0.0015)\n",
      "iteration 7871, train loss: 0.002493, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7872, train loss: 0.002376, validation loss: 0.001491\n",
      "tensor(0.0015)\n",
      "iteration 7873, train loss: 0.002378, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7874, train loss: 0.002409, validation loss: 0.001484\n",
      "tensor(0.0016)\n",
      "iteration 7875, train loss: 0.002362, validation loss: 0.001574\n",
      "tensor(0.0015)\n",
      "iteration 7876, train loss: 0.002531, validation loss: 0.001484\n",
      "tensor(0.0015)\n",
      "iteration 7877, train loss: 0.0024, validation loss: 0.001494\n",
      "tensor(0.0015)\n",
      "iteration 7878, train loss: 0.002408, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7879, train loss: 0.002461, validation loss: 0.001475\n",
      "tensor(0.0016)\n",
      "iteration 7880, train loss: 0.00235, validation loss: 0.001555\n",
      "tensor(0.0015)\n",
      "iteration 7881, train loss: 0.00244, validation loss: 0.00153\n",
      "tensor(0.0015)\n",
      "iteration 7882, train loss: 0.002386, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7883, train loss: 0.002432, validation loss: 0.001524\n",
      "tensor(0.0015)\n",
      "iteration 7884, train loss: 0.002419, validation loss: 0.001496\n",
      "tensor(0.0015)\n",
      "iteration 7885, train loss: 0.002398, validation loss: 0.001548\n",
      "tensor(0.0015)\n",
      "iteration 7886, train loss: 0.00234, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7887, train loss: 0.002455, validation loss: 0.001478\n",
      "tensor(0.0015)\n",
      "iteration 7888, train loss: 0.002328, validation loss: 0.001531\n",
      "tensor(0.0015)\n",
      "iteration 7889, train loss: 0.002394, validation loss: 0.001544\n",
      "tensor(0.0015)\n",
      "iteration 7890, train loss: 0.002367, validation loss: 0.001529\n",
      "tensor(0.0015)\n",
      "iteration 7891, train loss: 0.002396, validation loss: 0.001488\n",
      "tensor(0.0015)\n",
      "iteration 7892, train loss: 0.002429, validation loss: 0.001505\n",
      "tensor(0.0015)\n",
      "iteration 7893, train loss: 0.002395, validation loss: 0.001546\n",
      "tensor(0.0015)\n",
      "iteration 7894, train loss: 0.002434, validation loss: 0.001499\n",
      "tensor(0.0015)\n",
      "iteration 7895, train loss: 0.002355, validation loss: 0.001503\n",
      "tensor(0.0015)\n",
      "iteration 7896, train loss: 0.002482, validation loss: 0.001513\n",
      "tensor(0.0016)\n",
      "iteration 7897, train loss: 0.002436, validation loss: 0.001552\n",
      "tensor(0.0015)\n",
      "iteration 7898, train loss: 0.002387, validation loss: 0.001492\n",
      "tensor(0.0016)\n",
      "iteration 7899, train loss: 0.002319, validation loss: 0.001591\n",
      "tensor(0.0015)\n",
      "iteration 7900, train loss: 0.002438, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7901, train loss: 0.00241, validation loss: 0.001541\n",
      "tensor(0.0015)\n",
      "iteration 7902, train loss: 0.002407, validation loss: 0.001514\n",
      "tensor(0.0016)\n",
      "iteration 7903, train loss: 0.002432, validation loss: 0.001556\n",
      "tensor(0.0015)\n",
      "iteration 7904, train loss: 0.002564, validation loss: 0.001543\n",
      "tensor(0.0015)\n",
      "iteration 7905, train loss: 0.002426, validation loss: \u001b[92m0.001472\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7906, train loss: 0.002461, validation loss: 0.001525\n",
      "tensor(0.0015)\n",
      "iteration 7907, train loss: 0.002421, validation loss: 0.001535\n",
      "tensor(0.0016)\n",
      "iteration 7908, train loss: 0.002388, validation loss: 0.0016\n",
      "tensor(0.0015)\n",
      "iteration 7909, train loss: 0.002506, validation loss: 0.001534\n",
      "tensor(0.0015)\n",
      "iteration 7910, train loss: 0.002412, validation loss: 0.001503\n",
      "tensor(0.0016)\n",
      "iteration 7911, train loss: 0.00241, validation loss: 0.001613\n",
      "tensor(0.0015)\n",
      "iteration 7912, train loss: 0.00252, validation loss: 0.00149\n",
      "tensor(0.0016)\n",
      "iteration 7913, train loss: 0.002323, validation loss: 0.001602\n",
      "tensor(0.0015)\n",
      "iteration 7914, train loss: 0.002596, validation loss: 0.001487\n",
      "tensor(0.0016)\n",
      "iteration 7915, train loss: 0.002448, validation loss: 0.001595\n",
      "tensor(0.0015)\n",
      "iteration 7916, train loss: 0.002494, validation loss: 0.001494\n",
      "tensor(0.0015)\n",
      "iteration 7917, train loss: 0.002368, validation loss: 0.001527\n",
      "tensor(0.0016)\n",
      "iteration 7918, train loss: 0.002354, validation loss: 0.001611\n",
      "tensor(0.0015)\n",
      "iteration 7919, train loss: 0.002398, validation loss: 0.00153\n",
      "tensor(0.0016)\n",
      "iteration 7920, train loss: 0.00242, validation loss: 0.001576\n",
      "tensor(0.0015)\n",
      "iteration 7921, train loss: 0.002599, validation loss: 0.001484\n",
      "tensor(0.0016)\n",
      "iteration 7922, train loss: 0.002319, validation loss: 0.001589\n",
      "tensor(0.0015)\n",
      "iteration 7923, train loss: 0.002395, validation loss: 0.001492\n",
      "tensor(0.0016)\n",
      "iteration 7924, train loss: 0.002322, validation loss: 0.001596\n",
      "tensor(0.0016)\n",
      "iteration 7925, train loss: 0.002569, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 7926, train loss: 0.002447, validation loss: 0.001574\n",
      "tensor(0.0017)\n",
      "iteration 7927, train loss: 0.002439, validation loss: 0.001661\n",
      "tensor(0.0015)\n",
      "iteration 7928, train loss: 0.002705, validation loss: 0.001498\n",
      "tensor(0.0017)\n",
      "iteration 7929, train loss: 0.002363, validation loss: 0.001665\n",
      "tensor(0.0015)\n",
      "iteration 7930, train loss: 0.002503, validation loss: \u001b[92m0.00146\u001b[0m\n",
      "tensor(0.0016)\n",
      "iteration 7931, train loss: 0.002326, validation loss: 0.001574\n",
      "tensor(0.0015)\n",
      "iteration 7932, train loss: 0.002447, validation loss: 0.001462\n",
      "tensor(0.0017)\n",
      "iteration 7933, train loss: 0.002324, validation loss: 0.001657\n",
      "tensor(0.0015)\n",
      "iteration 7934, train loss: 0.002411, validation loss: 0.001501\n",
      "tensor(0.0015)\n",
      "iteration 7935, train loss: 0.002344, validation loss: 0.001496\n",
      "tensor(0.0015)\n",
      "iteration 7936, train loss: 0.00242, validation loss: 0.001472\n",
      "tensor(0.0015)\n",
      "iteration 7937, train loss: 0.002339, validation loss: 0.001491\n",
      "tensor(0.0015)\n",
      "iteration 7938, train loss: 0.002354, validation loss: 0.001474\n",
      "tensor(0.0015)\n",
      "iteration 7939, train loss: 0.002315, validation loss: 0.00148\n",
      "tensor(0.0015)\n",
      "iteration 7940, train loss: 0.002344, validation loss: \u001b[92m0.001457\u001b[0m\n",
      "tensor(0.0015)\n",
      "iteration 7941, train loss: 0.002333, validation loss: 0.001466\n",
      "tensor(0.0015)\n",
      "iteration 7942, train loss: 0.002407, validation loss: 0.001506\n",
      "tensor(0.0015)\n",
      "iteration 7943, train loss: 0.00233, validation loss: 0.001493\n",
      "tensor(0.0015)\n",
      "iteration 7944, train loss: 0.002379, validation loss: 0.00147\n",
      "tensor(0.0015)\n",
      "iteration 7945, train loss: 0.002399, validation loss: 0.001472\n",
      "tensor(0.0015)\n",
      "iteration 7946, train loss: 0.002329, validation loss: 0.001475\n",
      "tensor(0.0015)\n",
      "iteration 7947, train loss: 0.002288, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7948, train loss: 0.002431, validation loss: 0.001486\n",
      "tensor(0.0015)\n",
      "iteration 7949, train loss: 0.002333, validation loss: 0.001467\n",
      "tensor(0.0016)\n",
      "iteration 7950, train loss: 0.00244, validation loss: 0.001568\n",
      "tensor(0.0015)\n",
      "iteration 7951, train loss: 0.002437, validation loss: 0.001537\n",
      "tensor(0.0015)\n",
      "iteration 7952, train loss: 0.002399, validation loss: 0.001522\n",
      "tensor(0.0015)\n",
      "iteration 7953, train loss: 0.002398, validation loss: 0.001498\n",
      "tensor(0.0016)\n",
      "iteration 7954, train loss: 0.002446, validation loss: 0.001592\n",
      "tensor(0.0015)\n",
      "iteration 7955, train loss: 0.002409, validation loss: 0.001475\n",
      "tensor(0.0016)\n",
      "iteration 7956, train loss: 0.002367, validation loss: 0.001561\n",
      "tensor(0.0015)\n",
      "iteration 7957, train loss: 0.002505, validation loss: 0.001478\n",
      "tensor(0.0015)\n",
      "iteration 7958, train loss: 0.002432, validation loss: 0.001505\n",
      "tensor(0.0015)\n",
      "iteration 7959, train loss: 0.002387, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7960, train loss: 0.002392, validation loss: 0.001495\n",
      "tensor(0.0015)\n",
      "iteration 7961, train loss: 0.002358, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7962, train loss: 0.002362, validation loss: 0.001486\n",
      "tensor(0.0015)\n",
      "iteration 7963, train loss: 0.002293, validation loss: 0.001462\n",
      "tensor(0.0015)\n",
      "iteration 7964, train loss: 0.002345, validation loss: 0.0015\n",
      "tensor(0.0015)\n",
      "iteration 7965, train loss: 0.002428, validation loss: 0.001485\n",
      "tensor(0.0015)\n",
      "iteration 7966, train loss: 0.002386, validation loss: 0.001475\n",
      "tensor(0.0016)\n",
      "iteration 7967, train loss: 0.002391, validation loss: 0.001564\n",
      "tensor(0.0015)\n",
      "iteration 7968, train loss: 0.00246, validation loss: 0.001521\n",
      "tensor(0.0016)\n",
      "iteration 7969, train loss: 0.002437, validation loss: 0.001557\n",
      "tensor(0.0015)\n",
      "iteration 7970, train loss: 0.002456, validation loss: 0.001507\n",
      "tensor(0.0016)\n",
      "iteration 7971, train loss: 0.002406, validation loss: 0.001596\n",
      "tensor(0.0015)\n",
      "iteration 7972, train loss: 0.002476, validation loss: 0.001544\n",
      "tensor(0.0015)\n",
      "iteration 7973, train loss: 0.002521, validation loss: 0.001471\n",
      "tensor(0.0016)\n",
      "iteration 7974, train loss: 0.002526, validation loss: 0.001554\n",
      "tensor(0.0016)\n",
      "iteration 7975, train loss: 0.002439, validation loss: 0.001554\n",
      "tensor(0.0016)\n",
      "iteration 7976, train loss: 0.002462, validation loss: 0.001551\n",
      "tensor(0.0016)\n",
      "iteration 7977, train loss: 0.002353, validation loss: 0.001592\n",
      "tensor(0.0015)\n",
      "iteration 7978, train loss: 0.002491, validation loss: 0.001507\n",
      "tensor(0.0015)\n",
      "iteration 7979, train loss: 0.002491, validation loss: 0.001515\n",
      "tensor(0.0015)\n",
      "iteration 7980, train loss: 0.00245, validation loss: 0.001484\n",
      "tensor(0.0015)\n",
      "iteration 7981, train loss: 0.002433, validation loss: 0.00152\n",
      "tensor(0.0016)\n",
      "iteration 7982, train loss: 0.002335, validation loss: 0.001563\n",
      "tensor(0.0015)\n",
      "iteration 7983, train loss: 0.002425, validation loss: 0.001516\n",
      "tensor(0.0015)\n",
      "iteration 7984, train loss: 0.002326, validation loss: 0.001528\n",
      "tensor(0.0015)\n",
      "iteration 7985, train loss: 0.002422, validation loss: 0.001488\n",
      "tensor(0.0015)\n",
      "iteration 7986, train loss: 0.002425, validation loss: 0.00151\n",
      "tensor(0.0015)\n",
      "iteration 7987, train loss: 0.002424, validation loss: 0.001506\n",
      "tensor(0.0015)\n",
      "iteration 7988, train loss: 0.002383, validation loss: 0.001475\n",
      "tensor(0.0015)\n",
      "iteration 7989, train loss: 0.002361, validation loss: 0.001523\n",
      "tensor(0.0016)\n",
      "iteration 7990, train loss: 0.002361, validation loss: 0.001578\n",
      "tensor(0.0016)\n",
      "iteration 7991, train loss: 0.002398, validation loss: 0.001556\n",
      "tensor(0.0015)\n",
      "iteration 7992, train loss: 0.002468, validation loss: 0.001532\n",
      "tensor(0.0015)\n",
      "iteration 7993, train loss: 0.002424, validation loss: 0.001464\n",
      "tensor(0.0015)\n",
      "iteration 7994, train loss: 0.002322, validation loss: 0.001504\n",
      "tensor(0.0015)\n",
      "iteration 7995, train loss: 0.002313, validation loss: 0.00153\n",
      "tensor(0.0015)\n",
      "iteration 7996, train loss: 0.002492, validation loss: 0.001487\n",
      "tensor(0.0015)\n",
      "iteration 7997, train loss: 0.002392, validation loss: 0.001524\n",
      "tensor(0.0016)\n",
      "iteration 7998, train loss: 0.002381, validation loss: 0.001553\n",
      "tensor(0.0016)\n",
      "iteration 7999, train loss: 0.002389, validation loss: 0.001628\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "rnn_trained, train_losses, val_losses, net_params = trainer.run_training(train_mask=mask, same_batch=same_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a4868d00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize loss\n",
    "\n",
    "fig_trainloss = plt.figure(figsize=(10, 3))\n",
    "plt.plot(train_losses, color='r', label='train loss (log scale)')\n",
    "plt.plot(val_losses, color='b', label='valid loss (log scale)')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"# epoch\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=np.zeros(N),\n",
    "                      y_init=net_params[\"y_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_valid, target_batch_valid = task.get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_timeseries=input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "output = RNN_valid.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize RNN output\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "print(n_r, n_l, n_c)\n",
    "ch_names = [\"Left\", \"Right\"]\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(output[i,:,:n_r], axis=1), c='tab:blue', label=\"right\")\n",
    "    plt.plot(np.average(output[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left\")\n",
    "    plt.plot(np.average(output[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch\")\n",
    "    plt.title(f\"Output Channle: {ch_names[i]}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_trained, train_losses, val_losses, net_params\n",
    "# save RNN data\n",
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "neural_traces = RNN_valid.get_history()\n",
    "traces_data = {}\n",
    "traces_data[\"inputs\"] = input_batch_valid\n",
    "traces_data[\"targets\"] = target_batch_valid\n",
    "traces_data[\"traces\"] = neural_traces\n",
    "traces_data[\"outputs\"] = RNN_valid.get_output()\n",
    "traces_data[\"net_params\"] = net_params\n",
    "\n",
    "path_to_traces = os.path.join(\"../data/trained_RNNs/ALM/\",\"test\",\"trained_RNN_updated_go.pkl\")\n",
    "pickle.dump(traces_data, open(path_to_traces, \"wb+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_coach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
