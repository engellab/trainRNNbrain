{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.DataSaver import DataSaver\n",
    "from src.DynamicSystemAnalyzer import *\n",
    "from src.PerformanceAnalyzer import *\n",
    "from src.RNN_numpy import RNN_numpy\n",
    "from src.utils import numpify\n",
    "from src.Trainer import Trainer\n",
    "from src.RNN_torch import RNN_torch\n",
    "from src.Task import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file last updated:  2024-02-12 17:06:39.008962\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"/Users/jiayizhang/Documents/code_base/rnn-coach/\"\n",
    "task_name = \"ALM\"\n",
    "activation = \"relu\"\n",
    "config_dict = json.load(open(os.path.join(dict_path, \"data\", \"configs\", f'train_config_{task_name}_{activation}_lambda_orth=0.json'), mode=\"r\"))\n",
    "print(\"config file last updated: \", config_dict[\"last_compiled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "n_steps = config_dict[\"n_steps\"]\n",
    "task_params = config_dict[\"task_params\"]\n",
    "input_size = config_dict[\"num_inputs\"]\n",
    "output_size = config_dict[\"num_outputs\"]\n",
    "\n",
    "# intialize task\n",
    "task = eval(\"Task\" + task_name)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "\n",
    "# Trainer:\n",
    "lambda_orth = config_dict[\"lambda_orth\"]\n",
    "orth_input_only = config_dict[\"orth_input_only\"]\n",
    "lambda_r = config_dict[\"lambda_r\"]\n",
    "max_iter = config_dict[\"max_iter\"]\n",
    "tol = config_dict[\"tol\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "same_batch = config_dict[\"same_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000 False\n"
     ]
    }
   ],
   "source": [
    "print(lambda_orth, max_iter, orth_input_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize inputs and targets\n",
    "inputs, targets = task.get_batch()\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "channel_names = [\"Left\", \"Right\", \"Go\"]\n",
    "\n",
    "for i in range(input_size):  # left, right, go\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(inputs[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Input Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(targets[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Output Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN config:\n",
    "N = config_dict[\"N\"]\n",
    "dt = config_dict[\"dt\"]\n",
    "tau = config_dict[\"tau\"]\n",
    "mask = np.array(config_dict[\"mask\"])\n",
    "\n",
    "if (activation == \"relu\"):\n",
    "    activation = lambda x: torch.maximum(x, torch.tensor(0))\n",
    "constrained = config_dict[\"constrained\"]\n",
    "sigma_inp = config_dict[\"sigma_inp\"]\n",
    "sigma_rec = config_dict[\"sigma_rec\"]\n",
    "connectivity_density_rec = config_dict[\"connectivity_density_rec\"]\n",
    "spectral_rad = config_dict[\"sr\"]\n",
    "seed = config_dict[\"seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 1 # train this many RNNs\n",
    "# save models here\n",
    "model_path = os.path.join(\"../data/trained_RNNs/ALM/\",\"batch_RNNs_0212\")\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BATCH TRAINING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for RNN!\n",
      "iteration 0, train loss: \u001b[92m0.97443\u001b[0m, validation loss: \u001b[92m0.072701\u001b[0m\n",
      "iteration 1, train loss: \u001b[92m0.087249\u001b[0m, validation loss: 0.080064\n",
      "iteration 2, train loss: \u001b[92m0.085573\u001b[0m, validation loss: 0.083601\n",
      "iteration 3, train loss: 0.088111, validation loss: 0.085664\n",
      "iteration 4, train loss: 0.090833, validation loss: 0.087102\n",
      "iteration 5, train loss: 0.090538, validation loss: 0.088183\n",
      "iteration 6, train loss: 0.091003, validation loss: 0.089022\n",
      "iteration 7, train loss: 0.093641, validation loss: 0.089665\n",
      "iteration 8, train loss: 0.092902, validation loss: 0.090158\n",
      "iteration 9, train loss: 0.094471, validation loss: 0.090529\n",
      "iteration 10, train loss: 0.092709, validation loss: 0.090809\n",
      "iteration 11, train loss: 0.094073, validation loss: 0.091013\n",
      "iteration 12, train loss: 0.094234, validation loss: 0.091162\n",
      "iteration 13, train loss: 0.093344, validation loss: 0.091259\n",
      "iteration 14, train loss: 0.094117, validation loss: 0.091311\n",
      "iteration 15, train loss: 0.095321, validation loss: 0.091321\n",
      "iteration 16, train loss: 0.094671, validation loss: 0.091289\n",
      "iteration 17, train loss: 0.093585, validation loss: 0.091222\n",
      "iteration 18, train loss: 0.093766, validation loss: 0.091121\n",
      "iteration 19, train loss: 0.094198, validation loss: 0.090987\n",
      "iteration 20, train loss: 0.093479, validation loss: 0.090819\n",
      "iteration 21, train loss: 0.093177, validation loss: 0.090612\n",
      "iteration 22, train loss: 0.095001, validation loss: 0.09037\n",
      "iteration 23, train loss: 0.093346, validation loss: 0.090096\n",
      "iteration 24, train loss: 0.094044, validation loss: 0.089784\n",
      "iteration 25, train loss: 0.091224, validation loss: 0.089435\n",
      "iteration 26, train loss: 0.092791, validation loss: 0.08905\n",
      "iteration 27, train loss: 0.091792, validation loss: 0.088623\n",
      "iteration 28, train loss: 0.092047, validation loss: 0.08815\n",
      "iteration 29, train loss: 0.091887, validation loss: 0.087628\n",
      "iteration 30, train loss: 0.089671, validation loss: 0.087047\n",
      "iteration 31, train loss: 0.091084, validation loss: 0.0864\n",
      "iteration 32, train loss: 0.092469, validation loss: 0.085687\n",
      "iteration 33, train loss: 0.090097, validation loss: 0.084904\n",
      "iteration 34, train loss: 0.089479, validation loss: 0.08404\n",
      "iteration 35, train loss: 0.087568, validation loss: 0.083091\n",
      "iteration 36, train loss: 0.08816, validation loss: 0.082042\n",
      "iteration 37, train loss: 0.088305, validation loss: 0.080862\n",
      "iteration 38, train loss: \u001b[92m0.085025\u001b[0m, validation loss: 0.079555\n",
      "iteration 39, train loss: 0.085805, validation loss: 0.078128\n",
      "iteration 40, train loss: \u001b[92m0.082609\u001b[0m, validation loss: 0.076573\n",
      "iteration 41, train loss: \u001b[92m0.0815\u001b[0m, validation loss: 0.074887\n",
      "iteration 42, train loss: \u001b[92m0.079522\u001b[0m, validation loss: 0.073053\n",
      "iteration 43, train loss: \u001b[92m0.077566\u001b[0m, validation loss: \u001b[92m0.071087\u001b[0m\n",
      "iteration 44, train loss: \u001b[92m0.074348\u001b[0m, validation loss: \u001b[92m0.069011\u001b[0m\n",
      "iteration 45, train loss: \u001b[92m0.073551\u001b[0m, validation loss: \u001b[92m0.066861\u001b[0m\n",
      "iteration 46, train loss: \u001b[92m0.072453\u001b[0m, validation loss: \u001b[92m0.064673\u001b[0m\n",
      "iteration 47, train loss: \u001b[92m0.071603\u001b[0m, validation loss: \u001b[92m0.062556\u001b[0m\n",
      "iteration 48, train loss: \u001b[92m0.066479\u001b[0m, validation loss: \u001b[92m0.060643\u001b[0m\n",
      "iteration 49, train loss: \u001b[92m0.064634\u001b[0m, validation loss: \u001b[92m0.059091\u001b[0m\n",
      "iteration 50, train loss: 0.065578, validation loss: \u001b[92m0.058094\u001b[0m\n",
      "iteration 51, train loss: \u001b[92m0.062923\u001b[0m, validation loss: \u001b[92m0.057841\u001b[0m\n",
      "iteration 52, train loss: 0.063936, validation loss: 0.058305\n",
      "iteration 53, train loss: 0.065667, validation loss: 0.059033\n",
      "iteration 54, train loss: 0.064675, validation loss: 0.059614\n",
      "iteration 55, train loss: 0.06645, validation loss: 0.05958\n",
      "iteration 56, train loss: 0.06577, validation loss: 0.059069\n",
      "iteration 57, train loss: 0.065463, validation loss: 0.058348\n",
      "iteration 58, train loss: 0.06316, validation loss: \u001b[92m0.057677\u001b[0m\n",
      "iteration 59, train loss: \u001b[92m0.062669\u001b[0m, validation loss: \u001b[92m0.057213\u001b[0m\n",
      "iteration 60, train loss: 0.0644, validation loss: \u001b[92m0.056948\u001b[0m\n",
      "iteration 61, train loss: \u001b[92m0.061494\u001b[0m, validation loss: \u001b[92m0.056856\u001b[0m\n",
      "iteration 62, train loss: 0.06257, validation loss: 0.056887\n",
      "iteration 63, train loss: 0.061651, validation loss: 0.056984\n",
      "iteration 64, train loss: 0.063567, validation loss: 0.057086\n",
      "iteration 65, train loss: 0.062771, validation loss: 0.057156\n",
      "iteration 66, train loss: 0.063375, validation loss: 0.057178\n",
      "iteration 67, train loss: \u001b[92m0.061002\u001b[0m, validation loss: 0.05716\n",
      "iteration 68, train loss: 0.062992, validation loss: 0.057092\n",
      "iteration 69, train loss: 0.06242, validation loss: 0.056986\n",
      "iteration 70, train loss: \u001b[92m0.060802\u001b[0m, validation loss: \u001b[92m0.056852\u001b[0m\n",
      "iteration 71, train loss: 0.063106, validation loss: \u001b[92m0.056694\u001b[0m\n",
      "iteration 72, train loss: 0.062032, validation loss: \u001b[92m0.056523\u001b[0m\n",
      "iteration 73, train loss: \u001b[92m0.060456\u001b[0m, validation loss: \u001b[92m0.056372\u001b[0m\n",
      "iteration 74, train loss: 0.063436, validation loss: \u001b[92m0.056239\u001b[0m\n",
      "iteration 75, train loss: 0.061255, validation loss: \u001b[92m0.056146\u001b[0m\n",
      "iteration 76, train loss: \u001b[92m0.060282\u001b[0m, validation loss: \u001b[92m0.056099\u001b[0m\n",
      "iteration 77, train loss: 0.061369, validation loss: \u001b[92m0.056095\u001b[0m\n",
      "iteration 78, train loss: 0.061331, validation loss: 0.056122\n",
      "iteration 79, train loss: 0.062258, validation loss: 0.056155\n",
      "iteration 80, train loss: 0.060475, validation loss: 0.056187\n",
      "iteration 81, train loss: \u001b[92m0.060094\u001b[0m, validation loss: 0.056186\n",
      "iteration 82, train loss: 0.060595, validation loss: 0.056153\n",
      "iteration 83, train loss: 0.061093, validation loss: \u001b[92m0.056064\u001b[0m\n",
      "iteration 84, train loss: 0.060849, validation loss: \u001b[92m0.055956\u001b[0m\n",
      "iteration 85, train loss: 0.061655, validation loss: \u001b[92m0.055847\u001b[0m\n",
      "iteration 86, train loss: 0.061653, validation loss: \u001b[92m0.055761\u001b[0m\n",
      "iteration 87, train loss: 0.060464, validation loss: \u001b[92m0.055692\u001b[0m\n",
      "iteration 88, train loss: \u001b[92m0.059913\u001b[0m, validation loss: \u001b[92m0.055645\u001b[0m\n",
      "iteration 89, train loss: \u001b[92m0.05977\u001b[0m, validation loss: \u001b[92m0.05561\u001b[0m\n",
      "iteration 90, train loss: 0.060338, validation loss: \u001b[92m0.055583\u001b[0m\n",
      "iteration 91, train loss: 0.061226, validation loss: \u001b[92m0.055561\u001b[0m\n",
      "iteration 92, train loss: \u001b[92m0.059416\u001b[0m, validation loss: \u001b[92m0.055543\u001b[0m\n",
      "iteration 93, train loss: 0.060318, validation loss: \u001b[92m0.055528\u001b[0m\n",
      "iteration 94, train loss: 0.059955, validation loss: \u001b[92m0.055509\u001b[0m\n",
      "iteration 95, train loss: 0.060059, validation loss: \u001b[92m0.055485\u001b[0m\n",
      "iteration 96, train loss: 0.060848, validation loss: \u001b[92m0.055449\u001b[0m\n",
      "iteration 97, train loss: 0.061521, validation loss: \u001b[92m0.055412\u001b[0m\n",
      "iteration 98, train loss: 0.059465, validation loss: \u001b[92m0.055371\u001b[0m\n",
      "iteration 99, train loss: 0.06041, validation loss: \u001b[92m0.055331\u001b[0m\n",
      "iteration 100, train loss: 0.060679, validation loss: \u001b[92m0.055289\u001b[0m\n",
      "iteration 101, train loss: 0.059664, validation loss: \u001b[92m0.055248\u001b[0m\n",
      "iteration 102, train loss: 0.05993, validation loss: \u001b[92m0.055211\u001b[0m\n",
      "iteration 103, train loss: \u001b[92m0.058911\u001b[0m, validation loss: \u001b[92m0.055177\u001b[0m\n",
      "iteration 104, train loss: \u001b[92m0.057738\u001b[0m, validation loss: \u001b[92m0.055147\u001b[0m\n",
      "iteration 105, train loss: 0.0601, validation loss: \u001b[92m0.055119\u001b[0m\n",
      "iteration 106, train loss: 0.060086, validation loss: \u001b[92m0.055092\u001b[0m\n",
      "iteration 107, train loss: 0.0602, validation loss: \u001b[92m0.055065\u001b[0m\n",
      "iteration 108, train loss: 0.059999, validation loss: \u001b[92m0.055038\u001b[0m\n",
      "iteration 109, train loss: 0.060017, validation loss: \u001b[92m0.055012\u001b[0m\n",
      "iteration 110, train loss: 0.060795, validation loss: \u001b[92m0.054985\u001b[0m\n",
      "iteration 111, train loss: 0.059883, validation loss: \u001b[92m0.054959\u001b[0m\n",
      "iteration 112, train loss: 0.06005, validation loss: \u001b[92m0.054933\u001b[0m\n",
      "iteration 113, train loss: 0.060535, validation loss: \u001b[92m0.054908\u001b[0m\n",
      "iteration 114, train loss: 0.059021, validation loss: \u001b[92m0.054885\u001b[0m\n",
      "iteration 115, train loss: 0.058655, validation loss: \u001b[92m0.054863\u001b[0m\n",
      "iteration 116, train loss: 0.059047, validation loss: \u001b[92m0.054842\u001b[0m\n",
      "iteration 117, train loss: 0.058823, validation loss: \u001b[92m0.054825\u001b[0m\n",
      "iteration 118, train loss: 0.058326, validation loss: \u001b[92m0.054807\u001b[0m\n",
      "iteration 119, train loss: 0.060038, validation loss: \u001b[92m0.054789\u001b[0m\n",
      "iteration 120, train loss: 0.060769, validation loss: \u001b[92m0.05477\u001b[0m\n",
      "iteration 121, train loss: 0.059977, validation loss: \u001b[92m0.054747\u001b[0m\n",
      "iteration 122, train loss: 0.059042, validation loss: \u001b[92m0.054721\u001b[0m\n",
      "iteration 123, train loss: 0.059156, validation loss: \u001b[92m0.054694\u001b[0m\n",
      "iteration 124, train loss: 0.059028, validation loss: \u001b[92m0.054667\u001b[0m\n",
      "iteration 125, train loss: 0.059603, validation loss: \u001b[92m0.054637\u001b[0m\n",
      "iteration 126, train loss: 0.058981, validation loss: \u001b[92m0.054608\u001b[0m\n",
      "iteration 127, train loss: 0.058348, validation loss: \u001b[92m0.054584\u001b[0m\n",
      "iteration 128, train loss: 0.05978, validation loss: \u001b[92m0.054561\u001b[0m\n",
      "iteration 129, train loss: 0.058815, validation loss: \u001b[92m0.054541\u001b[0m\n",
      "iteration 130, train loss: 0.060184, validation loss: \u001b[92m0.054523\u001b[0m\n",
      "iteration 131, train loss: 0.059537, validation loss: \u001b[92m0.054505\u001b[0m\n",
      "iteration 132, train loss: 0.058246, validation loss: \u001b[92m0.054487\u001b[0m\n",
      "iteration 133, train loss: 0.058813, validation loss: \u001b[92m0.054469\u001b[0m\n",
      "iteration 134, train loss: 0.058364, validation loss: \u001b[92m0.054448\u001b[0m\n",
      "iteration 135, train loss: 0.058452, validation loss: \u001b[92m0.054428\u001b[0m\n",
      "iteration 136, train loss: 0.058706, validation loss: \u001b[92m0.054408\u001b[0m\n",
      "iteration 137, train loss: 0.058132, validation loss: \u001b[92m0.054392\u001b[0m\n",
      "iteration 138, train loss: 0.060022, validation loss: \u001b[92m0.054376\u001b[0m\n",
      "iteration 139, train loss: \u001b[92m0.05757\u001b[0m, validation loss: \u001b[92m0.054361\u001b[0m\n",
      "iteration 140, train loss: 0.058558, validation loss: \u001b[92m0.054345\u001b[0m\n",
      "iteration 141, train loss: 0.058575, validation loss: \u001b[92m0.054331\u001b[0m\n",
      "iteration 142, train loss: 0.057812, validation loss: \u001b[92m0.05432\u001b[0m\n",
      "iteration 143, train loss: 0.058703, validation loss: \u001b[92m0.054307\u001b[0m\n",
      "iteration 144, train loss: 0.057915, validation loss: \u001b[92m0.054294\u001b[0m\n",
      "iteration 145, train loss: 0.058409, validation loss: \u001b[92m0.054281\u001b[0m\n",
      "iteration 146, train loss: 0.058851, validation loss: \u001b[92m0.054264\u001b[0m\n",
      "iteration 147, train loss: 0.058566, validation loss: \u001b[92m0.054245\u001b[0m\n",
      "iteration 148, train loss: 0.058477, validation loss: \u001b[92m0.054225\u001b[0m\n",
      "iteration 149, train loss: 0.058474, validation loss: \u001b[92m0.054207\u001b[0m\n",
      "iteration 150, train loss: 0.05801, validation loss: \u001b[92m0.054189\u001b[0m\n",
      "iteration 151, train loss: 0.057598, validation loss: \u001b[92m0.054172\u001b[0m\n",
      "iteration 152, train loss: 0.058037, validation loss: \u001b[92m0.054156\u001b[0m\n",
      "iteration 153, train loss: 0.058855, validation loss: \u001b[92m0.054141\u001b[0m\n",
      "iteration 154, train loss: 0.057757, validation loss: \u001b[92m0.05413\u001b[0m\n",
      "iteration 155, train loss: 0.059217, validation loss: \u001b[92m0.054113\u001b[0m\n",
      "iteration 156, train loss: 0.058749, validation loss: \u001b[92m0.054098\u001b[0m\n",
      "iteration 157, train loss: 0.057972, validation loss: \u001b[92m0.05408\u001b[0m\n",
      "iteration 158, train loss: 0.059733, validation loss: \u001b[92m0.054059\u001b[0m\n",
      "iteration 159, train loss: 0.058352, validation loss: \u001b[92m0.054039\u001b[0m\n",
      "iteration 160, train loss: 0.057661, validation loss: \u001b[92m0.05402\u001b[0m\n",
      "iteration 161, train loss: 0.057985, validation loss: \u001b[92m0.054002\u001b[0m\n",
      "iteration 162, train loss: 0.059082, validation loss: \u001b[92m0.053985\u001b[0m\n",
      "iteration 163, train loss: 0.057705, validation loss: \u001b[92m0.053972\u001b[0m\n",
      "iteration 164, train loss: 0.058177, validation loss: \u001b[92m0.053961\u001b[0m\n",
      "iteration 165, train loss: 0.058069, validation loss: \u001b[92m0.053948\u001b[0m\n",
      "iteration 166, train loss: \u001b[92m0.057213\u001b[0m, validation loss: \u001b[92m0.053936\u001b[0m\n",
      "iteration 167, train loss: 0.05759, validation loss: \u001b[92m0.053923\u001b[0m\n",
      "iteration 168, train loss: 0.058668, validation loss: \u001b[92m0.053915\u001b[0m\n",
      "iteration 169, train loss: 0.058448, validation loss: \u001b[92m0.053908\u001b[0m\n",
      "iteration 170, train loss: 0.058521, validation loss: \u001b[92m0.053897\u001b[0m\n",
      "iteration 171, train loss: 0.058555, validation loss: \u001b[92m0.053886\u001b[0m\n",
      "iteration 172, train loss: 0.057417, validation loss: \u001b[92m0.053874\u001b[0m\n",
      "iteration 173, train loss: 0.058329, validation loss: \u001b[92m0.053852\u001b[0m\n",
      "iteration 174, train loss: \u001b[92m0.056902\u001b[0m, validation loss: \u001b[92m0.053827\u001b[0m\n",
      "iteration 175, train loss: 0.058272, validation loss: \u001b[92m0.053809\u001b[0m\n",
      "iteration 176, train loss: 0.057605, validation loss: \u001b[92m0.053791\u001b[0m\n",
      "iteration 177, train loss: 0.05816, validation loss: \u001b[92m0.053772\u001b[0m\n",
      "iteration 178, train loss: 0.057191, validation loss: \u001b[92m0.053755\u001b[0m\n",
      "iteration 179, train loss: 0.057337, validation loss: \u001b[92m0.053738\u001b[0m\n",
      "iteration 180, train loss: 0.057142, validation loss: \u001b[92m0.053724\u001b[0m\n",
      "iteration 181, train loss: 0.059062, validation loss: \u001b[92m0.053715\u001b[0m\n",
      "iteration 182, train loss: 0.058051, validation loss: \u001b[92m0.053704\u001b[0m\n",
      "iteration 183, train loss: 0.058338, validation loss: \u001b[92m0.053694\u001b[0m\n",
      "iteration 184, train loss: 0.057558, validation loss: \u001b[92m0.053686\u001b[0m\n",
      "iteration 185, train loss: 0.058096, validation loss: \u001b[92m0.053677\u001b[0m\n",
      "iteration 186, train loss: 0.058524, validation loss: \u001b[92m0.053666\u001b[0m\n",
      "iteration 187, train loss: 0.057753, validation loss: \u001b[92m0.053655\u001b[0m\n",
      "iteration 188, train loss: \u001b[92m0.056883\u001b[0m, validation loss: \u001b[92m0.053645\u001b[0m\n",
      "iteration 189, train loss: 0.057726, validation loss: \u001b[92m0.053639\u001b[0m\n",
      "iteration 190, train loss: 0.058114, validation loss: \u001b[92m0.053632\u001b[0m\n",
      "iteration 191, train loss: 0.058323, validation loss: \u001b[92m0.053625\u001b[0m\n",
      "iteration 192, train loss: \u001b[92m0.056651\u001b[0m, validation loss: \u001b[92m0.053616\u001b[0m\n",
      "iteration 193, train loss: \u001b[92m0.056634\u001b[0m, validation loss: \u001b[92m0.05361\u001b[0m\n",
      "iteration 194, train loss: 0.05772, validation loss: \u001b[92m0.053607\u001b[0m\n",
      "iteration 195, train loss: 0.056831, validation loss: \u001b[92m0.053604\u001b[0m\n",
      "iteration 196, train loss: 0.057045, validation loss: \u001b[92m0.0536\u001b[0m\n",
      "iteration 197, train loss: 0.057619, validation loss: 0.053601\n",
      "iteration 198, train loss: 0.057209, validation loss: 0.0536\n",
      "iteration 199, train loss: 0.057326, validation loss: 0.0536\n",
      "iteration 200, train loss: 0.057173, validation loss: \u001b[92m0.053598\u001b[0m\n",
      "iteration 201, train loss: 0.057611, validation loss: \u001b[92m0.053594\u001b[0m\n",
      "iteration 202, train loss: 0.057158, validation loss: \u001b[92m0.053583\u001b[0m\n",
      "iteration 203, train loss: 0.057857, validation loss: \u001b[92m0.053569\u001b[0m\n",
      "iteration 204, train loss: 0.057275, validation loss: \u001b[92m0.053553\u001b[0m\n",
      "iteration 205, train loss: 0.057194, validation loss: \u001b[92m0.053538\u001b[0m\n",
      "iteration 206, train loss: 0.057078, validation loss: \u001b[92m0.053525\u001b[0m\n",
      "iteration 207, train loss: 0.057479, validation loss: \u001b[92m0.053516\u001b[0m\n",
      "iteration 208, train loss: 0.057549, validation loss: \u001b[92m0.053504\u001b[0m\n",
      "iteration 209, train loss: \u001b[92m0.056614\u001b[0m, validation loss: \u001b[92m0.053493\u001b[0m\n",
      "iteration 210, train loss: 0.05784, validation loss: \u001b[92m0.053484\u001b[0m\n",
      "iteration 211, train loss: \u001b[92m0.056129\u001b[0m, validation loss: \u001b[92m0.053472\u001b[0m\n",
      "iteration 212, train loss: 0.057413, validation loss: \u001b[92m0.053458\u001b[0m\n",
      "iteration 213, train loss: 0.056407, validation loss: \u001b[92m0.053444\u001b[0m\n",
      "iteration 214, train loss: 0.057151, validation loss: \u001b[92m0.053433\u001b[0m\n",
      "iteration 215, train loss: 0.057221, validation loss: \u001b[92m0.053418\u001b[0m\n",
      "iteration 216, train loss: 0.057785, validation loss: \u001b[92m0.053407\u001b[0m\n",
      "iteration 217, train loss: 0.057066, validation loss: \u001b[92m0.053397\u001b[0m\n",
      "iteration 218, train loss: \u001b[92m0.055953\u001b[0m, validation loss: \u001b[92m0.053389\u001b[0m\n",
      "iteration 219, train loss: 0.057062, validation loss: \u001b[92m0.053383\u001b[0m\n",
      "iteration 220, train loss: 0.056093, validation loss: \u001b[92m0.053377\u001b[0m\n",
      "iteration 221, train loss: 0.056717, validation loss: \u001b[92m0.053372\u001b[0m\n",
      "iteration 222, train loss: 0.057597, validation loss: 0.053374\n",
      "iteration 223, train loss: 0.056324, validation loss: 0.053378\n",
      "iteration 224, train loss: 0.057582, validation loss: 0.053387\n",
      "iteration 225, train loss: 0.057789, validation loss: 0.053395\n",
      "iteration 226, train loss: 0.058369, validation loss: 0.053409\n",
      "iteration 227, train loss: 0.057479, validation loss: 0.053426\n",
      "iteration 228, train loss: 0.057153, validation loss: 0.053441\n",
      "iteration 229, train loss: 0.05777, validation loss: 0.053448\n",
      "iteration 230, train loss: 0.056968, validation loss: 0.053451\n",
      "iteration 231, train loss: 0.057106, validation loss: 0.05344\n",
      "iteration 232, train loss: \u001b[92m0.0558\u001b[0m, validation loss: 0.053422\n",
      "iteration 233, train loss: 0.055941, validation loss: 0.053402\n",
      "iteration 234, train loss: 0.056931, validation loss: 0.053383\n",
      "iteration 235, train loss: 0.056198, validation loss: \u001b[92m0.053365\u001b[0m\n",
      "iteration 236, train loss: 0.056908, validation loss: \u001b[92m0.053349\u001b[0m\n",
      "iteration 237, train loss: 0.056956, validation loss: \u001b[92m0.053338\u001b[0m\n",
      "iteration 238, train loss: \u001b[92m0.055718\u001b[0m, validation loss: \u001b[92m0.053324\u001b[0m\n",
      "iteration 239, train loss: 0.056984, validation loss: \u001b[92m0.053315\u001b[0m\n",
      "iteration 240, train loss: 0.056841, validation loss: \u001b[92m0.053306\u001b[0m\n",
      "iteration 241, train loss: 0.057235, validation loss: \u001b[92m0.053298\u001b[0m\n",
      "iteration 242, train loss: 0.056782, validation loss: \u001b[92m0.05329\u001b[0m\n",
      "iteration 243, train loss: 0.056863, validation loss: \u001b[92m0.053285\u001b[0m\n",
      "iteration 244, train loss: 0.056921, validation loss: \u001b[92m0.053281\u001b[0m\n",
      "iteration 245, train loss: 0.055853, validation loss: \u001b[92m0.053277\u001b[0m\n",
      "iteration 246, train loss: 0.05671, validation loss: \u001b[92m0.053274\u001b[0m\n",
      "iteration 247, train loss: 0.056525, validation loss: \u001b[92m0.053271\u001b[0m\n",
      "iteration 248, train loss: 0.0559, validation loss: \u001b[92m0.053268\u001b[0m\n",
      "iteration 249, train loss: \u001b[92m0.05566\u001b[0m, validation loss: \u001b[92m0.053268\u001b[0m\n",
      "iteration 250, train loss: 0.05671, validation loss: \u001b[92m0.053267\u001b[0m\n",
      "iteration 251, train loss: 0.056618, validation loss: \u001b[92m0.053265\u001b[0m\n",
      "iteration 252, train loss: 0.056833, validation loss: \u001b[92m0.05326\u001b[0m\n",
      "iteration 253, train loss: 0.057119, validation loss: \u001b[92m0.053254\u001b[0m\n",
      "iteration 254, train loss: 0.056662, validation loss: \u001b[92m0.053247\u001b[0m\n",
      "iteration 255, train loss: 0.057975, validation loss: \u001b[92m0.053241\u001b[0m\n",
      "iteration 256, train loss: 0.057052, validation loss: \u001b[92m0.053235\u001b[0m\n",
      "iteration 257, train loss: 0.056072, validation loss: \u001b[92m0.053231\u001b[0m\n",
      "iteration 258, train loss: 0.056189, validation loss: 0.053232\n",
      "iteration 259, train loss: 0.056454, validation loss: \u001b[92m0.053231\u001b[0m\n",
      "iteration 260, train loss: 0.056518, validation loss: \u001b[92m0.053229\u001b[0m\n",
      "iteration 261, train loss: 0.056658, validation loss: 0.053231\n",
      "iteration 262, train loss: 0.056309, validation loss: 0.053237\n",
      "iteration 263, train loss: 0.057042, validation loss: 0.05324\n",
      "iteration 264, train loss: 0.056637, validation loss: 0.053239\n",
      "iteration 265, train loss: 0.056514, validation loss: 0.05324\n",
      "iteration 266, train loss: 0.056376, validation loss: 0.053247\n",
      "iteration 267, train loss: 0.056588, validation loss: 0.053245\n",
      "iteration 268, train loss: \u001b[92m0.055558\u001b[0m, validation loss: 0.053244\n",
      "iteration 269, train loss: 0.057006, validation loss: 0.053248\n",
      "iteration 270, train loss: 0.05648, validation loss: 0.05325\n",
      "iteration 271, train loss: 0.056264, validation loss: 0.053252\n",
      "iteration 272, train loss: 0.056408, validation loss: 0.053251\n",
      "iteration 273, train loss: 0.056405, validation loss: 0.053247\n",
      "iteration 274, train loss: 0.057117, validation loss: 0.053233\n",
      "iteration 275, train loss: 0.055823, validation loss: \u001b[92m0.05322\u001b[0m\n",
      "iteration 276, train loss: 0.056222, validation loss: \u001b[92m0.053207\u001b[0m\n",
      "iteration 277, train loss: 0.057079, validation loss: \u001b[92m0.053194\u001b[0m\n",
      "iteration 278, train loss: 0.056385, validation loss: \u001b[92m0.053181\u001b[0m\n",
      "iteration 279, train loss: 0.0559, validation loss: \u001b[92m0.053172\u001b[0m\n",
      "iteration 280, train loss: 0.057202, validation loss: \u001b[92m0.053165\u001b[0m\n",
      "iteration 281, train loss: 0.057224, validation loss: \u001b[92m0.053158\u001b[0m\n",
      "iteration 282, train loss: 0.057504, validation loss: \u001b[92m0.053152\u001b[0m\n",
      "iteration 283, train loss: 0.055954, validation loss: \u001b[92m0.053149\u001b[0m\n",
      "iteration 284, train loss: 0.056966, validation loss: \u001b[92m0.053148\u001b[0m\n",
      "iteration 285, train loss: 0.056428, validation loss: \u001b[92m0.053147\u001b[0m\n",
      "iteration 286, train loss: 0.055612, validation loss: \u001b[92m0.053144\u001b[0m\n",
      "iteration 287, train loss: 0.055901, validation loss: \u001b[92m0.053141\u001b[0m\n",
      "iteration 288, train loss: 0.056794, validation loss: \u001b[92m0.053135\u001b[0m\n",
      "iteration 289, train loss: 0.056258, validation loss: \u001b[92m0.053134\u001b[0m\n",
      "iteration 290, train loss: 0.056464, validation loss: \u001b[92m0.053131\u001b[0m\n",
      "iteration 291, train loss: 0.05676, validation loss: \u001b[92m0.05313\u001b[0m\n",
      "iteration 292, train loss: 0.056718, validation loss: 0.053132\n",
      "iteration 293, train loss: 0.055583, validation loss: \u001b[92m0.05313\u001b[0m\n",
      "iteration 294, train loss: \u001b[92m0.055419\u001b[0m, validation loss: \u001b[92m0.053126\u001b[0m\n",
      "iteration 295, train loss: 0.056164, validation loss: \u001b[92m0.053119\u001b[0m\n",
      "iteration 296, train loss: 0.05695, validation loss: \u001b[92m0.053113\u001b[0m\n",
      "iteration 297, train loss: 0.056378, validation loss: \u001b[92m0.053109\u001b[0m\n",
      "iteration 298, train loss: 0.055445, validation loss: \u001b[92m0.053101\u001b[0m\n",
      "iteration 299, train loss: 0.056295, validation loss: \u001b[92m0.053094\u001b[0m\n",
      "iteration 300, train loss: 0.056233, validation loss: \u001b[92m0.053092\u001b[0m\n",
      "iteration 301, train loss: 0.056137, validation loss: 0.053092\n",
      "iteration 302, train loss: 0.055767, validation loss: \u001b[92m0.053091\u001b[0m\n",
      "iteration 303, train loss: 0.056444, validation loss: \u001b[92m0.05309\u001b[0m\n",
      "iteration 304, train loss: 0.055438, validation loss: \u001b[92m0.053088\u001b[0m\n",
      "iteration 305, train loss: 0.056073, validation loss: \u001b[92m0.053087\u001b[0m\n",
      "iteration 306, train loss: 0.056181, validation loss: \u001b[92m0.053086\u001b[0m\n",
      "iteration 307, train loss: 0.056398, validation loss: \u001b[92m0.053083\u001b[0m\n",
      "iteration 308, train loss: 0.055963, validation loss: \u001b[92m0.053082\u001b[0m\n",
      "iteration 309, train loss: \u001b[92m0.0554\u001b[0m, validation loss: \u001b[92m0.053081\u001b[0m\n",
      "iteration 310, train loss: 0.056307, validation loss: \u001b[92m0.05308\u001b[0m\n",
      "iteration 311, train loss: 0.056206, validation loss: \u001b[92m0.053077\u001b[0m\n",
      "iteration 312, train loss: 0.056905, validation loss: \u001b[92m0.053073\u001b[0m\n",
      "iteration 313, train loss: 0.056303, validation loss: 0.053073\n",
      "iteration 314, train loss: 0.055456, validation loss: 0.053075\n",
      "iteration 315, train loss: \u001b[92m0.055291\u001b[0m, validation loss: 0.053075\n",
      "iteration 316, train loss: \u001b[92m0.054871\u001b[0m, validation loss: 0.053076\n",
      "iteration 317, train loss: 0.055686, validation loss: 0.05308\n",
      "iteration 318, train loss: 0.055751, validation loss: 0.053082\n",
      "iteration 319, train loss: 0.055427, validation loss: 0.053086\n",
      "iteration 320, train loss: 0.056342, validation loss: 0.053094\n",
      "iteration 321, train loss: 0.056013, validation loss: 0.053108\n",
      "iteration 322, train loss: 0.056082, validation loss: 0.053119\n",
      "iteration 323, train loss: 0.055253, validation loss: 0.053126\n",
      "iteration 324, train loss: 0.05575, validation loss: 0.053125\n",
      "iteration 325, train loss: 0.056072, validation loss: 0.053111\n",
      "iteration 326, train loss: 0.055597, validation loss: 0.053092\n",
      "iteration 327, train loss: 0.056199, validation loss: \u001b[92m0.053071\u001b[0m\n",
      "iteration 328, train loss: 0.055941, validation loss: \u001b[92m0.053058\u001b[0m\n",
      "iteration 329, train loss: 0.055926, validation loss: \u001b[92m0.053049\u001b[0m\n",
      "iteration 330, train loss: 0.055756, validation loss: \u001b[92m0.053042\u001b[0m\n",
      "iteration 331, train loss: 0.056396, validation loss: \u001b[92m0.053037\u001b[0m\n",
      "iteration 332, train loss: 0.055699, validation loss: \u001b[92m0.053033\u001b[0m\n",
      "iteration 333, train loss: 0.055827, validation loss: \u001b[92m0.053029\u001b[0m\n",
      "iteration 334, train loss: 0.055945, validation loss: \u001b[92m0.053026\u001b[0m\n",
      "iteration 335, train loss: 0.055887, validation loss: \u001b[92m0.053025\u001b[0m\n",
      "iteration 336, train loss: 0.055104, validation loss: \u001b[92m0.053024\u001b[0m\n",
      "iteration 337, train loss: 0.055565, validation loss: \u001b[92m0.053021\u001b[0m\n",
      "iteration 338, train loss: 0.05604, validation loss: \u001b[92m0.05302\u001b[0m\n",
      "iteration 339, train loss: 0.055481, validation loss: \u001b[92m0.053017\u001b[0m\n",
      "iteration 340, train loss: 0.055383, validation loss: \u001b[92m0.053012\u001b[0m\n",
      "iteration 341, train loss: 0.055908, validation loss: \u001b[92m0.053009\u001b[0m\n",
      "iteration 342, train loss: 0.055842, validation loss: \u001b[92m0.053009\u001b[0m\n",
      "iteration 343, train loss: 0.055481, validation loss: 0.05301\n",
      "iteration 344, train loss: 0.055658, validation loss: 0.053012\n",
      "iteration 345, train loss: 0.055345, validation loss: 0.053015\n",
      "iteration 346, train loss: 0.055877, validation loss: 0.053021\n",
      "iteration 347, train loss: 0.055683, validation loss: 0.053025\n",
      "iteration 348, train loss: 0.055596, validation loss: 0.053028\n",
      "iteration 349, train loss: 0.056476, validation loss: 0.05303\n",
      "iteration 350, train loss: 0.056181, validation loss: 0.053031\n",
      "iteration 351, train loss: 0.05505, validation loss: 0.053029\n",
      "iteration 352, train loss: 0.055854, validation loss: 0.053025\n",
      "iteration 353, train loss: 0.0566, validation loss: 0.053022\n",
      "iteration 354, train loss: 0.055451, validation loss: 0.053022\n",
      "iteration 355, train loss: 0.055571, validation loss: 0.053019\n",
      "iteration 356, train loss: 0.056029, validation loss: 0.053015\n",
      "iteration 357, train loss: 0.05573, validation loss: \u001b[92m0.053005\u001b[0m\n",
      "iteration 358, train loss: 0.055079, validation loss: \u001b[92m0.052997\u001b[0m\n",
      "iteration 359, train loss: 0.0558, validation loss: \u001b[92m0.052988\u001b[0m\n",
      "iteration 360, train loss: 0.056135, validation loss: \u001b[92m0.05298\u001b[0m\n",
      "iteration 361, train loss: 0.055846, validation loss: \u001b[92m0.052973\u001b[0m\n",
      "iteration 362, train loss: 0.055362, validation loss: \u001b[92m0.052969\u001b[0m\n",
      "iteration 363, train loss: 0.055473, validation loss: \u001b[92m0.052965\u001b[0m\n",
      "iteration 364, train loss: 0.05569, validation loss: \u001b[92m0.052962\u001b[0m\n",
      "iteration 365, train loss: 0.055753, validation loss: \u001b[92m0.05296\u001b[0m\n",
      "iteration 366, train loss: 0.056161, validation loss: 0.05296\n",
      "iteration 367, train loss: 0.055063, validation loss: 0.052962\n",
      "iteration 368, train loss: 0.055413, validation loss: 0.052965\n",
      "iteration 369, train loss: 0.05546, validation loss: 0.052968\n",
      "iteration 370, train loss: 0.055547, validation loss: 0.052969\n",
      "iteration 371, train loss: 0.055033, validation loss: 0.05297\n",
      "iteration 372, train loss: 0.055478, validation loss: 0.05297\n",
      "iteration 373, train loss: \u001b[92m0.054761\u001b[0m, validation loss: 0.052969\n",
      "iteration 374, train loss: 0.056073, validation loss: 0.052966\n",
      "iteration 375, train loss: 0.05594, validation loss: 0.052962\n",
      "iteration 376, train loss: 0.055765, validation loss: \u001b[92m0.052959\u001b[0m\n",
      "iteration 377, train loss: 0.05556, validation loss: \u001b[92m0.052952\u001b[0m\n",
      "iteration 378, train loss: 0.055622, validation loss: \u001b[92m0.052944\u001b[0m\n",
      "iteration 379, train loss: 0.055424, validation loss: \u001b[92m0.05294\u001b[0m\n",
      "iteration 380, train loss: 0.056053, validation loss: \u001b[92m0.052938\u001b[0m\n",
      "iteration 381, train loss: 0.056314, validation loss: \u001b[92m0.052937\u001b[0m\n",
      "iteration 382, train loss: 0.055616, validation loss: 0.052939\n",
      "iteration 383, train loss: 0.055602, validation loss: \u001b[92m0.052934\u001b[0m\n",
      "iteration 384, train loss: 0.055531, validation loss: \u001b[92m0.05293\u001b[0m\n",
      "iteration 385, train loss: 0.055863, validation loss: \u001b[92m0.052927\u001b[0m\n",
      "iteration 386, train loss: 0.055681, validation loss: \u001b[92m0.052927\u001b[0m\n",
      "iteration 387, train loss: 0.055571, validation loss: 0.052928\n",
      "iteration 388, train loss: 0.055057, validation loss: 0.05293\n",
      "iteration 389, train loss: 0.055367, validation loss: 0.05293\n",
      "iteration 390, train loss: 0.055115, validation loss: 0.05293\n",
      "iteration 391, train loss: 0.055721, validation loss: 0.05293\n",
      "iteration 392, train loss: 0.055312, validation loss: 0.052928\n",
      "iteration 393, train loss: 0.055486, validation loss: \u001b[92m0.052927\u001b[0m\n",
      "iteration 394, train loss: 0.055966, validation loss: 0.052927\n",
      "iteration 395, train loss: 0.056039, validation loss: \u001b[92m0.052924\u001b[0m\n",
      "iteration 396, train loss: 0.055361, validation loss: \u001b[92m0.052921\u001b[0m\n",
      "iteration 397, train loss: 0.05563, validation loss: \u001b[92m0.05292\u001b[0m\n",
      "iteration 398, train loss: 0.055021, validation loss: \u001b[92m0.052916\u001b[0m\n",
      "iteration 399, train loss: 0.055598, validation loss: \u001b[92m0.052913\u001b[0m\n",
      "iteration 400, train loss: 0.05631, validation loss: \u001b[92m0.052911\u001b[0m\n",
      "iteration 401, train loss: 0.055224, validation loss: \u001b[92m0.052909\u001b[0m\n",
      "iteration 402, train loss: 0.055767, validation loss: \u001b[92m0.052908\u001b[0m\n",
      "iteration 403, train loss: 0.055308, validation loss: 0.052908\n",
      "iteration 404, train loss: 0.055681, validation loss: 0.052909\n",
      "iteration 405, train loss: 0.055183, validation loss: 0.052909\n",
      "iteration 406, train loss: 0.055008, validation loss: 0.05291\n",
      "iteration 407, train loss: 0.055085, validation loss: 0.052912\n",
      "iteration 408, train loss: 0.056206, validation loss: 0.052914\n",
      "iteration 409, train loss: 0.054947, validation loss: 0.052913\n",
      "iteration 410, train loss: 0.055215, validation loss: 0.052909\n",
      "iteration 411, train loss: 0.054856, validation loss: \u001b[92m0.052903\u001b[0m\n",
      "iteration 412, train loss: 0.055251, validation loss: \u001b[92m0.052896\u001b[0m\n",
      "iteration 413, train loss: 0.055458, validation loss: \u001b[92m0.052889\u001b[0m\n",
      "iteration 414, train loss: 0.055365, validation loss: \u001b[92m0.052882\u001b[0m\n",
      "iteration 415, train loss: 0.054783, validation loss: \u001b[92m0.052875\u001b[0m\n",
      "iteration 416, train loss: 0.055711, validation loss: \u001b[92m0.052871\u001b[0m\n",
      "iteration 417, train loss: 0.055064, validation loss: \u001b[92m0.052867\u001b[0m\n",
      "iteration 418, train loss: 0.054963, validation loss: \u001b[92m0.052867\u001b[0m\n",
      "iteration 419, train loss: 0.055351, validation loss: 0.052868\n",
      "iteration 420, train loss: 0.055793, validation loss: 0.05287\n",
      "iteration 421, train loss: 0.055499, validation loss: 0.052876\n",
      "iteration 422, train loss: 0.055557, validation loss: 0.052885\n",
      "iteration 423, train loss: 0.055509, validation loss: 0.052891\n",
      "iteration 424, train loss: 0.055811, validation loss: 0.052894\n",
      "iteration 425, train loss: 0.054956, validation loss: 0.052898\n",
      "iteration 426, train loss: 0.055092, validation loss: 0.052904\n",
      "iteration 427, train loss: 0.055895, validation loss: 0.052902\n",
      "iteration 428, train loss: 0.055292, validation loss: 0.052895\n",
      "iteration 429, train loss: 0.055751, validation loss: 0.052885\n",
      "iteration 430, train loss: 0.055426, validation loss: 0.052874\n",
      "iteration 431, train loss: 0.055539, validation loss: 0.052868\n",
      "iteration 432, train loss: 0.055678, validation loss: \u001b[92m0.052861\u001b[0m\n",
      "iteration 433, train loss: 0.055925, validation loss: \u001b[92m0.052857\u001b[0m\n",
      "iteration 434, train loss: 0.05541, validation loss: \u001b[92m0.052851\u001b[0m\n",
      "iteration 435, train loss: 0.055556, validation loss: \u001b[92m0.052847\u001b[0m\n",
      "iteration 436, train loss: 0.055605, validation loss: \u001b[92m0.052844\u001b[0m\n",
      "iteration 437, train loss: 0.055082, validation loss: \u001b[92m0.052841\u001b[0m\n",
      "iteration 438, train loss: 0.055775, validation loss: \u001b[92m0.052837\u001b[0m\n",
      "iteration 439, train loss: 0.055641, validation loss: \u001b[92m0.052833\u001b[0m\n",
      "iteration 440, train loss: 0.054871, validation loss: \u001b[92m0.052831\u001b[0m\n",
      "iteration 441, train loss: \u001b[92m0.054666\u001b[0m, validation loss: \u001b[92m0.052828\u001b[0m\n",
      "iteration 442, train loss: 0.055362, validation loss: \u001b[92m0.052828\u001b[0m\n",
      "iteration 443, train loss: 0.055615, validation loss: 0.052831\n",
      "iteration 444, train loss: 0.055354, validation loss: 0.05284\n",
      "iteration 445, train loss: 0.055228, validation loss: 0.052849\n",
      "iteration 446, train loss: 0.055262, validation loss: 0.052857\n",
      "iteration 447, train loss: 0.056058, validation loss: 0.05286\n",
      "iteration 448, train loss: 0.054884, validation loss: 0.052857\n",
      "iteration 449, train loss: 0.0552, validation loss: 0.052846\n",
      "iteration 450, train loss: 0.055238, validation loss: 0.052833\n",
      "iteration 451, train loss: 0.055319, validation loss: \u001b[92m0.052823\u001b[0m\n",
      "iteration 452, train loss: 0.055562, validation loss: \u001b[92m0.052816\u001b[0m\n",
      "iteration 453, train loss: 0.055196, validation loss: \u001b[92m0.05281\u001b[0m\n",
      "iteration 454, train loss: 0.055032, validation loss: \u001b[92m0.052806\u001b[0m\n",
      "iteration 455, train loss: 0.055239, validation loss: \u001b[92m0.052804\u001b[0m\n",
      "iteration 456, train loss: 0.054967, validation loss: \u001b[92m0.052802\u001b[0m\n",
      "iteration 457, train loss: 0.055386, validation loss: \u001b[92m0.052802\u001b[0m\n",
      "iteration 458, train loss: 0.055083, validation loss: 0.052803\n",
      "iteration 459, train loss: 0.055452, validation loss: 0.052807\n",
      "iteration 460, train loss: 0.055331, validation loss: 0.052812\n",
      "iteration 461, train loss: 0.055289, validation loss: 0.052819\n",
      "iteration 462, train loss: 0.055136, validation loss: 0.052828\n",
      "iteration 463, train loss: 0.0547, validation loss: 0.052836\n",
      "iteration 464, train loss: 0.055334, validation loss: 0.052843\n",
      "iteration 465, train loss: 0.055265, validation loss: 0.052847\n",
      "iteration 466, train loss: 0.054815, validation loss: 0.052847\n",
      "iteration 467, train loss: 0.054981, validation loss: 0.052843\n",
      "iteration 468, train loss: \u001b[92m0.054577\u001b[0m, validation loss: 0.052838\n",
      "iteration 469, train loss: 0.055014, validation loss: 0.052827\n",
      "iteration 470, train loss: 0.054867, validation loss: 0.052814\n",
      "iteration 471, train loss: 0.054877, validation loss: 0.052807\n",
      "iteration 472, train loss: 0.05516, validation loss: \u001b[92m0.052799\u001b[0m\n",
      "iteration 473, train loss: 0.055541, validation loss: \u001b[92m0.052792\u001b[0m\n",
      "iteration 474, train loss: 0.055198, validation loss: \u001b[92m0.052788\u001b[0m\n",
      "iteration 475, train loss: 0.055332, validation loss: \u001b[92m0.052785\u001b[0m\n",
      "iteration 476, train loss: 0.055135, validation loss: \u001b[92m0.052784\u001b[0m\n",
      "iteration 477, train loss: 0.055624, validation loss: \u001b[92m0.052781\u001b[0m\n",
      "iteration 478, train loss: 0.054729, validation loss: \u001b[92m0.052779\u001b[0m\n",
      "iteration 479, train loss: 0.055168, validation loss: \u001b[92m0.052779\u001b[0m\n",
      "iteration 480, train loss: 0.054977, validation loss: 0.052779\n",
      "iteration 481, train loss: 0.055254, validation loss: \u001b[92m0.052778\u001b[0m\n",
      "iteration 482, train loss: 0.055007, validation loss: 0.052781\n",
      "iteration 483, train loss: 0.055465, validation loss: 0.052784\n",
      "iteration 484, train loss: 0.055224, validation loss: 0.052787\n",
      "iteration 485, train loss: 0.055409, validation loss: 0.052796\n",
      "iteration 486, train loss: 0.055064, validation loss: 0.052803\n",
      "iteration 487, train loss: 0.055646, validation loss: 0.052805\n",
      "iteration 488, train loss: 0.055112, validation loss: 0.052801\n",
      "iteration 489, train loss: 0.054673, validation loss: 0.052794\n",
      "iteration 490, train loss: 0.055086, validation loss: 0.052788\n",
      "iteration 491, train loss: 0.055425, validation loss: 0.052783\n",
      "iteration 492, train loss: 0.055259, validation loss: 0.05278\n",
      "iteration 493, train loss: 0.055156, validation loss: 0.052779\n",
      "iteration 494, train loss: 0.05532, validation loss: \u001b[92m0.052777\u001b[0m\n",
      "iteration 495, train loss: 0.055157, validation loss: \u001b[92m0.052772\u001b[0m\n",
      "iteration 496, train loss: 0.054818, validation loss: \u001b[92m0.052768\u001b[0m\n",
      "iteration 497, train loss: 0.055325, validation loss: \u001b[92m0.052766\u001b[0m\n",
      "iteration 498, train loss: 0.054777, validation loss: \u001b[92m0.052765\u001b[0m\n",
      "iteration 499, train loss: 0.054836, validation loss: 0.052766\n",
      "iteration 500, train loss: 0.055273, validation loss: 0.052769\n",
      "iteration 501, train loss: \u001b[92m0.054404\u001b[0m, validation loss: 0.05277\n",
      "iteration 502, train loss: 0.054557, validation loss: 0.05277\n",
      "iteration 503, train loss: 0.055003, validation loss: 0.052772\n",
      "iteration 504, train loss: 0.054943, validation loss: 0.052774\n",
      "iteration 505, train loss: 0.055031, validation loss: 0.052776\n",
      "iteration 506, train loss: 0.054914, validation loss: 0.052777\n",
      "iteration 507, train loss: 0.054924, validation loss: 0.052776\n",
      "iteration 508, train loss: 0.054942, validation loss: 0.052776\n",
      "iteration 509, train loss: 0.054992, validation loss: 0.052773\n",
      "iteration 510, train loss: 0.055184, validation loss: 0.052772\n",
      "iteration 511, train loss: 0.055852, validation loss: 0.05277\n",
      "iteration 512, train loss: 0.055272, validation loss: 0.052768\n",
      "iteration 513, train loss: 0.054799, validation loss: 0.052767\n",
      "iteration 514, train loss: 0.055173, validation loss: 0.052767\n",
      "iteration 515, train loss: 0.054774, validation loss: 0.052769\n",
      "iteration 516, train loss: 0.05492, validation loss: 0.052768\n",
      "iteration 517, train loss: 0.0551, validation loss: 0.052766\n",
      "iteration 518, train loss: 0.055112, validation loss: \u001b[92m0.052763\u001b[0m\n",
      "iteration 519, train loss: 0.05505, validation loss: \u001b[92m0.052759\u001b[0m\n",
      "iteration 520, train loss: 0.055217, validation loss: \u001b[92m0.052756\u001b[0m\n",
      "iteration 521, train loss: 0.055104, validation loss: \u001b[92m0.052753\u001b[0m\n",
      "iteration 522, train loss: 0.054997, validation loss: \u001b[92m0.052752\u001b[0m\n",
      "iteration 523, train loss: 0.054781, validation loss: \u001b[92m0.052752\u001b[0m\n",
      "iteration 524, train loss: 0.054571, validation loss: \u001b[92m0.052752\u001b[0m\n",
      "iteration 525, train loss: 0.055395, validation loss: 0.052755\n",
      "iteration 526, train loss: 0.054538, validation loss: 0.052759\n",
      "iteration 527, train loss: 0.054763, validation loss: 0.052761\n",
      "iteration 528, train loss: 0.055679, validation loss: 0.052762\n",
      "iteration 529, train loss: 0.054876, validation loss: 0.05276\n",
      "iteration 530, train loss: 0.054801, validation loss: 0.052758\n",
      "iteration 531, train loss: 0.055322, validation loss: 0.052757\n",
      "iteration 532, train loss: 0.055098, validation loss: 0.052756\n",
      "iteration 533, train loss: 0.054973, validation loss: 0.052756\n",
      "iteration 534, train loss: 0.054965, validation loss: 0.052756\n",
      "iteration 535, train loss: 0.05487, validation loss: 0.052756\n",
      "iteration 536, train loss: 0.054967, validation loss: 0.052757\n",
      "iteration 537, train loss: 0.055371, validation loss: 0.052754\n",
      "iteration 538, train loss: 0.055146, validation loss: 0.052754\n",
      "iteration 539, train loss: 0.054976, validation loss: 0.052754\n",
      "iteration 540, train loss: 0.054994, validation loss: 0.052754\n",
      "iteration 541, train loss: 0.05533, validation loss: 0.052753\n",
      "iteration 542, train loss: 0.05473, validation loss: \u001b[92m0.052751\u001b[0m\n",
      "iteration 543, train loss: 0.055187, validation loss: \u001b[92m0.052748\u001b[0m\n",
      "iteration 544, train loss: 0.05495, validation loss: \u001b[92m0.052746\u001b[0m\n",
      "iteration 545, train loss: 0.054714, validation loss: \u001b[92m0.052744\u001b[0m\n",
      "iteration 546, train loss: 0.054758, validation loss: 0.052745\n",
      "iteration 547, train loss: 0.055073, validation loss: 0.052749\n",
      "iteration 548, train loss: 0.05489, validation loss: 0.052753\n",
      "iteration 549, train loss: 0.054838, validation loss: 0.052754\n",
      "iteration 550, train loss: 0.054878, validation loss: 0.052752\n",
      "iteration 551, train loss: 0.054938, validation loss: 0.052747\n",
      "iteration 552, train loss: 0.054714, validation loss: \u001b[92m0.052742\u001b[0m\n",
      "iteration 553, train loss: 0.054834, validation loss: \u001b[92m0.052737\u001b[0m\n",
      "iteration 554, train loss: 0.054962, validation loss: \u001b[92m0.052733\u001b[0m\n",
      "iteration 555, train loss: 0.054749, validation loss: \u001b[92m0.052732\u001b[0m\n",
      "iteration 556, train loss: 0.054911, validation loss: \u001b[92m0.05273\u001b[0m\n",
      "iteration 557, train loss: 0.054972, validation loss: \u001b[92m0.052729\u001b[0m\n",
      "iteration 558, train loss: 0.054653, validation loss: \u001b[92m0.052727\u001b[0m\n",
      "iteration 559, train loss: 0.054642, validation loss: \u001b[92m0.052725\u001b[0m\n",
      "iteration 560, train loss: 0.054751, validation loss: 0.052726\n",
      "iteration 561, train loss: 0.055467, validation loss: 0.052727\n",
      "iteration 562, train loss: 0.054809, validation loss: 0.052728\n",
      "iteration 563, train loss: 0.05495, validation loss: 0.052732\n",
      "iteration 564, train loss: 0.055051, validation loss: 0.052734\n",
      "iteration 565, train loss: 0.054742, validation loss: 0.05273\n",
      "iteration 566, train loss: 0.054932, validation loss: 0.052726\n",
      "iteration 567, train loss: 0.05494, validation loss: \u001b[92m0.052724\u001b[0m\n",
      "iteration 568, train loss: 0.054411, validation loss: 0.052724\n",
      "iteration 569, train loss: 0.055074, validation loss: 0.052726\n",
      "iteration 570, train loss: 0.054978, validation loss: 0.052726\n",
      "iteration 571, train loss: 0.054582, validation loss: 0.052726\n",
      "iteration 572, train loss: 0.055091, validation loss: 0.052724\n",
      "iteration 573, train loss: 0.055052, validation loss: \u001b[92m0.052724\u001b[0m\n",
      "iteration 574, train loss: 0.054715, validation loss: \u001b[92m0.052723\u001b[0m\n",
      "iteration 575, train loss: 0.055247, validation loss: \u001b[92m0.052719\u001b[0m\n",
      "iteration 576, train loss: 0.054781, validation loss: \u001b[92m0.052716\u001b[0m\n",
      "iteration 577, train loss: 0.054419, validation loss: \u001b[92m0.052714\u001b[0m\n",
      "iteration 578, train loss: 0.05498, validation loss: \u001b[92m0.052714\u001b[0m\n",
      "iteration 579, train loss: 0.054547, validation loss: \u001b[92m0.052713\u001b[0m\n",
      "iteration 580, train loss: 0.054772, validation loss: \u001b[92m0.052712\u001b[0m\n",
      "iteration 581, train loss: 0.054876, validation loss: \u001b[92m0.052711\u001b[0m\n",
      "iteration 582, train loss: 0.054437, validation loss: \u001b[92m0.052711\u001b[0m\n",
      "iteration 583, train loss: 0.05482, validation loss: \u001b[92m0.05271\u001b[0m\n",
      "iteration 584, train loss: 0.054752, validation loss: 0.052714\n",
      "iteration 585, train loss: 0.055187, validation loss: 0.052716\n",
      "iteration 586, train loss: 0.054763, validation loss: 0.052718\n",
      "iteration 587, train loss: 0.054825, validation loss: 0.052714\n",
      "iteration 588, train loss: 0.05449, validation loss: 0.052711\n",
      "iteration 589, train loss: 0.054942, validation loss: \u001b[92m0.052706\u001b[0m\n",
      "iteration 590, train loss: 0.054994, validation loss: \u001b[92m0.052701\u001b[0m\n",
      "iteration 591, train loss: 0.055215, validation loss: \u001b[92m0.052698\u001b[0m\n",
      "iteration 592, train loss: 0.054896, validation loss: \u001b[92m0.052696\u001b[0m\n",
      "iteration 593, train loss: 0.054851, validation loss: \u001b[92m0.052695\u001b[0m\n",
      "iteration 594, train loss: 0.055168, validation loss: \u001b[92m0.052694\u001b[0m\n",
      "iteration 595, train loss: 0.054997, validation loss: \u001b[92m0.052693\u001b[0m\n",
      "iteration 596, train loss: 0.054624, validation loss: 0.052694\n",
      "iteration 597, train loss: 0.055229, validation loss: 0.052698\n",
      "iteration 598, train loss: 0.054631, validation loss: 0.052702\n",
      "iteration 599, train loss: 0.055032, validation loss: 0.052704\n",
      "iteration 600, train loss: 0.054955, validation loss: 0.0527\n",
      "iteration 601, train loss: 0.054768, validation loss: 0.052696\n",
      "iteration 602, train loss: \u001b[92m0.054297\u001b[0m, validation loss: 0.052693\n",
      "iteration 603, train loss: 0.054699, validation loss: \u001b[92m0.05269\u001b[0m\n",
      "iteration 604, train loss: 0.054588, validation loss: \u001b[92m0.052687\u001b[0m\n",
      "iteration 605, train loss: 0.054508, validation loss: \u001b[92m0.052686\u001b[0m\n",
      "iteration 606, train loss: 0.055082, validation loss: \u001b[92m0.052686\u001b[0m\n",
      "iteration 607, train loss: 0.054992, validation loss: 0.052686\n",
      "iteration 608, train loss: 0.054684, validation loss: 0.052686\n",
      "iteration 609, train loss: 0.054714, validation loss: 0.052688\n",
      "iteration 610, train loss: 0.054829, validation loss: \u001b[92m0.052686\u001b[0m\n",
      "iteration 611, train loss: 0.055111, validation loss: \u001b[92m0.052684\u001b[0m\n",
      "iteration 612, train loss: 0.054484, validation loss: \u001b[92m0.052683\u001b[0m\n",
      "iteration 613, train loss: 0.054965, validation loss: \u001b[92m0.052683\u001b[0m\n",
      "iteration 614, train loss: 0.055052, validation loss: 0.052683\n",
      "iteration 615, train loss: 0.055121, validation loss: 0.052684\n",
      "iteration 616, train loss: 0.054655, validation loss: 0.052685\n",
      "iteration 617, train loss: 0.054556, validation loss: 0.052684\n",
      "iteration 618, train loss: 0.054862, validation loss: \u001b[92m0.052682\u001b[0m\n",
      "iteration 619, train loss: 0.054583, validation loss: \u001b[92m0.05268\u001b[0m\n",
      "iteration 620, train loss: 0.054905, validation loss: \u001b[92m0.052679\u001b[0m\n",
      "iteration 621, train loss: 0.055118, validation loss: \u001b[92m0.052678\u001b[0m\n",
      "iteration 622, train loss: 0.054385, validation loss: \u001b[92m0.052678\u001b[0m\n",
      "iteration 623, train loss: 0.054444, validation loss: 0.052678\n",
      "iteration 624, train loss: 0.054922, validation loss: \u001b[92m0.052677\u001b[0m\n",
      "iteration 625, train loss: 0.055555, validation loss: \u001b[92m0.052674\u001b[0m\n",
      "iteration 626, train loss: 0.054814, validation loss: \u001b[92m0.052673\u001b[0m\n",
      "iteration 627, train loss: 0.054901, validation loss: \u001b[92m0.052671\u001b[0m\n",
      "iteration 628, train loss: 0.054656, validation loss: 0.052671\n",
      "iteration 629, train loss: 0.054988, validation loss: 0.052673\n",
      "iteration 630, train loss: 0.054809, validation loss: 0.052675\n",
      "iteration 631, train loss: 0.054769, validation loss: 0.052677\n",
      "iteration 632, train loss: 0.055114, validation loss: 0.052676\n",
      "iteration 633, train loss: 0.054518, validation loss: 0.052672\n",
      "iteration 634, train loss: 0.054783, validation loss: \u001b[92m0.052669\u001b[0m\n",
      "iteration 635, train loss: 0.054779, validation loss: \u001b[92m0.052668\u001b[0m\n",
      "iteration 636, train loss: 0.054695, validation loss: \u001b[92m0.052668\u001b[0m\n",
      "iteration 637, train loss: 0.054603, validation loss: \u001b[92m0.052667\u001b[0m\n",
      "iteration 638, train loss: 0.0549, validation loss: \u001b[92m0.052663\u001b[0m\n",
      "iteration 639, train loss: 0.054747, validation loss: \u001b[92m0.052663\u001b[0m\n",
      "iteration 640, train loss: 0.05447, validation loss: \u001b[92m0.052662\u001b[0m\n",
      "iteration 641, train loss: 0.054915, validation loss: \u001b[92m0.052662\u001b[0m\n",
      "iteration 642, train loss: 0.054972, validation loss: \u001b[92m0.052661\u001b[0m\n",
      "iteration 643, train loss: 0.054676, validation loss: \u001b[92m0.05266\u001b[0m\n",
      "iteration 644, train loss: 0.054545, validation loss: \u001b[92m0.052658\u001b[0m\n",
      "iteration 645, train loss: 0.05476, validation loss: \u001b[92m0.052655\u001b[0m\n",
      "iteration 646, train loss: 0.0543, validation loss: \u001b[92m0.052653\u001b[0m\n",
      "iteration 647, train loss: 0.054439, validation loss: \u001b[92m0.05265\u001b[0m\n",
      "iteration 648, train loss: 0.054884, validation loss: \u001b[92m0.052647\u001b[0m\n",
      "iteration 649, train loss: 0.055114, validation loss: \u001b[92m0.052645\u001b[0m\n",
      "iteration 650, train loss: 0.054808, validation loss: \u001b[92m0.052642\u001b[0m\n",
      "iteration 651, train loss: 0.05478, validation loss: \u001b[92m0.052642\u001b[0m\n",
      "iteration 652, train loss: 0.05489, validation loss: 0.052643\n",
      "iteration 653, train loss: 0.054914, validation loss: 0.052644\n",
      "iteration 654, train loss: 0.054488, validation loss: 0.052646\n",
      "iteration 655, train loss: 0.054713, validation loss: 0.052645\n",
      "iteration 656, train loss: 0.054388, validation loss: 0.052645\n",
      "iteration 657, train loss: 0.054614, validation loss: 0.052643\n",
      "iteration 658, train loss: 0.055071, validation loss: 0.052643\n",
      "iteration 659, train loss: 0.054644, validation loss: 0.052646\n",
      "iteration 660, train loss: 0.054684, validation loss: 0.052652\n",
      "iteration 661, train loss: 0.054729, validation loss: 0.052657\n",
      "iteration 662, train loss: 0.055054, validation loss: 0.052665\n",
      "iteration 663, train loss: 0.054517, validation loss: 0.052668\n",
      "iteration 664, train loss: 0.05468, validation loss: 0.052666\n",
      "iteration 665, train loss: 0.054488, validation loss: 0.05266\n",
      "iteration 666, train loss: 0.054527, validation loss: 0.052654\n",
      "iteration 667, train loss: 0.054933, validation loss: 0.052651\n",
      "iteration 668, train loss: 0.054762, validation loss: 0.05265\n",
      "iteration 669, train loss: 0.054609, validation loss: 0.05265\n",
      "iteration 670, train loss: 0.054703, validation loss: 0.052651\n",
      "iteration 671, train loss: 0.05478, validation loss: 0.052651\n",
      "iteration 672, train loss: 0.054693, validation loss: 0.052652\n",
      "iteration 673, train loss: 0.054604, validation loss: 0.052653\n",
      "iteration 674, train loss: 0.054723, validation loss: 0.052653\n",
      "iteration 675, train loss: 0.054652, validation loss: 0.05265\n",
      "iteration 676, train loss: 0.054881, validation loss: 0.052654\n",
      "iteration 677, train loss: 0.054442, validation loss: 0.052665\n",
      "iteration 678, train loss: 0.054692, validation loss: 0.052675\n",
      "iteration 679, train loss: 0.054859, validation loss: 0.052677\n",
      "iteration 680, train loss: 0.054749, validation loss: 0.05267\n",
      "iteration 681, train loss: 0.054736, validation loss: 0.052659\n",
      "iteration 682, train loss: 0.05448, validation loss: 0.052649\n",
      "iteration 683, train loss: 0.054494, validation loss: 0.052645\n",
      "iteration 684, train loss: 0.054769, validation loss: 0.052647\n",
      "iteration 685, train loss: 0.054737, validation loss: 0.052654\n",
      "iteration 686, train loss: 0.054441, validation loss: 0.052659\n",
      "iteration 687, train loss: 0.054833, validation loss: 0.052659\n",
      "iteration 688, train loss: 0.054688, validation loss: 0.052657\n",
      "iteration 689, train loss: \u001b[92m0.054236\u001b[0m, validation loss: 0.052654\n",
      "iteration 690, train loss: 0.054514, validation loss: 0.052649\n",
      "iteration 691, train loss: \u001b[92m0.054222\u001b[0m, validation loss: 0.052647\n",
      "iteration 692, train loss: 0.054621, validation loss: 0.052644\n",
      "iteration 693, train loss: 0.054882, validation loss: \u001b[92m0.052642\u001b[0m\n",
      "iteration 694, train loss: 0.055042, validation loss: \u001b[92m0.052641\u001b[0m\n",
      "iteration 695, train loss: 0.054747, validation loss: 0.052642\n",
      "iteration 696, train loss: 0.054578, validation loss: 0.052643\n",
      "iteration 697, train loss: 0.054971, validation loss: 0.052643\n",
      "iteration 698, train loss: \u001b[92m0.054022\u001b[0m, validation loss: 0.052642\n",
      "iteration 699, train loss: 0.054828, validation loss: 0.052643\n",
      "iteration 700, train loss: 0.054342, validation loss: 0.052645\n",
      "iteration 701, train loss: 0.054674, validation loss: 0.052645\n",
      "iteration 702, train loss: 0.054454, validation loss: 0.052644\n",
      "iteration 703, train loss: 0.054511, validation loss: 0.052643\n",
      "iteration 704, train loss: 0.054625, validation loss: 0.052643\n",
      "iteration 705, train loss: 0.054568, validation loss: 0.052643\n",
      "iteration 706, train loss: 0.054902, validation loss: 0.052645\n",
      "iteration 707, train loss: 0.05433, validation loss: 0.052648\n",
      "iteration 708, train loss: 0.054838, validation loss: 0.052647\n",
      "iteration 709, train loss: 0.054968, validation loss: 0.052645\n",
      "iteration 710, train loss: 0.05424, validation loss: 0.052643\n",
      "iteration 711, train loss: 0.054919, validation loss: \u001b[92m0.052638\u001b[0m\n",
      "iteration 712, train loss: 0.054383, validation loss: \u001b[92m0.052636\u001b[0m\n",
      "iteration 713, train loss: 0.054592, validation loss: \u001b[92m0.052635\u001b[0m\n",
      "iteration 714, train loss: 0.054883, validation loss: 0.052636\n",
      "iteration 715, train loss: 0.054667, validation loss: \u001b[92m0.052635\u001b[0m\n",
      "iteration 716, train loss: 0.054928, validation loss: \u001b[92m0.052633\u001b[0m\n",
      "iteration 717, train loss: 0.054456, validation loss: \u001b[92m0.052633\u001b[0m\n",
      "iteration 718, train loss: 0.054464, validation loss: \u001b[92m0.052632\u001b[0m\n",
      "iteration 719, train loss: 0.054758, validation loss: \u001b[92m0.052631\u001b[0m\n",
      "iteration 720, train loss: 0.05453, validation loss: \u001b[92m0.052629\u001b[0m\n",
      "iteration 721, train loss: 0.054741, validation loss: \u001b[92m0.052627\u001b[0m\n",
      "iteration 722, train loss: 0.054226, validation loss: \u001b[92m0.052625\u001b[0m\n",
      "iteration 723, train loss: 0.054565, validation loss: \u001b[92m0.052625\u001b[0m\n",
      "iteration 724, train loss: 0.054634, validation loss: 0.052625\n",
      "iteration 725, train loss: 0.054583, validation loss: \u001b[92m0.052624\u001b[0m\n",
      "iteration 726, train loss: 0.054486, validation loss: 0.052627\n",
      "iteration 727, train loss: 0.054416, validation loss: 0.052627\n",
      "iteration 728, train loss: 0.054418, validation loss: 0.052625\n",
      "iteration 729, train loss: 0.054465, validation loss: \u001b[92m0.052623\u001b[0m\n",
      "iteration 730, train loss: 0.054167, validation loss: \u001b[92m0.05262\u001b[0m\n",
      "iteration 731, train loss: 0.054535, validation loss: \u001b[92m0.05262\u001b[0m\n",
      "iteration 732, train loss: 0.054501, validation loss: \u001b[92m0.052618\u001b[0m\n",
      "iteration 733, train loss: 0.054498, validation loss: \u001b[92m0.052615\u001b[0m\n",
      "iteration 734, train loss: 0.054731, validation loss: \u001b[92m0.05261\u001b[0m\n",
      "iteration 735, train loss: 0.054878, validation loss: \u001b[92m0.052609\u001b[0m\n",
      "iteration 736, train loss: 0.05443, validation loss: \u001b[92m0.052607\u001b[0m\n",
      "iteration 737, train loss: 0.055011, validation loss: \u001b[92m0.052605\u001b[0m\n",
      "iteration 738, train loss: 0.054401, validation loss: \u001b[92m0.052603\u001b[0m\n",
      "iteration 739, train loss: 0.054213, validation loss: \u001b[92m0.052601\u001b[0m\n",
      "iteration 740, train loss: 0.054615, validation loss: \u001b[92m0.0526\u001b[0m\n",
      "iteration 741, train loss: 0.054593, validation loss: 0.0526\n",
      "iteration 742, train loss: 0.054811, validation loss: \u001b[92m0.0526\u001b[0m\n",
      "iteration 743, train loss: 0.054759, validation loss: 0.052601\n",
      "iteration 744, train loss: 0.054793, validation loss: 0.052603\n",
      "iteration 745, train loss: 0.05463, validation loss: 0.052604\n",
      "iteration 746, train loss: 0.054509, validation loss: 0.052608\n",
      "iteration 747, train loss: 0.054892, validation loss: 0.052606\n",
      "iteration 748, train loss: 0.054421, validation loss: 0.052603\n",
      "iteration 749, train loss: 0.055061, validation loss: 0.052601\n",
      "iteration 750, train loss: 0.054794, validation loss: 0.052602\n",
      "iteration 751, train loss: 0.054484, validation loss: 0.052605\n",
      "iteration 752, train loss: 0.054559, validation loss: 0.052607\n",
      "iteration 753, train loss: 0.054918, validation loss: 0.05261\n",
      "iteration 754, train loss: 0.054741, validation loss: 0.052612\n",
      "iteration 755, train loss: 0.054469, validation loss: 0.052609\n",
      "iteration 756, train loss: 0.054513, validation loss: 0.052606\n",
      "iteration 757, train loss: 0.054426, validation loss: 0.052602\n",
      "iteration 758, train loss: 0.054231, validation loss: \u001b[92m0.052599\u001b[0m\n",
      "iteration 759, train loss: 0.054692, validation loss: 0.052603\n",
      "iteration 760, train loss: 0.054422, validation loss: 0.052606\n",
      "iteration 761, train loss: 0.054065, validation loss: 0.052618\n",
      "iteration 762, train loss: 0.054392, validation loss: 0.05263\n",
      "iteration 763, train loss: 0.054218, validation loss: 0.052641\n",
      "iteration 764, train loss: 0.054234, validation loss: 0.052652\n",
      "iteration 765, train loss: 0.054687, validation loss: 0.052649\n",
      "iteration 766, train loss: 0.054297, validation loss: 0.052637\n",
      "iteration 767, train loss: 0.05475, validation loss: 0.052625\n",
      "iteration 768, train loss: 0.054615, validation loss: 0.05262\n",
      "iteration 769, train loss: 0.054447, validation loss: 0.052613\n",
      "iteration 770, train loss: 0.054517, validation loss: 0.052607\n",
      "iteration 771, train loss: 0.05426, validation loss: 0.052601\n",
      "iteration 772, train loss: 0.054479, validation loss: \u001b[92m0.052599\u001b[0m\n",
      "iteration 773, train loss: 0.054808, validation loss: 0.052601\n",
      "iteration 774, train loss: 0.054182, validation loss: 0.052603\n",
      "iteration 775, train loss: 0.054563, validation loss: 0.052602\n",
      "iteration 776, train loss: 0.054367, validation loss: 0.052602\n",
      "iteration 777, train loss: 0.054659, validation loss: 0.052603\n",
      "iteration 778, train loss: 0.054345, validation loss: 0.052604\n",
      "iteration 779, train loss: 0.054209, validation loss: 0.052605\n",
      "iteration 780, train loss: 0.054758, validation loss: 0.052603\n",
      "iteration 781, train loss: 0.054482, validation loss: 0.052607\n",
      "iteration 782, train loss: 0.054714, validation loss: 0.052614\n",
      "iteration 783, train loss: 0.054276, validation loss: 0.052621\n",
      "iteration 784, train loss: 0.054709, validation loss: 0.052623\n",
      "iteration 785, train loss: 0.054567, validation loss: 0.052625\n",
      "iteration 786, train loss: 0.054637, validation loss: 0.05262\n",
      "iteration 787, train loss: 0.054281, validation loss: 0.052611\n",
      "iteration 788, train loss: 0.054573, validation loss: 0.052602\n",
      "iteration 789, train loss: 0.054238, validation loss: \u001b[92m0.052595\u001b[0m\n",
      "iteration 790, train loss: 0.054477, validation loss: \u001b[92m0.052588\u001b[0m\n",
      "iteration 791, train loss: 0.054629, validation loss: \u001b[92m0.052587\u001b[0m\n",
      "iteration 792, train loss: 0.05468, validation loss: \u001b[92m0.052584\u001b[0m\n",
      "iteration 793, train loss: 0.054308, validation loss: \u001b[92m0.052582\u001b[0m\n",
      "iteration 794, train loss: 0.054429, validation loss: \u001b[92m0.05258\u001b[0m\n",
      "iteration 795, train loss: 0.054803, validation loss: \u001b[92m0.052578\u001b[0m\n",
      "iteration 796, train loss: 0.054309, validation loss: \u001b[92m0.052573\u001b[0m\n",
      "iteration 797, train loss: 0.054459, validation loss: \u001b[92m0.052566\u001b[0m\n",
      "iteration 798, train loss: 0.055104, validation loss: \u001b[92m0.052559\u001b[0m\n",
      "iteration 799, train loss: 0.054435, validation loss: \u001b[92m0.052556\u001b[0m\n",
      "iteration 800, train loss: 0.054224, validation loss: \u001b[92m0.052556\u001b[0m\n",
      "iteration 801, train loss: 0.054565, validation loss: 0.052558\n",
      "iteration 802, train loss: 0.054948, validation loss: 0.05256\n",
      "iteration 803, train loss: 0.054496, validation loss: 0.052561\n",
      "iteration 804, train loss: 0.05444, validation loss: 0.052561\n",
      "iteration 805, train loss: 0.054994, validation loss: 0.052556\n",
      "iteration 806, train loss: 0.054426, validation loss: \u001b[92m0.052552\u001b[0m\n",
      "iteration 807, train loss: 0.054595, validation loss: 0.052553\n",
      "iteration 808, train loss: 0.054569, validation loss: 0.052554\n",
      "iteration 809, train loss: 0.054364, validation loss: 0.052556\n",
      "iteration 810, train loss: 0.054978, validation loss: 0.052561\n",
      "iteration 811, train loss: 0.054272, validation loss: 0.052569\n",
      "iteration 812, train loss: 0.054454, validation loss: 0.052573\n",
      "iteration 813, train loss: 0.054547, validation loss: 0.052575\n",
      "iteration 814, train loss: 0.054353, validation loss: 0.052577\n",
      "iteration 815, train loss: 0.054338, validation loss: 0.052578\n",
      "iteration 816, train loss: 0.054094, validation loss: 0.052581\n",
      "iteration 817, train loss: 0.054428, validation loss: 0.052578\n",
      "iteration 818, train loss: 0.054308, validation loss: 0.052572\n",
      "iteration 819, train loss: 0.054045, validation loss: 0.052567\n",
      "iteration 820, train loss: 0.054505, validation loss: 0.052561\n",
      "iteration 821, train loss: 0.054674, validation loss: 0.052558\n",
      "iteration 822, train loss: 0.054629, validation loss: 0.052556\n",
      "iteration 823, train loss: 0.054449, validation loss: 0.052553\n",
      "iteration 824, train loss: 0.05471, validation loss: 0.052556\n",
      "iteration 825, train loss: 0.054511, validation loss: 0.05256\n",
      "iteration 826, train loss: 0.05434, validation loss: 0.052563\n",
      "iteration 827, train loss: 0.054165, validation loss: 0.052571\n",
      "iteration 828, train loss: 0.054673, validation loss: 0.052583\n",
      "iteration 829, train loss: 0.054231, validation loss: 0.052583\n",
      "iteration 830, train loss: 0.054121, validation loss: 0.052576\n",
      "iteration 831, train loss: 0.054503, validation loss: 0.052566\n",
      "iteration 832, train loss: 0.054435, validation loss: 0.05256\n",
      "iteration 833, train loss: 0.054584, validation loss: 0.052557\n",
      "iteration 834, train loss: 0.054446, validation loss: 0.052556\n",
      "iteration 835, train loss: 0.05411, validation loss: 0.052555\n",
      "iteration 836, train loss: 0.054208, validation loss: 0.052555\n",
      "iteration 837, train loss: 0.054508, validation loss: 0.052556\n",
      "iteration 838, train loss: 0.054381, validation loss: 0.052556\n",
      "iteration 839, train loss: 0.054448, validation loss: 0.052556\n",
      "iteration 840, train loss: 0.054308, validation loss: 0.052554\n",
      "iteration 841, train loss: 0.054267, validation loss: 0.052556\n",
      "iteration 842, train loss: 0.05432, validation loss: 0.052553\n",
      "iteration 843, train loss: 0.054283, validation loss: \u001b[92m0.052551\u001b[0m\n",
      "iteration 844, train loss: 0.054665, validation loss: \u001b[92m0.052548\u001b[0m\n",
      "iteration 845, train loss: 0.054452, validation loss: 0.05255\n",
      "iteration 846, train loss: \u001b[92m0.053963\u001b[0m, validation loss: 0.052551\n",
      "iteration 847, train loss: 0.054345, validation loss: 0.052555\n",
      "iteration 848, train loss: 0.054452, validation loss: 0.052554\n",
      "iteration 849, train loss: 0.054402, validation loss: 0.052551\n",
      "iteration 850, train loss: 0.054035, validation loss: \u001b[92m0.052544\u001b[0m\n",
      "iteration 851, train loss: 0.054238, validation loss: \u001b[92m0.052538\u001b[0m\n",
      "iteration 852, train loss: 0.054312, validation loss: 0.052538\n",
      "iteration 853, train loss: 0.054202, validation loss: \u001b[92m0.052537\u001b[0m\n",
      "iteration 854, train loss: 0.054267, validation loss: \u001b[92m0.052537\u001b[0m\n",
      "iteration 855, train loss: 0.054629, validation loss: 0.052538\n",
      "iteration 856, train loss: 0.054221, validation loss: 0.052543\n",
      "iteration 857, train loss: 0.05446, validation loss: 0.052543\n",
      "iteration 858, train loss: 0.054638, validation loss: 0.052544\n",
      "iteration 859, train loss: 0.054131, validation loss: 0.052549\n",
      "iteration 860, train loss: 0.054362, validation loss: 0.052551\n",
      "iteration 861, train loss: 0.054416, validation loss: 0.05255\n",
      "iteration 862, train loss: 0.054426, validation loss: 0.05255\n",
      "iteration 863, train loss: 0.054646, validation loss: 0.052548\n",
      "iteration 864, train loss: 0.054097, validation loss: 0.05255\n",
      "iteration 865, train loss: 0.054299, validation loss: 0.052546\n",
      "iteration 866, train loss: 0.054175, validation loss: 0.05254\n",
      "iteration 867, train loss: 0.054108, validation loss: \u001b[92m0.052536\u001b[0m\n",
      "iteration 868, train loss: 0.054224, validation loss: \u001b[92m0.052528\u001b[0m\n",
      "iteration 869, train loss: 0.054501, validation loss: \u001b[92m0.052525\u001b[0m\n",
      "iteration 870, train loss: 0.054701, validation loss: \u001b[92m0.052522\u001b[0m\n",
      "iteration 871, train loss: 0.054234, validation loss: \u001b[92m0.052518\u001b[0m\n",
      "iteration 872, train loss: 0.054396, validation loss: \u001b[92m0.052514\u001b[0m\n",
      "iteration 873, train loss: 0.054153, validation loss: \u001b[92m0.052511\u001b[0m\n",
      "iteration 874, train loss: 0.054494, validation loss: 0.052515\n",
      "iteration 875, train loss: 0.054316, validation loss: 0.052521\n",
      "iteration 876, train loss: 0.054686, validation loss: 0.052525\n",
      "iteration 877, train loss: 0.05469, validation loss: 0.052533\n",
      "iteration 878, train loss: 0.054421, validation loss: 0.052527\n",
      "iteration 879, train loss: 0.0541, validation loss: 0.052522\n",
      "iteration 880, train loss: 0.054042, validation loss: 0.052518\n",
      "iteration 881, train loss: 0.054381, validation loss: 0.052518\n",
      "iteration 882, train loss: 0.054478, validation loss: 0.052524\n",
      "iteration 883, train loss: 0.054332, validation loss: 0.052528\n",
      "iteration 884, train loss: 0.054372, validation loss: 0.052525\n",
      "iteration 885, train loss: 0.054563, validation loss: 0.052519\n",
      "iteration 886, train loss: \u001b[92m0.053952\u001b[0m, validation loss: 0.052512\n",
      "iteration 887, train loss: 0.054415, validation loss: \u001b[92m0.052507\u001b[0m\n",
      "iteration 888, train loss: 0.054191, validation loss: \u001b[92m0.052505\u001b[0m\n",
      "iteration 889, train loss: 0.0543, validation loss: \u001b[92m0.052503\u001b[0m\n",
      "iteration 890, train loss: 0.054374, validation loss: \u001b[92m0.052501\u001b[0m\n",
      "iteration 891, train loss: \u001b[92m0.053876\u001b[0m, validation loss: 0.052502\n",
      "iteration 892, train loss: 0.054736, validation loss: 0.052502\n",
      "iteration 893, train loss: 0.054273, validation loss: 0.052509\n",
      "iteration 894, train loss: 0.054365, validation loss: 0.05252\n",
      "iteration 895, train loss: 0.054423, validation loss: 0.052533\n",
      "iteration 896, train loss: 0.054157, validation loss: 0.052543\n",
      "iteration 897, train loss: 0.054313, validation loss: 0.052543\n",
      "iteration 898, train loss: 0.054316, validation loss: 0.052531\n",
      "iteration 899, train loss: 0.053884, validation loss: 0.052514\n",
      "iteration 900, train loss: 0.05439, validation loss: 0.052504\n",
      "iteration 901, train loss: 0.054104, validation loss: \u001b[92m0.052499\u001b[0m\n",
      "iteration 902, train loss: 0.054068, validation loss: \u001b[92m0.052495\u001b[0m\n",
      "iteration 903, train loss: 0.054582, validation loss: \u001b[92m0.052491\u001b[0m\n",
      "iteration 904, train loss: 0.053986, validation loss: \u001b[92m0.052488\u001b[0m\n",
      "iteration 905, train loss: 0.054248, validation loss: \u001b[92m0.052488\u001b[0m\n",
      "iteration 906, train loss: 0.05446, validation loss: 0.052488\n",
      "iteration 907, train loss: 0.054251, validation loss: 0.052493\n",
      "iteration 908, train loss: 0.054355, validation loss: 0.052501\n",
      "iteration 909, train loss: 0.054421, validation loss: 0.052514\n",
      "iteration 910, train loss: 0.054443, validation loss: 0.052527\n",
      "iteration 911, train loss: 0.054578, validation loss: 0.052535\n",
      "iteration 912, train loss: 0.054578, validation loss: 0.052536\n",
      "iteration 913, train loss: 0.054254, validation loss: 0.052535\n",
      "iteration 914, train loss: 0.05451, validation loss: 0.052526\n",
      "iteration 915, train loss: 0.054168, validation loss: 0.052508\n",
      "iteration 916, train loss: 0.054229, validation loss: 0.052502\n",
      "iteration 917, train loss: 0.054224, validation loss: 0.052498\n",
      "iteration 918, train loss: 0.054376, validation loss: 0.052489\n",
      "iteration 919, train loss: 0.054537, validation loss: \u001b[92m0.052483\u001b[0m\n",
      "iteration 920, train loss: 0.054236, validation loss: \u001b[92m0.05248\u001b[0m\n",
      "iteration 921, train loss: 0.054283, validation loss: \u001b[92m0.052478\u001b[0m\n",
      "iteration 922, train loss: 0.054677, validation loss: 0.05248\n",
      "iteration 923, train loss: 0.054467, validation loss: 0.052484\n",
      "iteration 924, train loss: 0.054577, validation loss: 0.052493\n",
      "iteration 925, train loss: 0.053993, validation loss: 0.052502\n",
      "iteration 926, train loss: 0.054008, validation loss: 0.052509\n",
      "iteration 927, train loss: 0.054459, validation loss: 0.052514\n",
      "iteration 928, train loss: 0.05473, validation loss: 0.052513\n",
      "iteration 929, train loss: 0.054181, validation loss: 0.05251\n",
      "iteration 930, train loss: 0.054003, validation loss: 0.052512\n",
      "iteration 931, train loss: 0.054426, validation loss: 0.052508\n",
      "iteration 932, train loss: 0.054036, validation loss: 0.052506\n",
      "iteration 933, train loss: 0.054314, validation loss: 0.052494\n",
      "iteration 934, train loss: 0.054145, validation loss: 0.052488\n",
      "iteration 935, train loss: 0.054096, validation loss: 0.052478\n",
      "iteration 936, train loss: 0.054543, validation loss: \u001b[92m0.052466\u001b[0m\n",
      "iteration 937, train loss: 0.054515, validation loss: \u001b[92m0.052461\u001b[0m\n",
      "iteration 938, train loss: 0.054489, validation loss: \u001b[92m0.052454\u001b[0m\n",
      "iteration 939, train loss: 0.054568, validation loss: \u001b[92m0.052451\u001b[0m\n",
      "iteration 940, train loss: 0.054233, validation loss: 0.052454\n",
      "iteration 941, train loss: 0.054314, validation loss: \u001b[92m0.052451\u001b[0m\n",
      "iteration 942, train loss: \u001b[92m0.053725\u001b[0m, validation loss: \u001b[92m0.05245\u001b[0m\n",
      "iteration 943, train loss: 0.054344, validation loss: \u001b[92m0.052433\u001b[0m\n",
      "iteration 944, train loss: 0.054408, validation loss: \u001b[92m0.052418\u001b[0m\n",
      "iteration 945, train loss: 0.053949, validation loss: \u001b[92m0.052404\u001b[0m\n",
      "iteration 946, train loss: 0.05454, validation loss: \u001b[92m0.05239\u001b[0m\n",
      "iteration 947, train loss: 0.053858, validation loss: \u001b[92m0.052379\u001b[0m\n",
      "iteration 948, train loss: 0.054348, validation loss: \u001b[92m0.052363\u001b[0m\n",
      "iteration 949, train loss: 0.054379, validation loss: \u001b[92m0.052347\u001b[0m\n",
      "iteration 950, train loss: 0.053881, validation loss: \u001b[92m0.052331\u001b[0m\n",
      "iteration 951, train loss: 0.05417, validation loss: \u001b[92m0.052322\u001b[0m\n",
      "iteration 952, train loss: 0.054071, validation loss: 0.052322\n",
      "iteration 953, train loss: 0.054264, validation loss: \u001b[92m0.05231\u001b[0m\n",
      "iteration 954, train loss: 0.054479, validation loss: \u001b[92m0.052262\u001b[0m\n",
      "iteration 955, train loss: 0.054398, validation loss: \u001b[92m0.052194\u001b[0m\n",
      "iteration 956, train loss: 0.054099, validation loss: \u001b[92m0.05209\u001b[0m\n",
      "iteration 957, train loss: 0.054186, validation loss: \u001b[92m0.051934\u001b[0m\n",
      "iteration 958, train loss: 0.054197, validation loss: \u001b[92m0.051646\u001b[0m\n",
      "iteration 959, train loss: 0.054277, validation loss: \u001b[92m0.051125\u001b[0m\n",
      "iteration 960, train loss: 0.053892, validation loss: \u001b[92m0.050381\u001b[0m\n",
      "iteration 961, train loss: \u001b[92m0.053318\u001b[0m, validation loss: \u001b[92m0.049483\u001b[0m\n",
      "iteration 962, train loss: \u001b[92m0.052852\u001b[0m, validation loss: \u001b[92m0.048471\u001b[0m\n",
      "iteration 963, train loss: \u001b[92m0.052675\u001b[0m, validation loss: \u001b[92m0.047025\u001b[0m\n",
      "iteration 964, train loss: \u001b[92m0.051522\u001b[0m, validation loss: \u001b[92m0.044704\u001b[0m\n",
      "iteration 965, train loss: \u001b[92m0.04937\u001b[0m, validation loss: \u001b[92m0.043415\u001b[0m\n",
      "iteration 966, train loss: \u001b[92m0.048087\u001b[0m, validation loss: \u001b[92m0.03932\u001b[0m\n",
      "iteration 967, train loss: \u001b[92m0.043572\u001b[0m, validation loss: \u001b[92m0.035084\u001b[0m\n",
      "iteration 968, train loss: \u001b[92m0.041752\u001b[0m, validation loss: \u001b[92m0.031486\u001b[0m\n",
      "iteration 969, train loss: \u001b[92m0.037207\u001b[0m, validation loss: \u001b[92m0.027763\u001b[0m\n",
      "iteration 970, train loss: \u001b[92m0.033084\u001b[0m, validation loss: \u001b[92m0.024645\u001b[0m\n",
      "iteration 971, train loss: \u001b[92m0.029016\u001b[0m, validation loss: \u001b[92m0.02282\u001b[0m\n",
      "iteration 972, train loss: \u001b[92m0.028173\u001b[0m, validation loss: 0.025111\n",
      "iteration 973, train loss: 0.029294, validation loss: \u001b[92m0.021495\u001b[0m\n",
      "iteration 974, train loss: \u001b[92m0.025284\u001b[0m, validation loss: 0.021534\n",
      "iteration 975, train loss: 0.026274, validation loss: \u001b[92m0.021069\u001b[0m\n",
      "iteration 976, train loss: 0.025406, validation loss: \u001b[92m0.019309\u001b[0m\n",
      "iteration 977, train loss: \u001b[92m0.023197\u001b[0m, validation loss: \u001b[92m0.017927\u001b[0m\n",
      "iteration 978, train loss: \u001b[92m0.020418\u001b[0m, validation loss: 0.020707\n",
      "iteration 979, train loss: 0.022951, validation loss: 0.018342\n",
      "iteration 980, train loss: 0.021156, validation loss: 0.019678\n",
      "iteration 981, train loss: 0.02363, validation loss: 0.018049\n",
      "iteration 982, train loss: 0.021702, validation loss: 0.018432\n",
      "iteration 983, train loss: \u001b[92m0.020142\u001b[0m, validation loss: 0.018981\n",
      "iteration 984, train loss: 0.020506, validation loss: \u001b[92m0.015618\u001b[0m\n",
      "iteration 985, train loss: \u001b[92m0.018319\u001b[0m, validation loss: 0.017044\n",
      "iteration 986, train loss: 0.019141, validation loss: 0.016264\n",
      "iteration 987, train loss: 0.019392, validation loss: \u001b[92m0.014198\u001b[0m\n",
      "iteration 988, train loss: \u001b[92m0.017635\u001b[0m, validation loss: 0.01643\n",
      "iteration 989, train loss: 0.018852, validation loss: \u001b[92m0.0141\u001b[0m\n",
      "iteration 990, train loss: \u001b[92m0.017079\u001b[0m, validation loss: \u001b[92m0.013569\u001b[0m\n",
      "iteration 991, train loss: \u001b[92m0.01677\u001b[0m, validation loss: \u001b[92m0.013369\u001b[0m\n",
      "iteration 992, train loss: \u001b[92m0.016698\u001b[0m, validation loss: \u001b[92m0.012824\u001b[0m\n",
      "iteration 993, train loss: \u001b[92m0.015409\u001b[0m, validation loss: 0.013271\n",
      "iteration 994, train loss: 0.016369, validation loss: \u001b[92m0.012622\u001b[0m\n",
      "iteration 995, train loss: \u001b[92m0.014799\u001b[0m, validation loss: \u001b[92m0.012461\u001b[0m\n",
      "iteration 996, train loss: 0.014932, validation loss: \u001b[92m0.012317\u001b[0m\n",
      "iteration 997, train loss: 0.014911, validation loss: 0.012332\n",
      "iteration 998, train loss: 0.015172, validation loss: \u001b[92m0.011992\u001b[0m\n",
      "iteration 999, train loss: 0.014988, validation loss: \u001b[92m0.011572\u001b[0m\n",
      "iteration 1000, train loss: \u001b[92m0.014194\u001b[0m, validation loss: \u001b[92m0.011225\u001b[0m\n",
      "iteration 1001, train loss: \u001b[92m0.014149\u001b[0m, validation loss: \u001b[92m0.01101\u001b[0m\n",
      "iteration 1002, train loss: 0.014276, validation loss: \u001b[92m0.011008\u001b[0m\n",
      "iteration 1003, train loss: \u001b[92m0.014046\u001b[0m, validation loss: 0.011222\n",
      "iteration 1004, train loss: \u001b[92m0.013439\u001b[0m, validation loss: \u001b[92m0.010811\u001b[0m\n",
      "iteration 1005, train loss: 0.013694, validation loss: \u001b[92m0.010425\u001b[0m\n",
      "iteration 1006, train loss: \u001b[92m0.013396\u001b[0m, validation loss: \u001b[92m0.01032\u001b[0m\n",
      "iteration 1007, train loss: 0.014059, validation loss: \u001b[92m0.010249\u001b[0m\n",
      "iteration 1008, train loss: 0.013474, validation loss: 0.010535\n",
      "iteration 1009, train loss: 0.01382, validation loss: 0.011244\n",
      "iteration 1010, train loss: 0.013604, validation loss: 0.011132\n",
      "iteration 1011, train loss: \u001b[92m0.013377\u001b[0m, validation loss: 0.010358\n",
      "iteration 1012, train loss: \u001b[92m0.013028\u001b[0m, validation loss: \u001b[92m0.010154\u001b[0m\n",
      "iteration 1013, train loss: 0.013591, validation loss: 0.010157\n",
      "iteration 1014, train loss: 0.013127, validation loss: \u001b[92m0.009942\u001b[0m\n",
      "iteration 1015, train loss: 0.013739, validation loss: 0.010527\n",
      "iteration 1016, train loss: 0.013294, validation loss: 0.011175\n",
      "iteration 1017, train loss: 0.013794, validation loss: 0.011038\n",
      "iteration 1018, train loss: 0.01361, validation loss: \u001b[92m0.009744\u001b[0m\n",
      "iteration 1019, train loss: 0.013397, validation loss: \u001b[92m0.009675\u001b[0m\n",
      "iteration 1020, train loss: \u001b[92m0.012477\u001b[0m, validation loss: 0.009678\n",
      "iteration 1021, train loss: 0.013257, validation loss: 0.009736\n",
      "iteration 1022, train loss: 0.013372, validation loss: 0.009864\n",
      "iteration 1023, train loss: \u001b[92m0.012039\u001b[0m, validation loss: 0.009885\n",
      "iteration 1024, train loss: 0.01309, validation loss: 0.009787\n",
      "iteration 1025, train loss: 0.012558, validation loss: 0.009713\n",
      "iteration 1026, train loss: 0.012908, validation loss: \u001b[92m0.009628\u001b[0m\n",
      "iteration 1027, train loss: 0.012786, validation loss: \u001b[92m0.009614\u001b[0m\n",
      "iteration 1028, train loss: 0.013017, validation loss: 0.01007\n",
      "iteration 1029, train loss: 0.013201, validation loss: 0.010374\n",
      "iteration 1030, train loss: 0.01364, validation loss: \u001b[92m0.009521\u001b[0m\n",
      "iteration 1031, train loss: 0.012672, validation loss: 0.009698\n",
      "iteration 1032, train loss: 0.013236, validation loss: 0.009679\n",
      "iteration 1033, train loss: 0.013096, validation loss: \u001b[92m0.009419\u001b[0m\n",
      "iteration 1034, train loss: 0.012408, validation loss: 0.009723\n",
      "iteration 1035, train loss: 0.01275, validation loss: 0.01001\n",
      "iteration 1036, train loss: 0.012464, validation loss: 0.009766\n",
      "iteration 1037, train loss: 0.01276, validation loss: \u001b[92m0.009341\u001b[0m\n",
      "iteration 1038, train loss: 0.012348, validation loss: 0.009423\n",
      "iteration 1039, train loss: 0.012418, validation loss: 0.009378\n",
      "iteration 1040, train loss: 0.01253, validation loss: \u001b[92m0.009274\u001b[0m\n",
      "iteration 1041, train loss: 0.012857, validation loss: 0.009604\n",
      "iteration 1042, train loss: 0.012362, validation loss: 0.01106\n",
      "iteration 1043, train loss: 0.012886, validation loss: 0.010595\n",
      "iteration 1044, train loss: 0.012787, validation loss: 0.009892\n",
      "iteration 1045, train loss: 0.012412, validation loss: \u001b[92m0.009096\u001b[0m\n",
      "iteration 1046, train loss: 0.012297, validation loss: 0.009569\n",
      "iteration 1047, train loss: 0.012744, validation loss: 0.009584\n",
      "iteration 1048, train loss: 0.013357, validation loss: \u001b[92m0.009019\u001b[0m\n",
      "iteration 1049, train loss: \u001b[92m0.012011\u001b[0m, validation loss: 0.009642\n",
      "iteration 1050, train loss: 0.012552, validation loss: 0.010422\n",
      "iteration 1051, train loss: 0.012896, validation loss: 0.009128\n",
      "iteration 1052, train loss: 0.012065, validation loss: \u001b[92m0.008922\u001b[0m\n",
      "iteration 1053, train loss: 0.012585, validation loss: 0.008976\n",
      "iteration 1054, train loss: 0.012137, validation loss: \u001b[92m0.008904\u001b[0m\n",
      "iteration 1055, train loss: 0.0121, validation loss: \u001b[92m0.008898\u001b[0m\n",
      "iteration 1056, train loss: \u001b[92m0.011665\u001b[0m, validation loss: 0.009243\n",
      "iteration 1057, train loss: 0.01285, validation loss: 0.009382\n",
      "iteration 1058, train loss: 0.01283, validation loss: 0.009127\n",
      "iteration 1059, train loss: \u001b[92m0.011535\u001b[0m, validation loss: 0.008989\n",
      "iteration 1060, train loss: 0.011798, validation loss: \u001b[92m0.008826\u001b[0m\n",
      "iteration 1061, train loss: 0.01236, validation loss: \u001b[92m0.008782\u001b[0m\n",
      "iteration 1062, train loss: 0.012964, validation loss: 0.008946\n",
      "iteration 1063, train loss: 0.012347, validation loss: 0.009167\n",
      "iteration 1064, train loss: 0.011864, validation loss: 0.008845\n",
      "iteration 1065, train loss: 0.011731, validation loss: \u001b[92m0.008649\u001b[0m\n",
      "iteration 1066, train loss: 0.012345, validation loss: 0.008721\n",
      "iteration 1067, train loss: 0.013001, validation loss: 0.008693\n",
      "iteration 1068, train loss: 0.011878, validation loss: \u001b[92m0.008638\u001b[0m\n",
      "iteration 1069, train loss: 0.012114, validation loss: 0.008794\n",
      "iteration 1070, train loss: 0.01219, validation loss: 0.009546\n",
      "iteration 1071, train loss: 0.012715, validation loss: 0.009079\n",
      "iteration 1072, train loss: 0.012484, validation loss: \u001b[92m0.008598\u001b[0m\n",
      "iteration 1073, train loss: 0.012081, validation loss: 0.008699\n",
      "iteration 1074, train loss: 0.012217, validation loss: \u001b[92m0.008586\u001b[0m\n",
      "iteration 1075, train loss: \u001b[92m0.010923\u001b[0m, validation loss: \u001b[92m0.008496\u001b[0m\n",
      "iteration 1076, train loss: 0.011787, validation loss: 0.008906\n",
      "iteration 1077, train loss: 0.011495, validation loss: 0.009024\n",
      "iteration 1078, train loss: 0.012725, validation loss: 0.008558\n",
      "iteration 1079, train loss: 0.012195, validation loss: \u001b[92m0.008489\u001b[0m\n",
      "iteration 1080, train loss: 0.011336, validation loss: \u001b[92m0.008487\u001b[0m\n",
      "iteration 1081, train loss: 0.01202, validation loss: 0.008534\n",
      "iteration 1082, train loss: 0.01216, validation loss: 0.008537\n",
      "iteration 1083, train loss: 0.012395, validation loss: \u001b[92m0.008471\u001b[0m\n",
      "iteration 1084, train loss: 0.011376, validation loss: \u001b[92m0.008378\u001b[0m\n",
      "iteration 1085, train loss: 0.012202, validation loss: 0.008382\n",
      "iteration 1086, train loss: 0.011635, validation loss: 0.008401\n",
      "iteration 1087, train loss: 0.01189, validation loss: 0.008548\n",
      "iteration 1088, train loss: 0.011803, validation loss: \u001b[92m0.008323\u001b[0m\n",
      "iteration 1089, train loss: 0.011158, validation loss: \u001b[92m0.008265\u001b[0m\n",
      "iteration 1090, train loss: 0.011296, validation loss: 0.00827\n",
      "iteration 1091, train loss: 0.012164, validation loss: \u001b[92m0.008238\u001b[0m\n",
      "iteration 1092, train loss: 0.011437, validation loss: 0.008296\n",
      "iteration 1093, train loss: 0.012469, validation loss: 0.008289\n",
      "iteration 1094, train loss: 0.011762, validation loss: \u001b[92m0.008218\u001b[0m\n",
      "iteration 1095, train loss: 0.012594, validation loss: 0.008257\n",
      "iteration 1096, train loss: 0.011755, validation loss: 0.008219\n",
      "iteration 1097, train loss: 0.011263, validation loss: \u001b[92m0.008201\u001b[0m\n",
      "iteration 1098, train loss: 0.011039, validation loss: 0.008217\n",
      "iteration 1099, train loss: 0.012094, validation loss: 0.008262\n",
      "iteration 1100, train loss: 0.010931, validation loss: 0.00829\n",
      "iteration 1101, train loss: 0.012088, validation loss: \u001b[92m0.008099\u001b[0m\n",
      "iteration 1102, train loss: 0.011363, validation loss: \u001b[92m0.00804\u001b[0m\n",
      "iteration 1103, train loss: 0.011602, validation loss: \u001b[92m0.008015\u001b[0m\n",
      "iteration 1104, train loss: 0.011278, validation loss: 0.008038\n",
      "iteration 1105, train loss: \u001b[92m0.010918\u001b[0m, validation loss: 0.008065\n",
      "iteration 1106, train loss: \u001b[92m0.010729\u001b[0m, validation loss: \u001b[92m0.007937\u001b[0m\n",
      "iteration 1107, train loss: 0.011565, validation loss: 0.008053\n",
      "iteration 1108, train loss: 0.011491, validation loss: 0.008031\n",
      "iteration 1109, train loss: 0.010731, validation loss: \u001b[92m0.007887\u001b[0m\n",
      "iteration 1110, train loss: 0.010922, validation loss: 0.007899\n",
      "iteration 1111, train loss: 0.010854, validation loss: \u001b[92m0.007874\u001b[0m\n",
      "iteration 1112, train loss: 0.011106, validation loss: 0.00792\n",
      "iteration 1113, train loss: 0.011076, validation loss: 0.008203\n",
      "iteration 1114, train loss: 0.011587, validation loss: 0.007878\n",
      "iteration 1115, train loss: 0.01123, validation loss: \u001b[92m0.007833\u001b[0m\n",
      "iteration 1116, train loss: 0.011491, validation loss: \u001b[92m0.007782\u001b[0m\n",
      "iteration 1117, train loss: 0.011829, validation loss: 0.007874\n",
      "iteration 1118, train loss: 0.011608, validation loss: 0.007864\n",
      "iteration 1119, train loss: 0.01156, validation loss: 0.007794\n",
      "iteration 1120, train loss: 0.010819, validation loss: 0.007888\n",
      "iteration 1121, train loss: 0.011447, validation loss: 0.008025\n",
      "iteration 1122, train loss: 0.011078, validation loss: 0.007785\n",
      "iteration 1123, train loss: 0.011844, validation loss: 0.008315\n",
      "iteration 1124, train loss: 0.011148, validation loss: 0.008295\n",
      "iteration 1125, train loss: 0.011063, validation loss: \u001b[92m0.007738\u001b[0m\n",
      "iteration 1126, train loss: \u001b[92m0.010529\u001b[0m, validation loss: 0.007833\n",
      "iteration 1127, train loss: 0.010824, validation loss: 0.007865\n",
      "iteration 1128, train loss: 0.011729, validation loss: \u001b[92m0.007652\u001b[0m\n",
      "iteration 1129, train loss: 0.011471, validation loss: 0.008265\n",
      "iteration 1130, train loss: 0.011704, validation loss: 0.008191\n",
      "iteration 1131, train loss: 0.01182, validation loss: \u001b[92m0.007596\u001b[0m\n",
      "iteration 1132, train loss: \u001b[92m0.010474\u001b[0m, validation loss: 0.007731\n",
      "iteration 1133, train loss: 0.010547, validation loss: 0.007678\n",
      "iteration 1134, train loss: 0.010991, validation loss: \u001b[92m0.007482\u001b[0m\n",
      "iteration 1135, train loss: \u001b[92m0.009857\u001b[0m, validation loss: 0.007717\n",
      "iteration 1136, train loss: 0.010632, validation loss: 0.008299\n",
      "iteration 1137, train loss: 0.010761, validation loss: 0.008349\n",
      "iteration 1138, train loss: 0.011591, validation loss: \u001b[92m0.007429\u001b[0m\n",
      "iteration 1139, train loss: 0.010396, validation loss: 0.008155\n",
      "iteration 1140, train loss: 0.012135, validation loss: 0.007934\n",
      "iteration 1141, train loss: 0.011797, validation loss: \u001b[92m0.007331\u001b[0m\n",
      "iteration 1142, train loss: 0.011019, validation loss: 0.008629\n",
      "iteration 1143, train loss: 0.011883, validation loss: 0.009563\n",
      "iteration 1144, train loss: 0.011807, validation loss: 0.0077\n",
      "iteration 1145, train loss: 0.011598, validation loss: 0.007595\n",
      "iteration 1146, train loss: 0.011318, validation loss: 0.008211\n",
      "iteration 1147, train loss: 0.011852, validation loss: 0.007719\n",
      "iteration 1148, train loss: 0.011069, validation loss: 0.007423\n",
      "iteration 1149, train loss: 0.010966, validation loss: 0.00835\n",
      "iteration 1150, train loss: 0.011776, validation loss: 0.007705\n",
      "iteration 1151, train loss: 0.01079, validation loss: 0.007413\n",
      "iteration 1152, train loss: 0.010499, validation loss: 0.007851\n",
      "iteration 1153, train loss: 0.011682, validation loss: 0.007715\n",
      "iteration 1154, train loss: 0.010878, validation loss: 0.007371\n",
      "iteration 1155, train loss: 0.01088, validation loss: 0.007862\n",
      "iteration 1156, train loss: 0.01084, validation loss: 0.007665\n",
      "iteration 1157, train loss: 0.010645, validation loss: \u001b[92m0.007305\u001b[0m\n",
      "iteration 1158, train loss: 0.010427, validation loss: 0.007515\n",
      "iteration 1159, train loss: 0.010726, validation loss: 0.007756\n",
      "iteration 1160, train loss: 0.011141, validation loss: 0.007351\n",
      "iteration 1161, train loss: 0.010458, validation loss: \u001b[92m0.007255\u001b[0m\n",
      "iteration 1162, train loss: 0.010725, validation loss: 0.007407\n",
      "iteration 1163, train loss: 0.01104, validation loss: \u001b[92m0.007211\u001b[0m\n",
      "iteration 1164, train loss: 0.010683, validation loss: \u001b[92m0.007107\u001b[0m\n",
      "iteration 1165, train loss: 0.010435, validation loss: 0.007198\n",
      "iteration 1166, train loss: 0.010864, validation loss: \u001b[92m0.007102\u001b[0m\n",
      "iteration 1167, train loss: 0.011448, validation loss: \u001b[92m0.007024\u001b[0m\n",
      "iteration 1168, train loss: \u001b[92m0.009759\u001b[0m, validation loss: 0.007118\n",
      "iteration 1169, train loss: 0.010858, validation loss: 0.007148\n",
      "iteration 1170, train loss: 0.011758, validation loss: \u001b[92m0.006991\u001b[0m\n",
      "iteration 1171, train loss: 0.010125, validation loss: \u001b[92m0.006955\u001b[0m\n",
      "iteration 1172, train loss: 0.01085, validation loss: 0.007042\n",
      "iteration 1173, train loss: 0.010583, validation loss: 0.006979\n",
      "iteration 1174, train loss: 0.010598, validation loss: \u001b[92m0.006889\u001b[0m\n",
      "iteration 1175, train loss: 0.010257, validation loss: \u001b[92m0.00684\u001b[0m\n",
      "iteration 1176, train loss: 0.010654, validation loss: 0.006872\n",
      "iteration 1177, train loss: \u001b[92m0.009637\u001b[0m, validation loss: 0.00709\n",
      "iteration 1178, train loss: 0.010518, validation loss: 0.007382\n",
      "iteration 1179, train loss: 0.010983, validation loss: 0.00706\n",
      "iteration 1180, train loss: 0.010315, validation loss: \u001b[92m0.006775\u001b[0m\n",
      "iteration 1181, train loss: 0.010322, validation loss: 0.006806\n",
      "iteration 1182, train loss: 0.010365, validation loss: \u001b[92m0.006757\u001b[0m\n",
      "iteration 1183, train loss: 0.010184, validation loss: 0.006857\n",
      "iteration 1184, train loss: 0.010328, validation loss: 0.007126\n",
      "iteration 1185, train loss: 0.010301, validation loss: 0.007276\n",
      "iteration 1186, train loss: 0.009751, validation loss: 0.00694\n",
      "iteration 1187, train loss: 0.010315, validation loss: \u001b[92m0.006686\u001b[0m\n",
      "iteration 1188, train loss: 0.009687, validation loss: 0.006757\n",
      "iteration 1189, train loss: 0.010277, validation loss: \u001b[92m0.00668\u001b[0m\n",
      "iteration 1190, train loss: 0.010437, validation loss: 0.006893\n",
      "iteration 1191, train loss: 0.010876, validation loss: 0.007187\n",
      "iteration 1192, train loss: 0.010232, validation loss: 0.006924\n",
      "iteration 1193, train loss: 0.010703, validation loss: 0.006708\n",
      "iteration 1194, train loss: 0.010121, validation loss: \u001b[92m0.006655\u001b[0m\n",
      "iteration 1195, train loss: 0.00965, validation loss: \u001b[92m0.006636\u001b[0m\n",
      "iteration 1196, train loss: \u001b[92m0.009453\u001b[0m, validation loss: \u001b[92m0.006612\u001b[0m\n",
      "iteration 1197, train loss: 0.010057, validation loss: 0.006773\n",
      "iteration 1198, train loss: 0.010546, validation loss: 0.006795\n",
      "iteration 1199, train loss: 0.00996, validation loss: 0.00664\n",
      "iteration 1200, train loss: 0.009667, validation loss: \u001b[92m0.006593\u001b[0m\n",
      "iteration 1201, train loss: 0.010689, validation loss: \u001b[92m0.006589\u001b[0m\n",
      "iteration 1202, train loss: 0.009959, validation loss: 0.0066\n",
      "iteration 1203, train loss: 0.009714, validation loss: 0.006823\n",
      "iteration 1204, train loss: 0.010065, validation loss: 0.006664\n",
      "iteration 1205, train loss: 0.009881, validation loss: \u001b[92m0.006539\u001b[0m\n",
      "iteration 1206, train loss: 0.010077, validation loss: \u001b[92m0.006504\u001b[0m\n",
      "iteration 1207, train loss: 0.010124, validation loss: 0.006513\n",
      "iteration 1208, train loss: 0.009895, validation loss: 0.006917\n",
      "iteration 1209, train loss: 0.009951, validation loss: 0.006907\n",
      "iteration 1210, train loss: 0.009917, validation loss: 0.006632\n",
      "iteration 1211, train loss: 0.009779, validation loss: \u001b[92m0.006488\u001b[0m\n",
      "iteration 1212, train loss: 0.009707, validation loss: 0.006516\n",
      "iteration 1213, train loss: 0.009795, validation loss: 0.006493\n",
      "iteration 1214, train loss: 0.010227, validation loss: 0.006518\n",
      "iteration 1215, train loss: 0.009852, validation loss: 0.006723\n",
      "iteration 1216, train loss: 0.009761, validation loss: 0.006637\n",
      "iteration 1217, train loss: 0.010365, validation loss: \u001b[92m0.006404\u001b[0m\n",
      "iteration 1218, train loss: 0.009754, validation loss: 0.006735\n",
      "iteration 1219, train loss: 0.010231, validation loss: 0.006663\n",
      "iteration 1220, train loss: \u001b[92m0.00941\u001b[0m, validation loss: 0.006409\n",
      "iteration 1221, train loss: \u001b[92m0.009125\u001b[0m, validation loss: 0.006901\n",
      "iteration 1222, train loss: 0.010187, validation loss: 0.007793\n",
      "iteration 1223, train loss: 0.010715, validation loss: 0.006585\n",
      "iteration 1224, train loss: 0.009712, validation loss: 0.006573\n",
      "iteration 1225, train loss: 0.009808, validation loss: 0.006978\n",
      "iteration 1226, train loss: 0.011441, validation loss: 0.006484\n",
      "iteration 1227, train loss: 0.009451, validation loss: 0.006682\n",
      "iteration 1228, train loss: 0.009656, validation loss: 0.007207\n",
      "iteration 1229, train loss: 0.01042, validation loss: 0.006614\n",
      "iteration 1230, train loss: 0.009608, validation loss: \u001b[92m0.006356\u001b[0m\n",
      "iteration 1231, train loss: 0.009632, validation loss: 0.00666\n",
      "iteration 1232, train loss: 0.010306, validation loss: 0.006445\n",
      "iteration 1233, train loss: 0.009544, validation loss: \u001b[92m0.006331\u001b[0m\n",
      "iteration 1234, train loss: 0.009481, validation loss: 0.006529\n",
      "iteration 1235, train loss: 0.00978, validation loss: 0.006485\n",
      "iteration 1236, train loss: 0.009693, validation loss: 0.006361\n",
      "iteration 1237, train loss: \u001b[92m0.009016\u001b[0m, validation loss: \u001b[92m0.006254\u001b[0m\n",
      "iteration 1238, train loss: 0.009877, validation loss: \u001b[92m0.006212\u001b[0m\n",
      "iteration 1239, train loss: 0.009357, validation loss: \u001b[92m0.006187\u001b[0m\n",
      "iteration 1240, train loss: 0.009419, validation loss: \u001b[92m0.006174\u001b[0m\n",
      "iteration 1241, train loss: 0.009432, validation loss: \u001b[92m0.00614\u001b[0m\n",
      "iteration 1242, train loss: \u001b[92m0.008536\u001b[0m, validation loss: 0.006142\n",
      "iteration 1243, train loss: 0.0091, validation loss: 0.006159\n",
      "iteration 1244, train loss: 0.00959, validation loss: \u001b[92m0.006055\u001b[0m\n",
      "iteration 1245, train loss: 0.009431, validation loss: 0.006082\n",
      "iteration 1246, train loss: 0.010053, validation loss: \u001b[92m0.006042\u001b[0m\n",
      "iteration 1247, train loss: 0.008859, validation loss: 0.006232\n",
      "iteration 1248, train loss: 0.009871, validation loss: 0.006308\n",
      "iteration 1249, train loss: 0.009199, validation loss: 0.006052\n",
      "iteration 1250, train loss: 0.009609, validation loss: 0.006054\n",
      "iteration 1251, train loss: 0.009365, validation loss: \u001b[92m0.006012\u001b[0m\n",
      "iteration 1252, train loss: 0.008944, validation loss: 0.006054\n",
      "iteration 1253, train loss: 0.010081, validation loss: \u001b[92m0.00597\u001b[0m\n",
      "iteration 1254, train loss: \u001b[92m0.008505\u001b[0m, validation loss: \u001b[92m0.005935\u001b[0m\n",
      "iteration 1255, train loss: 0.009468, validation loss: \u001b[92m0.005918\u001b[0m\n",
      "iteration 1256, train loss: 0.009284, validation loss: 0.00594\n",
      "iteration 1257, train loss: 0.008697, validation loss: 0.006077\n",
      "iteration 1258, train loss: 0.009069, validation loss: 0.00611\n",
      "iteration 1259, train loss: 0.008788, validation loss: 0.006037\n",
      "iteration 1260, train loss: 0.008993, validation loss: \u001b[92m0.005897\u001b[0m\n",
      "iteration 1261, train loss: 0.008667, validation loss: 0.005958\n",
      "iteration 1262, train loss: 0.00867, validation loss: 0.005953\n",
      "iteration 1263, train loss: 0.008882, validation loss: 0.0061\n",
      "iteration 1264, train loss: 0.009736, validation loss: 0.006357\n",
      "iteration 1265, train loss: 0.009459, validation loss: 0.006052\n",
      "iteration 1266, train loss: 0.009558, validation loss: \u001b[92m0.005891\u001b[0m\n",
      "iteration 1267, train loss: 0.008844, validation loss: 0.006397\n",
      "iteration 1268, train loss: 0.009753, validation loss: \u001b[92m0.00586\u001b[0m\n",
      "iteration 1269, train loss: 0.00869, validation loss: 0.006623\n",
      "iteration 1270, train loss: 0.00921, validation loss: 0.006316\n",
      "iteration 1271, train loss: 0.009618, validation loss: \u001b[92m0.00583\u001b[0m\n",
      "iteration 1272, train loss: 0.009138, validation loss: 0.006228\n",
      "iteration 1273, train loss: 0.009085, validation loss: 0.006142\n",
      "iteration 1274, train loss: 0.009019, validation loss: \u001b[92m0.00583\u001b[0m\n",
      "iteration 1275, train loss: 0.008996, validation loss: 0.006168\n",
      "iteration 1276, train loss: 0.008992, validation loss: 0.006578\n",
      "iteration 1277, train loss: 0.008906, validation loss: 0.006264\n",
      "iteration 1278, train loss: 0.009006, validation loss: 0.005857\n",
      "iteration 1279, train loss: 0.009841, validation loss: 0.006069\n",
      "iteration 1280, train loss: 0.00962, validation loss: 0.005851\n",
      "iteration 1281, train loss: 0.00928, validation loss: 0.006713\n",
      "iteration 1282, train loss: 0.009745, validation loss: 0.00676\n",
      "iteration 1283, train loss: 0.009882, validation loss: 0.006174\n",
      "iteration 1284, train loss: 0.009903, validation loss: 0.005967\n",
      "iteration 1285, train loss: 0.009426, validation loss: 0.005966\n",
      "iteration 1286, train loss: 0.008859, validation loss: \u001b[92m0.005793\u001b[0m\n",
      "iteration 1287, train loss: 0.008738, validation loss: 0.006252\n",
      "iteration 1288, train loss: 0.008762, validation loss: 0.006643\n",
      "iteration 1289, train loss: 0.009575, validation loss: 0.005828\n",
      "iteration 1290, train loss: 0.008875, validation loss: 0.006227\n",
      "iteration 1291, train loss: 0.008611, validation loss: 0.006359\n",
      "iteration 1292, train loss: 0.009247, validation loss: 0.006198\n",
      "iteration 1293, train loss: 0.009, validation loss: 0.006641\n",
      "iteration 1294, train loss: 0.009293, validation loss: 0.006997\n",
      "iteration 1295, train loss: 0.009822, validation loss: 0.006569\n",
      "iteration 1296, train loss: 0.009858, validation loss: \u001b[92m0.005786\u001b[0m\n",
      "iteration 1297, train loss: 0.009116, validation loss: 0.006095\n",
      "iteration 1298, train loss: 0.009506, validation loss: 0.005952\n",
      "iteration 1299, train loss: 0.009334, validation loss: 0.006044\n",
      "iteration 1300, train loss: 0.008555, validation loss: 0.006887\n",
      "iteration 1301, train loss: 0.009285, validation loss: 0.006965\n",
      "iteration 1302, train loss: 0.010473, validation loss: 0.005881\n",
      "iteration 1303, train loss: 0.009264, validation loss: 0.006003\n",
      "iteration 1304, train loss: \u001b[92m0.008478\u001b[0m, validation loss: 0.006292\n",
      "iteration 1305, train loss: 0.009855, validation loss: 0.005862\n",
      "iteration 1306, train loss: 0.0093, validation loss: 0.006934\n",
      "iteration 1307, train loss: 0.009635, validation loss: 0.007503\n",
      "iteration 1308, train loss: 0.009777, validation loss: 0.006733\n",
      "iteration 1309, train loss: 0.01014, validation loss: 0.005938\n",
      "iteration 1310, train loss: 0.008631, validation loss: 0.006134\n",
      "iteration 1311, train loss: 0.009628, validation loss: 0.005868\n",
      "iteration 1312, train loss: \u001b[92m0.008335\u001b[0m, validation loss: \u001b[92m0.005666\u001b[0m\n",
      "iteration 1313, train loss: \u001b[92m0.008291\u001b[0m, validation loss: 0.005812\n",
      "iteration 1314, train loss: 0.008445, validation loss: 0.00588\n",
      "iteration 1315, train loss: \u001b[92m0.008286\u001b[0m, validation loss: 0.005769\n",
      "iteration 1316, train loss: 0.009406, validation loss: 0.005691\n",
      "iteration 1317, train loss: 0.008484, validation loss: 0.005764\n",
      "iteration 1318, train loss: 0.008649, validation loss: 0.005713\n",
      "iteration 1319, train loss: 0.009157, validation loss: 0.00575\n",
      "iteration 1320, train loss: 0.008797, validation loss: 0.005968\n",
      "iteration 1321, train loss: 0.008667, validation loss: 0.005827\n",
      "iteration 1322, train loss: \u001b[92m0.008186\u001b[0m, validation loss: \u001b[92m0.005625\u001b[0m\n",
      "iteration 1323, train loss: 0.009135, validation loss: \u001b[92m0.005607\u001b[0m\n",
      "iteration 1324, train loss: 0.008873, validation loss: 0.005623\n",
      "iteration 1325, train loss: 0.008656, validation loss: 0.005626\n",
      "iteration 1326, train loss: 0.008357, validation loss: 0.00565\n",
      "iteration 1327, train loss: 0.008449, validation loss: 0.005677\n",
      "iteration 1328, train loss: 0.008605, validation loss: 0.005715\n",
      "iteration 1329, train loss: 0.008844, validation loss: 0.005647\n",
      "iteration 1330, train loss: 0.009122, validation loss: 0.005609\n",
      "iteration 1331, train loss: 0.008559, validation loss: 0.005669\n",
      "iteration 1332, train loss: 0.008907, validation loss: 0.005802\n",
      "iteration 1333, train loss: 0.008825, validation loss: 0.005959\n",
      "iteration 1334, train loss: 0.00837, validation loss: 0.006114\n",
      "iteration 1335, train loss: 0.008666, validation loss: 0.00571\n",
      "iteration 1336, train loss: 0.008328, validation loss: 0.005671\n",
      "iteration 1337, train loss: 0.009031, validation loss: 0.005836\n",
      "iteration 1338, train loss: 0.009239, validation loss: 0.005797\n",
      "iteration 1339, train loss: 0.00875, validation loss: 0.00593\n",
      "iteration 1340, train loss: 0.008806, validation loss: 0.005889\n",
      "iteration 1341, train loss: 0.008727, validation loss: 0.00577\n",
      "iteration 1342, train loss: 0.008571, validation loss: 0.00564\n",
      "iteration 1343, train loss: 0.009207, validation loss: \u001b[92m0.005599\u001b[0m\n",
      "iteration 1344, train loss: 0.008262, validation loss: 0.005612\n",
      "iteration 1345, train loss: 0.008505, validation loss: 0.005606\n",
      "iteration 1346, train loss: 0.009216, validation loss: 0.006012\n",
      "iteration 1347, train loss: 0.009043, validation loss: 0.00637\n",
      "iteration 1348, train loss: 0.008735, validation loss: 0.006074\n",
      "iteration 1349, train loss: 0.008847, validation loss: \u001b[92m0.005546\u001b[0m\n",
      "iteration 1350, train loss: 0.00929, validation loss: 0.0059\n",
      "iteration 1351, train loss: 0.009446, validation loss: 0.005715\n",
      "iteration 1352, train loss: 0.009055, validation loss: 0.005682\n",
      "iteration 1353, train loss: 0.008401, validation loss: 0.006048\n",
      "iteration 1354, train loss: 0.009719, validation loss: 0.005746\n",
      "iteration 1355, train loss: 0.009064, validation loss: 0.005751\n",
      "iteration 1356, train loss: 0.008569, validation loss: 0.005948\n",
      "iteration 1357, train loss: 0.009028, validation loss: \u001b[92m0.005524\u001b[0m\n",
      "iteration 1358, train loss: 0.008296, validation loss: 0.005901\n",
      "iteration 1359, train loss: 0.008712, validation loss: 0.005782\n",
      "iteration 1360, train loss: 0.009225, validation loss: 0.005729\n",
      "iteration 1361, train loss: 0.009066, validation loss: 0.005713\n",
      "iteration 1362, train loss: 0.008756, validation loss: 0.00567\n",
      "iteration 1363, train loss: 0.008881, validation loss: 0.006167\n",
      "iteration 1364, train loss: 0.00875, validation loss: 0.006135\n",
      "iteration 1365, train loss: 0.009339, validation loss: 0.00556\n",
      "iteration 1366, train loss: \u001b[92m0.008111\u001b[0m, validation loss: 0.005635\n",
      "iteration 1367, train loss: 0.009015, validation loss: \u001b[92m0.005517\u001b[0m\n",
      "iteration 1368, train loss: 0.00828, validation loss: 0.005931\n",
      "iteration 1369, train loss: 0.00887, validation loss: 0.006079\n",
      "iteration 1370, train loss: 0.009336, validation loss: 0.005725\n",
      "iteration 1371, train loss: 0.008884, validation loss: 0.005541\n",
      "iteration 1372, train loss: 0.008822, validation loss: 0.005641\n",
      "iteration 1373, train loss: 0.008321, validation loss: 0.00576\n",
      "iteration 1374, train loss: 0.008395, validation loss: 0.005949\n",
      "iteration 1375, train loss: 0.008624, validation loss: 0.005772\n",
      "iteration 1376, train loss: 0.008724, validation loss: 0.005632\n",
      "iteration 1377, train loss: 0.008587, validation loss: 0.005695\n",
      "iteration 1378, train loss: 0.008583, validation loss: 0.005787\n",
      "iteration 1379, train loss: 0.00827, validation loss: 0.005745\n",
      "iteration 1380, train loss: 0.008515, validation loss: 0.00568\n",
      "iteration 1381, train loss: 0.008959, validation loss: 0.005519\n",
      "iteration 1382, train loss: 0.009289, validation loss: \u001b[92m0.005514\u001b[0m\n",
      "iteration 1383, train loss: 0.008884, validation loss: 0.005529\n",
      "iteration 1384, train loss: 0.008397, validation loss: 0.005707\n",
      "iteration 1385, train loss: 0.008696, validation loss: 0.005768\n",
      "iteration 1386, train loss: 0.008185, validation loss: 0.005623\n",
      "iteration 1387, train loss: 0.008765, validation loss: 0.005664\n",
      "iteration 1388, train loss: 0.008681, validation loss: 0.005714\n",
      "iteration 1389, train loss: 0.008585, validation loss: 0.005647\n",
      "iteration 1390, train loss: 0.008189, validation loss: 0.005541\n",
      "iteration 1391, train loss: 0.00839, validation loss: 0.005533\n",
      "iteration 1392, train loss: 0.008473, validation loss: 0.005545\n",
      "iteration 1393, train loss: 0.009086, validation loss: 0.005594\n",
      "iteration 1394, train loss: \u001b[92m0.007653\u001b[0m, validation loss: 0.00593\n",
      "iteration 1395, train loss: 0.008465, validation loss: 0.005816\n",
      "iteration 1396, train loss: 0.009195, validation loss: 0.005599\n",
      "iteration 1397, train loss: 0.009351, validation loss: 0.005837\n",
      "iteration 1398, train loss: 0.009203, validation loss: \u001b[92m0.00544\u001b[0m\n",
      "iteration 1399, train loss: 0.008387, validation loss: 0.005786\n",
      "iteration 1400, train loss: 0.008445, validation loss: 0.006065\n",
      "iteration 1401, train loss: 0.009155, validation loss: 0.005552\n",
      "iteration 1402, train loss: 0.008366, validation loss: \u001b[92m0.005414\u001b[0m\n",
      "iteration 1403, train loss: 0.008134, validation loss: 0.005447\n",
      "iteration 1404, train loss: 0.008245, validation loss: 0.00559\n",
      "iteration 1405, train loss: 0.008669, validation loss: 0.005569\n",
      "iteration 1406, train loss: 0.008787, validation loss: 0.005465\n",
      "iteration 1407, train loss: 0.008831, validation loss: 0.005458\n",
      "iteration 1408, train loss: 0.008375, validation loss: 0.005604\n",
      "iteration 1409, train loss: 0.008207, validation loss: 0.005593\n",
      "iteration 1410, train loss: 0.008483, validation loss: 0.005543\n",
      "iteration 1411, train loss: 0.008146, validation loss: 0.005438\n",
      "iteration 1412, train loss: 0.008711, validation loss: \u001b[92m0.005401\u001b[0m\n",
      "iteration 1413, train loss: 0.008009, validation loss: 0.005406\n",
      "iteration 1414, train loss: 0.008604, validation loss: 0.005408\n",
      "iteration 1415, train loss: 0.008388, validation loss: 0.005416\n",
      "iteration 1416, train loss: 0.008594, validation loss: 0.005457\n",
      "iteration 1417, train loss: 0.008438, validation loss: 0.005541\n",
      "iteration 1418, train loss: 0.00886, validation loss: 0.00546\n",
      "iteration 1419, train loss: 0.008342, validation loss: 0.005422\n",
      "iteration 1420, train loss: 0.008133, validation loss: \u001b[92m0.005398\u001b[0m\n",
      "iteration 1421, train loss: 0.008797, validation loss: 0.005475\n",
      "iteration 1422, train loss: 0.008574, validation loss: 0.005441\n",
      "iteration 1423, train loss: 0.008245, validation loss: 0.005599\n",
      "iteration 1424, train loss: 0.007981, validation loss: 0.005563\n",
      "iteration 1425, train loss: 0.009137, validation loss: \u001b[92m0.005378\u001b[0m\n",
      "iteration 1426, train loss: 0.008332, validation loss: 0.00545\n",
      "iteration 1427, train loss: 0.008327, validation loss: 0.005401\n",
      "iteration 1428, train loss: 0.008306, validation loss: 0.005466\n",
      "iteration 1429, train loss: 0.0082, validation loss: 0.005748\n",
      "iteration 1430, train loss: 0.008113, validation loss: 0.005497\n",
      "iteration 1431, train loss: 0.007786, validation loss: \u001b[92m0.00537\u001b[0m\n",
      "iteration 1432, train loss: 0.008861, validation loss: 0.005408\n",
      "iteration 1433, train loss: 0.008519, validation loss: 0.005433\n",
      "iteration 1434, train loss: 0.007977, validation loss: 0.00543\n",
      "iteration 1435, train loss: 0.009127, validation loss: \u001b[92m0.005342\u001b[0m\n",
      "iteration 1436, train loss: 0.008002, validation loss: \u001b[92m0.005342\u001b[0m\n",
      "iteration 1437, train loss: 0.008226, validation loss: 0.005391\n",
      "iteration 1438, train loss: 0.008951, validation loss: 0.005405\n",
      "iteration 1439, train loss: 0.00836, validation loss: 0.005729\n",
      "iteration 1440, train loss: 0.008352, validation loss: 0.005716\n",
      "iteration 1441, train loss: 0.009065, validation loss: 0.005354\n",
      "iteration 1442, train loss: 0.008648, validation loss: 0.005448\n",
      "iteration 1443, train loss: 0.008344, validation loss: 0.005449\n",
      "iteration 1444, train loss: 0.007706, validation loss: 0.005434\n",
      "iteration 1445, train loss: 0.008582, validation loss: 0.005591\n",
      "iteration 1446, train loss: 0.008408, validation loss: 0.005521\n",
      "iteration 1447, train loss: 0.008555, validation loss: 0.005464\n",
      "iteration 1448, train loss: 0.008257, validation loss: 0.005604\n",
      "iteration 1449, train loss: 0.008594, validation loss: 0.005415\n",
      "iteration 1450, train loss: 0.008098, validation loss: 0.005467\n",
      "iteration 1451, train loss: 0.008386, validation loss: 0.005605\n",
      "iteration 1452, train loss: 0.008374, validation loss: 0.005687\n",
      "iteration 1453, train loss: 0.008357, validation loss: 0.005615\n",
      "iteration 1454, train loss: 0.007857, validation loss: 0.005485\n",
      "iteration 1455, train loss: 0.008171, validation loss: 0.00546\n",
      "iteration 1456, train loss: 0.008169, validation loss: 0.005413\n",
      "iteration 1457, train loss: 0.008438, validation loss: 0.005649\n",
      "iteration 1458, train loss: 0.00872, validation loss: 0.005699\n",
      "iteration 1459, train loss: 0.00816, validation loss: 0.005374\n",
      "iteration 1460, train loss: 0.00867, validation loss: 0.005943\n",
      "iteration 1461, train loss: 0.009298, validation loss: 0.005354\n",
      "iteration 1462, train loss: 0.007722, validation loss: 0.006012\n",
      "iteration 1463, train loss: 0.00864, validation loss: 0.006521\n",
      "iteration 1464, train loss: 0.008994, validation loss: 0.005776\n",
      "iteration 1465, train loss: 0.008073, validation loss: \u001b[92m0.005332\u001b[0m\n",
      "iteration 1466, train loss: 0.008528, validation loss: 0.005715\n",
      "iteration 1467, train loss: 0.008765, validation loss: 0.005584\n",
      "iteration 1468, train loss: 0.008237, validation loss: 0.005799\n",
      "iteration 1469, train loss: 0.008504, validation loss: 0.006291\n",
      "iteration 1470, train loss: 0.008716, validation loss: 0.006055\n",
      "iteration 1471, train loss: 0.00857, validation loss: 0.005602\n",
      "iteration 1472, train loss: 0.008845, validation loss: 0.005545\n",
      "iteration 1473, train loss: 0.008256, validation loss: 0.005492\n",
      "iteration 1474, train loss: 0.008231, validation loss: 0.005399\n",
      "iteration 1475, train loss: 0.008472, validation loss: 0.00603\n",
      "iteration 1476, train loss: 0.008477, validation loss: 0.005954\n",
      "iteration 1477, train loss: 0.009217, validation loss: \u001b[92m0.005327\u001b[0m\n",
      "iteration 1478, train loss: 0.008371, validation loss: 0.005464\n",
      "iteration 1479, train loss: 0.008691, validation loss: 0.005333\n",
      "iteration 1480, train loss: 0.008146, validation loss: 0.005379\n",
      "iteration 1481, train loss: 0.007925, validation loss: 0.005563\n",
      "iteration 1482, train loss: 0.008031, validation loss: 0.005636\n",
      "iteration 1483, train loss: 0.008022, validation loss: 0.005553\n",
      "iteration 1484, train loss: 0.00904, validation loss: 0.005555\n",
      "iteration 1485, train loss: 0.008327, validation loss: 0.005496\n",
      "iteration 1486, train loss: 0.008964, validation loss: 0.00546\n",
      "iteration 1487, train loss: 0.008424, validation loss: 0.005544\n",
      "iteration 1488, train loss: 0.008497, validation loss: 0.005475\n",
      "iteration 1489, train loss: 0.008147, validation loss: 0.005396\n",
      "iteration 1490, train loss: 0.00861, validation loss: 0.005438\n",
      "iteration 1491, train loss: 0.009194, validation loss: 0.005346\n",
      "iteration 1492, train loss: 0.008133, validation loss: 0.0054\n",
      "iteration 1493, train loss: 0.008093, validation loss: 0.005455\n",
      "iteration 1494, train loss: 0.008439, validation loss: 0.005407\n",
      "iteration 1495, train loss: 0.008101, validation loss: 0.005459\n",
      "iteration 1496, train loss: 0.008269, validation loss: 0.00566\n",
      "iteration 1497, train loss: 0.009254, validation loss: 0.005428\n",
      "iteration 1498, train loss: 0.008352, validation loss: 0.005631\n",
      "iteration 1499, train loss: 0.008629, validation loss: 0.005809\n",
      "iteration 1500, train loss: 0.008602, validation loss: 0.00567\n",
      "iteration 1501, train loss: 0.008595, validation loss: 0.005375\n",
      "iteration 1502, train loss: 0.0084, validation loss: \u001b[92m0.005302\u001b[0m\n",
      "iteration 1503, train loss: 0.007815, validation loss: 0.005474\n",
      "iteration 1504, train loss: 0.008691, validation loss: \u001b[92m0.005296\u001b[0m\n",
      "iteration 1505, train loss: 0.00846, validation loss: 0.005769\n",
      "iteration 1506, train loss: 0.009024, validation loss: 0.005962\n",
      "iteration 1507, train loss: 0.00864, validation loss: 0.005699\n",
      "iteration 1508, train loss: 0.008398, validation loss: 0.005575\n",
      "iteration 1509, train loss: 0.008409, validation loss: 0.005353\n",
      "iteration 1510, train loss: 0.008185, validation loss: 0.005401\n",
      "iteration 1511, train loss: 0.007869, validation loss: 0.005427\n",
      "iteration 1512, train loss: 0.008175, validation loss: 0.005302\n",
      "iteration 1513, train loss: 0.008875, validation loss: \u001b[92m0.005287\u001b[0m\n",
      "iteration 1514, train loss: 0.008262, validation loss: 0.005339\n",
      "iteration 1515, train loss: 0.008456, validation loss: 0.005353\n",
      "iteration 1516, train loss: 0.008222, validation loss: 0.005328\n",
      "iteration 1517, train loss: 0.00832, validation loss: 0.005351\n",
      "iteration 1518, train loss: 0.008275, validation loss: \u001b[92m0.005262\u001b[0m\n",
      "iteration 1519, train loss: 0.008372, validation loss: 0.005287\n",
      "iteration 1520, train loss: 0.008587, validation loss: \u001b[92m0.005247\u001b[0m\n",
      "iteration 1521, train loss: 0.008488, validation loss: 0.005495\n",
      "iteration 1522, train loss: 0.00841, validation loss: 0.005398\n",
      "iteration 1523, train loss: 0.007885, validation loss: 0.005327\n",
      "iteration 1524, train loss: 0.008547, validation loss: 0.00536\n",
      "iteration 1525, train loss: 0.008164, validation loss: 0.00528\n",
      "iteration 1526, train loss: 0.008147, validation loss: 0.005314\n",
      "iteration 1527, train loss: 0.007942, validation loss: 0.005451\n",
      "iteration 1528, train loss: 0.008139, validation loss: 0.005294\n",
      "iteration 1529, train loss: 0.008055, validation loss: \u001b[92m0.005244\u001b[0m\n",
      "iteration 1530, train loss: 0.008316, validation loss: 0.005336\n",
      "iteration 1531, train loss: 0.007988, validation loss: 0.005492\n",
      "iteration 1532, train loss: 0.008379, validation loss: 0.005345\n",
      "iteration 1533, train loss: 0.008258, validation loss: 0.005326\n",
      "iteration 1534, train loss: 0.008307, validation loss: 0.005322\n",
      "iteration 1535, train loss: 0.008353, validation loss: 0.005494\n",
      "iteration 1536, train loss: \u001b[92m0.007631\u001b[0m, validation loss: 0.005666\n",
      "iteration 1537, train loss: 0.008191, validation loss: 0.00566\n",
      "iteration 1538, train loss: 0.007761, validation loss: 0.005497\n",
      "iteration 1539, train loss: 0.007797, validation loss: \u001b[92m0.005237\u001b[0m\n",
      "iteration 1540, train loss: 0.00792, validation loss: 0.005301\n",
      "iteration 1541, train loss: 0.008478, validation loss: 0.005276\n",
      "iteration 1542, train loss: 0.007634, validation loss: 0.005725\n",
      "iteration 1543, train loss: 0.00872, validation loss: 0.005896\n",
      "iteration 1544, train loss: 0.008545, validation loss: 0.005481\n",
      "iteration 1545, train loss: 0.007913, validation loss: 0.005312\n",
      "iteration 1546, train loss: 0.008229, validation loss: 0.005341\n",
      "iteration 1547, train loss: 0.008344, validation loss: 0.005393\n",
      "iteration 1548, train loss: 0.00778, validation loss: 0.005554\n",
      "iteration 1549, train loss: 0.007795, validation loss: 0.005537\n",
      "iteration 1550, train loss: 0.008582, validation loss: \u001b[92m0.005228\u001b[0m\n",
      "iteration 1551, train loss: 0.007919, validation loss: 0.005379\n",
      "iteration 1552, train loss: 0.008001, validation loss: 0.005445\n",
      "iteration 1553, train loss: 0.007862, validation loss: \u001b[92m0.005223\u001b[0m\n",
      "iteration 1554, train loss: 0.007695, validation loss: 0.005457\n",
      "iteration 1555, train loss: 0.008289, validation loss: 0.005381\n",
      "iteration 1556, train loss: 0.008274, validation loss: 0.005249\n",
      "iteration 1557, train loss: 0.007844, validation loss: 0.005235\n",
      "iteration 1558, train loss: 0.008047, validation loss: \u001b[92m0.005216\u001b[0m\n",
      "iteration 1559, train loss: 0.008022, validation loss: 0.005242\n",
      "iteration 1560, train loss: 0.007788, validation loss: 0.005293\n",
      "iteration 1561, train loss: 0.007681, validation loss: 0.005242\n",
      "iteration 1562, train loss: 0.00828, validation loss: \u001b[92m0.005172\u001b[0m\n",
      "iteration 1563, train loss: 0.008229, validation loss: 0.005173\n",
      "iteration 1564, train loss: 0.008195, validation loss: 0.00525\n",
      "iteration 1565, train loss: 0.008264, validation loss: 0.005351\n",
      "iteration 1566, train loss: 0.008196, validation loss: 0.005316\n",
      "iteration 1567, train loss: 0.008181, validation loss: 0.005247\n",
      "iteration 1568, train loss: 0.008152, validation loss: 0.005173\n",
      "iteration 1569, train loss: 0.008221, validation loss: 0.005231\n",
      "iteration 1570, train loss: 0.008034, validation loss: 0.005258\n",
      "iteration 1571, train loss: 0.008687, validation loss: 0.005192\n",
      "iteration 1572, train loss: 0.008031, validation loss: 0.005239\n",
      "iteration 1573, train loss: 0.008073, validation loss: 0.005259\n",
      "iteration 1574, train loss: 0.008062, validation loss: 0.00522\n",
      "iteration 1575, train loss: 0.008237, validation loss: 0.005225\n",
      "iteration 1576, train loss: 0.008107, validation loss: 0.005277\n",
      "iteration 1577, train loss: \u001b[92m0.007631\u001b[0m, validation loss: \u001b[92m0.005156\u001b[0m\n",
      "iteration 1578, train loss: \u001b[92m0.007562\u001b[0m, validation loss: 0.005181\n",
      "iteration 1579, train loss: 0.008054, validation loss: 0.005205\n",
      "iteration 1580, train loss: 0.008545, validation loss: 0.005371\n",
      "iteration 1581, train loss: 0.007922, validation loss: 0.005721\n",
      "iteration 1582, train loss: 0.008026, validation loss: 0.005403\n",
      "iteration 1583, train loss: \u001b[92m0.007381\u001b[0m, validation loss: \u001b[92m0.00514\u001b[0m\n",
      "iteration 1584, train loss: 0.007559, validation loss: 0.005187\n",
      "iteration 1585, train loss: 0.007694, validation loss: 0.005157\n",
      "iteration 1586, train loss: 0.007721, validation loss: 0.005523\n",
      "iteration 1587, train loss: 0.007642, validation loss: 0.005647\n",
      "iteration 1588, train loss: 0.008252, validation loss: 0.005241\n",
      "iteration 1589, train loss: 0.00814, validation loss: 0.005143\n",
      "iteration 1590, train loss: 0.007587, validation loss: 0.005185\n",
      "iteration 1591, train loss: 0.007885, validation loss: \u001b[92m0.005136\u001b[0m\n",
      "iteration 1592, train loss: 0.008046, validation loss: 0.005139\n",
      "iteration 1593, train loss: 0.007965, validation loss: 0.00518\n",
      "iteration 1594, train loss: 0.008086, validation loss: 0.005319\n",
      "iteration 1595, train loss: 0.008665, validation loss: 0.005277\n",
      "iteration 1596, train loss: 0.008521, validation loss: \u001b[92m0.00513\u001b[0m\n",
      "iteration 1597, train loss: 0.007648, validation loss: 0.005449\n",
      "iteration 1598, train loss: 0.008596, validation loss: 0.005142\n",
      "iteration 1599, train loss: 0.007993, validation loss: 0.005507\n",
      "iteration 1600, train loss: 0.008523, validation loss: 0.005925\n",
      "iteration 1601, train loss: 0.008351, validation loss: 0.00565\n",
      "iteration 1602, train loss: 0.008356, validation loss: 0.0052\n",
      "iteration 1603, train loss: 0.007758, validation loss: 0.0053\n",
      "iteration 1604, train loss: 0.008284, validation loss: 0.005399\n",
      "iteration 1605, train loss: 0.008547, validation loss: 0.005145\n",
      "iteration 1606, train loss: 0.007582, validation loss: 0.005328\n",
      "iteration 1607, train loss: 0.007932, validation loss: 0.005399\n",
      "iteration 1608, train loss: 0.008615, validation loss: 0.005241\n",
      "iteration 1609, train loss: 0.008477, validation loss: 0.005209\n",
      "iteration 1610, train loss: 0.007886, validation loss: 0.005399\n",
      "iteration 1611, train loss: 0.008183, validation loss: 0.00526\n",
      "iteration 1612, train loss: 0.008208, validation loss: 0.005382\n",
      "iteration 1613, train loss: 0.0075, validation loss: 0.005527\n",
      "iteration 1614, train loss: 0.008046, validation loss: 0.005287\n",
      "iteration 1615, train loss: 0.008093, validation loss: 0.005204\n",
      "iteration 1616, train loss: 0.008387, validation loss: 0.005295\n",
      "iteration 1617, train loss: 0.008645, validation loss: 0.005172\n",
      "iteration 1618, train loss: 0.008254, validation loss: 0.005259\n",
      "iteration 1619, train loss: 0.007739, validation loss: 0.005296\n",
      "iteration 1620, train loss: 0.007939, validation loss: 0.005186\n",
      "iteration 1621, train loss: 0.008418, validation loss: 0.005183\n",
      "iteration 1622, train loss: 0.008245, validation loss: 0.005174\n",
      "iteration 1623, train loss: 0.008054, validation loss: 0.005259\n",
      "iteration 1624, train loss: 0.008594, validation loss: 0.005203\n",
      "iteration 1625, train loss: 0.008248, validation loss: 0.005208\n",
      "iteration 1626, train loss: 0.007929, validation loss: 0.005355\n",
      "iteration 1627, train loss: 0.008076, validation loss: 0.005422\n",
      "iteration 1628, train loss: 0.007744, validation loss: 0.005326\n",
      "iteration 1629, train loss: 0.007655, validation loss: 0.005253\n",
      "iteration 1630, train loss: 0.008307, validation loss: \u001b[92m0.005127\u001b[0m\n",
      "iteration 1631, train loss: 0.008157, validation loss: 0.005231\n",
      "iteration 1632, train loss: 0.008131, validation loss: 0.005168\n",
      "iteration 1633, train loss: 0.00785, validation loss: 0.005202\n",
      "iteration 1634, train loss: 0.007957, validation loss: \u001b[92m0.005122\u001b[0m\n",
      "iteration 1635, train loss: 0.008375, validation loss: \u001b[92m0.005095\u001b[0m\n",
      "iteration 1636, train loss: 0.008216, validation loss: 0.00517\n",
      "iteration 1637, train loss: 0.008094, validation loss: 0.005204\n",
      "iteration 1638, train loss: 0.007612, validation loss: \u001b[92m0.005082\u001b[0m\n",
      "iteration 1639, train loss: 0.007542, validation loss: 0.005213\n",
      "iteration 1640, train loss: 0.008185, validation loss: 0.00534\n",
      "iteration 1641, train loss: 0.008026, validation loss: 0.005254\n",
      "iteration 1642, train loss: 0.008192, validation loss: 0.005361\n",
      "iteration 1643, train loss: 0.008346, validation loss: 0.005115\n",
      "iteration 1644, train loss: 0.008104, validation loss: 0.005099\n",
      "iteration 1645, train loss: 0.008071, validation loss: \u001b[92m0.005074\u001b[0m\n",
      "iteration 1646, train loss: 0.008199, validation loss: 0.005186\n",
      "iteration 1647, train loss: 0.007682, validation loss: 0.005164\n",
      "iteration 1648, train loss: 0.008216, validation loss: \u001b[92m0.005068\u001b[0m\n",
      "iteration 1649, train loss: 0.007621, validation loss: 0.005124\n",
      "iteration 1650, train loss: 0.007759, validation loss: 0.005113\n",
      "iteration 1651, train loss: 0.007884, validation loss: 0.005174\n",
      "iteration 1652, train loss: 0.00782, validation loss: 0.005408\n",
      "iteration 1653, train loss: 0.008287, validation loss: 0.005289\n",
      "iteration 1654, train loss: 0.007734, validation loss: 0.005178\n",
      "iteration 1655, train loss: 0.007754, validation loss: 0.005157\n",
      "iteration 1656, train loss: 0.007617, validation loss: 0.005186\n",
      "iteration 1657, train loss: 0.00842, validation loss: 0.005244\n",
      "iteration 1658, train loss: 0.008693, validation loss: 0.005639\n",
      "iteration 1659, train loss: 0.008234, validation loss: 0.005307\n",
      "iteration 1660, train loss: 0.007962, validation loss: 0.00516\n",
      "iteration 1661, train loss: 0.008105, validation loss: 0.005277\n",
      "iteration 1662, train loss: 0.007835, validation loss: 0.005132\n",
      "iteration 1663, train loss: 0.007641, validation loss: 0.005601\n",
      "iteration 1664, train loss: 0.008512, validation loss: 0.005766\n",
      "iteration 1665, train loss: 0.00798, validation loss: 0.005505\n",
      "iteration 1666, train loss: 0.007482, validation loss: 0.005114\n",
      "iteration 1667, train loss: 0.008257, validation loss: 0.00535\n",
      "iteration 1668, train loss: 0.008746, validation loss: 0.005182\n",
      "iteration 1669, train loss: 0.008261, validation loss: 0.005509\n",
      "iteration 1670, train loss: 0.007732, validation loss: 0.005922\n",
      "iteration 1671, train loss: 0.008528, validation loss: 0.005528\n",
      "iteration 1672, train loss: 0.007882, validation loss: 0.005163\n",
      "iteration 1673, train loss: 0.008593, validation loss: 0.005168\n",
      "iteration 1674, train loss: 0.007851, validation loss: 0.005677\n",
      "iteration 1675, train loss: 0.009618, validation loss: 0.00523\n",
      "iteration 1676, train loss: 0.007879, validation loss: 0.005947\n",
      "iteration 1677, train loss: 0.008533, validation loss: 0.005776\n",
      "iteration 1678, train loss: 0.008184, validation loss: 0.005388\n",
      "iteration 1679, train loss: 0.008016, validation loss: 0.005584\n",
      "iteration 1680, train loss: 0.008094, validation loss: 0.005481\n",
      "iteration 1681, train loss: 0.008563, validation loss: 0.005309\n",
      "iteration 1682, train loss: 0.008645, validation loss: 0.005584\n",
      "iteration 1683, train loss: 0.008258, validation loss: 0.005748\n",
      "iteration 1684, train loss: 0.007891, validation loss: 0.005367\n",
      "iteration 1685, train loss: 0.008027, validation loss: 0.00529\n",
      "iteration 1686, train loss: 0.008164, validation loss: 0.005225\n",
      "iteration 1687, train loss: 0.007711, validation loss: 0.005274\n",
      "iteration 1688, train loss: 0.00762, validation loss: 0.005402\n",
      "iteration 1689, train loss: 0.008225, validation loss: 0.005483\n",
      "iteration 1690, train loss: 0.007851, validation loss: 0.005267\n",
      "iteration 1691, train loss: 0.008001, validation loss: 0.005166\n",
      "iteration 1692, train loss: 0.007503, validation loss: 0.005333\n",
      "iteration 1693, train loss: 0.008102, validation loss: 0.005233\n",
      "iteration 1694, train loss: 0.007806, validation loss: 0.005104\n",
      "iteration 1695, train loss: 0.008284, validation loss: \u001b[92m0.005064\u001b[0m\n",
      "iteration 1696, train loss: 0.008094, validation loss: 0.005227\n",
      "iteration 1697, train loss: 0.007763, validation loss: 0.005459\n",
      "iteration 1698, train loss: 0.008194, validation loss: 0.005307\n",
      "iteration 1699, train loss: 0.007968, validation loss: 0.005139\n",
      "iteration 1700, train loss: 0.007977, validation loss: 0.00507\n",
      "iteration 1701, train loss: 0.007841, validation loss: 0.005079\n",
      "iteration 1702, train loss: 0.008195, validation loss: 0.005129\n",
      "iteration 1703, train loss: 0.00795, validation loss: 0.005146\n",
      "iteration 1704, train loss: 0.008165, validation loss: 0.005182\n",
      "iteration 1705, train loss: 0.007632, validation loss: 0.005212\n",
      "iteration 1706, train loss: \u001b[92m0.007028\u001b[0m, validation loss: 0.005154\n",
      "iteration 1707, train loss: 0.008412, validation loss: 0.005136\n",
      "iteration 1708, train loss: 0.007703, validation loss: 0.00512\n",
      "iteration 1709, train loss: 0.007493, validation loss: \u001b[92m0.005054\u001b[0m\n",
      "iteration 1710, train loss: 0.0078, validation loss: 0.005246\n",
      "iteration 1711, train loss: 0.007646, validation loss: 0.005502\n",
      "iteration 1712, train loss: 0.007662, validation loss: 0.005473\n",
      "iteration 1713, train loss: 0.008093, validation loss: 0.005219\n",
      "iteration 1714, train loss: 0.008006, validation loss: \u001b[92m0.005041\u001b[0m\n",
      "iteration 1715, train loss: 0.007345, validation loss: \u001b[92m0.004998\u001b[0m\n",
      "iteration 1716, train loss: 0.008094, validation loss: 0.005041\n",
      "iteration 1717, train loss: 0.007974, validation loss: 0.005031\n",
      "iteration 1718, train loss: 0.007922, validation loss: 0.005054\n",
      "iteration 1719, train loss: 0.00825, validation loss: 0.005136\n",
      "iteration 1720, train loss: 0.007852, validation loss: 0.005109\n",
      "iteration 1721, train loss: 0.008248, validation loss: \u001b[92m0.004996\u001b[0m\n",
      "iteration 1722, train loss: 0.007746, validation loss: \u001b[92m0.004995\u001b[0m\n",
      "iteration 1723, train loss: 0.007638, validation loss: 0.005027\n",
      "iteration 1724, train loss: 0.007422, validation loss: 0.005087\n",
      "iteration 1725, train loss: 0.007799, validation loss: 0.005052\n",
      "iteration 1726, train loss: 0.007757, validation loss: 0.005105\n",
      "iteration 1727, train loss: 0.007777, validation loss: 0.005177\n",
      "iteration 1728, train loss: 0.008299, validation loss: 0.005145\n",
      "iteration 1729, train loss: 0.007103, validation loss: 0.005073\n",
      "iteration 1730, train loss: 0.008245, validation loss: 0.00502\n",
      "iteration 1731, train loss: 0.007471, validation loss: 0.005006\n",
      "iteration 1732, train loss: 0.007543, validation loss: 0.005004\n",
      "iteration 1733, train loss: 0.007501, validation loss: 0.005177\n",
      "iteration 1734, train loss: 0.007687, validation loss: 0.005137\n",
      "iteration 1735, train loss: 0.007864, validation loss: 0.005054\n",
      "iteration 1736, train loss: 0.00778, validation loss: 0.00509\n",
      "iteration 1737, train loss: 0.008262, validation loss: 0.005146\n",
      "iteration 1738, train loss: 0.007066, validation loss: 0.005299\n",
      "iteration 1739, train loss: 0.008107, validation loss: 0.005271\n",
      "iteration 1740, train loss: 0.007974, validation loss: 0.005134\n",
      "iteration 1741, train loss: 0.008108, validation loss: 0.005039\n",
      "iteration 1742, train loss: 0.007503, validation loss: 0.005237\n",
      "iteration 1743, train loss: 0.007866, validation loss: 0.005029\n",
      "iteration 1744, train loss: 0.007299, validation loss: 0.005049\n",
      "iteration 1745, train loss: 0.007336, validation loss: 0.005373\n",
      "iteration 1746, train loss: 0.008261, validation loss: 0.005293\n",
      "iteration 1747, train loss: 0.007884, validation loss: 0.005042\n",
      "iteration 1748, train loss: 0.008001, validation loss: 0.005059\n",
      "iteration 1749, train loss: 0.007615, validation loss: 0.005063\n",
      "iteration 1750, train loss: 0.00715, validation loss: 0.0053\n",
      "iteration 1751, train loss: 0.007631, validation loss: 0.00542\n",
      "iteration 1752, train loss: 0.00799, validation loss: 0.004997\n",
      "iteration 1753, train loss: 0.007742, validation loss: 0.005104\n",
      "iteration 1754, train loss: 0.007859, validation loss: 0.005158\n",
      "iteration 1755, train loss: 0.007882, validation loss: 0.005316\n",
      "iteration 1756, train loss: 0.007597, validation loss: 0.005296\n",
      "iteration 1757, train loss: 0.007563, validation loss: 0.005109\n",
      "iteration 1758, train loss: 0.007653, validation loss: 0.005025\n",
      "iteration 1759, train loss: 0.007811, validation loss: 0.005035\n",
      "iteration 1760, train loss: 0.008096, validation loss: \u001b[92m0.004992\u001b[0m\n",
      "iteration 1761, train loss: 0.00729, validation loss: \u001b[92m0.00498\u001b[0m\n",
      "iteration 1762, train loss: 0.007599, validation loss: 0.005025\n",
      "iteration 1763, train loss: 0.00817, validation loss: 0.005106\n",
      "iteration 1764, train loss: 0.008125, validation loss: 0.005225\n",
      "iteration 1765, train loss: 0.007915, validation loss: 0.005041\n",
      "iteration 1766, train loss: 0.007827, validation loss: 0.0053\n",
      "iteration 1767, train loss: 0.008515, validation loss: 0.005146\n",
      "iteration 1768, train loss: 0.008086, validation loss: 0.005081\n",
      "iteration 1769, train loss: 0.007393, validation loss: 0.005395\n",
      "iteration 1770, train loss: 0.007734, validation loss: 0.005563\n",
      "iteration 1771, train loss: 0.007797, validation loss: 0.005142\n",
      "iteration 1772, train loss: 0.007436, validation loss: 0.005051\n",
      "iteration 1773, train loss: 0.008036, validation loss: 0.005066\n",
      "iteration 1774, train loss: 0.007486, validation loss: \u001b[92m0.004965\u001b[0m\n",
      "iteration 1775, train loss: 0.007917, validation loss: 0.005373\n",
      "iteration 1776, train loss: 0.008171, validation loss: 0.005521\n",
      "iteration 1777, train loss: 0.007494, validation loss: 0.005128\n",
      "iteration 1778, train loss: 0.00773, validation loss: 0.005051\n",
      "iteration 1779, train loss: 0.00788, validation loss: 0.005162\n",
      "iteration 1780, train loss: 0.007761, validation loss: \u001b[92m0.004963\u001b[0m\n",
      "iteration 1781, train loss: 0.007547, validation loss: 0.005142\n",
      "iteration 1782, train loss: 0.007409, validation loss: 0.005139\n",
      "iteration 1783, train loss: 0.008086, validation loss: 0.004978\n",
      "iteration 1784, train loss: 0.007571, validation loss: 0.005188\n",
      "iteration 1785, train loss: 0.008407, validation loss: 0.005112\n",
      "iteration 1786, train loss: 0.00782, validation loss: 0.005027\n",
      "iteration 1787, train loss: 0.007619, validation loss: 0.005517\n",
      "iteration 1788, train loss: 0.007649, validation loss: 0.005402\n",
      "iteration 1789, train loss: 0.007561, validation loss: 0.005017\n",
      "iteration 1790, train loss: 0.007583, validation loss: 0.00501\n",
      "iteration 1791, train loss: 0.007578, validation loss: 0.004986\n",
      "iteration 1792, train loss: 0.007412, validation loss: \u001b[92m0.004912\u001b[0m\n",
      "iteration 1793, train loss: 0.00767, validation loss: 0.004948\n",
      "iteration 1794, train loss: 0.007075, validation loss: 0.004964\n",
      "iteration 1795, train loss: 0.007708, validation loss: 0.004978\n",
      "iteration 1796, train loss: 0.008062, validation loss: 0.004935\n",
      "iteration 1797, train loss: 0.007766, validation loss: 0.004972\n",
      "iteration 1798, train loss: 0.007621, validation loss: 0.004969\n",
      "iteration 1799, train loss: 0.007511, validation loss: 0.004943\n",
      "iteration 1800, train loss: 0.007653, validation loss: 0.004949\n",
      "iteration 1801, train loss: 0.00743, validation loss: 0.005008\n",
      "iteration 1802, train loss: 0.007599, validation loss: 0.004914\n",
      "iteration 1803, train loss: 0.007504, validation loss: 0.004924\n",
      "iteration 1804, train loss: 0.007432, validation loss: 0.005\n",
      "iteration 1805, train loss: 0.007634, validation loss: 0.005196\n",
      "iteration 1806, train loss: 0.007964, validation loss: 0.005149\n",
      "iteration 1807, train loss: 0.007882, validation loss: 0.005016\n",
      "iteration 1808, train loss: 0.007976, validation loss: 0.004978\n",
      "iteration 1809, train loss: 0.007687, validation loss: 0.004945\n",
      "iteration 1810, train loss: 0.007904, validation loss: \u001b[92m0.004898\u001b[0m\n",
      "iteration 1811, train loss: 0.007107, validation loss: 0.005013\n",
      "iteration 1812, train loss: 0.007359, validation loss: 0.005102\n",
      "iteration 1813, train loss: 0.008251, validation loss: 0.005008\n",
      "iteration 1814, train loss: 0.007853, validation loss: 0.00491\n",
      "iteration 1815, train loss: 0.007187, validation loss: 0.004979\n",
      "iteration 1816, train loss: 0.007619, validation loss: 0.005149\n",
      "iteration 1817, train loss: 0.008221, validation loss: 0.005059\n",
      "iteration 1818, train loss: 0.007692, validation loss: 0.004952\n",
      "iteration 1819, train loss: 0.007822, validation loss: 0.005111\n",
      "iteration 1820, train loss: 0.008102, validation loss: 0.005078\n",
      "iteration 1821, train loss: 0.007341, validation loss: 0.005177\n",
      "iteration 1822, train loss: 0.007497, validation loss: 0.005325\n",
      "iteration 1823, train loss: 0.00786, validation loss: 0.004952\n",
      "iteration 1824, train loss: 0.007808, validation loss: \u001b[92m0.004896\u001b[0m\n",
      "iteration 1825, train loss: 0.008084, validation loss: \u001b[92m0.004874\u001b[0m\n",
      "iteration 1826, train loss: 0.007427, validation loss: 0.004879\n",
      "iteration 1827, train loss: \u001b[92m0.006939\u001b[0m, validation loss: 0.004931\n",
      "iteration 1828, train loss: 0.007576, validation loss: 0.004963\n",
      "iteration 1829, train loss: 0.007506, validation loss: 0.00495\n",
      "iteration 1830, train loss: 0.007728, validation loss: 0.00489\n",
      "iteration 1831, train loss: 0.007437, validation loss: 0.004877\n",
      "iteration 1832, train loss: 0.00773, validation loss: 0.004995\n",
      "iteration 1833, train loss: 0.00774, validation loss: 0.004971\n",
      "iteration 1834, train loss: 0.007432, validation loss: 0.004956\n",
      "iteration 1835, train loss: 0.007905, validation loss: 0.004888\n",
      "iteration 1836, train loss: 0.008292, validation loss: 0.004904\n",
      "iteration 1837, train loss: 0.007654, validation loss: \u001b[92m0.004871\u001b[0m\n",
      "iteration 1838, train loss: 0.007402, validation loss: 0.004948\n",
      "iteration 1839, train loss: 0.007782, validation loss: 0.004912\n",
      "iteration 1840, train loss: 0.007394, validation loss: 0.004934\n",
      "iteration 1841, train loss: 0.007483, validation loss: \u001b[92m0.00485\u001b[0m\n",
      "iteration 1842, train loss: 0.007347, validation loss: 0.004946\n",
      "iteration 1843, train loss: 0.00807, validation loss: 0.004918\n",
      "iteration 1844, train loss: 0.007506, validation loss: 0.005054\n",
      "iteration 1845, train loss: 0.007866, validation loss: 0.005015\n",
      "iteration 1846, train loss: 0.007755, validation loss: 0.005112\n",
      "iteration 1847, train loss: 0.007464, validation loss: 0.0051\n",
      "iteration 1848, train loss: 0.007752, validation loss: 0.004928\n",
      "iteration 1849, train loss: 0.007628, validation loss: 0.005235\n",
      "iteration 1850, train loss: 0.008123, validation loss: 0.004873\n",
      "iteration 1851, train loss: 0.007729, validation loss: 0.005462\n",
      "iteration 1852, train loss: 0.007809, validation loss: 0.005751\n",
      "iteration 1853, train loss: 0.007482, validation loss: 0.005582\n",
      "iteration 1854, train loss: 0.007965, validation loss: 0.005079\n",
      "iteration 1855, train loss: 0.007341, validation loss: 0.004918\n",
      "iteration 1856, train loss: 0.007765, validation loss: 0.005138\n",
      "iteration 1857, train loss: 0.008176, validation loss: 0.005023\n",
      "iteration 1858, train loss: 0.00842, validation loss: 0.005248\n",
      "iteration 1859, train loss: 0.007651, validation loss: 0.005492\n",
      "iteration 1860, train loss: 0.007996, validation loss: 0.005378\n",
      "iteration 1861, train loss: 0.008407, validation loss: 0.005181\n",
      "iteration 1862, train loss: 0.007645, validation loss: 0.004969\n",
      "iteration 1863, train loss: 0.008086, validation loss: 0.004985\n",
      "iteration 1864, train loss: 0.007074, validation loss: 0.005536\n",
      "iteration 1865, train loss: 0.008217, validation loss: 0.00543\n",
      "iteration 1866, train loss: 0.008129, validation loss: 0.004998\n",
      "iteration 1867, train loss: 0.007427, validation loss: 0.004921\n",
      "iteration 1868, train loss: 0.007781, validation loss: 0.005116\n",
      "iteration 1869, train loss: 0.007884, validation loss: 0.005094\n",
      "iteration 1870, train loss: 0.007679, validation loss: 0.004969\n",
      "iteration 1871, train loss: 0.00769, validation loss: 0.00513\n",
      "iteration 1872, train loss: 0.007808, validation loss: 0.005083\n",
      "iteration 1873, train loss: 0.00725, validation loss: 0.00498\n",
      "iteration 1874, train loss: 0.008092, validation loss: 0.004878\n",
      "iteration 1875, train loss: 0.0073, validation loss: 0.004931\n",
      "iteration 1876, train loss: 0.00754, validation loss: 0.005102\n",
      "iteration 1877, train loss: 0.007188, validation loss: 0.005106\n",
      "iteration 1878, train loss: 0.007732, validation loss: 0.005042\n",
      "iteration 1879, train loss: 0.007518, validation loss: 0.004899\n",
      "iteration 1880, train loss: 0.007713, validation loss: 0.004874\n",
      "iteration 1881, train loss: 0.007405, validation loss: 0.004911\n",
      "iteration 1882, train loss: 0.007345, validation loss: 0.004947\n",
      "iteration 1883, train loss: 0.008019, validation loss: 0.004908\n",
      "iteration 1884, train loss: 0.007594, validation loss: 0.0049\n",
      "iteration 1885, train loss: 0.00712, validation loss: 0.004992\n",
      "iteration 1886, train loss: 0.007362, validation loss: 0.005074\n",
      "iteration 1887, train loss: 0.007521, validation loss: 0.005029\n",
      "iteration 1888, train loss: 0.008096, validation loss: 0.004909\n",
      "iteration 1889, train loss: 0.00724, validation loss: \u001b[92m0.004842\u001b[0m\n",
      "iteration 1890, train loss: 0.007553, validation loss: 0.004935\n",
      "iteration 1891, train loss: 0.00775, validation loss: 0.005017\n",
      "iteration 1892, train loss: 0.008131, validation loss: 0.004872\n",
      "iteration 1893, train loss: 0.007438, validation loss: 0.004863\n",
      "iteration 1894, train loss: 0.007641, validation loss: 0.00492\n",
      "iteration 1895, train loss: 0.007385, validation loss: 0.004921\n",
      "iteration 1896, train loss: 0.007458, validation loss: 0.004923\n",
      "iteration 1897, train loss: 0.007076, validation loss: 0.00496\n",
      "iteration 1898, train loss: 0.007384, validation loss: 0.004937\n",
      "iteration 1899, train loss: 0.007526, validation loss: 0.005063\n",
      "iteration 1900, train loss: 0.007465, validation loss: 0.00513\n",
      "iteration 1901, train loss: 0.007455, validation loss: 0.005073\n",
      "iteration 1902, train loss: 0.007332, validation loss: 0.004883\n",
      "iteration 1903, train loss: 0.0074, validation loss: 0.004943\n",
      "iteration 1904, train loss: 0.007692, validation loss: 0.005224\n",
      "iteration 1905, train loss: 0.008224, validation loss: 0.004849\n",
      "iteration 1906, train loss: 0.007733, validation loss: 0.005222\n",
      "iteration 1907, train loss: 0.007818, validation loss: 0.005475\n",
      "iteration 1908, train loss: 0.008334, validation loss: \u001b[92m0.004806\u001b[0m\n",
      "iteration 1909, train loss: 0.007628, validation loss: 0.004886\n",
      "iteration 1910, train loss: 0.007546, validation loss: 0.004922\n",
      "iteration 1911, train loss: 0.007561, validation loss: 0.0049\n",
      "iteration 1912, train loss: 0.007448, validation loss: 0.005158\n",
      "iteration 1913, train loss: 0.008283, validation loss: 0.005029\n",
      "iteration 1914, train loss: 0.007816, validation loss: \u001b[92m0.004763\u001b[0m\n",
      "iteration 1915, train loss: 0.007703, validation loss: 0.004792\n",
      "iteration 1916, train loss: 0.007718, validation loss: 0.004797\n",
      "iteration 1917, train loss: 0.007517, validation loss: 0.005089\n",
      "iteration 1918, train loss: 0.00739, validation loss: 0.005362\n",
      "iteration 1919, train loss: 0.007963, validation loss: 0.005061\n",
      "iteration 1920, train loss: 0.00781, validation loss: 0.004774\n",
      "iteration 1921, train loss: 0.007809, validation loss: 0.005008\n",
      "iteration 1922, train loss: 0.007554, validation loss: 0.005037\n",
      "iteration 1923, train loss: 0.007841, validation loss: 0.004786\n",
      "iteration 1924, train loss: 0.008038, validation loss: 0.005218\n",
      "iteration 1925, train loss: 0.008151, validation loss: 0.005103\n",
      "iteration 1926, train loss: 0.008074, validation loss: 0.004774\n",
      "iteration 1927, train loss: 0.007205, validation loss: 0.0048\n",
      "iteration 1928, train loss: 0.008049, validation loss: 0.004781\n",
      "iteration 1929, train loss: 0.007206, validation loss: \u001b[92m0.004762\u001b[0m\n",
      "iteration 1930, train loss: 0.007389, validation loss: 0.004829\n",
      "iteration 1931, train loss: 0.007205, validation loss: 0.004935\n",
      "iteration 1932, train loss: 0.00792, validation loss: 0.004795\n",
      "iteration 1933, train loss: 0.007523, validation loss: 0.004782\n",
      "iteration 1934, train loss: 0.007221, validation loss: 0.004786\n",
      "iteration 1935, train loss: 0.007925, validation loss: 0.004806\n",
      "iteration 1936, train loss: 0.007266, validation loss: 0.004868\n",
      "iteration 1937, train loss: 0.007476, validation loss: 0.004872\n",
      "iteration 1938, train loss: 0.007541, validation loss: 0.00483\n",
      "iteration 1939, train loss: 0.007653, validation loss: 0.004795\n",
      "iteration 1940, train loss: 0.007912, validation loss: 0.004792\n",
      "iteration 1941, train loss: 0.007587, validation loss: 0.004844\n",
      "iteration 1942, train loss: 0.006944, validation loss: 0.004872\n",
      "iteration 1943, train loss: 0.007868, validation loss: 0.004825\n",
      "iteration 1944, train loss: 0.007542, validation loss: 0.004839\n",
      "iteration 1945, train loss: 0.006944, validation loss: 0.004859\n",
      "iteration 1946, train loss: 0.007297, validation loss: 0.004803\n",
      "iteration 1947, train loss: 0.007146, validation loss: 0.004838\n",
      "iteration 1948, train loss: 0.008057, validation loss: 0.004777\n",
      "iteration 1949, train loss: 0.007675, validation loss: 0.005016\n",
      "iteration 1950, train loss: 0.007408, validation loss: 0.004999\n",
      "iteration 1951, train loss: 0.007501, validation loss: 0.004837\n",
      "iteration 1952, train loss: 0.007341, validation loss: 0.004866\n",
      "iteration 1953, train loss: 0.007833, validation loss: 0.004943\n",
      "iteration 1954, train loss: 0.007134, validation loss: 0.00488\n",
      "iteration 1955, train loss: 0.00735, validation loss: 0.005118\n",
      "iteration 1956, train loss: 0.007521, validation loss: 0.005204\n",
      "iteration 1957, train loss: 0.007567, validation loss: 0.004915\n",
      "iteration 1958, train loss: 0.007635, validation loss: 0.004821\n",
      "iteration 1959, train loss: 0.007237, validation loss: 0.004892\n",
      "iteration 1960, train loss: 0.008267, validation loss: 0.004857\n",
      "iteration 1961, train loss: 0.007479, validation loss: 0.005113\n",
      "iteration 1962, train loss: 0.007573, validation loss: 0.005086\n",
      "iteration 1963, train loss: 0.00759, validation loss: 0.005022\n",
      "iteration 1964, train loss: 0.007747, validation loss: 0.004879\n",
      "iteration 1965, train loss: 0.00766, validation loss: 0.004781\n",
      "iteration 1966, train loss: 0.007761, validation loss: 0.00481\n",
      "iteration 1967, train loss: 0.00765, validation loss: 0.005345\n",
      "iteration 1968, train loss: 0.00754, validation loss: 0.005003\n",
      "iteration 1969, train loss: 0.007071, validation loss: \u001b[92m0.004734\u001b[0m\n",
      "iteration 1970, train loss: 0.0075, validation loss: 0.004902\n",
      "iteration 1971, train loss: 0.00788, validation loss: 0.00489\n",
      "iteration 1972, train loss: 0.007591, validation loss: 0.00499\n",
      "iteration 1973, train loss: 0.007347, validation loss: 0.005084\n",
      "iteration 1974, train loss: 0.007263, validation loss: 0.004984\n",
      "iteration 1975, train loss: 0.008077, validation loss: 0.004766\n",
      "iteration 1976, train loss: 0.00769, validation loss: 0.004802\n",
      "iteration 1977, train loss: 0.007728, validation loss: 0.004752\n",
      "iteration 1978, train loss: 0.007862, validation loss: 0.004839\n",
      "iteration 1979, train loss: 0.006999, validation loss: 0.005083\n",
      "iteration 1980, train loss: 0.007712, validation loss: 0.005052\n",
      "iteration 1981, train loss: 0.007342, validation loss: 0.004855\n",
      "iteration 1982, train loss: 0.007417, validation loss: 0.004751\n",
      "iteration 1983, train loss: 0.007616, validation loss: 0.004762\n",
      "iteration 1984, train loss: 0.007696, validation loss: 0.004979\n",
      "iteration 1985, train loss: 0.007319, validation loss: 0.004989\n",
      "iteration 1986, train loss: 0.007311, validation loss: 0.004904\n",
      "iteration 1987, train loss: 0.007467, validation loss: 0.004749\n",
      "iteration 1988, train loss: 0.007627, validation loss: 0.004737\n",
      "iteration 1989, train loss: 0.007372, validation loss: 0.004813\n",
      "iteration 1990, train loss: 0.007613, validation loss: 0.004747\n",
      "iteration 1991, train loss: 0.007531, validation loss: 0.004863\n",
      "iteration 1992, train loss: 0.007537, validation loss: 0.005096\n",
      "iteration 1993, train loss: 0.007364, validation loss: 0.005033\n",
      "iteration 1994, train loss: 0.007459, validation loss: 0.004931\n",
      "iteration 1995, train loss: 0.007352, validation loss: 0.004852\n",
      "iteration 1996, train loss: 0.0074, validation loss: 0.004784\n",
      "iteration 1997, train loss: 0.007267, validation loss: 0.004842\n",
      "iteration 1998, train loss: 0.007502, validation loss: 0.00485\n",
      "iteration 1999, train loss: 0.007522, validation loss: 0.00486\n",
      "iteration 2000, train loss: 0.007692, validation loss: 0.004874\n",
      "iteration 2001, train loss: 0.007152, validation loss: 0.005037\n",
      "iteration 2002, train loss: 0.007503, validation loss: 0.004958\n",
      "iteration 2003, train loss: 0.007682, validation loss: 0.004782\n",
      "iteration 2004, train loss: 0.007161, validation loss: 0.004866\n",
      "iteration 2005, train loss: 0.007641, validation loss: 0.004934\n",
      "iteration 2006, train loss: 0.007172, validation loss: 0.004836\n",
      "iteration 2007, train loss: 0.007367, validation loss: 0.004741\n",
      "iteration 2008, train loss: 0.007358, validation loss: 0.004926\n",
      "iteration 2009, train loss: 0.007765, validation loss: 0.005196\n",
      "iteration 2010, train loss: 0.007141, validation loss: 0.005252\n",
      "iteration 2011, train loss: 0.007542, validation loss: 0.004816\n",
      "iteration 2012, train loss: 0.007424, validation loss: 0.004841\n",
      "iteration 2013, train loss: 0.00718, validation loss: 0.005069\n",
      "iteration 2014, train loss: 0.007758, validation loss: \u001b[92m0.004722\u001b[0m\n",
      "iteration 2015, train loss: 0.007281, validation loss: 0.005167\n",
      "iteration 2016, train loss: 0.007894, validation loss: 0.005408\n",
      "iteration 2017, train loss: 0.007212, validation loss: 0.005073\n",
      "iteration 2018, train loss: 0.007944, validation loss: \u001b[92m0.004715\u001b[0m\n",
      "iteration 2019, train loss: \u001b[92m0.006833\u001b[0m, validation loss: 0.004961\n",
      "iteration 2020, train loss: 0.007634, validation loss: 0.004893\n",
      "iteration 2021, train loss: 0.007455, validation loss: 0.004876\n",
      "iteration 2022, train loss: 0.007608, validation loss: 0.005318\n",
      "iteration 2023, train loss: 0.008215, validation loss: 0.005242\n",
      "iteration 2024, train loss: 0.007566, validation loss: 0.005033\n",
      "iteration 2025, train loss: 0.007911, validation loss: 0.004849\n",
      "iteration 2026, train loss: 0.007147, validation loss: 0.004767\n",
      "iteration 2027, train loss: 0.007179, validation loss: 0.004823\n",
      "iteration 2028, train loss: 0.007273, validation loss: 0.004944\n",
      "iteration 2029, train loss: 0.006883, validation loss: 0.004798\n",
      "iteration 2030, train loss: 0.007135, validation loss: 0.004732\n",
      "iteration 2031, train loss: 0.00721, validation loss: 0.004769\n",
      "iteration 2032, train loss: 0.007933, validation loss: 0.004726\n",
      "iteration 2033, train loss: 0.007762, validation loss: 0.004913\n",
      "iteration 2034, train loss: 0.007849, validation loss: 0.005038\n",
      "iteration 2035, train loss: 0.007284, validation loss: 0.00503\n",
      "iteration 2036, train loss: 0.007805, validation loss: 0.004836\n",
      "iteration 2037, train loss: \u001b[92m0.006692\u001b[0m, validation loss: 0.004863\n",
      "iteration 2038, train loss: 0.006822, validation loss: 0.004892\n",
      "iteration 2039, train loss: 0.007753, validation loss: \u001b[92m0.004684\u001b[0m\n",
      "iteration 2040, train loss: 0.00762, validation loss: 0.004896\n",
      "iteration 2041, train loss: 0.007361, validation loss: 0.005102\n",
      "iteration 2042, train loss: 0.007584, validation loss: 0.004896\n",
      "iteration 2043, train loss: 0.006913, validation loss: 0.004815\n",
      "iteration 2044, train loss: 0.006815, validation loss: 0.004869\n",
      "iteration 2045, train loss: 0.007766, validation loss: 0.004823\n",
      "iteration 2046, train loss: 0.008447, validation loss: 0.004882\n",
      "iteration 2047, train loss: 0.00758, validation loss: 0.00483\n",
      "iteration 2048, train loss: 0.007438, validation loss: 0.004789\n",
      "iteration 2049, train loss: 0.00779, validation loss: 0.005002\n",
      "iteration 2050, train loss: 0.008191, validation loss: 0.005079\n",
      "iteration 2051, train loss: 0.007611, validation loss: 0.005038\n",
      "iteration 2052, train loss: 0.007476, validation loss: 0.004732\n",
      "iteration 2053, train loss: 0.007904, validation loss: 0.00501\n",
      "iteration 2054, train loss: 0.007276, validation loss: 0.005288\n",
      "iteration 2055, train loss: 0.007772, validation loss: 0.004802\n",
      "iteration 2056, train loss: 0.007114, validation loss: 0.005119\n",
      "iteration 2057, train loss: 0.007729, validation loss: 0.005114\n",
      "iteration 2058, train loss: 0.007909, validation loss: 0.004811\n",
      "iteration 2059, train loss: 0.007225, validation loss: 0.005524\n",
      "iteration 2060, train loss: 0.008102, validation loss: 0.005697\n",
      "iteration 2061, train loss: 0.007982, validation loss: 0.005013\n",
      "iteration 2062, train loss: 0.007305, validation loss: 0.004822\n",
      "iteration 2063, train loss: 0.007584, validation loss: 0.005027\n",
      "iteration 2064, train loss: 0.007938, validation loss: 0.004726\n",
      "iteration 2065, train loss: 0.007296, validation loss: 0.004836\n",
      "iteration 2066, train loss: 0.007459, validation loss: 0.005007\n",
      "iteration 2067, train loss: 0.007824, validation loss: 0.004826\n",
      "iteration 2068, train loss: 0.006964, validation loss: 0.00473\n",
      "iteration 2069, train loss: 0.007076, validation loss: \u001b[92m0.004681\u001b[0m\n",
      "iteration 2070, train loss: 0.00725, validation loss: 0.004839\n",
      "iteration 2071, train loss: 0.007041, validation loss: 0.004946\n",
      "iteration 2072, train loss: 0.007711, validation loss: 0.004895\n",
      "iteration 2073, train loss: 0.007423, validation loss: 0.004818\n",
      "iteration 2074, train loss: 0.006856, validation loss: 0.00482\n",
      "iteration 2075, train loss: 0.007015, validation loss: 0.004933\n",
      "iteration 2076, train loss: 0.007478, validation loss: 0.004988\n",
      "iteration 2077, train loss: 0.007369, validation loss: 0.004791\n",
      "iteration 2078, train loss: 0.007283, validation loss: 0.00476\n",
      "iteration 2079, train loss: 0.007357, validation loss: 0.004716\n",
      "iteration 2080, train loss: 0.007126, validation loss: 0.004856\n",
      "iteration 2081, train loss: 0.006851, validation loss: 0.004944\n",
      "iteration 2082, train loss: 0.007413, validation loss: 0.004823\n",
      "iteration 2083, train loss: 0.007554, validation loss: 0.00472\n",
      "iteration 2084, train loss: 0.007111, validation loss: 0.00476\n",
      "iteration 2085, train loss: 0.007375, validation loss: 0.004734\n",
      "iteration 2086, train loss: 0.007324, validation loss: 0.00488\n",
      "iteration 2087, train loss: 0.007543, validation loss: 0.005164\n",
      "iteration 2088, train loss: 0.007051, validation loss: 0.005014\n",
      "iteration 2089, train loss: 0.007627, validation loss: 0.004784\n",
      "iteration 2090, train loss: 0.007648, validation loss: 0.004785\n",
      "iteration 2091, train loss: 0.007529, validation loss: 0.004758\n",
      "iteration 2092, train loss: 0.006815, validation loss: 0.004776\n",
      "iteration 2093, train loss: 0.007122, validation loss: 0.004871\n",
      "iteration 2094, train loss: 0.007371, validation loss: 0.004821\n",
      "iteration 2095, train loss: 0.00731, validation loss: 0.004837\n",
      "iteration 2096, train loss: 0.007404, validation loss: 0.004733\n",
      "iteration 2097, train loss: 0.006901, validation loss: 0.004866\n",
      "iteration 2098, train loss: 0.007673, validation loss: 0.004867\n",
      "iteration 2099, train loss: 0.007089, validation loss: 0.004793\n",
      "iteration 2100, train loss: 0.0073, validation loss: 0.004904\n",
      "iteration 2101, train loss: 0.007174, validation loss: 0.004781\n",
      "iteration 2102, train loss: 0.007177, validation loss: \u001b[92m0.004679\u001b[0m\n",
      "iteration 2103, train loss: 0.007388, validation loss: 0.004846\n",
      "iteration 2104, train loss: 0.007635, validation loss: 0.00477\n",
      "iteration 2105, train loss: 0.007398, validation loss: 0.004745\n",
      "iteration 2106, train loss: 0.007377, validation loss: 0.00512\n",
      "iteration 2107, train loss: 0.007299, validation loss: 0.005038\n",
      "iteration 2108, train loss: 0.007366, validation loss: 0.00479\n",
      "iteration 2109, train loss: 0.007063, validation loss: 0.004747\n",
      "iteration 2110, train loss: 0.006879, validation loss: 0.004746\n",
      "iteration 2111, train loss: 0.007464, validation loss: \u001b[92m0.004653\u001b[0m\n",
      "iteration 2112, train loss: \u001b[92m0.006557\u001b[0m, validation loss: 0.004665\n",
      "iteration 2113, train loss: 0.006808, validation loss: 0.004692\n",
      "iteration 2114, train loss: 0.00703, validation loss: 0.004701\n",
      "iteration 2115, train loss: 0.007693, validation loss: 0.004846\n",
      "iteration 2116, train loss: 0.007233, validation loss: 0.004879\n",
      "iteration 2117, train loss: 0.006664, validation loss: 0.004834\n",
      "iteration 2118, train loss: 0.007172, validation loss: 0.004661\n",
      "iteration 2119, train loss: 0.007125, validation loss: 0.004675\n",
      "iteration 2120, train loss: 0.007302, validation loss: 0.004655\n",
      "iteration 2121, train loss: 0.007327, validation loss: 0.004654\n",
      "iteration 2122, train loss: 0.006859, validation loss: 0.00473\n",
      "iteration 2123, train loss: 0.007094, validation loss: 0.004922\n",
      "iteration 2124, train loss: 0.007233, validation loss: 0.004926\n",
      "iteration 2125, train loss: 0.007639, validation loss: 0.004692\n",
      "iteration 2126, train loss: 0.007372, validation loss: \u001b[92m0.004653\u001b[0m\n",
      "iteration 2127, train loss: 0.006806, validation loss: 0.004667\n",
      "iteration 2128, train loss: 0.007014, validation loss: 0.004662\n",
      "iteration 2129, train loss: 0.007021, validation loss: 0.004656\n",
      "iteration 2130, train loss: 0.007081, validation loss: 0.004734\n",
      "iteration 2131, train loss: 0.007373, validation loss: 0.004678\n",
      "iteration 2132, train loss: 0.007227, validation loss: 0.004701\n",
      "iteration 2133, train loss: 0.007492, validation loss: 0.004719\n",
      "iteration 2134, train loss: 0.007027, validation loss: 0.004704\n",
      "iteration 2135, train loss: 0.007065, validation loss: 0.004687\n",
      "iteration 2136, train loss: 0.007261, validation loss: 0.004797\n",
      "iteration 2137, train loss: 0.007392, validation loss: 0.004862\n",
      "iteration 2138, train loss: 0.0072, validation loss: 0.004667\n",
      "iteration 2139, train loss: 0.00723, validation loss: 0.004786\n",
      "iteration 2140, train loss: 0.007328, validation loss: 0.004848\n",
      "iteration 2141, train loss: 0.00749, validation loss: 0.005069\n",
      "iteration 2142, train loss: 0.007183, validation loss: 0.0053\n",
      "iteration 2143, train loss: 0.00763, validation loss: 0.004983\n",
      "iteration 2144, train loss: 0.007345, validation loss: 0.004656\n",
      "iteration 2145, train loss: 0.006609, validation loss: 0.004857\n",
      "iteration 2146, train loss: 0.007396, validation loss: 0.004653\n",
      "iteration 2147, train loss: 0.006822, validation loss: 0.004801\n",
      "iteration 2148, train loss: 0.006942, validation loss: 0.004999\n",
      "iteration 2149, train loss: 0.007743, validation loss: 0.004982\n",
      "iteration 2150, train loss: 0.007335, validation loss: 0.004793\n",
      "iteration 2151, train loss: 0.006873, validation loss: 0.004668\n",
      "iteration 2152, train loss: 0.006932, validation loss: 0.004674\n",
      "iteration 2153, train loss: 0.007354, validation loss: 0.004658\n",
      "iteration 2154, train loss: 0.007105, validation loss: 0.004913\n",
      "iteration 2155, train loss: 0.007125, validation loss: 0.005055\n",
      "iteration 2156, train loss: 0.007426, validation loss: 0.004773\n",
      "iteration 2157, train loss: 0.007167, validation loss: 0.004693\n",
      "iteration 2158, train loss: 0.006925, validation loss: \u001b[92m0.004647\u001b[0m\n",
      "iteration 2159, train loss: 0.007036, validation loss: \u001b[92m0.004621\u001b[0m\n",
      "iteration 2160, train loss: 0.007169, validation loss: 0.004669\n",
      "iteration 2161, train loss: 0.006644, validation loss: 0.004765\n",
      "iteration 2162, train loss: 0.007895, validation loss: 0.004674\n",
      "iteration 2163, train loss: 0.007262, validation loss: \u001b[92m0.004615\u001b[0m\n",
      "iteration 2164, train loss: 0.007126, validation loss: 0.004654\n",
      "iteration 2165, train loss: 0.007328, validation loss: 0.004865\n",
      "iteration 2166, train loss: 0.006845, validation loss: 0.005037\n",
      "iteration 2167, train loss: 0.007497, validation loss: 0.004798\n",
      "iteration 2168, train loss: 0.007225, validation loss: 0.004639\n",
      "iteration 2169, train loss: 0.007234, validation loss: 0.004634\n",
      "iteration 2170, train loss: 0.007101, validation loss: 0.004627\n",
      "iteration 2171, train loss: 0.007122, validation loss: 0.004667\n",
      "iteration 2172, train loss: 0.006828, validation loss: 0.00472\n",
      "iteration 2173, train loss: 0.00689, validation loss: 0.004701\n",
      "iteration 2174, train loss: 0.007078, validation loss: 0.004615\n",
      "iteration 2175, train loss: 0.007294, validation loss: 0.004623\n",
      "iteration 2176, train loss: 0.007221, validation loss: 0.004652\n",
      "iteration 2177, train loss: 0.007601, validation loss: 0.004738\n",
      "iteration 2178, train loss: 0.007334, validation loss: 0.00471\n",
      "iteration 2179, train loss: 0.007006, validation loss: 0.004822\n",
      "iteration 2180, train loss: 0.007025, validation loss: 0.004794\n",
      "iteration 2181, train loss: 0.007194, validation loss: 0.004749\n",
      "iteration 2182, train loss: 0.006745, validation loss: 0.004717\n",
      "iteration 2183, train loss: 0.007178, validation loss: 0.004703\n",
      "iteration 2184, train loss: 0.00684, validation loss: 0.004828\n",
      "iteration 2185, train loss: 0.006788, validation loss: 0.004881\n",
      "iteration 2186, train loss: 0.007303, validation loss: 0.004735\n",
      "iteration 2187, train loss: 0.006917, validation loss: 0.004764\n",
      "iteration 2188, train loss: 0.007365, validation loss: 0.004744\n",
      "iteration 2189, train loss: 0.007584, validation loss: 0.004621\n",
      "iteration 2190, train loss: \u001b[92m0.006541\u001b[0m, validation loss: 0.00464\n",
      "iteration 2191, train loss: 0.007318, validation loss: 0.004781\n",
      "iteration 2192, train loss: 0.006956, validation loss: 0.004688\n",
      "iteration 2193, train loss: 0.007196, validation loss: 0.00462\n",
      "iteration 2194, train loss: 0.007093, validation loss: 0.004649\n",
      "iteration 2195, train loss: 0.007581, validation loss: 0.004718\n",
      "iteration 2196, train loss: 0.007067, validation loss: 0.004645\n",
      "iteration 2197, train loss: 0.007194, validation loss: \u001b[92m0.004601\u001b[0m\n",
      "iteration 2198, train loss: 0.007067, validation loss: 0.004603\n",
      "iteration 2199, train loss: 0.006603, validation loss: 0.004615\n",
      "iteration 2200, train loss: 0.006652, validation loss: 0.004656\n",
      "iteration 2201, train loss: 0.007031, validation loss: 0.004797\n",
      "iteration 2202, train loss: 0.006828, validation loss: 0.004693\n",
      "iteration 2203, train loss: 0.007041, validation loss: 0.004719\n",
      "iteration 2204, train loss: 0.007129, validation loss: 0.004806\n",
      "iteration 2205, train loss: 0.007147, validation loss: 0.004707\n",
      "iteration 2206, train loss: 0.007188, validation loss: 0.004695\n",
      "iteration 2207, train loss: 0.007104, validation loss: 0.005107\n",
      "iteration 2208, train loss: 0.007498, validation loss: 0.004952\n",
      "iteration 2209, train loss: 0.007629, validation loss: 0.004621\n",
      "iteration 2210, train loss: 0.007128, validation loss: 0.004797\n",
      "iteration 2211, train loss: 0.007377, validation loss: 0.004834\n",
      "iteration 2212, train loss: 0.007133, validation loss: 0.004699\n",
      "iteration 2213, train loss: 0.007297, validation loss: 0.004847\n",
      "iteration 2214, train loss: 0.007115, validation loss: 0.00518\n",
      "iteration 2215, train loss: 0.006842, validation loss: 0.005161\n",
      "iteration 2216, train loss: 0.0076, validation loss: 0.004665\n",
      "iteration 2217, train loss: 0.006698, validation loss: 0.004751\n",
      "iteration 2218, train loss: 0.007229, validation loss: 0.004787\n",
      "iteration 2219, train loss: 0.007225, validation loss: 0.004634\n",
      "iteration 2220, train loss: 0.00705, validation loss: 0.004958\n",
      "iteration 2221, train loss: 0.007289, validation loss: 0.005005\n",
      "iteration 2222, train loss: 0.007051, validation loss: 0.004921\n",
      "iteration 2223, train loss: 0.007152, validation loss: 0.004765\n",
      "iteration 2224, train loss: 0.00711, validation loss: 0.004772\n",
      "iteration 2225, train loss: \u001b[92m0.006426\u001b[0m, validation loss: 0.00479\n",
      "iteration 2226, train loss: 0.007079, validation loss: 0.004656\n",
      "iteration 2227, train loss: 0.006869, validation loss: 0.005136\n",
      "iteration 2228, train loss: 0.006975, validation loss: 0.005505\n",
      "iteration 2229, train loss: 0.008226, validation loss: 0.004877\n",
      "iteration 2230, train loss: 0.006513, validation loss: 0.004753\n",
      "iteration 2231, train loss: 0.00704, validation loss: 0.00481\n",
      "iteration 2232, train loss: 0.007038, validation loss: 0.004609\n",
      "iteration 2233, train loss: 0.006861, validation loss: 0.004652\n",
      "iteration 2234, train loss: 0.006995, validation loss: 0.004869\n",
      "iteration 2235, train loss: 0.007229, validation loss: 0.004639\n",
      "iteration 2236, train loss: 0.006833, validation loss: 0.004699\n",
      "iteration 2237, train loss: 0.007244, validation loss: 0.004839\n",
      "iteration 2238, train loss: 0.007079, validation loss: 0.004822\n",
      "iteration 2239, train loss: 0.007222, validation loss: 0.004748\n",
      "iteration 2240, train loss: 0.007075, validation loss: 0.004611\n",
      "iteration 2241, train loss: \u001b[92m0.006413\u001b[0m, validation loss: 0.004634\n",
      "iteration 2242, train loss: 0.006755, validation loss: 0.004711\n",
      "iteration 2243, train loss: 0.007361, validation loss: 0.004622\n",
      "iteration 2244, train loss: 0.00706, validation loss: 0.004618\n",
      "iteration 2245, train loss: 0.006913, validation loss: 0.004701\n",
      "iteration 2246, train loss: 0.006737, validation loss: 0.004735\n",
      "iteration 2247, train loss: 0.006846, validation loss: 0.004798\n",
      "iteration 2248, train loss: 0.006462, validation loss: 0.004756\n",
      "iteration 2249, train loss: 0.006665, validation loss: 0.004646\n",
      "iteration 2250, train loss: 0.006915, validation loss: 0.004652\n",
      "iteration 2251, train loss: 0.006723, validation loss: 0.004688\n",
      "iteration 2252, train loss: 0.007101, validation loss: 0.004692\n",
      "iteration 2253, train loss: 0.00706, validation loss: 0.004999\n",
      "iteration 2254, train loss: 0.007351, validation loss: 0.005\n",
      "iteration 2255, train loss: 0.007273, validation loss: 0.004651\n",
      "iteration 2256, train loss: \u001b[92m0.006189\u001b[0m, validation loss: \u001b[92m0.004566\u001b[0m\n",
      "iteration 2257, train loss: 0.00724, validation loss: 0.004673\n",
      "iteration 2258, train loss: 0.006601, validation loss: 0.004671\n",
      "iteration 2259, train loss: 0.007199, validation loss: 0.004674\n",
      "iteration 2260, train loss: 0.006788, validation loss: 0.004848\n",
      "iteration 2261, train loss: 0.007719, validation loss: 0.004642\n",
      "iteration 2262, train loss: 0.007213, validation loss: 0.004753\n",
      "iteration 2263, train loss: 0.007203, validation loss: 0.004751\n",
      "iteration 2264, train loss: 0.007112, validation loss: 0.004641\n",
      "iteration 2265, train loss: 0.006935, validation loss: 0.004651\n",
      "iteration 2266, train loss: 0.007334, validation loss: 0.004592\n",
      "iteration 2267, train loss: 0.006913, validation loss: 0.004815\n",
      "iteration 2268, train loss: 0.006683, validation loss: 0.004912\n",
      "iteration 2269, train loss: 0.007537, validation loss: 0.00465\n",
      "iteration 2270, train loss: 0.006934, validation loss: 0.004981\n",
      "iteration 2271, train loss: 0.007148, validation loss: 0.005044\n",
      "iteration 2272, train loss: 0.007524, validation loss: 0.00475\n",
      "iteration 2273, train loss: 0.006892, validation loss: 0.004582\n",
      "iteration 2274, train loss: 0.007149, validation loss: 0.00488\n",
      "iteration 2275, train loss: 0.006931, validation loss: 0.004762\n",
      "iteration 2276, train loss: 0.007301, validation loss: 0.004651\n",
      "iteration 2277, train loss: 0.007073, validation loss: 0.004981\n",
      "iteration 2278, train loss: 0.00688, validation loss: 0.005115\n",
      "iteration 2279, train loss: 0.007238, validation loss: 0.004903\n",
      "iteration 2280, train loss: 0.007101, validation loss: 0.004647\n",
      "iteration 2281, train loss: 0.006937, validation loss: 0.0047\n",
      "iteration 2282, train loss: 0.006637, validation loss: 0.004657\n",
      "iteration 2283, train loss: 0.007278, validation loss: 0.004678\n",
      "iteration 2284, train loss: 0.007054, validation loss: 0.004743\n",
      "iteration 2285, train loss: 0.006861, validation loss: 0.004664\n",
      "iteration 2286, train loss: 0.006558, validation loss: 0.004633\n",
      "iteration 2287, train loss: 0.007015, validation loss: 0.004636\n",
      "iteration 2288, train loss: 0.00706, validation loss: 0.004653\n",
      "iteration 2289, train loss: 0.007245, validation loss: 0.004605\n",
      "iteration 2290, train loss: 0.007405, validation loss: 0.004614\n",
      "iteration 2291, train loss: 0.006687, validation loss: 0.004628\n",
      "iteration 2292, train loss: 0.00684, validation loss: 0.004645\n",
      "iteration 2293, train loss: 0.006815, validation loss: 0.004591\n",
      "iteration 2294, train loss: 0.006419, validation loss: 0.004614\n",
      "iteration 2295, train loss: 0.007024, validation loss: 0.004647\n",
      "iteration 2296, train loss: 0.006621, validation loss: 0.004659\n",
      "iteration 2297, train loss: 0.007201, validation loss: 0.004855\n",
      "iteration 2298, train loss: \u001b[92m0.005925\u001b[0m, validation loss: 0.004876\n",
      "iteration 2299, train loss: 0.006705, validation loss: 0.004674\n",
      "iteration 2300, train loss: 0.007319, validation loss: 0.004584\n",
      "iteration 2301, train loss: 0.007017, validation loss: 0.004784\n",
      "iteration 2302, train loss: 0.006916, validation loss: 0.004815\n",
      "iteration 2303, train loss: 0.006708, validation loss: 0.004669\n",
      "iteration 2304, train loss: 0.006817, validation loss: 0.005051\n",
      "iteration 2305, train loss: 0.007374, validation loss: 0.0051\n",
      "iteration 2306, train loss: 0.007717, validation loss: 0.004754\n",
      "iteration 2307, train loss: 0.00682, validation loss: 0.004629\n",
      "iteration 2308, train loss: 0.006845, validation loss: 0.005054\n",
      "iteration 2309, train loss: 0.007245, validation loss: 0.004776\n",
      "iteration 2310, train loss: 0.006683, validation loss: 0.004594\n",
      "iteration 2311, train loss: 0.006692, validation loss: 0.0049\n",
      "iteration 2312, train loss: 0.006837, validation loss: 0.005192\n",
      "iteration 2313, train loss: 0.007238, validation loss: 0.005025\n",
      "iteration 2314, train loss: 0.006844, validation loss: 0.004776\n",
      "iteration 2315, train loss: 0.007159, validation loss: 0.004758\n",
      "iteration 2316, train loss: 0.007338, validation loss: 0.004719\n",
      "iteration 2317, train loss: 0.007147, validation loss: 0.004691\n",
      "iteration 2318, train loss: 0.006622, validation loss: 0.004814\n",
      "iteration 2319, train loss: 0.006788, validation loss: 0.004682\n",
      "iteration 2320, train loss: 0.006549, validation loss: 0.004689\n",
      "iteration 2321, train loss: 0.006772, validation loss: 0.004759\n",
      "iteration 2322, train loss: 0.006691, validation loss: 0.004766\n",
      "iteration 2323, train loss: 0.006862, validation loss: 0.004703\n",
      "iteration 2324, train loss: 0.006446, validation loss: 0.004619\n",
      "iteration 2325, train loss: 0.007038, validation loss: 0.004673\n",
      "iteration 2326, train loss: 0.006595, validation loss: 0.004768\n",
      "iteration 2327, train loss: 0.006787, validation loss: 0.004774\n",
      "iteration 2328, train loss: 0.006845, validation loss: 0.004628\n",
      "iteration 2329, train loss: 0.006838, validation loss: 0.004631\n",
      "iteration 2330, train loss: 0.006752, validation loss: 0.004722\n",
      "iteration 2331, train loss: 0.006719, validation loss: 0.004758\n",
      "iteration 2332, train loss: 0.006797, validation loss: 0.004679\n",
      "iteration 2333, train loss: 0.006517, validation loss: 0.004652\n",
      "iteration 2334, train loss: 0.006051, validation loss: 0.004662\n",
      "iteration 2335, train loss: 0.006865, validation loss: 0.004713\n",
      "iteration 2336, train loss: 0.007143, validation loss: 0.004699\n",
      "iteration 2337, train loss: 0.006553, validation loss: 0.004673\n",
      "iteration 2338, train loss: 0.006752, validation loss: 0.00483\n",
      "iteration 2339, train loss: 0.006717, validation loss: 0.004778\n",
      "iteration 2340, train loss: 0.006667, validation loss: 0.004612\n",
      "iteration 2341, train loss: 0.006434, validation loss: 0.004638\n",
      "iteration 2342, train loss: 0.006775, validation loss: 0.004671\n",
      "iteration 2343, train loss: 0.006965, validation loss: 0.004613\n",
      "iteration 2344, train loss: 0.006848, validation loss: 0.004672\n",
      "iteration 2345, train loss: 0.007052, validation loss: 0.0048\n",
      "iteration 2346, train loss: 0.006556, validation loss: 0.004742\n",
      "iteration 2347, train loss: 0.006963, validation loss: 0.004979\n",
      "iteration 2348, train loss: 0.007645, validation loss: 0.004721\n",
      "iteration 2349, train loss: 0.006063, validation loss: 0.004683\n",
      "iteration 2350, train loss: 0.006743, validation loss: 0.00469\n",
      "iteration 2351, train loss: 0.006816, validation loss: 0.004645\n",
      "iteration 2352, train loss: 0.006599, validation loss: 0.004645\n",
      "iteration 2353, train loss: 0.006867, validation loss: 0.004625\n",
      "iteration 2354, train loss: 0.006619, validation loss: 0.004594\n",
      "iteration 2355, train loss: 0.00633, validation loss: 0.004591\n",
      "iteration 2356, train loss: 0.006428, validation loss: 0.004673\n",
      "iteration 2357, train loss: 0.006908, validation loss: 0.004833\n",
      "iteration 2358, train loss: 0.006973, validation loss: 0.004676\n",
      "iteration 2359, train loss: 0.006946, validation loss: 0.004619\n",
      "iteration 2360, train loss: 0.006796, validation loss: 0.004678\n",
      "iteration 2361, train loss: 0.00707, validation loss: 0.004694\n",
      "iteration 2362, train loss: 0.00669, validation loss: 0.004849\n",
      "iteration 2363, train loss: 0.006403, validation loss: 0.004967\n",
      "iteration 2364, train loss: 0.00679, validation loss: 0.004735\n",
      "iteration 2365, train loss: 0.006821, validation loss: 0.004571\n",
      "iteration 2366, train loss: 0.006841, validation loss: 0.004609\n",
      "iteration 2367, train loss: 0.006024, validation loss: 0.004668\n",
      "iteration 2368, train loss: 0.006643, validation loss: 0.004738\n",
      "iteration 2369, train loss: 0.006255, validation loss: 0.004723\n",
      "iteration 2370, train loss: 0.006707, validation loss: 0.00472\n",
      "iteration 2371, train loss: 0.006942, validation loss: 0.004858\n",
      "iteration 2372, train loss: 0.006913, validation loss: 0.004762\n",
      "iteration 2373, train loss: 0.006624, validation loss: 0.00468\n",
      "iteration 2374, train loss: 0.006307, validation loss: 0.004786\n",
      "iteration 2375, train loss: 0.006857, validation loss: 0.004756\n",
      "iteration 2376, train loss: 0.006549, validation loss: 0.004657\n",
      "iteration 2377, train loss: 0.006526, validation loss: 0.004672\n",
      "iteration 2378, train loss: 0.006535, validation loss: 0.004761\n",
      "iteration 2379, train loss: 0.006467, validation loss: 0.004731\n",
      "iteration 2380, train loss: 0.006357, validation loss: 0.004741\n",
      "iteration 2381, train loss: 0.006311, validation loss: 0.004631\n",
      "iteration 2382, train loss: 0.006705, validation loss: 0.004751\n",
      "iteration 2383, train loss: 0.006928, validation loss: 0.004819\n",
      "iteration 2384, train loss: 0.006632, validation loss: \u001b[92m0.004526\u001b[0m\n",
      "iteration 2385, train loss: \u001b[92m0.005902\u001b[0m, validation loss: 0.004758\n",
      "iteration 2386, train loss: 0.006444, validation loss: 0.005153\n",
      "iteration 2387, train loss: 0.007192, validation loss: 0.005006\n",
      "iteration 2388, train loss: 0.006992, validation loss: 0.004747\n",
      "iteration 2389, train loss: 0.00616, validation loss: 0.004662\n",
      "iteration 2390, train loss: 0.006749, validation loss: 0.004819\n",
      "iteration 2391, train loss: 0.006498, validation loss: 0.004627\n",
      "iteration 2392, train loss: 0.006461, validation loss: 0.004673\n",
      "iteration 2393, train loss: 0.006346, validation loss: 0.004663\n",
      "iteration 2394, train loss: 0.00651, validation loss: 0.004719\n",
      "iteration 2395, train loss: 0.006313, validation loss: 0.004739\n",
      "iteration 2396, train loss: 0.006356, validation loss: 0.00477\n",
      "iteration 2397, train loss: 0.00678, validation loss: 0.004741\n",
      "iteration 2398, train loss: 0.006526, validation loss: 0.004662\n",
      "iteration 2399, train loss: 0.006361, validation loss: 0.004721\n",
      "iteration 2400, train loss: 0.006828, validation loss: 0.004627\n",
      "iteration 2401, train loss: 0.00697, validation loss: 0.004651\n",
      "iteration 2402, train loss: 0.006175, validation loss: 0.004834\n",
      "iteration 2403, train loss: 0.006443, validation loss: 0.004878\n",
      "iteration 2404, train loss: 0.0063, validation loss: 0.004761\n",
      "iteration 2405, train loss: 0.006545, validation loss: 0.004761\n",
      "iteration 2406, train loss: 0.006642, validation loss: 0.004825\n",
      "iteration 2407, train loss: 0.006472, validation loss: 0.004674\n",
      "iteration 2408, train loss: 0.006648, validation loss: 0.00454\n",
      "iteration 2409, train loss: 0.00646, validation loss: 0.004552\n",
      "iteration 2410, train loss: 0.006932, validation loss: 0.004593\n",
      "iteration 2411, train loss: 0.006819, validation loss: 0.004575\n",
      "iteration 2412, train loss: 0.006505, validation loss: 0.004531\n",
      "iteration 2413, train loss: 0.006494, validation loss: 0.004571\n",
      "iteration 2414, train loss: 0.005964, validation loss: 0.004675\n",
      "iteration 2415, train loss: 0.006203, validation loss: 0.004696\n",
      "iteration 2416, train loss: 0.006201, validation loss: 0.004822\n",
      "iteration 2417, train loss: 0.006816, validation loss: 0.004585\n",
      "iteration 2418, train loss: 0.006693, validation loss: 0.004568\n",
      "iteration 2419, train loss: 0.006526, validation loss: 0.00458\n",
      "iteration 2420, train loss: 0.006096, validation loss: 0.004621\n",
      "iteration 2421, train loss: 0.006797, validation loss: 0.004908\n",
      "iteration 2422, train loss: 0.006644, validation loss: 0.00459\n",
      "iteration 2423, train loss: 0.006579, validation loss: 0.004527\n",
      "iteration 2424, train loss: 0.006278, validation loss: 0.004572\n",
      "iteration 2425, train loss: 0.006179, validation loss: 0.004622\n",
      "iteration 2426, train loss: 0.006541, validation loss: 0.00456\n",
      "iteration 2427, train loss: 0.006647, validation loss: 0.004539\n",
      "iteration 2428, train loss: 0.006375, validation loss: \u001b[92m0.004526\u001b[0m\n",
      "iteration 2429, train loss: 0.006682, validation loss: 0.004697\n",
      "iteration 2430, train loss: 0.006975, validation loss: 0.004779\n",
      "iteration 2431, train loss: 0.006833, validation loss: 0.004657\n",
      "iteration 2432, train loss: 0.006807, validation loss: 0.004545\n",
      "iteration 2433, train loss: \u001b[92m0.0058\u001b[0m, validation loss: 0.004587\n",
      "iteration 2434, train loss: 0.006767, validation loss: 0.004567\n",
      "iteration 2435, train loss: 0.006031, validation loss: 0.004589\n",
      "iteration 2436, train loss: 0.006078, validation loss: 0.00468\n",
      "iteration 2437, train loss: 0.006338, validation loss: 0.004758\n",
      "iteration 2438, train loss: 0.006586, validation loss: 0.004626\n",
      "iteration 2439, train loss: 0.006208, validation loss: 0.004555\n",
      "iteration 2440, train loss: 0.006417, validation loss: 0.004762\n",
      "iteration 2441, train loss: 0.006411, validation loss: 0.004602\n",
      "iteration 2442, train loss: 0.006343, validation loss: 0.004624\n",
      "iteration 2443, train loss: 0.006166, validation loss: 0.004962\n",
      "iteration 2444, train loss: 0.006262, validation loss: 0.004802\n",
      "iteration 2445, train loss: 0.006222, validation loss: 0.00471\n",
      "iteration 2446, train loss: 0.006539, validation loss: 0.005007\n",
      "iteration 2447, train loss: 0.006373, validation loss: 0.004781\n",
      "iteration 2448, train loss: 0.006136, validation loss: 0.004614\n",
      "iteration 2449, train loss: 0.006084, validation loss: 0.004716\n",
      "iteration 2450, train loss: 0.006039, validation loss: 0.004599\n",
      "iteration 2451, train loss: 0.005865, validation loss: 0.004572\n",
      "iteration 2452, train loss: 0.006085, validation loss: 0.004614\n",
      "iteration 2453, train loss: 0.00663, validation loss: 0.004585\n",
      "iteration 2454, train loss: 0.006455, validation loss: 0.00467\n",
      "iteration 2455, train loss: 0.006342, validation loss: 0.004764\n",
      "iteration 2456, train loss: 0.006529, validation loss: 0.004574\n",
      "iteration 2457, train loss: 0.006225, validation loss: 0.004584\n",
      "iteration 2458, train loss: 0.006631, validation loss: 0.004612\n",
      "iteration 2459, train loss: 0.006286, validation loss: 0.004578\n",
      "iteration 2460, train loss: 0.006183, validation loss: 0.004561\n",
      "iteration 2461, train loss: 0.006275, validation loss: 0.004569\n",
      "iteration 2462, train loss: 0.00645, validation loss: 0.004584\n",
      "iteration 2463, train loss: 0.005993, validation loss: 0.004542\n",
      "iteration 2464, train loss: 0.005997, validation loss: 0.004727\n",
      "iteration 2465, train loss: 0.006332, validation loss: 0.004872\n",
      "iteration 2466, train loss: 0.006745, validation loss: 0.004617\n",
      "iteration 2467, train loss: 0.006244, validation loss: 0.004781\n",
      "iteration 2468, train loss: 0.006047, validation loss: 0.004883\n",
      "iteration 2469, train loss: 0.006536, validation loss: 0.004623\n",
      "iteration 2470, train loss: 0.006355, validation loss: 0.004716\n",
      "iteration 2471, train loss: 0.005981, validation loss: 0.004942\n",
      "iteration 2472, train loss: 0.006868, validation loss: 0.004652\n",
      "iteration 2473, train loss: 0.006297, validation loss: 0.004633\n",
      "iteration 2474, train loss: 0.006372, validation loss: 0.004711\n",
      "iteration 2475, train loss: 0.006813, validation loss: 0.00454\n",
      "iteration 2476, train loss: 0.006221, validation loss: 0.004744\n",
      "iteration 2477, train loss: 0.006422, validation loss: 0.004615\n",
      "iteration 2478, train loss: 0.006156, validation loss: 0.004572\n",
      "iteration 2479, train loss: 0.006024, validation loss: 0.004794\n",
      "iteration 2480, train loss: 0.006186, validation loss: 0.004916\n",
      "iteration 2481, train loss: 0.006689, validation loss: 0.004641\n",
      "iteration 2482, train loss: 0.006, validation loss: 0.004586\n",
      "iteration 2483, train loss: 0.00616, validation loss: 0.004684\n",
      "iteration 2484, train loss: 0.005953, validation loss: 0.004613\n",
      "iteration 2485, train loss: 0.00653, validation loss: 0.004692\n",
      "iteration 2486, train loss: 0.005841, validation loss: 0.004707\n",
      "iteration 2487, train loss: 0.006118, validation loss: 0.004574\n",
      "iteration 2488, train loss: 0.006125, validation loss: 0.004637\n",
      "iteration 2489, train loss: 0.006811, validation loss: 0.004537\n",
      "iteration 2490, train loss: 0.006375, validation loss: 0.004604\n",
      "iteration 2491, train loss: 0.006337, validation loss: 0.00477\n",
      "iteration 2492, train loss: \u001b[92m0.005695\u001b[0m, validation loss: 0.004734\n",
      "iteration 2493, train loss: 0.00627, validation loss: 0.004621\n",
      "iteration 2494, train loss: 0.00591, validation loss: 0.004785\n",
      "iteration 2495, train loss: 0.00659, validation loss: 0.004592\n",
      "iteration 2496, train loss: 0.005757, validation loss: 0.00467\n",
      "iteration 2497, train loss: 0.006231, validation loss: 0.004594\n",
      "iteration 2498, train loss: 0.005986, validation loss: 0.004584\n",
      "iteration 2499, train loss: 0.005724, validation loss: 0.004694\n",
      "iteration 2500, train loss: 0.005808, validation loss: 0.004773\n",
      "iteration 2501, train loss: 0.005919, validation loss: 0.004805\n",
      "iteration 2502, train loss: \u001b[92m0.005682\u001b[0m, validation loss: 0.004886\n",
      "iteration 2503, train loss: 0.005912, validation loss: 0.004699\n",
      "iteration 2504, train loss: 0.005969, validation loss: 0.004673\n",
      "iteration 2505, train loss: 0.006125, validation loss: 0.004749\n",
      "iteration 2506, train loss: 0.006414, validation loss: 0.004555\n",
      "iteration 2507, train loss: 0.005889, validation loss: 0.00468\n",
      "iteration 2508, train loss: \u001b[92m0.005656\u001b[0m, validation loss: 0.004841\n",
      "iteration 2509, train loss: 0.005934, validation loss: 0.004834\n",
      "iteration 2510, train loss: 0.0061, validation loss: 0.004627\n",
      "iteration 2511, train loss: \u001b[92m0.005608\u001b[0m, validation loss: \u001b[92m0.00451\u001b[0m\n",
      "iteration 2512, train loss: 0.006193, validation loss: 0.004604\n",
      "iteration 2513, train loss: 0.006181, validation loss: 0.004694\n",
      "iteration 2514, train loss: 0.006041, validation loss: 0.004674\n",
      "iteration 2515, train loss: \u001b[92m0.005491\u001b[0m, validation loss: 0.00468\n",
      "iteration 2516, train loss: 0.005936, validation loss: 0.004692\n",
      "iteration 2517, train loss: 0.006102, validation loss: 0.004576\n",
      "iteration 2518, train loss: 0.006171, validation loss: 0.004521\n",
      "iteration 2519, train loss: 0.005658, validation loss: 0.004527\n",
      "iteration 2520, train loss: 0.006223, validation loss: \u001b[92m0.00451\u001b[0m\n",
      "iteration 2521, train loss: 0.00599, validation loss: 0.004518\n",
      "iteration 2522, train loss: 0.006008, validation loss: 0.004698\n",
      "iteration 2523, train loss: 0.006038, validation loss: 0.005076\n",
      "iteration 2524, train loss: 0.005798, validation loss: 0.004881\n",
      "iteration 2525, train loss: 0.006044, validation loss: 0.004603\n",
      "iteration 2526, train loss: 0.005896, validation loss: 0.004804\n",
      "iteration 2527, train loss: 0.006863, validation loss: 0.004634\n",
      "iteration 2528, train loss: 0.006326, validation loss: 0.004725\n",
      "iteration 2529, train loss: 0.005891, validation loss: 0.004787\n",
      "iteration 2530, train loss: 0.00571, validation loss: 0.004713\n",
      "iteration 2531, train loss: 0.006161, validation loss: 0.004843\n",
      "iteration 2532, train loss: 0.006502, validation loss: 0.004756\n",
      "iteration 2533, train loss: 0.006592, validation loss: 0.00462\n",
      "iteration 2534, train loss: 0.005827, validation loss: 0.004721\n",
      "iteration 2535, train loss: 0.006051, validation loss: 0.004588\n",
      "iteration 2536, train loss: 0.006028, validation loss: 0.005001\n",
      "iteration 2537, train loss: 0.006627, validation loss: 0.004879\n",
      "iteration 2538, train loss: 0.005848, validation loss: 0.004894\n",
      "iteration 2539, train loss: 0.006368, validation loss: 0.005284\n",
      "iteration 2540, train loss: 0.006105, validation loss: 0.005299\n",
      "iteration 2541, train loss: 0.006528, validation loss: 0.004683\n",
      "iteration 2542, train loss: 0.005996, validation loss: 0.004617\n",
      "iteration 2543, train loss: 0.00619, validation loss: 0.004833\n",
      "iteration 2544, train loss: 0.006703, validation loss: 0.004554\n",
      "iteration 2545, train loss: 0.006214, validation loss: 0.00466\n",
      "iteration 2546, train loss: 0.006017, validation loss: 0.004843\n",
      "iteration 2547, train loss: 0.006003, validation loss: 0.004733\n",
      "iteration 2548, train loss: 0.005781, validation loss: 0.004578\n",
      "iteration 2549, train loss: 0.005812, validation loss: 0.004512\n",
      "iteration 2550, train loss: 0.005771, validation loss: 0.004589\n",
      "iteration 2551, train loss: 0.00574, validation loss: 0.004699\n",
      "iteration 2552, train loss: 0.006279, validation loss: 0.004727\n",
      "iteration 2553, train loss: 0.006121, validation loss: 0.004616\n",
      "iteration 2554, train loss: 0.005627, validation loss: 0.004518\n",
      "iteration 2555, train loss: 0.005593, validation loss: 0.004774\n",
      "iteration 2556, train loss: \u001b[92m0.005431\u001b[0m, validation loss: 0.004999\n",
      "iteration 2557, train loss: 0.00647, validation loss: 0.00481\n",
      "iteration 2558, train loss: 0.005839, validation loss: 0.004669\n",
      "iteration 2559, train loss: 0.005519, validation loss: 0.004782\n",
      "iteration 2560, train loss: 0.005704, validation loss: 0.004786\n",
      "iteration 2561, train loss: 0.006795, validation loss: 0.004527\n",
      "iteration 2562, train loss: 0.006203, validation loss: 0.004774\n",
      "iteration 2563, train loss: 0.00654, validation loss: 0.004685\n",
      "iteration 2564, train loss: 0.0058, validation loss: 0.004583\n",
      "iteration 2565, train loss: 0.005865, validation loss: 0.004583\n",
      "iteration 2566, train loss: 0.005641, validation loss: 0.004517\n",
      "iteration 2567, train loss: 0.005964, validation loss: \u001b[92m0.004419\u001b[0m\n",
      "iteration 2568, train loss: 0.006157, validation loss: 0.004538\n",
      "iteration 2569, train loss: \u001b[92m0.005335\u001b[0m, validation loss: 0.004636\n",
      "iteration 2570, train loss: 0.005394, validation loss: 0.004674\n",
      "iteration 2571, train loss: 0.00614, validation loss: 0.004723\n",
      "iteration 2572, train loss: 0.006292, validation loss: 0.004681\n",
      "iteration 2573, train loss: 0.006246, validation loss: 0.00468\n",
      "iteration 2574, train loss: 0.005977, validation loss: 0.004579\n",
      "iteration 2575, train loss: 0.006271, validation loss: 0.004467\n",
      "iteration 2576, train loss: 0.006124, validation loss: \u001b[92m0.004396\u001b[0m\n",
      "iteration 2577, train loss: 0.00579, validation loss: \u001b[92m0.004361\u001b[0m\n",
      "iteration 2578, train loss: 0.005493, validation loss: \u001b[92m0.004298\u001b[0m\n",
      "iteration 2579, train loss: 0.005479, validation loss: 0.004301\n",
      "iteration 2580, train loss: 0.00587, validation loss: 0.004437\n",
      "iteration 2581, train loss: 0.005845, validation loss: 0.004489\n",
      "iteration 2582, train loss: 0.005677, validation loss: 0.004849\n",
      "iteration 2583, train loss: 0.005924, validation loss: 0.005028\n",
      "iteration 2584, train loss: 0.006142, validation loss: 0.004682\n",
      "iteration 2585, train loss: 0.005912, validation loss: 0.004512\n",
      "iteration 2586, train loss: 0.006236, validation loss: \u001b[92m0.00429\u001b[0m\n",
      "iteration 2587, train loss: 0.005501, validation loss: 0.004317\n",
      "iteration 2588, train loss: 0.006161, validation loss: \u001b[92m0.00407\u001b[0m\n",
      "iteration 2589, train loss: 0.006014, validation loss: \u001b[92m0.003988\u001b[0m\n",
      "iteration 2590, train loss: 0.006017, validation loss: 0.004017\n",
      "iteration 2591, train loss: \u001b[92m0.005272\u001b[0m, validation loss: 0.004208\n",
      "iteration 2592, train loss: 0.00575, validation loss: 0.004354\n",
      "iteration 2593, train loss: 0.005769, validation loss: 0.004403\n",
      "iteration 2594, train loss: 0.006225, validation loss: 0.004411\n",
      "iteration 2595, train loss: 0.006048, validation loss: 0.004566\n",
      "iteration 2596, train loss: 0.005305, validation loss: 0.004476\n",
      "iteration 2597, train loss: 0.005848, validation loss: 0.004123\n",
      "iteration 2598, train loss: 0.005442, validation loss: \u001b[92m0.003891\u001b[0m\n",
      "iteration 2599, train loss: 0.005462, validation loss: 0.003926\n",
      "iteration 2600, train loss: 0.006298, validation loss: 0.004368\n",
      "iteration 2601, train loss: 0.006158, validation loss: 0.004563\n",
      "iteration 2602, train loss: 0.00566, validation loss: 0.004766\n",
      "iteration 2603, train loss: 0.005722, validation loss: 0.004608\n",
      "iteration 2604, train loss: 0.006, validation loss: 0.004333\n",
      "iteration 2605, train loss: 0.005946, validation loss: 0.004537\n",
      "iteration 2606, train loss: 0.006034, validation loss: 0.00442\n",
      "iteration 2607, train loss: 0.006032, validation loss: 0.004378\n",
      "iteration 2608, train loss: 0.005588, validation loss: 0.00414\n",
      "iteration 2609, train loss: 0.006042, validation loss: 0.004054\n",
      "iteration 2610, train loss: 0.005972, validation loss: 0.004337\n",
      "iteration 2611, train loss: 0.005671, validation loss: 0.004652\n",
      "iteration 2612, train loss: 0.005724, validation loss: 0.004458\n",
      "iteration 2613, train loss: 0.006225, validation loss: 0.004691\n",
      "iteration 2614, train loss: 0.005773, validation loss: 0.005124\n",
      "iteration 2615, train loss: 0.006067, validation loss: 0.004895\n",
      "iteration 2616, train loss: 0.006387, validation loss: 0.004291\n",
      "iteration 2617, train loss: 0.005812, validation loss: 0.004221\n",
      "iteration 2618, train loss: 0.005863, validation loss: 0.004207\n",
      "iteration 2619, train loss: 0.006116, validation loss: 0.003902\n",
      "iteration 2620, train loss: 0.005631, validation loss: 0.004404\n",
      "iteration 2621, train loss: 0.006059, validation loss: 0.004693\n",
      "iteration 2622, train loss: 0.006277, validation loss: 0.004616\n",
      "iteration 2623, train loss: 0.006206, validation loss: 0.004262\n",
      "iteration 2624, train loss: 0.005742, validation loss: 0.004174\n",
      "iteration 2625, train loss: 0.005612, validation loss: 0.004649\n",
      "iteration 2626, train loss: 0.006455, validation loss: 0.00454\n",
      "iteration 2627, train loss: 0.006028, validation loss: 0.00417\n",
      "iteration 2628, train loss: 0.005529, validation loss: 0.004364\n",
      "iteration 2629, train loss: 0.005609, validation loss: 0.00472\n",
      "iteration 2630, train loss: 0.006226, validation loss: 0.004548\n",
      "iteration 2631, train loss: 0.006244, validation loss: 0.004083\n",
      "iteration 2632, train loss: 0.005628, validation loss: 0.004155\n",
      "iteration 2633, train loss: 0.005358, validation loss: 0.004643\n",
      "iteration 2634, train loss: 0.006874, validation loss: 0.004056\n",
      "iteration 2635, train loss: 0.005745, validation loss: 0.00426\n",
      "iteration 2636, train loss: 0.005987, validation loss: 0.004389\n",
      "iteration 2637, train loss: 0.005613, validation loss: 0.00428\n",
      "iteration 2638, train loss: 0.006031, validation loss: 0.004083\n",
      "iteration 2639, train loss: 0.005724, validation loss: 0.004002\n",
      "iteration 2640, train loss: 0.005809, validation loss: \u001b[92m0.003866\u001b[0m\n",
      "iteration 2641, train loss: 0.006066, validation loss: 0.004009\n",
      "iteration 2642, train loss: 0.005663, validation loss: 0.004387\n",
      "iteration 2643, train loss: 0.005474, validation loss: 0.004277\n",
      "iteration 2644, train loss: 0.005605, validation loss: 0.004178\n",
      "iteration 2645, train loss: 0.005712, validation loss: 0.004127\n",
      "iteration 2646, train loss: 0.006632, validation loss: 0.00425\n",
      "iteration 2647, train loss: 0.005911, validation loss: 0.004194\n",
      "iteration 2648, train loss: 0.005792, validation loss: 0.004245\n",
      "iteration 2649, train loss: 0.005583, validation loss: 0.004506\n",
      "iteration 2650, train loss: 0.006206, validation loss: 0.004445\n",
      "iteration 2651, train loss: 0.005825, validation loss: 0.004202\n",
      "iteration 2652, train loss: 0.005879, validation loss: \u001b[92m0.00386\u001b[0m\n",
      "iteration 2653, train loss: \u001b[92m0.005185\u001b[0m, validation loss: \u001b[92m0.003817\u001b[0m\n",
      "iteration 2654, train loss: 0.006434, validation loss: \u001b[92m0.003666\u001b[0m\n",
      "iteration 2655, train loss: 0.005865, validation loss: 0.003774\n",
      "iteration 2656, train loss: 0.005454, validation loss: 0.004064\n",
      "iteration 2657, train loss: 0.005814, validation loss: 0.004141\n",
      "iteration 2658, train loss: 0.006006, validation loss: 0.003988\n",
      "iteration 2659, train loss: 0.005901, validation loss: 0.003858\n",
      "iteration 2660, train loss: 0.005698, validation loss: 0.003834\n",
      "iteration 2661, train loss: 0.005622, validation loss: 0.003813\n",
      "iteration 2662, train loss: 0.005742, validation loss: 0.003676\n",
      "iteration 2663, train loss: 0.005617, validation loss: 0.003703\n",
      "iteration 2664, train loss: \u001b[92m0.005057\u001b[0m, validation loss: 0.003723\n",
      "iteration 2665, train loss: 0.005554, validation loss: 0.00392\n",
      "iteration 2666, train loss: 0.005475, validation loss: 0.00406\n",
      "iteration 2667, train loss: 0.00601, validation loss: 0.003971\n",
      "iteration 2668, train loss: 0.0058, validation loss: \u001b[92m0.003631\u001b[0m\n",
      "iteration 2669, train loss: \u001b[92m0.004946\u001b[0m, validation loss: 0.003705\n",
      "iteration 2670, train loss: 0.005863, validation loss: 0.003703\n",
      "iteration 2671, train loss: 0.005513, validation loss: \u001b[92m0.003628\u001b[0m\n",
      "iteration 2672, train loss: 0.005269, validation loss: 0.003851\n",
      "iteration 2673, train loss: 0.005419, validation loss: 0.003996\n",
      "iteration 2674, train loss: 0.005566, validation loss: 0.00403\n",
      "iteration 2675, train loss: 0.005543, validation loss: 0.003907\n",
      "iteration 2676, train loss: 0.00567, validation loss: 0.003796\n",
      "iteration 2677, train loss: 0.005925, validation loss: 0.003831\n",
      "iteration 2678, train loss: 0.005847, validation loss: 0.003975\n",
      "iteration 2679, train loss: 0.00646, validation loss: 0.003835\n",
      "iteration 2680, train loss: 0.005594, validation loss: 0.004379\n",
      "iteration 2681, train loss: 0.005633, validation loss: 0.004454\n",
      "iteration 2682, train loss: 0.005512, validation loss: 0.004409\n",
      "iteration 2683, train loss: 0.006227, validation loss: 0.003938\n",
      "iteration 2684, train loss: 0.005286, validation loss: \u001b[92m0.003622\u001b[0m\n",
      "iteration 2685, train loss: 0.005131, validation loss: 0.004078\n",
      "iteration 2686, train loss: 0.006209, validation loss: 0.004278\n",
      "iteration 2687, train loss: 0.00596, validation loss: 0.004062\n",
      "iteration 2688, train loss: 0.005998, validation loss: 0.003875\n",
      "iteration 2689, train loss: 0.005487, validation loss: 0.0044\n",
      "iteration 2690, train loss: 0.005503, validation loss: 0.004819\n",
      "iteration 2691, train loss: 0.006598, validation loss: 0.004307\n",
      "iteration 2692, train loss: 0.005666, validation loss: 0.003948\n",
      "iteration 2693, train loss: 0.005342, validation loss: 0.004134\n",
      "iteration 2694, train loss: 0.006105, validation loss: 0.004025\n",
      "iteration 2695, train loss: 0.005339, validation loss: 0.003717\n",
      "iteration 2696, train loss: 0.00587, validation loss: \u001b[92m0.003604\u001b[0m\n",
      "iteration 2697, train loss: 0.005268, validation loss: 0.003836\n",
      "iteration 2698, train loss: 0.005507, validation loss: 0.004056\n",
      "iteration 2699, train loss: 0.005902, validation loss: 0.004062\n",
      "iteration 2700, train loss: 0.005435, validation loss: 0.003987\n",
      "iteration 2701, train loss: 0.005394, validation loss: 0.003898\n",
      "iteration 2702, train loss: 0.005316, validation loss: 0.003827\n",
      "iteration 2703, train loss: 0.005625, validation loss: 0.003707\n",
      "iteration 2704, train loss: 0.005737, validation loss: 0.003634\n",
      "iteration 2705, train loss: 0.005328, validation loss: \u001b[92m0.003548\u001b[0m\n",
      "iteration 2706, train loss: 0.005527, validation loss: \u001b[92m0.003498\u001b[0m\n",
      "iteration 2707, train loss: 0.005365, validation loss: 0.003842\n",
      "iteration 2708, train loss: 0.005164, validation loss: 0.004072\n",
      "iteration 2709, train loss: 0.005736, validation loss: 0.003739\n",
      "iteration 2710, train loss: 0.005235, validation loss: 0.003556\n",
      "iteration 2711, train loss: 0.005696, validation loss: 0.0036\n",
      "iteration 2712, train loss: 0.005271, validation loss: 0.003684\n",
      "iteration 2713, train loss: 0.00537, validation loss: 0.003824\n",
      "iteration 2714, train loss: 0.005175, validation loss: 0.003772\n",
      "iteration 2715, train loss: \u001b[92m0.004935\u001b[0m, validation loss: 0.003534\n",
      "iteration 2716, train loss: 0.005143, validation loss: 0.003634\n",
      "iteration 2717, train loss: 0.005444, validation loss: 0.003552\n",
      "iteration 2718, train loss: 0.005306, validation loss: 0.003515\n",
      "iteration 2719, train loss: 0.00506, validation loss: 0.003776\n",
      "iteration 2720, train loss: 0.005501, validation loss: 0.003626\n",
      "iteration 2721, train loss: 0.005184, validation loss: 0.003787\n",
      "iteration 2722, train loss: 0.005273, validation loss: 0.003859\n",
      "iteration 2723, train loss: 0.005095, validation loss: 0.003651\n",
      "iteration 2724, train loss: 0.005828, validation loss: \u001b[92m0.00344\u001b[0m\n",
      "iteration 2725, train loss: \u001b[92m0.004859\u001b[0m, validation loss: 0.00363\n",
      "iteration 2726, train loss: 0.005065, validation loss: 0.003986\n",
      "iteration 2727, train loss: 0.005434, validation loss: 0.00379\n",
      "iteration 2728, train loss: 0.00514, validation loss: 0.004041\n",
      "iteration 2729, train loss: 0.00523, validation loss: 0.003994\n",
      "iteration 2730, train loss: 0.005208, validation loss: 0.003546\n",
      "iteration 2731, train loss: 0.005111, validation loss: 0.003659\n",
      "iteration 2732, train loss: 0.005038, validation loss: 0.004026\n",
      "iteration 2733, train loss: 0.005405, validation loss: 0.003758\n",
      "iteration 2734, train loss: 0.004954, validation loss: 0.003623\n",
      "iteration 2735, train loss: 0.004911, validation loss: 0.004132\n",
      "iteration 2736, train loss: 0.006012, validation loss: 0.003592\n",
      "iteration 2737, train loss: 0.005008, validation loss: 0.003738\n",
      "iteration 2738, train loss: \u001b[92m0.004846\u001b[0m, validation loss: 0.00408\n",
      "iteration 2739, train loss: 0.00503, validation loss: 0.003829\n",
      "iteration 2740, train loss: \u001b[92m0.004614\u001b[0m, validation loss: 0.003649\n",
      "iteration 2741, train loss: 0.004984, validation loss: 0.003532\n",
      "iteration 2742, train loss: 0.005554, validation loss: 0.003748\n",
      "iteration 2743, train loss: 0.005186, validation loss: 0.003885\n",
      "iteration 2744, train loss: 0.005269, validation loss: 0.003655\n",
      "iteration 2745, train loss: 0.005269, validation loss: 0.003506\n",
      "iteration 2746, train loss: 0.005374, validation loss: 0.003696\n",
      "iteration 2747, train loss: 0.005115, validation loss: 0.003935\n",
      "iteration 2748, train loss: 0.005267, validation loss: \u001b[92m0.003426\u001b[0m\n",
      "iteration 2749, train loss: 0.005003, validation loss: 0.003632\n",
      "iteration 2750, train loss: 0.00503, validation loss: 0.004092\n",
      "iteration 2751, train loss: 0.005432, validation loss: 0.003821\n",
      "iteration 2752, train loss: 0.005049, validation loss: 0.00372\n",
      "iteration 2753, train loss: 0.005466, validation loss: 0.003705\n",
      "iteration 2754, train loss: 0.005095, validation loss: 0.00351\n",
      "iteration 2755, train loss: 0.004932, validation loss: 0.003714\n",
      "iteration 2756, train loss: 0.005027, validation loss: 0.004073\n",
      "iteration 2757, train loss: 0.005638, validation loss: 0.003949\n",
      "iteration 2758, train loss: 0.005374, validation loss: 0.003881\n",
      "iteration 2759, train loss: 0.005333, validation loss: 0.004065\n",
      "iteration 2760, train loss: 0.005943, validation loss: 0.003989\n",
      "iteration 2761, train loss: 0.005168, validation loss: 0.004089\n",
      "iteration 2762, train loss: 0.005543, validation loss: 0.004572\n",
      "iteration 2763, train loss: 0.006265, validation loss: 0.00394\n",
      "iteration 2764, train loss: 0.005536, validation loss: 0.003674\n",
      "iteration 2765, train loss: 0.005002, validation loss: 0.004209\n",
      "iteration 2766, train loss: 0.005226, validation loss: 0.004369\n",
      "iteration 2767, train loss: 0.005996, validation loss: 0.004065\n",
      "iteration 2768, train loss: 0.005477, validation loss: 0.003902\n",
      "iteration 2769, train loss: 0.005246, validation loss: 0.004066\n",
      "iteration 2770, train loss: 0.005387, validation loss: 0.00371\n",
      "iteration 2771, train loss: 0.005455, validation loss: 0.003585\n",
      "iteration 2772, train loss: 0.004942, validation loss: 0.003678\n",
      "iteration 2773, train loss: 0.004934, validation loss: 0.003731\n",
      "iteration 2774, train loss: 0.00492, validation loss: 0.003612\n",
      "iteration 2775, train loss: 0.0049, validation loss: 0.003738\n",
      "iteration 2776, train loss: 0.005188, validation loss: 0.003553\n",
      "iteration 2777, train loss: 0.004989, validation loss: 0.003436\n",
      "iteration 2778, train loss: 0.00499, validation loss: 0.003658\n",
      "iteration 2779, train loss: 0.005323, validation loss: 0.003519\n",
      "iteration 2780, train loss: 0.004995, validation loss: 0.003494\n",
      "iteration 2781, train loss: 0.005063, validation loss: 0.003428\n",
      "iteration 2782, train loss: 0.005224, validation loss: 0.003783\n",
      "iteration 2783, train loss: 0.005058, validation loss: 0.004378\n",
      "iteration 2784, train loss: 0.005225, validation loss: 0.004169\n",
      "iteration 2785, train loss: 0.005486, validation loss: \u001b[92m0.003421\u001b[0m\n",
      "iteration 2786, train loss: 0.00484, validation loss: 0.003437\n",
      "iteration 2787, train loss: 0.005055, validation loss: \u001b[92m0.003356\u001b[0m\n",
      "iteration 2788, train loss: 0.005361, validation loss: 0.003935\n",
      "iteration 2789, train loss: 0.005102, validation loss: 0.004212\n",
      "iteration 2790, train loss: 0.005278, validation loss: 0.004251\n",
      "iteration 2791, train loss: 0.005166, validation loss: 0.004104\n",
      "iteration 2792, train loss: 0.00492, validation loss: 0.003577\n",
      "iteration 2793, train loss: 0.004949, validation loss: \u001b[92m0.003299\u001b[0m\n",
      "iteration 2794, train loss: 0.004978, validation loss: 0.003472\n",
      "iteration 2795, train loss: 0.005147, validation loss: 0.003606\n",
      "iteration 2796, train loss: 0.004843, validation loss: 0.004172\n",
      "iteration 2797, train loss: 0.005199, validation loss: 0.003761\n",
      "iteration 2798, train loss: \u001b[92m0.004499\u001b[0m, validation loss: 0.003555\n",
      "iteration 2799, train loss: 0.005082, validation loss: 0.003385\n",
      "iteration 2800, train loss: 0.004906, validation loss: 0.003437\n",
      "iteration 2801, train loss: 0.004625, validation loss: 0.003591\n",
      "iteration 2802, train loss: 0.005272, validation loss: 0.00374\n",
      "iteration 2803, train loss: 0.005256, validation loss: 0.003885\n",
      "iteration 2804, train loss: 0.00467, validation loss: 0.004105\n",
      "iteration 2805, train loss: 0.004924, validation loss: 0.003567\n",
      "iteration 2806, train loss: 0.004705, validation loss: 0.003614\n",
      "iteration 2807, train loss: 0.005199, validation loss: 0.003653\n",
      "iteration 2808, train loss: 0.004887, validation loss: 0.003643\n",
      "iteration 2809, train loss: 0.004738, validation loss: 0.003899\n",
      "iteration 2810, train loss: 0.005223, validation loss: 0.003474\n",
      "iteration 2811, train loss: 0.005001, validation loss: 0.003591\n",
      "iteration 2812, train loss: 0.005239, validation loss: 0.003365\n",
      "iteration 2813, train loss: 0.004886, validation loss: 0.003386\n",
      "iteration 2814, train loss: 0.004589, validation loss: 0.003855\n",
      "iteration 2815, train loss: 0.005274, validation loss: 0.004112\n",
      "iteration 2816, train loss: 0.0049, validation loss: 0.003599\n",
      "iteration 2817, train loss: 0.005027, validation loss: \u001b[92m0.003188\u001b[0m\n",
      "iteration 2818, train loss: 0.005422, validation loss: 0.003349\n",
      "iteration 2819, train loss: 0.005199, validation loss: 0.003624\n",
      "iteration 2820, train loss: 0.004654, validation loss: 0.003804\n",
      "iteration 2821, train loss: 0.004798, validation loss: 0.003862\n",
      "iteration 2822, train loss: 0.00523, validation loss: 0.00351\n",
      "iteration 2823, train loss: 0.004907, validation loss: 0.003248\n",
      "iteration 2824, train loss: 0.005056, validation loss: 0.003259\n",
      "iteration 2825, train loss: 0.004794, validation loss: 0.003455\n",
      "iteration 2826, train loss: 0.00515, validation loss: 0.003673\n",
      "iteration 2827, train loss: 0.005059, validation loss: 0.003474\n",
      "iteration 2828, train loss: 0.00485, validation loss: 0.003331\n",
      "iteration 2829, train loss: 0.004558, validation loss: 0.003343\n",
      "iteration 2830, train loss: 0.004984, validation loss: 0.003461\n",
      "iteration 2831, train loss: 0.004993, validation loss: 0.003309\n",
      "iteration 2832, train loss: 0.004577, validation loss: 0.003405\n",
      "iteration 2833, train loss: 0.004752, validation loss: 0.003809\n",
      "iteration 2834, train loss: 0.005056, validation loss: 0.004226\n",
      "iteration 2835, train loss: 0.005407, validation loss: 0.003593\n",
      "iteration 2836, train loss: 0.005149, validation loss: 0.003634\n",
      "iteration 2837, train loss: 0.005331, validation loss: 0.003579\n",
      "iteration 2838, train loss: 0.00502, validation loss: 0.003625\n",
      "iteration 2839, train loss: 0.005492, validation loss: 0.004403\n",
      "iteration 2840, train loss: 0.005326, validation loss: 0.004538\n",
      "iteration 2841, train loss: 0.005548, validation loss: 0.004116\n",
      "iteration 2842, train loss: 0.00542, validation loss: 0.004075\n",
      "iteration 2843, train loss: 0.005202, validation loss: 0.003514\n",
      "iteration 2844, train loss: 0.005428, validation loss: 0.003322\n",
      "iteration 2845, train loss: 0.005089, validation loss: 0.003661\n",
      "iteration 2846, train loss: 0.005265, validation loss: 0.003812\n",
      "iteration 2847, train loss: 0.005081, validation loss: 0.003881\n",
      "iteration 2848, train loss: 0.004891, validation loss: 0.003655\n",
      "iteration 2849, train loss: 0.005325, validation loss: 0.003202\n",
      "iteration 2850, train loss: 0.004985, validation loss: 0.003607\n",
      "iteration 2851, train loss: 0.005076, validation loss: 0.003617\n",
      "iteration 2852, train loss: 0.004712, validation loss: 0.003906\n",
      "iteration 2853, train loss: 0.005052, validation loss: 0.004064\n",
      "iteration 2854, train loss: 0.005218, validation loss: 0.003912\n",
      "iteration 2855, train loss: 0.005467, validation loss: 0.003358\n",
      "iteration 2856, train loss: 0.004778, validation loss: 0.003531\n",
      "iteration 2857, train loss: 0.006069, validation loss: 0.003394\n",
      "iteration 2858, train loss: 0.005215, validation loss: 0.003646\n",
      "iteration 2859, train loss: 0.004983, validation loss: 0.004056\n",
      "iteration 2860, train loss: 0.005115, validation loss: 0.003857\n",
      "iteration 2861, train loss: 0.004863, validation loss: 0.003576\n",
      "iteration 2862, train loss: 0.004947, validation loss: 0.00374\n",
      "iteration 2863, train loss: 0.005269, validation loss: 0.00339\n",
      "iteration 2864, train loss: 0.005083, validation loss: 0.00338\n",
      "iteration 2865, train loss: 0.005142, validation loss: 0.003711\n",
      "iteration 2866, train loss: 0.004636, validation loss: 0.003965\n",
      "iteration 2867, train loss: 0.005134, validation loss: 0.003942\n",
      "iteration 2868, train loss: 0.005303, validation loss: 0.003738\n",
      "iteration 2869, train loss: 0.00502, validation loss: 0.003423\n",
      "iteration 2870, train loss: 0.005016, validation loss: 0.003194\n",
      "iteration 2871, train loss: 0.005002, validation loss: 0.003344\n",
      "iteration 2872, train loss: 0.004566, validation loss: 0.003666\n",
      "iteration 2873, train loss: 0.005054, validation loss: 0.003837\n",
      "iteration 2874, train loss: 0.00484, validation loss: 0.003616\n",
      "iteration 2875, train loss: 0.004897, validation loss: 0.003415\n",
      "iteration 2876, train loss: 0.005461, validation loss: \u001b[92m0.003102\u001b[0m\n",
      "iteration 2877, train loss: 0.004731, validation loss: 0.003195\n",
      "iteration 2878, train loss: 0.004824, validation loss: 0.0033\n",
      "iteration 2879, train loss: \u001b[92m0.004379\u001b[0m, validation loss: 0.003418\n",
      "iteration 2880, train loss: 0.004697, validation loss: 0.003566\n",
      "iteration 2881, train loss: 0.004519, validation loss: 0.003584\n",
      "iteration 2882, train loss: 0.004734, validation loss: 0.00334\n",
      "iteration 2883, train loss: 0.004861, validation loss: 0.00325\n",
      "iteration 2884, train loss: 0.004733, validation loss: 0.003275\n",
      "iteration 2885, train loss: 0.004837, validation loss: 0.00319\n",
      "iteration 2886, train loss: 0.004992, validation loss: 0.00343\n",
      "iteration 2887, train loss: 0.004847, validation loss: 0.003788\n",
      "iteration 2888, train loss: 0.004739, validation loss: 0.003739\n",
      "iteration 2889, train loss: 0.004621, validation loss: 0.003513\n",
      "iteration 2890, train loss: 0.004572, validation loss: 0.003333\n",
      "iteration 2891, train loss: 0.004726, validation loss: 0.003227\n",
      "iteration 2892, train loss: 0.005174, validation loss: 0.003184\n",
      "iteration 2893, train loss: 0.004688, validation loss: 0.003537\n",
      "iteration 2894, train loss: 0.00502, validation loss: 0.003532\n",
      "iteration 2895, train loss: 0.004677, validation loss: 0.003442\n",
      "iteration 2896, train loss: 0.005018, validation loss: 0.003264\n",
      "iteration 2897, train loss: 0.004637, validation loss: 0.003444\n",
      "iteration 2898, train loss: 0.004922, validation loss: 0.003218\n",
      "iteration 2899, train loss: 0.004821, validation loss: 0.003414\n",
      "iteration 2900, train loss: 0.004482, validation loss: 0.004237\n",
      "iteration 2901, train loss: 0.00497, validation loss: 0.004075\n",
      "iteration 2902, train loss: 0.005042, validation loss: 0.003527\n",
      "iteration 2903, train loss: 0.005088, validation loss: 0.003588\n",
      "iteration 2904, train loss: 0.005578, validation loss: 0.003278\n",
      "iteration 2905, train loss: 0.004689, validation loss: 0.003689\n",
      "iteration 2906, train loss: 0.005035, validation loss: 0.003947\n",
      "iteration 2907, train loss: 0.004961, validation loss: 0.003987\n",
      "iteration 2908, train loss: 0.005015, validation loss: 0.003687\n",
      "iteration 2909, train loss: 0.005222, validation loss: 0.003862\n",
      "iteration 2910, train loss: 0.005189, validation loss: 0.00434\n",
      "iteration 2911, train loss: 0.005754, validation loss: 0.003707\n",
      "iteration 2912, train loss: 0.005058, validation loss: 0.00332\n",
      "iteration 2913, train loss: 0.004848, validation loss: 0.003558\n",
      "iteration 2914, train loss: 0.005332, validation loss: 0.003419\n",
      "iteration 2915, train loss: 0.004831, validation loss: 0.003494\n",
      "iteration 2916, train loss: 0.005171, validation loss: 0.003849\n",
      "iteration 2917, train loss: 0.004908, validation loss: 0.003698\n",
      "iteration 2918, train loss: 0.004964, validation loss: 0.003631\n",
      "iteration 2919, train loss: 0.005441, validation loss: 0.003484\n",
      "iteration 2920, train loss: 0.00481, validation loss: 0.003191\n",
      "iteration 2921, train loss: 0.004717, validation loss: 0.003747\n",
      "iteration 2922, train loss: 0.004944, validation loss: 0.004038\n",
      "iteration 2923, train loss: 0.00508, validation loss: 0.003754\n",
      "iteration 2924, train loss: 0.004806, validation loss: 0.003737\n",
      "iteration 2925, train loss: 0.004992, validation loss: 0.003544\n",
      "iteration 2926, train loss: 0.005041, validation loss: 0.003321\n",
      "iteration 2927, train loss: 0.004913, validation loss: 0.003655\n",
      "iteration 2928, train loss: 0.005277, validation loss: 0.003861\n",
      "iteration 2929, train loss: 0.004797, validation loss: 0.004001\n",
      "iteration 2930, train loss: 0.005253, validation loss: 0.003517\n",
      "iteration 2931, train loss: 0.00493, validation loss: 0.003147\n",
      "iteration 2932, train loss: 0.004662, validation loss: \u001b[92m0.003091\u001b[0m\n",
      "iteration 2933, train loss: 0.00502, validation loss: 0.003323\n",
      "iteration 2934, train loss: 0.004751, validation loss: 0.003583\n",
      "iteration 2935, train loss: 0.004863, validation loss: 0.003848\n",
      "iteration 2936, train loss: 0.004971, validation loss: 0.003717\n",
      "iteration 2937, train loss: 0.004907, validation loss: 0.003335\n",
      "iteration 2938, train loss: 0.004724, validation loss: \u001b[92m0.003081\u001b[0m\n",
      "iteration 2939, train loss: 0.004651, validation loss: \u001b[92m0.00302\u001b[0m\n",
      "iteration 2940, train loss: 0.004783, validation loss: 0.00308\n",
      "iteration 2941, train loss: \u001b[92m0.004332\u001b[0m, validation loss: 0.003463\n",
      "iteration 2942, train loss: 0.004663, validation loss: 0.003988\n",
      "iteration 2943, train loss: 0.005151, validation loss: 0.003852\n",
      "iteration 2944, train loss: 0.004792, validation loss: 0.003288\n",
      "iteration 2945, train loss: 0.004955, validation loss: 0.003094\n",
      "iteration 2946, train loss: 0.004474, validation loss: 0.003498\n",
      "iteration 2947, train loss: 0.004839, validation loss: 0.003775\n",
      "iteration 2948, train loss: 0.005215, validation loss: 0.003633\n",
      "iteration 2949, train loss: 0.004858, validation loss: 0.003757\n",
      "iteration 2950, train loss: 0.004887, validation loss: 0.003786\n",
      "iteration 2951, train loss: 0.0052, validation loss: 0.00342\n",
      "iteration 2952, train loss: 0.004793, validation loss: 0.003323\n",
      "iteration 2953, train loss: 0.004351, validation loss: 0.003805\n",
      "iteration 2954, train loss: 0.005403, validation loss: 0.003831\n",
      "iteration 2955, train loss: 0.005114, validation loss: 0.00351\n",
      "iteration 2956, train loss: 0.005116, validation loss: 0.003652\n",
      "iteration 2957, train loss: 0.00512, validation loss: 0.003919\n",
      "iteration 2958, train loss: 0.005313, validation loss: 0.003629\n",
      "iteration 2959, train loss: 0.004833, validation loss: 0.003953\n",
      "iteration 2960, train loss: 0.005049, validation loss: 0.004195\n",
      "iteration 2961, train loss: 0.005312, validation loss: 0.003705\n",
      "iteration 2962, train loss: 0.004573, validation loss: 0.003763\n",
      "iteration 2963, train loss: 0.004941, validation loss: 0.003758\n",
      "iteration 2964, train loss: 0.004817, validation loss: 0.003166\n",
      "iteration 2965, train loss: 0.005155, validation loss: 0.003391\n",
      "iteration 2966, train loss: 0.005017, validation loss: 0.003987\n",
      "iteration 2967, train loss: 0.004892, validation loss: 0.004116\n",
      "iteration 2968, train loss: 0.005017, validation loss: 0.00417\n",
      "iteration 2969, train loss: 0.005587, validation loss: 0.003546\n",
      "iteration 2970, train loss: 0.004565, validation loss: 0.003151\n",
      "iteration 2971, train loss: 0.00467, validation loss: 0.003158\n",
      "iteration 2972, train loss: 0.004804, validation loss: 0.003383\n",
      "iteration 2973, train loss: 0.004819, validation loss: 0.003794\n",
      "iteration 2974, train loss: 0.005186, validation loss: 0.003717\n",
      "iteration 2975, train loss: 0.004627, validation loss: 0.003476\n",
      "iteration 2976, train loss: 0.00543, validation loss: 0.003233\n",
      "iteration 2977, train loss: 0.004758, validation loss: 0.003216\n",
      "iteration 2978, train loss: 0.004923, validation loss: 0.003518\n",
      "iteration 2979, train loss: 0.004472, validation loss: 0.003969\n",
      "iteration 2980, train loss: 0.004915, validation loss: 0.004316\n",
      "iteration 2981, train loss: 0.005726, validation loss: 0.003719\n",
      "iteration 2982, train loss: 0.005096, validation loss: 0.0031\n",
      "iteration 2983, train loss: 0.004746, validation loss: 0.0034\n",
      "iteration 2984, train loss: 0.005001, validation loss: 0.003565\n",
      "iteration 2985, train loss: 0.005604, validation loss: 0.003885\n",
      "iteration 2986, train loss: 0.004977, validation loss: 0.004273\n",
      "iteration 2987, train loss: 0.005097, validation loss: 0.003789\n",
      "iteration 2988, train loss: 0.004905, validation loss: 0.003255\n",
      "iteration 2989, train loss: 0.0047, validation loss: 0.003422\n",
      "iteration 2990, train loss: 0.004932, validation loss: 0.003533\n",
      "iteration 2991, train loss: 0.004911, validation loss: 0.003757\n",
      "iteration 2992, train loss: 0.004912, validation loss: 0.003949\n",
      "iteration 2993, train loss: 0.004956, validation loss: 0.004053\n",
      "iteration 2994, train loss: 0.005228, validation loss: 0.003552\n",
      "iteration 2995, train loss: 0.004722, validation loss: 0.003087\n",
      "iteration 2996, train loss: 0.004704, validation loss: 0.00337\n",
      "iteration 2997, train loss: 0.004828, validation loss: 0.003603\n",
      "iteration 2998, train loss: 0.005098, validation loss: 0.003518\n",
      "iteration 2999, train loss: 0.004542, validation loss: 0.003612\n",
      "iteration 3000, train loss: 0.004618, validation loss: 0.003573\n",
      "iteration 3001, train loss: 0.004641, validation loss: 0.003421\n",
      "iteration 3002, train loss: 0.004968, validation loss: 0.003079\n",
      "iteration 3003, train loss: \u001b[92m0.004266\u001b[0m, validation loss: 0.003427\n",
      "iteration 3004, train loss: 0.005243, validation loss: 0.003474\n",
      "iteration 3005, train loss: 0.005171, validation loss: 0.003191\n",
      "iteration 3006, train loss: 0.004837, validation loss: 0.003473\n",
      "iteration 3007, train loss: 0.004497, validation loss: 0.003947\n",
      "iteration 3008, train loss: 0.005493, validation loss: 0.003659\n",
      "iteration 3009, train loss: 0.004689, validation loss: 0.003256\n",
      "iteration 3010, train loss: 0.005063, validation loss: 0.003252\n",
      "iteration 3011, train loss: 0.004686, validation loss: 0.003532\n",
      "iteration 3012, train loss: 0.004773, validation loss: 0.003477\n",
      "iteration 3013, train loss: 0.005039, validation loss: 0.003207\n",
      "iteration 3014, train loss: 0.004583, validation loss: 0.003456\n",
      "iteration 3015, train loss: 0.004397, validation loss: 0.003736\n",
      "iteration 3016, train loss: 0.004884, validation loss: 0.003656\n",
      "iteration 3017, train loss: 0.004849, validation loss: 0.003315\n",
      "iteration 3018, train loss: 0.0044, validation loss: 0.003111\n",
      "iteration 3019, train loss: 0.004418, validation loss: 0.003054\n",
      "iteration 3020, train loss: 0.00465, validation loss: 0.003064\n",
      "iteration 3021, train loss: 0.004671, validation loss: 0.003192\n",
      "iteration 3022, train loss: \u001b[92m0.004224\u001b[0m, validation loss: 0.003466\n",
      "iteration 3023, train loss: 0.004458, validation loss: 0.003613\n",
      "iteration 3024, train loss: 0.005115, validation loss: 0.003351\n",
      "iteration 3025, train loss: 0.004675, validation loss: 0.003208\n",
      "iteration 3026, train loss: 0.004639, validation loss: 0.00312\n",
      "iteration 3027, train loss: 0.004805, validation loss: \u001b[92m0.002991\u001b[0m\n",
      "iteration 3028, train loss: 0.004558, validation loss: 0.003184\n",
      "iteration 3029, train loss: 0.004657, validation loss: 0.003457\n",
      "iteration 3030, train loss: 0.004439, validation loss: 0.003536\n",
      "iteration 3031, train loss: 0.004445, validation loss: 0.003448\n",
      "iteration 3032, train loss: 0.004442, validation loss: 0.003428\n",
      "iteration 3033, train loss: 0.004751, validation loss: 0.003202\n",
      "iteration 3034, train loss: 0.004436, validation loss: 0.003102\n",
      "iteration 3035, train loss: 0.004716, validation loss: 0.003283\n",
      "iteration 3036, train loss: 0.004653, validation loss: 0.00327\n",
      "iteration 3037, train loss: 0.004423, validation loss: 0.003214\n",
      "iteration 3038, train loss: 0.004587, validation loss: 0.003195\n",
      "iteration 3039, train loss: 0.004234, validation loss: 0.003262\n",
      "iteration 3040, train loss: 0.004702, validation loss: 0.00322\n",
      "iteration 3041, train loss: 0.00515, validation loss: 0.003186\n",
      "iteration 3042, train loss: 0.004436, validation loss: 0.003425\n",
      "iteration 3043, train loss: 0.004863, validation loss: 0.003395\n",
      "iteration 3044, train loss: 0.004447, validation loss: 0.003256\n",
      "iteration 3045, train loss: 0.004454, validation loss: 0.003065\n",
      "iteration 3046, train loss: 0.004825, validation loss: 0.003097\n",
      "iteration 3047, train loss: 0.004307, validation loss: 0.003304\n",
      "iteration 3048, train loss: 0.00482, validation loss: 0.003397\n",
      "iteration 3049, train loss: 0.004265, validation loss: 0.003542\n",
      "iteration 3050, train loss: 0.004597, validation loss: 0.003434\n",
      "iteration 3051, train loss: 0.004454, validation loss: 0.003259\n",
      "iteration 3052, train loss: 0.004269, validation loss: 0.002995\n",
      "iteration 3053, train loss: 0.004682, validation loss: 0.003019\n",
      "iteration 3054, train loss: 0.004796, validation loss: 0.00325\n",
      "iteration 3055, train loss: 0.004594, validation loss: 0.003457\n",
      "iteration 3056, train loss: 0.004297, validation loss: 0.003684\n",
      "iteration 3057, train loss: 0.004651, validation loss: 0.003731\n",
      "iteration 3058, train loss: 0.004836, validation loss: 0.003452\n",
      "iteration 3059, train loss: 0.004705, validation loss: 0.003203\n",
      "iteration 3060, train loss: 0.004624, validation loss: 0.002996\n",
      "iteration 3061, train loss: 0.004802, validation loss: 0.00326\n",
      "iteration 3062, train loss: 0.004552, validation loss: 0.003736\n",
      "iteration 3063, train loss: 0.004292, validation loss: 0.00413\n",
      "iteration 3064, train loss: 0.004823, validation loss: 0.003801\n",
      "iteration 3065, train loss: 0.005232, validation loss: 0.003139\n",
      "iteration 3066, train loss: 0.004652, validation loss: 0.003138\n",
      "iteration 3067, train loss: 0.004905, validation loss: 0.003347\n",
      "iteration 3068, train loss: 0.004427, validation loss: 0.003474\n",
      "iteration 3069, train loss: 0.004856, validation loss: 0.00325\n",
      "iteration 3070, train loss: 0.004587, validation loss: 0.003637\n",
      "iteration 3071, train loss: 0.005207, validation loss: 0.003402\n",
      "iteration 3072, train loss: 0.004951, validation loss: 0.003055\n",
      "iteration 3073, train loss: 0.004589, validation loss: 0.003497\n",
      "iteration 3074, train loss: 0.004699, validation loss: 0.003583\n",
      "iteration 3075, train loss: 0.004845, validation loss: 0.003225\n",
      "iteration 3076, train loss: 0.004263, validation loss: \u001b[92m0.002947\u001b[0m\n",
      "iteration 3077, train loss: 0.004301, validation loss: 0.00297\n",
      "iteration 3078, train loss: 0.004588, validation loss: 0.003052\n",
      "iteration 3079, train loss: 0.004647, validation loss: 0.003142\n",
      "iteration 3080, train loss: 0.004443, validation loss: 0.003338\n",
      "iteration 3081, train loss: 0.004458, validation loss: 0.003503\n",
      "iteration 3082, train loss: 0.004932, validation loss: 0.003228\n",
      "iteration 3083, train loss: 0.004699, validation loss: 0.00299\n",
      "iteration 3084, train loss: 0.004675, validation loss: \u001b[92m0.002943\u001b[0m\n",
      "iteration 3085, train loss: 0.00435, validation loss: 0.003314\n",
      "iteration 3086, train loss: 0.004358, validation loss: 0.003507\n",
      "iteration 3087, train loss: 0.004403, validation loss: 0.003411\n",
      "iteration 3088, train loss: 0.004619, validation loss: 0.003158\n",
      "iteration 3089, train loss: 0.004334, validation loss: 0.003178\n",
      "iteration 3090, train loss: 0.004369, validation loss: 0.003105\n",
      "iteration 3091, train loss: 0.004453, validation loss: 0.003067\n",
      "iteration 3092, train loss: 0.004641, validation loss: 0.003309\n",
      "iteration 3093, train loss: 0.004643, validation loss: 0.003366\n",
      "iteration 3094, train loss: 0.004626, validation loss: 0.003267\n",
      "iteration 3095, train loss: 0.004306, validation loss: 0.003282\n",
      "iteration 3096, train loss: 0.004348, validation loss: 0.003393\n",
      "iteration 3097, train loss: 0.004739, validation loss: 0.003091\n",
      "iteration 3098, train loss: 0.005066, validation loss: 0.003204\n",
      "iteration 3099, train loss: 0.004595, validation loss: 0.003584\n",
      "iteration 3100, train loss: 0.005186, validation loss: 0.003379\n",
      "iteration 3101, train loss: 0.004408, validation loss: 0.003034\n",
      "iteration 3102, train loss: 0.004579, validation loss: 0.003174\n",
      "iteration 3103, train loss: 0.004882, validation loss: 0.003216\n",
      "iteration 3104, train loss: 0.004564, validation loss: 0.003154\n",
      "iteration 3105, train loss: 0.005259, validation loss: 0.003345\n",
      "iteration 3106, train loss: 0.004588, validation loss: 0.003808\n",
      "iteration 3107, train loss: 0.005644, validation loss: 0.00334\n",
      "iteration 3108, train loss: 0.004817, validation loss: 0.003436\n",
      "iteration 3109, train loss: 0.004441, validation loss: 0.003767\n",
      "iteration 3110, train loss: 0.00474, validation loss: 0.003291\n",
      "iteration 3111, train loss: 0.004773, validation loss: 0.003026\n",
      "iteration 3112, train loss: 0.004291, validation loss: 0.003015\n",
      "iteration 3113, train loss: 0.004575, validation loss: 0.003174\n",
      "iteration 3114, train loss: 0.004955, validation loss: 0.00338\n",
      "iteration 3115, train loss: \u001b[92m0.004208\u001b[0m, validation loss: 0.003898\n",
      "iteration 3116, train loss: 0.004667, validation loss: 0.003676\n",
      "iteration 3117, train loss: 0.005106, validation loss: 0.003234\n",
      "iteration 3118, train loss: 0.00454, validation loss: 0.003067\n",
      "iteration 3119, train loss: 0.004566, validation loss: 0.003106\n",
      "iteration 3120, train loss: 0.004876, validation loss: 0.003254\n",
      "iteration 3121, train loss: 0.005067, validation loss: 0.003436\n",
      "iteration 3122, train loss: 0.00454, validation loss: 0.003611\n",
      "iteration 3123, train loss: 0.004749, validation loss: 0.003541\n",
      "iteration 3124, train loss: 0.00442, validation loss: 0.003254\n",
      "iteration 3125, train loss: 0.004974, validation loss: 0.003096\n",
      "iteration 3126, train loss: 0.004908, validation loss: 0.003276\n",
      "iteration 3127, train loss: 0.00485, validation loss: 0.003528\n",
      "iteration 3128, train loss: 0.004612, validation loss: 0.003639\n",
      "iteration 3129, train loss: 0.00459, validation loss: 0.003477\n",
      "iteration 3130, train loss: 0.004735, validation loss: 0.003369\n",
      "iteration 3131, train loss: 0.005502, validation loss: \u001b[92m0.002917\u001b[0m\n",
      "iteration 3132, train loss: 0.004558, validation loss: 0.003267\n",
      "iteration 3133, train loss: 0.004843, validation loss: 0.003767\n",
      "iteration 3134, train loss: 0.004883, validation loss: 0.003288\n",
      "iteration 3135, train loss: 0.004328, validation loss: 0.003265\n",
      "iteration 3136, train loss: 0.004248, validation loss: 0.003586\n",
      "iteration 3137, train loss: 0.004869, validation loss: 0.003218\n",
      "iteration 3138, train loss: 0.004465, validation loss: 0.003172\n",
      "iteration 3139, train loss: 0.004546, validation loss: 0.003421\n",
      "iteration 3140, train loss: 0.004941, validation loss: 0.003201\n",
      "iteration 3141, train loss: 0.004407, validation loss: 0.003038\n",
      "iteration 3142, train loss: 0.004364, validation loss: 0.003334\n",
      "iteration 3143, train loss: 0.004663, validation loss: 0.003565\n",
      "iteration 3144, train loss: 0.004571, validation loss: 0.003365\n",
      "iteration 3145, train loss: 0.004756, validation loss: 0.003171\n",
      "iteration 3146, train loss: 0.004283, validation loss: 0.003099\n",
      "iteration 3147, train loss: 0.004873, validation loss: 0.003014\n",
      "iteration 3148, train loss: 0.004706, validation loss: 0.003214\n",
      "iteration 3149, train loss: \u001b[92m0.004099\u001b[0m, validation loss: 0.003509\n",
      "iteration 3150, train loss: 0.004773, validation loss: 0.003398\n",
      "iteration 3151, train loss: 0.004572, validation loss: 0.003151\n",
      "iteration 3152, train loss: 0.004645, validation loss: 0.003291\n",
      "iteration 3153, train loss: 0.004295, validation loss: 0.003825\n",
      "iteration 3154, train loss: 0.005235, validation loss: 0.003439\n",
      "iteration 3155, train loss: 0.004984, validation loss: 0.00298\n",
      "iteration 3156, train loss: 0.004448, validation loss: 0.003433\n",
      "iteration 3157, train loss: 0.004724, validation loss: 0.003769\n",
      "iteration 3158, train loss: 0.004828, validation loss: 0.003738\n",
      "iteration 3159, train loss: 0.005332, validation loss: 0.003345\n",
      "iteration 3160, train loss: 0.004909, validation loss: 0.003098\n",
      "iteration 3161, train loss: 0.004758, validation loss: 0.00319\n",
      "iteration 3162, train loss: 0.0046, validation loss: 0.003384\n",
      "iteration 3163, train loss: 0.004569, validation loss: 0.003303\n",
      "iteration 3164, train loss: 0.00486, validation loss: 0.003221\n",
      "iteration 3165, train loss: 0.004427, validation loss: 0.003244\n",
      "iteration 3166, train loss: 0.004776, validation loss: 0.003928\n",
      "iteration 3167, train loss: 0.00525, validation loss: 0.004118\n",
      "iteration 3168, train loss: 0.005281, validation loss: 0.00335\n",
      "iteration 3169, train loss: 0.004705, validation loss: 0.003505\n",
      "iteration 3170, train loss: 0.00497, validation loss: 0.004305\n",
      "iteration 3171, train loss: 0.005331, validation loss: 0.004037\n",
      "iteration 3172, train loss: 0.005546, validation loss: 0.00321\n",
      "iteration 3173, train loss: 0.004327, validation loss: 0.003543\n",
      "iteration 3174, train loss: 0.004758, validation loss: 0.003936\n",
      "iteration 3175, train loss: 0.005455, validation loss: 0.00348\n",
      "iteration 3176, train loss: 0.004717, validation loss: 0.003144\n",
      "iteration 3177, train loss: 0.004219, validation loss: 0.003608\n",
      "iteration 3178, train loss: 0.004661, validation loss: 0.003993\n",
      "iteration 3179, train loss: 0.005066, validation loss: 0.003535\n",
      "iteration 3180, train loss: 0.004746, validation loss: 0.003157\n",
      "iteration 3181, train loss: 0.004572, validation loss: 0.003434\n",
      "iteration 3182, train loss: 0.004614, validation loss: 0.003288\n",
      "iteration 3183, train loss: 0.00485, validation loss: 0.002929\n",
      "iteration 3184, train loss: 0.004849, validation loss: 0.003227\n",
      "iteration 3185, train loss: 0.004494, validation loss: 0.003598\n",
      "iteration 3186, train loss: 0.005406, validation loss: 0.003336\n",
      "iteration 3187, train loss: 0.00467, validation loss: 0.002966\n",
      "iteration 3188, train loss: 0.004563, validation loss: 0.00303\n",
      "iteration 3189, train loss: 0.004102, validation loss: 0.003108\n",
      "iteration 3190, train loss: 0.005306, validation loss: 0.003057\n",
      "iteration 3191, train loss: 0.004317, validation loss: 0.003361\n",
      "iteration 3192, train loss: 0.004387, validation loss: 0.003402\n",
      "iteration 3193, train loss: 0.00448, validation loss: 0.003203\n",
      "iteration 3194, train loss: 0.004449, validation loss: 0.00302\n",
      "iteration 3195, train loss: 0.004445, validation loss: 0.003105\n",
      "iteration 3196, train loss: 0.004537, validation loss: 0.002992\n",
      "iteration 3197, train loss: 0.004754, validation loss: 0.003078\n",
      "iteration 3198, train loss: 0.004171, validation loss: 0.003421\n",
      "iteration 3199, train loss: 0.004591, validation loss: 0.00341\n",
      "iteration 3200, train loss: 0.004487, validation loss: 0.003259\n",
      "iteration 3201, train loss: 0.004358, validation loss: 0.003199\n",
      "iteration 3202, train loss: 0.004245, validation loss: 0.003139\n",
      "iteration 3203, train loss: 0.004459, validation loss: 0.002986\n",
      "iteration 3204, train loss: 0.004575, validation loss: 0.00305\n",
      "iteration 3205, train loss: 0.004357, validation loss: 0.003458\n",
      "iteration 3206, train loss: 0.004431, validation loss: 0.003717\n",
      "iteration 3207, train loss: 0.00481, validation loss: 0.0037\n",
      "iteration 3208, train loss: 0.004864, validation loss: 0.003523\n",
      "iteration 3209, train loss: 0.004675, validation loss: 0.00312\n",
      "iteration 3210, train loss: 0.004621, validation loss: \u001b[92m0.002862\u001b[0m\n",
      "iteration 3211, train loss: 0.004433, validation loss: 0.003154\n",
      "iteration 3212, train loss: 0.004527, validation loss: 0.003375\n",
      "iteration 3213, train loss: 0.005014, validation loss: 0.003406\n",
      "iteration 3214, train loss: 0.004457, validation loss: 0.003672\n",
      "iteration 3215, train loss: 0.004945, validation loss: 0.003262\n",
      "iteration 3216, train loss: 0.004598, validation loss: 0.002977\n",
      "iteration 3217, train loss: 0.004924, validation loss: 0.002915\n",
      "iteration 3218, train loss: 0.004391, validation loss: 0.003053\n",
      "iteration 3219, train loss: 0.004438, validation loss: 0.00305\n",
      "iteration 3220, train loss: \u001b[92m0.00409\u001b[0m, validation loss: 0.003059\n",
      "iteration 3221, train loss: 0.004688, validation loss: 0.002914\n",
      "iteration 3222, train loss: 0.00446, validation loss: 0.002913\n",
      "iteration 3223, train loss: 0.004388, validation loss: 0.003012\n",
      "iteration 3224, train loss: 0.004722, validation loss: 0.00311\n",
      "iteration 3225, train loss: 0.004694, validation loss: 0.003291\n",
      "iteration 3226, train loss: 0.004519, validation loss: 0.003313\n",
      "iteration 3227, train loss: 0.004634, validation loss: 0.002944\n",
      "iteration 3228, train loss: 0.004313, validation loss: 0.002968\n",
      "iteration 3229, train loss: 0.004224, validation loss: 0.003138\n",
      "iteration 3230, train loss: 0.004793, validation loss: 0.003012\n",
      "iteration 3231, train loss: 0.004169, validation loss: 0.003051\n",
      "iteration 3232, train loss: 0.004326, validation loss: 0.003218\n",
      "iteration 3233, train loss: 0.004855, validation loss: 0.003383\n",
      "iteration 3234, train loss: 0.004191, validation loss: 0.003508\n",
      "iteration 3235, train loss: 0.004286, validation loss: 0.003312\n",
      "iteration 3236, train loss: 0.00434, validation loss: 0.002862\n",
      "iteration 3237, train loss: 0.004371, validation loss: 0.002864\n",
      "iteration 3238, train loss: 0.004519, validation loss: 0.002937\n",
      "iteration 3239, train loss: 0.004379, validation loss: 0.003168\n",
      "iteration 3240, train loss: 0.004404, validation loss: 0.003393\n",
      "iteration 3241, train loss: 0.004493, validation loss: 0.003259\n",
      "iteration 3242, train loss: 0.004195, validation loss: 0.003016\n",
      "iteration 3243, train loss: 0.004351, validation loss: 0.002906\n",
      "iteration 3244, train loss: 0.004603, validation loss: 0.002911\n",
      "iteration 3245, train loss: 0.004491, validation loss: 0.003118\n",
      "iteration 3246, train loss: 0.00445, validation loss: 0.003244\n",
      "iteration 3247, train loss: 0.004353, validation loss: 0.003289\n",
      "iteration 3248, train loss: 0.004769, validation loss: 0.00305\n",
      "iteration 3249, train loss: 0.004726, validation loss: 0.003296\n",
      "iteration 3250, train loss: 0.005065, validation loss: 0.003277\n",
      "iteration 3251, train loss: 0.004634, validation loss: 0.003156\n",
      "iteration 3252, train loss: 0.004384, validation loss: 0.003244\n",
      "iteration 3253, train loss: 0.004379, validation loss: 0.003177\n",
      "iteration 3254, train loss: 0.004835, validation loss: 0.003082\n",
      "iteration 3255, train loss: 0.004506, validation loss: 0.003149\n",
      "iteration 3256, train loss: 0.004179, validation loss: 0.003223\n",
      "iteration 3257, train loss: 0.004246, validation loss: 0.003181\n",
      "iteration 3258, train loss: 0.004303, validation loss: 0.00304\n",
      "iteration 3259, train loss: 0.004323, validation loss: 0.002949\n",
      "iteration 3260, train loss: 0.004395, validation loss: 0.003023\n",
      "iteration 3261, train loss: 0.004466, validation loss: 0.003062\n",
      "iteration 3262, train loss: 0.004792, validation loss: 0.003037\n",
      "iteration 3263, train loss: 0.004607, validation loss: 0.002876\n",
      "iteration 3264, train loss: \u001b[92m0.003981\u001b[0m, validation loss: 0.00287\n",
      "iteration 3265, train loss: 0.004503, validation loss: \u001b[92m0.002842\u001b[0m\n",
      "iteration 3266, train loss: 0.004461, validation loss: 0.002903\n",
      "iteration 3267, train loss: 0.004222, validation loss: 0.003174\n",
      "iteration 3268, train loss: 0.004095, validation loss: 0.003433\n",
      "iteration 3269, train loss: 0.004421, validation loss: 0.003215\n",
      "iteration 3270, train loss: 0.004325, validation loss: 0.002944\n",
      "iteration 3271, train loss: 0.004771, validation loss: 0.002993\n",
      "iteration 3272, train loss: 0.004453, validation loss: 0.003128\n",
      "iteration 3273, train loss: 0.004573, validation loss: 0.002979\n",
      "iteration 3274, train loss: 0.004075, validation loss: 0.003189\n",
      "iteration 3275, train loss: 0.004587, validation loss: 0.003339\n",
      "iteration 3276, train loss: 0.004937, validation loss: 0.003147\n",
      "iteration 3277, train loss: 0.004414, validation loss: 0.003087\n",
      "iteration 3278, train loss: 0.004544, validation loss: 0.003069\n",
      "iteration 3279, train loss: 0.004437, validation loss: 0.002865\n",
      "iteration 3280, train loss: 0.004512, validation loss: 0.003093\n",
      "iteration 3281, train loss: 0.004306, validation loss: 0.003344\n",
      "iteration 3282, train loss: 0.004493, validation loss: 0.003226\n",
      "iteration 3283, train loss: 0.004441, validation loss: 0.00303\n",
      "iteration 3284, train loss: 0.004147, validation loss: 0.003048\n",
      "iteration 3285, train loss: 0.004459, validation loss: 0.003069\n",
      "iteration 3286, train loss: 0.004185, validation loss: 0.003081\n",
      "iteration 3287, train loss: 0.00446, validation loss: 0.002982\n",
      "iteration 3288, train loss: \u001b[92m0.003929\u001b[0m, validation loss: 0.003011\n",
      "iteration 3289, train loss: 0.004323, validation loss: 0.002916\n",
      "iteration 3290, train loss: 0.004875, validation loss: 0.002879\n",
      "iteration 3291, train loss: 0.004521, validation loss: 0.003233\n",
      "iteration 3292, train loss: 0.004091, validation loss: 0.003439\n",
      "iteration 3293, train loss: 0.004576, validation loss: 0.003018\n",
      "iteration 3294, train loss: 0.00431, validation loss: 0.002941\n",
      "iteration 3295, train loss: 0.004639, validation loss: 0.003092\n",
      "iteration 3296, train loss: 0.004531, validation loss: 0.003032\n",
      "iteration 3297, train loss: 0.004312, validation loss: 0.003464\n",
      "iteration 3298, train loss: 0.00442, validation loss: 0.003598\n",
      "iteration 3299, train loss: 0.004529, validation loss: 0.003284\n",
      "iteration 3300, train loss: 0.004335, validation loss: 0.003221\n",
      "iteration 3301, train loss: 0.004336, validation loss: 0.003131\n",
      "iteration 3302, train loss: 0.004933, validation loss: 0.00291\n",
      "iteration 3303, train loss: 0.004357, validation loss: 0.003295\n",
      "iteration 3304, train loss: 0.004982, validation loss: 0.003242\n",
      "iteration 3305, train loss: 0.004664, validation loss: 0.002964\n",
      "iteration 3306, train loss: 0.004596, validation loss: 0.003123\n",
      "iteration 3307, train loss: 0.004305, validation loss: 0.003532\n",
      "iteration 3308, train loss: 0.004642, validation loss: 0.003335\n",
      "iteration 3309, train loss: 0.004397, validation loss: 0.002938\n",
      "iteration 3310, train loss: 0.004059, validation loss: 0.00292\n",
      "iteration 3311, train loss: 0.00487, validation loss: 0.003052\n",
      "iteration 3312, train loss: 0.00484, validation loss: 0.003021\n",
      "iteration 3313, train loss: 0.004561, validation loss: 0.003337\n",
      "iteration 3314, train loss: 0.004156, validation loss: 0.003453\n",
      "iteration 3315, train loss: 0.004565, validation loss: 0.003222\n",
      "iteration 3316, train loss: 0.00424, validation loss: 0.002998\n",
      "iteration 3317, train loss: 0.004507, validation loss: 0.00294\n",
      "iteration 3318, train loss: 0.004472, validation loss: 0.002891\n",
      "iteration 3319, train loss: 0.004593, validation loss: 0.00323\n",
      "iteration 3320, train loss: 0.004346, validation loss: 0.003516\n",
      "iteration 3321, train loss: 0.004864, validation loss: 0.003129\n",
      "iteration 3322, train loss: 0.004346, validation loss: 0.002897\n",
      "iteration 3323, train loss: 0.00403, validation loss: 0.003081\n",
      "iteration 3324, train loss: 0.00481, validation loss: 0.00312\n",
      "iteration 3325, train loss: 0.004545, validation loss: 0.00303\n",
      "iteration 3326, train loss: 0.004691, validation loss: 0.003419\n",
      "iteration 3327, train loss: 0.00487, validation loss: 0.003522\n",
      "iteration 3328, train loss: 0.004393, validation loss: 0.003176\n",
      "iteration 3329, train loss: 0.004475, validation loss: 0.002935\n",
      "iteration 3330, train loss: 0.004508, validation loss: 0.002978\n",
      "iteration 3331, train loss: 0.004627, validation loss: 0.002959\n",
      "iteration 3332, train loss: 0.003955, validation loss: 0.00362\n",
      "iteration 3333, train loss: 0.004324, validation loss: 0.003987\n",
      "iteration 3334, train loss: 0.005047, validation loss: 0.003386\n",
      "iteration 3335, train loss: 0.004897, validation loss: 0.003069\n",
      "iteration 3336, train loss: 0.004509, validation loss: 0.003558\n",
      "iteration 3337, train loss: 0.005117, validation loss: 0.003212\n",
      "iteration 3338, train loss: 0.004962, validation loss: 0.003046\n",
      "iteration 3339, train loss: 0.004523, validation loss: 0.003793\n",
      "iteration 3340, train loss: 0.005087, validation loss: 0.003737\n",
      "iteration 3341, train loss: 0.004872, validation loss: 0.003139\n",
      "iteration 3342, train loss: 0.004103, validation loss: 0.00322\n",
      "iteration 3343, train loss: 0.004622, validation loss: 0.003502\n",
      "iteration 3344, train loss: 0.004327, validation loss: 0.003387\n",
      "iteration 3345, train loss: 0.004982, validation loss: 0.00304\n",
      "iteration 3346, train loss: 0.004208, validation loss: 0.00367\n",
      "iteration 3347, train loss: 0.004773, validation loss: 0.003755\n",
      "iteration 3348, train loss: 0.004987, validation loss: 0.003278\n",
      "iteration 3349, train loss: 0.004562, validation loss: 0.00291\n",
      "iteration 3350, train loss: 0.003968, validation loss: 0.003058\n",
      "iteration 3351, train loss: 0.004847, validation loss: 0.003\n",
      "iteration 3352, train loss: 0.004243, validation loss: 0.002974\n",
      "iteration 3353, train loss: 0.004307, validation loss: 0.003124\n",
      "iteration 3354, train loss: 0.004487, validation loss: 0.003268\n",
      "iteration 3355, train loss: 0.00429, validation loss: 0.003234\n",
      "iteration 3356, train loss: 0.004528, validation loss: 0.002925\n",
      "iteration 3357, train loss: 0.004515, validation loss: \u001b[92m0.002837\u001b[0m\n",
      "iteration 3358, train loss: 0.0045, validation loss: \u001b[92m0.002799\u001b[0m\n",
      "iteration 3359, train loss: 0.004351, validation loss: 0.002948\n",
      "iteration 3360, train loss: 0.004309, validation loss: 0.002961\n",
      "iteration 3361, train loss: 0.00457, validation loss: 0.003018\n",
      "iteration 3362, train loss: 0.004479, validation loss: 0.003084\n",
      "iteration 3363, train loss: 0.004373, validation loss: 0.003069\n",
      "iteration 3364, train loss: 0.004349, validation loss: 0.003041\n",
      "iteration 3365, train loss: 0.0043, validation loss: 0.002978\n",
      "iteration 3366, train loss: 0.004297, validation loss: 0.002979\n",
      "iteration 3367, train loss: 0.004082, validation loss: 0.003039\n",
      "iteration 3368, train loss: 0.004242, validation loss: 0.003076\n",
      "iteration 3369, train loss: 0.00459, validation loss: 0.003006\n",
      "iteration 3370, train loss: 0.004568, validation loss: 0.002932\n",
      "iteration 3371, train loss: 0.004491, validation loss: 0.003158\n",
      "iteration 3372, train loss: 0.004529, validation loss: 0.003322\n",
      "iteration 3373, train loss: 0.004182, validation loss: 0.003312\n",
      "iteration 3374, train loss: 0.004277, validation loss: 0.003337\n",
      "iteration 3375, train loss: 0.004477, validation loss: 0.003133\n",
      "iteration 3376, train loss: 0.004413, validation loss: 0.002902\n",
      "iteration 3377, train loss: 0.00426, validation loss: \u001b[92m0.002758\u001b[0m\n",
      "iteration 3378, train loss: 0.004625, validation loss: 0.003069\n",
      "iteration 3379, train loss: 0.004355, validation loss: 0.003443\n",
      "iteration 3380, train loss: 0.004348, validation loss: 0.003377\n",
      "iteration 3381, train loss: 0.004329, validation loss: 0.003203\n",
      "iteration 3382, train loss: 0.004295, validation loss: 0.00328\n",
      "iteration 3383, train loss: 0.004276, validation loss: 0.00325\n",
      "iteration 3384, train loss: 0.004567, validation loss: 0.002921\n",
      "iteration 3385, train loss: \u001b[92m0.003914\u001b[0m, validation loss: 0.002839\n",
      "iteration 3386, train loss: 0.004009, validation loss: 0.00333\n",
      "iteration 3387, train loss: 0.004441, validation loss: 0.00356\n",
      "iteration 3388, train loss: 0.004856, validation loss: 0.003292\n",
      "iteration 3389, train loss: 0.004396, validation loss: 0.00326\n",
      "iteration 3390, train loss: 0.004099, validation loss: 0.003509\n",
      "iteration 3391, train loss: 0.004784, validation loss: 0.003172\n",
      "iteration 3392, train loss: 0.004914, validation loss: 0.002864\n",
      "iteration 3393, train loss: 0.004155, validation loss: 0.003506\n",
      "iteration 3394, train loss: 0.004388, validation loss: 0.003736\n",
      "iteration 3395, train loss: 0.004828, validation loss: 0.003235\n",
      "iteration 3396, train loss: 0.00412, validation loss: 0.003097\n",
      "iteration 3397, train loss: 0.004209, validation loss: 0.003557\n",
      "iteration 3398, train loss: 0.00499, validation loss: 0.003384\n",
      "iteration 3399, train loss: 0.004716, validation loss: 0.003209\n",
      "iteration 3400, train loss: 0.004338, validation loss: 0.003536\n",
      "iteration 3401, train loss: 0.00462, validation loss: 0.003798\n",
      "iteration 3402, train loss: 0.004885, validation loss: 0.003314\n",
      "iteration 3403, train loss: 0.004508, validation loss: 0.003062\n",
      "iteration 3404, train loss: 0.004204, validation loss: 0.003415\n",
      "iteration 3405, train loss: 0.004633, validation loss: 0.003391\n",
      "iteration 3406, train loss: 0.004332, validation loss: 0.003166\n",
      "iteration 3407, train loss: 0.00401, validation loss: 0.00295\n",
      "iteration 3408, train loss: 0.004155, validation loss: 0.003095\n",
      "iteration 3409, train loss: 0.004712, validation loss: 0.003327\n",
      "iteration 3410, train loss: 0.004951, validation loss: 0.003111\n",
      "iteration 3411, train loss: 0.004805, validation loss: 0.003113\n",
      "iteration 3412, train loss: 0.003956, validation loss: 0.003317\n",
      "iteration 3413, train loss: 0.004319, validation loss: 0.003118\n",
      "iteration 3414, train loss: 0.004279, validation loss: 0.002927\n",
      "iteration 3415, train loss: 0.004045, validation loss: 0.002869\n",
      "iteration 3416, train loss: 0.004505, validation loss: 0.002929\n",
      "iteration 3417, train loss: 0.00455, validation loss: 0.003083\n",
      "iteration 3418, train loss: 0.004294, validation loss: 0.003248\n",
      "iteration 3419, train loss: 0.004782, validation loss: 0.002944\n",
      "iteration 3420, train loss: 0.004475, validation loss: 0.00282\n",
      "iteration 3421, train loss: 0.003952, validation loss: 0.002896\n",
      "iteration 3422, train loss: 0.004922, validation loss: \u001b[92m0.002743\u001b[0m\n",
      "iteration 3423, train loss: 0.004066, validation loss: 0.002826\n",
      "iteration 3424, train loss: 0.004415, validation loss: 0.003288\n",
      "iteration 3425, train loss: 0.004258, validation loss: 0.003573\n",
      "iteration 3426, train loss: 0.004498, validation loss: 0.003283\n",
      "iteration 3427, train loss: 0.004621, validation loss: 0.002903\n",
      "iteration 3428, train loss: 0.004477, validation loss: 0.002832\n",
      "iteration 3429, train loss: 0.004628, validation loss: 0.002743\n",
      "iteration 3430, train loss: 0.004672, validation loss: 0.002806\n",
      "iteration 3431, train loss: 0.004271, validation loss: 0.003129\n",
      "iteration 3432, train loss: 0.004357, validation loss: 0.003169\n",
      "iteration 3433, train loss: 0.004297, validation loss: 0.002974\n",
      "iteration 3434, train loss: 0.004402, validation loss: 0.002792\n",
      "iteration 3435, train loss: 0.004182, validation loss: 0.002889\n",
      "iteration 3436, train loss: 0.004256, validation loss: 0.00295\n",
      "iteration 3437, train loss: 0.004217, validation loss: 0.00335\n",
      "iteration 3438, train loss: 0.004195, validation loss: 0.003813\n",
      "iteration 3439, train loss: 0.004617, validation loss: 0.003647\n",
      "iteration 3440, train loss: 0.004409, validation loss: 0.003091\n",
      "iteration 3441, train loss: 0.00465, validation loss: 0.002746\n",
      "iteration 3442, train loss: 0.004153, validation loss: 0.002971\n",
      "iteration 3443, train loss: 0.004249, validation loss: 0.002878\n",
      "iteration 3444, train loss: 0.004559, validation loss: 0.002847\n",
      "iteration 3445, train loss: 0.004205, validation loss: 0.003239\n",
      "iteration 3446, train loss: 0.004388, validation loss: 0.00326\n",
      "iteration 3447, train loss: 0.004027, validation loss: 0.003079\n",
      "iteration 3448, train loss: 0.004387, validation loss: 0.002765\n",
      "iteration 3449, train loss: 0.004371, validation loss: 0.003119\n",
      "iteration 3450, train loss: 0.004616, validation loss: 0.003131\n",
      "iteration 3451, train loss: 0.004766, validation loss: 0.002854\n",
      "iteration 3452, train loss: 0.004269, validation loss: 0.003158\n",
      "iteration 3453, train loss: 0.004739, validation loss: 0.003234\n",
      "iteration 3454, train loss: 0.004669, validation loss: 0.002927\n",
      "iteration 3455, train loss: 0.004244, validation loss: 0.002836\n",
      "iteration 3456, train loss: 0.004063, validation loss: 0.002992\n",
      "iteration 3457, train loss: 0.00435, validation loss: 0.00288\n",
      "iteration 3458, train loss: 0.004038, validation loss: 0.002867\n",
      "iteration 3459, train loss: 0.004533, validation loss: 0.003175\n",
      "iteration 3460, train loss: 0.004345, validation loss: 0.003263\n",
      "iteration 3461, train loss: 0.004548, validation loss: 0.002975\n",
      "iteration 3462, train loss: 0.004093, validation loss: 0.003001\n",
      "iteration 3463, train loss: 0.004195, validation loss: 0.003164\n",
      "iteration 3464, train loss: 0.003992, validation loss: 0.00311\n",
      "iteration 3465, train loss: 0.004143, validation loss: 0.002793\n",
      "iteration 3466, train loss: 0.004025, validation loss: 0.002958\n",
      "iteration 3467, train loss: 0.004209, validation loss: 0.00321\n",
      "iteration 3468, train loss: 0.00429, validation loss: 0.003099\n",
      "iteration 3469, train loss: 0.004595, validation loss: 0.002929\n",
      "iteration 3470, train loss: \u001b[92m0.00388\u001b[0m, validation loss: 0.002893\n",
      "iteration 3471, train loss: 0.004251, validation loss: 0.003036\n",
      "iteration 3472, train loss: 0.004034, validation loss: 0.003022\n",
      "iteration 3473, train loss: 0.004191, validation loss: 0.002909\n",
      "iteration 3474, train loss: 0.004413, validation loss: 0.002847\n",
      "iteration 3475, train loss: 0.00422, validation loss: 0.002837\n",
      "iteration 3476, train loss: 0.004305, validation loss: 0.002848\n",
      "iteration 3477, train loss: 0.004622, validation loss: 0.002768\n",
      "iteration 3478, train loss: 0.004472, validation loss: 0.002762\n",
      "iteration 3479, train loss: 0.004411, validation loss: 0.00303\n",
      "iteration 3480, train loss: 0.00466, validation loss: 0.003176\n",
      "iteration 3481, train loss: 0.004783, validation loss: 0.003109\n",
      "iteration 3482, train loss: 0.003938, validation loss: 0.003212\n",
      "iteration 3483, train loss: 0.004268, validation loss: 0.003086\n",
      "iteration 3484, train loss: 0.004287, validation loss: 0.002877\n",
      "iteration 3485, train loss: 0.004242, validation loss: 0.002977\n",
      "iteration 3486, train loss: 0.004208, validation loss: 0.00303\n",
      "iteration 3487, train loss: 0.004164, validation loss: 0.002921\n",
      "iteration 3488, train loss: 0.004067, validation loss: 0.002849\n",
      "iteration 3489, train loss: 0.00418, validation loss: 0.00298\n",
      "iteration 3490, train loss: 0.004141, validation loss: 0.003135\n",
      "iteration 3491, train loss: 0.004084, validation loss: 0.003003\n",
      "iteration 3492, train loss: 0.004101, validation loss: 0.002916\n",
      "iteration 3493, train loss: \u001b[92m0.00388\u001b[0m, validation loss: 0.002847\n",
      "iteration 3494, train loss: 0.003946, validation loss: \u001b[92m0.002724\u001b[0m\n",
      "iteration 3495, train loss: 0.004296, validation loss: 0.002792\n",
      "iteration 3496, train loss: 0.004059, validation loss: 0.002907\n",
      "iteration 3497, train loss: 0.004054, validation loss: 0.003079\n",
      "iteration 3498, train loss: 0.004216, validation loss: 0.003195\n",
      "iteration 3499, train loss: 0.004414, validation loss: 0.003057\n",
      "iteration 3500, train loss: 0.004167, validation loss: 0.002772\n",
      "iteration 3501, train loss: 0.004062, validation loss: 0.002758\n",
      "iteration 3502, train loss: 0.004358, validation loss: 0.002797\n",
      "iteration 3503, train loss: 0.00431, validation loss: 0.002856\n",
      "iteration 3504, train loss: 0.004128, validation loss: 0.002979\n",
      "iteration 3505, train loss: 0.004016, validation loss: 0.002909\n",
      "iteration 3506, train loss: 0.00415, validation loss: 0.002839\n",
      "iteration 3507, train loss: 0.004065, validation loss: 0.002757\n",
      "iteration 3508, train loss: 0.00405, validation loss: 0.002781\n",
      "iteration 3509, train loss: 0.00437, validation loss: 0.003246\n",
      "iteration 3510, train loss: 0.004544, validation loss: 0.003135\n",
      "iteration 3511, train loss: 0.004451, validation loss: 0.003079\n",
      "iteration 3512, train loss: 0.003881, validation loss: 0.003226\n",
      "iteration 3513, train loss: 0.004674, validation loss: 0.002856\n",
      "iteration 3514, train loss: 0.004097, validation loss: 0.002884\n",
      "iteration 3515, train loss: 0.003995, validation loss: 0.003225\n",
      "iteration 3516, train loss: 0.004237, validation loss: 0.003351\n",
      "iteration 3517, train loss: 0.004586, validation loss: 0.002857\n",
      "iteration 3518, train loss: 0.004254, validation loss: 0.002977\n",
      "iteration 3519, train loss: 0.004957, validation loss: 0.00302\n",
      "iteration 3520, train loss: 0.004292, validation loss: 0.002965\n",
      "iteration 3521, train loss: 0.00409, validation loss: 0.003308\n",
      "iteration 3522, train loss: 0.004706, validation loss: 0.003084\n",
      "iteration 3523, train loss: 0.004635, validation loss: \u001b[92m0.002721\u001b[0m\n",
      "iteration 3524, train loss: 0.004015, validation loss: 0.003042\n",
      "iteration 3525, train loss: 0.004376, validation loss: 0.002976\n",
      "iteration 3526, train loss: 0.004544, validation loss: 0.003123\n",
      "iteration 3527, train loss: 0.004201, validation loss: 0.003825\n",
      "iteration 3528, train loss: 0.005171, validation loss: 0.003536\n",
      "iteration 3529, train loss: 0.004738, validation loss: 0.002789\n",
      "iteration 3530, train loss: 0.003906, validation loss: 0.003254\n",
      "iteration 3531, train loss: 0.00463, validation loss: 0.003492\n",
      "iteration 3532, train loss: 0.005482, validation loss: 0.002953\n",
      "iteration 3533, train loss: 0.004295, validation loss: 0.003723\n",
      "iteration 3534, train loss: 0.00456, validation loss: 0.004095\n",
      "iteration 3535, train loss: 0.004622, validation loss: 0.003445\n",
      "iteration 3536, train loss: 0.004652, validation loss: 0.00278\n",
      "iteration 3537, train loss: 0.003968, validation loss: 0.003207\n",
      "iteration 3538, train loss: 0.00492, validation loss: 0.003067\n",
      "iteration 3539, train loss: 0.004204, validation loss: 0.002794\n",
      "iteration 3540, train loss: 0.003945, validation loss: 0.003074\n",
      "iteration 3541, train loss: 0.004141, validation loss: 0.00337\n",
      "iteration 3542, train loss: 0.004636, validation loss: 0.003173\n",
      "iteration 3543, train loss: 0.004275, validation loss: 0.002837\n",
      "iteration 3544, train loss: 0.004442, validation loss: 0.002916\n",
      "iteration 3545, train loss: 0.004898, validation loss: 0.0031\n",
      "iteration 3546, train loss: 0.004958, validation loss: 0.003037\n",
      "iteration 3547, train loss: 0.004542, validation loss: 0.002872\n",
      "iteration 3548, train loss: 0.004273, validation loss: 0.003165\n",
      "iteration 3549, train loss: 0.004502, validation loss: 0.003229\n",
      "iteration 3550, train loss: 0.004392, validation loss: 0.002891\n",
      "iteration 3551, train loss: 0.003925, validation loss: 0.002758\n",
      "iteration 3552, train loss: \u001b[92m0.0037\u001b[0m, validation loss: 0.002848\n",
      "iteration 3553, train loss: 0.004283, validation loss: 0.002784\n",
      "iteration 3554, train loss: 0.003912, validation loss: 0.002767\n",
      "iteration 3555, train loss: 0.003874, validation loss: 0.002816\n",
      "iteration 3556, train loss: 0.004134, validation loss: 0.002945\n",
      "iteration 3557, train loss: 0.004273, validation loss: 0.002807\n",
      "iteration 3558, train loss: 0.004211, validation loss: 0.002799\n",
      "iteration 3559, train loss: 0.003958, validation loss: 0.003099\n",
      "iteration 3560, train loss: 0.004432, validation loss: 0.003292\n",
      "iteration 3561, train loss: 0.004244, validation loss: 0.003078\n",
      "iteration 3562, train loss: 0.004238, validation loss: 0.002805\n",
      "iteration 3563, train loss: 0.004205, validation loss: 0.002808\n",
      "iteration 3564, train loss: 0.003922, validation loss: 0.002901\n",
      "iteration 3565, train loss: 0.004191, validation loss: 0.002873\n",
      "iteration 3566, train loss: 0.004397, validation loss: 0.002783\n",
      "iteration 3567, train loss: 0.004284, validation loss: 0.002813\n",
      "iteration 3568, train loss: 0.004451, validation loss: 0.002968\n",
      "iteration 3569, train loss: 0.004208, validation loss: 0.003041\n",
      "iteration 3570, train loss: 0.004232, validation loss: 0.002926\n",
      "iteration 3571, train loss: 0.003963, validation loss: 0.002883\n",
      "iteration 3572, train loss: 0.004157, validation loss: 0.002864\n",
      "iteration 3573, train loss: 0.00444, validation loss: 0.002919\n",
      "iteration 3574, train loss: 0.00422, validation loss: 0.002955\n",
      "iteration 3575, train loss: 0.004418, validation loss: 0.002873\n",
      "iteration 3576, train loss: 0.004086, validation loss: 0.003039\n",
      "iteration 3577, train loss: 0.004267, validation loss: 0.00295\n",
      "iteration 3578, train loss: 0.003937, validation loss: 0.002768\n",
      "iteration 3579, train loss: 0.004197, validation loss: 0.002893\n",
      "iteration 3580, train loss: 0.004265, validation loss: 0.003059\n",
      "iteration 3581, train loss: 0.004243, validation loss: 0.003005\n",
      "iteration 3582, train loss: 0.003914, validation loss: 0.002932\n",
      "iteration 3583, train loss: 0.0045, validation loss: 0.002777\n",
      "iteration 3584, train loss: 0.004128, validation loss: 0.002812\n",
      "iteration 3585, train loss: 0.004203, validation loss: 0.002849\n",
      "iteration 3586, train loss: 0.004008, validation loss: 0.002942\n",
      "iteration 3587, train loss: 0.004287, validation loss: 0.002912\n",
      "iteration 3588, train loss: 0.004122, validation loss: 0.00297\n",
      "iteration 3589, train loss: 0.00386, validation loss: 0.002971\n",
      "iteration 3590, train loss: 0.004709, validation loss: 0.002765\n",
      "iteration 3591, train loss: 0.004571, validation loss: 0.003082\n",
      "iteration 3592, train loss: 0.004469, validation loss: 0.003219\n",
      "iteration 3593, train loss: 0.00423, validation loss: 0.002946\n",
      "iteration 3594, train loss: 0.00401, validation loss: 0.00327\n",
      "iteration 3595, train loss: 0.003849, validation loss: 0.003828\n",
      "iteration 3596, train loss: 0.00443, validation loss: 0.003494\n",
      "iteration 3597, train loss: 0.005016, validation loss: 0.002857\n",
      "iteration 3598, train loss: 0.004066, validation loss: 0.003341\n",
      "iteration 3599, train loss: 0.004164, validation loss: 0.003837\n",
      "iteration 3600, train loss: 0.005133, validation loss: 0.003296\n",
      "iteration 3601, train loss: 0.004478, validation loss: 0.002902\n",
      "iteration 3602, train loss: 0.004031, validation loss: 0.003393\n",
      "iteration 3603, train loss: 0.004378, validation loss: 0.003486\n",
      "iteration 3604, train loss: 0.004726, validation loss: 0.003103\n",
      "iteration 3605, train loss: 0.004288, validation loss: 0.002784\n",
      "iteration 3606, train loss: 0.004211, validation loss: 0.003093\n",
      "iteration 3607, train loss: 0.003998, validation loss: 0.003249\n",
      "iteration 3608, train loss: 0.004872, validation loss: 0.002982\n",
      "iteration 3609, train loss: 0.004271, validation loss: 0.003137\n",
      "iteration 3610, train loss: 0.004648, validation loss: 0.003452\n",
      "iteration 3611, train loss: 0.004502, validation loss: 0.003206\n",
      "iteration 3612, train loss: 0.004067, validation loss: 0.00296\n",
      "iteration 3613, train loss: 0.004131, validation loss: 0.003096\n",
      "iteration 3614, train loss: 0.004532, validation loss: 0.003142\n",
      "iteration 3615, train loss: 0.004628, validation loss: 0.003091\n",
      "iteration 3616, train loss: 0.004821, validation loss: 0.00335\n",
      "iteration 3617, train loss: 0.004656, validation loss: 0.003664\n",
      "iteration 3618, train loss: 0.004405, validation loss: 0.003871\n",
      "iteration 3619, train loss: 0.004433, validation loss: 0.003854\n",
      "iteration 3620, train loss: 0.00433, validation loss: 0.003236\n",
      "iteration 3621, train loss: 0.004255, validation loss: 0.002902\n",
      "iteration 3622, train loss: 0.004089, validation loss: 0.003311\n",
      "iteration 3623, train loss: 0.004777, validation loss: 0.003529\n",
      "iteration 3624, train loss: 0.004525, validation loss: 0.003515\n",
      "iteration 3625, train loss: 0.004807, validation loss: 0.003231\n",
      "iteration 3626, train loss: 0.0043, validation loss: 0.003436\n",
      "iteration 3627, train loss: 0.004479, validation loss: 0.003612\n",
      "iteration 3628, train loss: 0.005232, validation loss: 0.003083\n",
      "iteration 3629, train loss: 0.004651, validation loss: 0.00289\n",
      "iteration 3630, train loss: 0.004053, validation loss: 0.00366\n",
      "iteration 3631, train loss: 0.004791, validation loss: 0.003668\n",
      "iteration 3632, train loss: 0.004936, validation loss: 0.003009\n",
      "iteration 3633, train loss: 0.00426, validation loss: 0.002835\n",
      "iteration 3634, train loss: 0.004484, validation loss: 0.002992\n",
      "iteration 3635, train loss: 0.004349, validation loss: 0.00295\n",
      "iteration 3636, train loss: 0.004612, validation loss: 0.002828\n",
      "iteration 3637, train loss: 0.004216, validation loss: 0.003138\n",
      "iteration 3638, train loss: 0.004198, validation loss: 0.003492\n",
      "iteration 3639, train loss: 0.004579, validation loss: 0.003259\n",
      "iteration 3640, train loss: 0.004376, validation loss: 0.002766\n",
      "iteration 3641, train loss: 0.00411, validation loss: 0.002739\n",
      "iteration 3642, train loss: 0.003994, validation loss: 0.00301\n",
      "iteration 3643, train loss: 0.004519, validation loss: 0.002993\n",
      "iteration 3644, train loss: 0.004756, validation loss: 0.002929\n",
      "iteration 3645, train loss: 0.004582, validation loss: 0.003187\n",
      "iteration 3646, train loss: 0.004332, validation loss: 0.003233\n",
      "iteration 3647, train loss: 0.004277, validation loss: 0.003009\n",
      "iteration 3648, train loss: 0.004373, validation loss: 0.003019\n",
      "iteration 3649, train loss: 0.004306, validation loss: 0.003175\n",
      "iteration 3650, train loss: 0.004905, validation loss: 0.003033\n",
      "iteration 3651, train loss: 0.004891, validation loss: 0.002971\n",
      "iteration 3652, train loss: 0.004351, validation loss: 0.00331\n",
      "iteration 3653, train loss: 0.004534, validation loss: 0.003415\n",
      "iteration 3654, train loss: 0.004927, validation loss: 0.003272\n",
      "iteration 3655, train loss: 0.004154, validation loss: 0.003112\n",
      "iteration 3656, train loss: 0.004656, validation loss: 0.003094\n",
      "iteration 3657, train loss: 0.00411, validation loss: 0.003143\n",
      "iteration 3658, train loss: 0.004294, validation loss: 0.003105\n",
      "iteration 3659, train loss: 0.003887, validation loss: 0.002968\n",
      "iteration 3660, train loss: 0.004317, validation loss: 0.002801\n",
      "iteration 3661, train loss: 0.004368, validation loss: 0.002898\n",
      "iteration 3662, train loss: 0.004446, validation loss: 0.003065\n",
      "iteration 3663, train loss: 0.004059, validation loss: 0.003147\n",
      "iteration 3664, train loss: 0.004491, validation loss: 0.002985\n",
      "iteration 3665, train loss: 0.004393, validation loss: 0.00295\n",
      "iteration 3666, train loss: 0.003974, validation loss: 0.00304\n",
      "iteration 3667, train loss: 0.004093, validation loss: 0.00302\n",
      "iteration 3668, train loss: 0.004545, validation loss: \u001b[92m0.002717\u001b[0m\n",
      "iteration 3669, train loss: 0.004184, validation loss: 0.003027\n",
      "iteration 3670, train loss: 0.004213, validation loss: 0.00329\n",
      "iteration 3671, train loss: 0.004441, validation loss: 0.003\n",
      "iteration 3672, train loss: 0.003909, validation loss: 0.002768\n",
      "iteration 3673, train loss: 0.003908, validation loss: 0.002892\n",
      "iteration 3674, train loss: 0.004408, validation loss: 0.003006\n",
      "iteration 3675, train loss: 0.004678, validation loss: 0.002731\n",
      "iteration 3676, train loss: 0.004462, validation loss: 0.003012\n",
      "iteration 3677, train loss: 0.004115, validation loss: 0.003478\n",
      "iteration 3678, train loss: 0.004509, validation loss: 0.003285\n",
      "iteration 3679, train loss: 0.004574, validation loss: 0.002863\n",
      "iteration 3680, train loss: \u001b[92m0.003674\u001b[0m, validation loss: 0.00289\n",
      "iteration 3681, train loss: 0.004774, validation loss: 0.00286\n",
      "iteration 3682, train loss: 0.004118, validation loss: 0.002814\n",
      "iteration 3683, train loss: 0.004341, validation loss: 0.00286\n",
      "iteration 3684, train loss: 0.004167, validation loss: 0.003104\n",
      "iteration 3685, train loss: 0.00455, validation loss: 0.003186\n",
      "iteration 3686, train loss: 0.004396, validation loss: 0.002941\n",
      "iteration 3687, train loss: 0.003958, validation loss: 0.002913\n",
      "iteration 3688, train loss: 0.004058, validation loss: 0.002937\n",
      "iteration 3689, train loss: 0.004701, validation loss: 0.002921\n",
      "iteration 3690, train loss: 0.004531, validation loss: 0.002832\n",
      "iteration 3691, train loss: 0.004092, validation loss: 0.002872\n",
      "iteration 3692, train loss: 0.004162, validation loss: 0.003127\n",
      "iteration 3693, train loss: 0.004338, validation loss: 0.003041\n",
      "iteration 3694, train loss: 0.004203, validation loss: 0.002747\n",
      "iteration 3695, train loss: 0.003915, validation loss: 0.002734\n",
      "iteration 3696, train loss: 0.004284, validation loss: 0.002883\n",
      "iteration 3697, train loss: 0.004081, validation loss: 0.002992\n",
      "iteration 3698, train loss: 0.004018, validation loss: 0.003017\n",
      "iteration 3699, train loss: 0.004109, validation loss: 0.002965\n",
      "iteration 3700, train loss: 0.003864, validation loss: 0.002911\n",
      "iteration 3701, train loss: 0.004223, validation loss: 0.002767\n",
      "iteration 3702, train loss: 0.004264, validation loss: 0.0029\n",
      "iteration 3703, train loss: 0.004695, validation loss: 0.002891\n",
      "iteration 3704, train loss: 0.004365, validation loss: 0.002937\n",
      "iteration 3705, train loss: 0.004179, validation loss: 0.003228\n",
      "iteration 3706, train loss: 0.00441, validation loss: 0.002979\n",
      "iteration 3707, train loss: 0.004442, validation loss: \u001b[92m0.002689\u001b[0m\n",
      "iteration 3708, train loss: 0.00427, validation loss: 0.003008\n",
      "iteration 3709, train loss: 0.004558, validation loss: 0.003154\n",
      "iteration 3710, train loss: 0.004602, validation loss: 0.002962\n",
      "iteration 3711, train loss: 0.004164, validation loss: 0.003287\n",
      "iteration 3712, train loss: 0.004313, validation loss: 0.003489\n",
      "iteration 3713, train loss: 0.004467, validation loss: 0.003189\n",
      "iteration 3714, train loss: 0.004579, validation loss: 0.002743\n",
      "iteration 3715, train loss: 0.003905, validation loss: 0.003024\n",
      "iteration 3716, train loss: 0.004422, validation loss: 0.003163\n",
      "iteration 3717, train loss: 0.00433, validation loss: 0.002902\n",
      "iteration 3718, train loss: 0.004031, validation loss: 0.002946\n",
      "iteration 3719, train loss: 0.003943, validation loss: 0.003335\n",
      "iteration 3720, train loss: 0.004316, validation loss: 0.003238\n",
      "iteration 3721, train loss: 0.004183, validation loss: 0.002839\n",
      "iteration 3722, train loss: 0.004351, validation loss: 0.002949\n",
      "iteration 3723, train loss: 0.004253, validation loss: 0.00328\n",
      "iteration 3724, train loss: 0.004709, validation loss: 0.003208\n",
      "iteration 3725, train loss: 0.004742, validation loss: 0.002861\n",
      "iteration 3726, train loss: 0.004175, validation loss: 0.003364\n",
      "iteration 3727, train loss: 0.004484, validation loss: 0.003749\n",
      "iteration 3728, train loss: 0.005166, validation loss: 0.00324\n",
      "iteration 3729, train loss: 0.004313, validation loss: 0.002791\n",
      "iteration 3730, train loss: 0.004186, validation loss: 0.003041\n",
      "iteration 3731, train loss: 0.004325, validation loss: 0.003258\n",
      "iteration 3732, train loss: 0.0043, validation loss: 0.00309\n",
      "iteration 3733, train loss: 0.004194, validation loss: 0.002901\n",
      "iteration 3734, train loss: 0.003876, validation loss: 0.003027\n",
      "iteration 3735, train loss: 0.004094, validation loss: 0.003118\n",
      "iteration 3736, train loss: 0.004551, validation loss: 0.002894\n",
      "iteration 3737, train loss: 0.004128, validation loss: 0.002981\n",
      "iteration 3738, train loss: 0.004709, validation loss: 0.003309\n",
      "iteration 3739, train loss: 0.004645, validation loss: 0.003263\n",
      "iteration 3740, train loss: 0.004579, validation loss: 0.002763\n",
      "iteration 3741, train loss: 0.004204, validation loss: 0.002727\n",
      "iteration 3742, train loss: 0.004094, validation loss: 0.003018\n",
      "iteration 3743, train loss: 0.004172, validation loss: 0.003212\n",
      "iteration 3744, train loss: 0.004187, validation loss: 0.003098\n",
      "iteration 3745, train loss: 0.004155, validation loss: 0.002873\n",
      "iteration 3746, train loss: 0.004251, validation loss: 0.002802\n",
      "iteration 3747, train loss: 0.004743, validation loss: 0.002723\n",
      "iteration 3748, train loss: 0.004046, validation loss: \u001b[92m0.002657\u001b[0m\n",
      "iteration 3749, train loss: 0.004281, validation loss: 0.002776\n",
      "iteration 3750, train loss: 0.003883, validation loss: 0.002972\n",
      "iteration 3751, train loss: 0.004314, validation loss: 0.002954\n",
      "iteration 3752, train loss: 0.003746, validation loss: 0.002855\n",
      "iteration 3753, train loss: 0.003979, validation loss: 0.002718\n",
      "iteration 3754, train loss: 0.004051, validation loss: \u001b[92m0.002625\u001b[0m\n",
      "iteration 3755, train loss: 0.004306, validation loss: 0.002688\n",
      "iteration 3756, train loss: 0.004379, validation loss: 0.002861\n",
      "iteration 3757, train loss: 0.004122, validation loss: 0.002938\n",
      "iteration 3758, train loss: 0.00426, validation loss: 0.002894\n",
      "iteration 3759, train loss: 0.003887, validation loss: 0.002918\n",
      "iteration 3760, train loss: 0.004863, validation loss: 0.002858\n",
      "iteration 3761, train loss: 0.004064, validation loss: 0.002818\n",
      "iteration 3762, train loss: 0.003901, validation loss: 0.002791\n",
      "iteration 3763, train loss: 0.00386, validation loss: 0.002756\n",
      "iteration 3764, train loss: 0.004196, validation loss: 0.002759\n",
      "iteration 3765, train loss: 0.004415, validation loss: 0.002849\n",
      "iteration 3766, train loss: 0.004188, validation loss: 0.002841\n",
      "iteration 3767, train loss: 0.003987, validation loss: 0.002814\n",
      "iteration 3768, train loss: 0.003962, validation loss: 0.002763\n",
      "iteration 3769, train loss: 0.003973, validation loss: 0.002926\n",
      "iteration 3770, train loss: 0.004341, validation loss: 0.002816\n",
      "iteration 3771, train loss: 0.004305, validation loss: 0.002771\n",
      "iteration 3772, train loss: 0.004176, validation loss: 0.002828\n",
      "iteration 3773, train loss: 0.004016, validation loss: 0.002877\n",
      "iteration 3774, train loss: 0.004214, validation loss: 0.002793\n",
      "iteration 3775, train loss: 0.00447, validation loss: 0.002705\n",
      "iteration 3776, train loss: 0.003926, validation loss: 0.002725\n",
      "iteration 3777, train loss: 0.003971, validation loss: 0.002747\n",
      "iteration 3778, train loss: 0.003762, validation loss: 0.00276\n",
      "iteration 3779, train loss: 0.004005, validation loss: 0.002943\n",
      "iteration 3780, train loss: 0.004241, validation loss: 0.002861\n",
      "iteration 3781, train loss: 0.004144, validation loss: 0.002795\n",
      "iteration 3782, train loss: 0.004105, validation loss: 0.002783\n",
      "iteration 3783, train loss: 0.004451, validation loss: 0.00274\n",
      "iteration 3784, train loss: 0.004115, validation loss: 0.002769\n",
      "iteration 3785, train loss: 0.003943, validation loss: 0.002891\n",
      "iteration 3786, train loss: 0.003953, validation loss: 0.002961\n",
      "iteration 3787, train loss: 0.00385, validation loss: 0.002955\n",
      "iteration 3788, train loss: 0.004001, validation loss: 0.002791\n",
      "iteration 3789, train loss: 0.004275, validation loss: 0.00268\n",
      "iteration 3790, train loss: 0.004088, validation loss: 0.002712\n",
      "iteration 3791, train loss: 0.004038, validation loss: 0.002662\n",
      "iteration 3792, train loss: 0.003909, validation loss: 0.00292\n",
      "iteration 3793, train loss: 0.004239, validation loss: 0.003156\n",
      "iteration 3794, train loss: 0.004323, validation loss: 0.003139\n",
      "iteration 3795, train loss: 0.003872, validation loss: 0.003066\n",
      "iteration 3796, train loss: 0.004261, validation loss: 0.002871\n",
      "iteration 3797, train loss: 0.004109, validation loss: 0.002674\n",
      "iteration 3798, train loss: 0.004095, validation loss: 0.002717\n",
      "iteration 3799, train loss: 0.003927, validation loss: 0.002948\n",
      "iteration 3800, train loss: 0.00411, validation loss: 0.003111\n",
      "iteration 3801, train loss: 0.004481, validation loss: 0.003091\n",
      "iteration 3802, train loss: 0.00404, validation loss: 0.003017\n",
      "iteration 3803, train loss: 0.003821, validation loss: 0.002972\n",
      "iteration 3804, train loss: 0.004851, validation loss: 0.002678\n",
      "iteration 3805, train loss: 0.00404, validation loss: 0.002905\n",
      "iteration 3806, train loss: 0.00402, validation loss: 0.003131\n",
      "iteration 3807, train loss: 0.004739, validation loss: 0.002994\n",
      "iteration 3808, train loss: 0.004078, validation loss: 0.002827\n",
      "iteration 3809, train loss: 0.003995, validation loss: 0.002824\n",
      "iteration 3810, train loss: 0.004059, validation loss: 0.002767\n",
      "iteration 3811, train loss: 0.004033, validation loss: 0.002714\n",
      "iteration 3812, train loss: 0.004171, validation loss: 0.002845\n",
      "iteration 3813, train loss: 0.004073, validation loss: 0.00286\n",
      "iteration 3814, train loss: 0.003904, validation loss: 0.002778\n",
      "iteration 3815, train loss: 0.003791, validation loss: 0.002678\n",
      "iteration 3816, train loss: 0.003939, validation loss: 0.002663\n",
      "iteration 3817, train loss: 0.00399, validation loss: 0.002826\n",
      "iteration 3818, train loss: 0.004082, validation loss: 0.003057\n",
      "iteration 3819, train loss: 0.004016, validation loss: 0.003162\n",
      "iteration 3820, train loss: 0.004304, validation loss: 0.002949\n",
      "iteration 3821, train loss: 0.004605, validation loss: 0.002924\n",
      "iteration 3822, train loss: 0.003974, validation loss: 0.003147\n",
      "iteration 3823, train loss: 0.004362, validation loss: 0.002884\n",
      "iteration 3824, train loss: 0.003882, validation loss: 0.002748\n",
      "iteration 3825, train loss: 0.003995, validation loss: 0.002922\n",
      "iteration 3826, train loss: 0.004135, validation loss: 0.003039\n",
      "iteration 3827, train loss: 0.004155, validation loss: 0.002901\n",
      "iteration 3828, train loss: 0.004303, validation loss: 0.002786\n",
      "iteration 3829, train loss: 0.004629, validation loss: 0.002851\n",
      "iteration 3830, train loss: 0.004121, validation loss: 0.002952\n",
      "iteration 3831, train loss: 0.004166, validation loss: 0.003199\n",
      "iteration 3832, train loss: 0.00425, validation loss: 0.003242\n",
      "iteration 3833, train loss: 0.004156, validation loss: 0.003045\n",
      "iteration 3834, train loss: 0.004103, validation loss: 0.002737\n",
      "iteration 3835, train loss: 0.003954, validation loss: \u001b[92m0.002622\u001b[0m\n",
      "iteration 3836, train loss: 0.003922, validation loss: 0.002686\n",
      "iteration 3837, train loss: 0.003796, validation loss: 0.002879\n",
      "iteration 3838, train loss: 0.004184, validation loss: 0.002957\n",
      "iteration 3839, train loss: 0.004202, validation loss: 0.002786\n",
      "iteration 3840, train loss: 0.00411, validation loss: 0.002802\n",
      "iteration 3841, train loss: 0.004003, validation loss: 0.003347\n",
      "iteration 3842, train loss: 0.00486, validation loss: 0.003037\n",
      "iteration 3843, train loss: 0.004527, validation loss: 0.002822\n",
      "iteration 3844, train loss: 0.00402, validation loss: 0.00374\n",
      "iteration 3845, train loss: 0.004859, validation loss: 0.003973\n",
      "iteration 3846, train loss: 0.004633, validation loss: 0.003236\n",
      "iteration 3847, train loss: 0.004638, validation loss: 0.002665\n",
      "iteration 3848, train loss: 0.004144, validation loss: 0.003183\n",
      "iteration 3849, train loss: 0.004819, validation loss: 0.003154\n",
      "iteration 3850, train loss: 0.003987, validation loss: 0.002911\n",
      "iteration 3851, train loss: 0.004118, validation loss: 0.002859\n",
      "iteration 3852, train loss: 0.004044, validation loss: 0.003248\n",
      "iteration 3853, train loss: 0.004149, validation loss: 0.003402\n",
      "iteration 3854, train loss: 0.004849, validation loss: 0.00287\n",
      "iteration 3855, train loss: 0.004011, validation loss: 0.002637\n",
      "iteration 3856, train loss: 0.004067, validation loss: 0.002943\n",
      "iteration 3857, train loss: 0.004218, validation loss: 0.003223\n",
      "iteration 3858, train loss: 0.004617, validation loss: 0.003113\n",
      "iteration 3859, train loss: 0.004075, validation loss: 0.00298\n",
      "iteration 3860, train loss: 0.003877, validation loss: 0.002858\n",
      "iteration 3861, train loss: 0.004024, validation loss: 0.00296\n",
      "iteration 3862, train loss: 0.004267, validation loss: 0.002927\n",
      "iteration 3863, train loss: 0.004541, validation loss: \u001b[92m0.002597\u001b[0m\n",
      "iteration 3864, train loss: 0.004098, validation loss: 0.002779\n",
      "iteration 3865, train loss: 0.003928, validation loss: 0.003285\n",
      "iteration 3866, train loss: 0.003959, validation loss: 0.003303\n",
      "iteration 3867, train loss: 0.004025, validation loss: 0.002983\n",
      "iteration 3868, train loss: 0.004124, validation loss: 0.002636\n",
      "iteration 3869, train loss: 0.003882, validation loss: 0.00272\n",
      "iteration 3870, train loss: 0.004055, validation loss: 0.002754\n",
      "iteration 3871, train loss: 0.003929, validation loss: 0.002669\n",
      "iteration 3872, train loss: 0.003958, validation loss: 0.002797\n",
      "iteration 3873, train loss: 0.004162, validation loss: 0.002899\n",
      "iteration 3874, train loss: 0.004279, validation loss: 0.002791\n",
      "iteration 3875, train loss: 0.004173, validation loss: 0.002761\n",
      "iteration 3876, train loss: 0.003823, validation loss: 0.00269\n",
      "iteration 3877, train loss: \u001b[92m0.003635\u001b[0m, validation loss: 0.002773\n",
      "iteration 3878, train loss: 0.004122, validation loss: 0.002987\n",
      "iteration 3879, train loss: 0.004404, validation loss: 0.00281\n",
      "iteration 3880, train loss: 0.004206, validation loss: 0.002767\n",
      "iteration 3881, train loss: 0.003981, validation loss: 0.003038\n",
      "iteration 3882, train loss: 0.004206, validation loss: 0.003001\n",
      "iteration 3883, train loss: 0.004061, validation loss: 0.002759\n",
      "iteration 3884, train loss: 0.00407, validation loss: 0.00277\n",
      "iteration 3885, train loss: 0.003945, validation loss: 0.00303\n",
      "iteration 3886, train loss: 0.004585, validation loss: 0.002724\n",
      "iteration 3887, train loss: 0.004012, validation loss: 0.00277\n",
      "iteration 3888, train loss: 0.003764, validation loss: 0.003422\n",
      "iteration 3889, train loss: 0.004175, validation loss: 0.003672\n",
      "iteration 3890, train loss: 0.004684, validation loss: 0.003359\n",
      "iteration 3891, train loss: 0.004741, validation loss: 0.002791\n",
      "iteration 3892, train loss: 0.003796, validation loss: 0.003088\n",
      "iteration 3893, train loss: 0.004222, validation loss: 0.003151\n",
      "iteration 3894, train loss: 0.005086, validation loss: 0.002653\n",
      "iteration 3895, train loss: 0.004208, validation loss: 0.00312\n",
      "iteration 3896, train loss: 0.004175, validation loss: 0.003822\n",
      "iteration 3897, train loss: 0.004757, validation loss: 0.00346\n",
      "iteration 3898, train loss: 0.004725, validation loss: 0.002807\n",
      "iteration 3899, train loss: 0.003799, validation loss: 0.002972\n",
      "iteration 3900, train loss: 0.004337, validation loss: 0.002909\n",
      "iteration 3901, train loss: 0.004726, validation loss: 0.002715\n",
      "iteration 3902, train loss: 0.004039, validation loss: 0.003071\n",
      "iteration 3903, train loss: 0.004237, validation loss: 0.003364\n",
      "iteration 3904, train loss: 0.004797, validation loss: 0.003243\n",
      "iteration 3905, train loss: 0.003991, validation loss: 0.003073\n",
      "iteration 3906, train loss: 0.004381, validation loss: 0.002871\n",
      "iteration 3907, train loss: 0.00443, validation loss: 0.00268\n",
      "iteration 3908, train loss: 0.00424, validation loss: 0.002886\n",
      "iteration 3909, train loss: 0.004153, validation loss: 0.003012\n",
      "iteration 3910, train loss: 0.004682, validation loss: 0.002784\n",
      "iteration 3911, train loss: 0.004165, validation loss: 0.002962\n",
      "iteration 3912, train loss: 0.003974, validation loss: 0.003528\n",
      "iteration 3913, train loss: 0.004558, validation loss: 0.003442\n",
      "iteration 3914, train loss: 0.004983, validation loss: 0.002965\n",
      "iteration 3915, train loss: 0.003876, validation loss: 0.002983\n",
      "iteration 3916, train loss: 0.003875, validation loss: 0.003065\n",
      "iteration 3917, train loss: 0.004422, validation loss: 0.003011\n",
      "iteration 3918, train loss: 0.004842, validation loss: 0.002936\n",
      "iteration 3919, train loss: 0.004086, validation loss: 0.003093\n",
      "iteration 3920, train loss: 0.004411, validation loss: 0.00304\n",
      "iteration 3921, train loss: 0.004271, validation loss: 0.003352\n",
      "iteration 3922, train loss: 0.004578, validation loss: 0.003616\n",
      "iteration 3923, train loss: 0.004446, validation loss: 0.003179\n",
      "iteration 3924, train loss: 0.00405, validation loss: 0.002764\n",
      "iteration 3925, train loss: 0.003983, validation loss: \u001b[92m0.00259\u001b[0m\n",
      "iteration 3926, train loss: 0.003863, validation loss: 0.002813\n",
      "iteration 3927, train loss: 0.004464, validation loss: 0.00282\n",
      "iteration 3928, train loss: 0.003702, validation loss: 0.002925\n",
      "iteration 3929, train loss: 0.003849, validation loss: 0.003046\n",
      "iteration 3930, train loss: 0.003948, validation loss: 0.00301\n",
      "iteration 3931, train loss: 0.003916, validation loss: 0.002791\n",
      "iteration 3932, train loss: 0.004218, validation loss: 0.002803\n",
      "iteration 3933, train loss: 0.003936, validation loss: 0.002848\n",
      "iteration 3934, train loss: 0.004371, validation loss: 0.002732\n",
      "iteration 3935, train loss: 0.003788, validation loss: 0.003043\n",
      "iteration 3936, train loss: 0.004482, validation loss: 0.00313\n",
      "iteration 3937, train loss: 0.004225, validation loss: 0.002789\n",
      "iteration 3938, train loss: 0.004093, validation loss: 0.002629\n",
      "iteration 3939, train loss: 0.004155, validation loss: 0.002763\n",
      "iteration 3940, train loss: 0.004177, validation loss: 0.002791\n",
      "iteration 3941, train loss: 0.003878, validation loss: 0.002683\n",
      "iteration 3942, train loss: 0.00374, validation loss: 0.002878\n",
      "iteration 3943, train loss: 0.004031, validation loss: 0.003219\n",
      "iteration 3944, train loss: 0.004088, validation loss: 0.002988\n",
      "iteration 3945, train loss: 0.003966, validation loss: 0.002653\n",
      "iteration 3946, train loss: 0.003856, validation loss: 0.002846\n",
      "iteration 3947, train loss: 0.00396, validation loss: 0.003346\n",
      "iteration 3948, train loss: 0.004518, validation loss: 0.00334\n",
      "iteration 3949, train loss: 0.004848, validation loss: 0.002905\n",
      "iteration 3950, train loss: 0.003979, validation loss: 0.002849\n",
      "iteration 3951, train loss: 0.004055, validation loss: 0.003063\n",
      "iteration 3952, train loss: 0.004479, validation loss: 0.00305\n",
      "iteration 3953, train loss: 0.004453, validation loss: 0.002913\n",
      "iteration 3954, train loss: 0.00393, validation loss: 0.002823\n",
      "iteration 3955, train loss: 0.004117, validation loss: 0.002684\n",
      "iteration 3956, train loss: 0.004053, validation loss: \u001b[92m0.002538\u001b[0m\n",
      "iteration 3957, train loss: 0.003811, validation loss: 0.002666\n",
      "iteration 3958, train loss: 0.003943, validation loss: 0.002769\n",
      "iteration 3959, train loss: 0.003965, validation loss: 0.002745\n",
      "iteration 3960, train loss: 0.004191, validation loss: 0.0027\n",
      "iteration 3961, train loss: 0.004056, validation loss: 0.002706\n",
      "iteration 3962, train loss: 0.00411, validation loss: 0.002735\n",
      "iteration 3963, train loss: 0.004318, validation loss: 0.002763\n",
      "iteration 3964, train loss: 0.00404, validation loss: 0.002865\n",
      "iteration 3965, train loss: 0.004052, validation loss: 0.002733\n",
      "iteration 3966, train loss: 0.003852, validation loss: 0.00287\n",
      "iteration 3967, train loss: 0.004395, validation loss: 0.002984\n",
      "iteration 3968, train loss: 0.004222, validation loss: 0.002867\n",
      "iteration 3969, train loss: 0.003705, validation loss: 0.002788\n",
      "iteration 3970, train loss: 0.003809, validation loss: 0.00277\n",
      "iteration 3971, train loss: 0.003773, validation loss: 0.002921\n",
      "iteration 3972, train loss: 0.004535, validation loss: 0.002899\n",
      "iteration 3973, train loss: 0.003796, validation loss: 0.002864\n",
      "iteration 3974, train loss: 0.003727, validation loss: 0.003006\n",
      "iteration 3975, train loss: 0.004518, validation loss: 0.002909\n",
      "iteration 3976, train loss: 0.003987, validation loss: 0.002848\n",
      "iteration 3977, train loss: 0.004269, validation loss: 0.002706\n",
      "iteration 3978, train loss: 0.003998, validation loss: 0.002664\n",
      "iteration 3979, train loss: 0.00423, validation loss: 0.002997\n",
      "iteration 3980, train loss: 0.004313, validation loss: 0.003117\n",
      "iteration 3981, train loss: 0.004676, validation loss: 0.002779\n",
      "iteration 3982, train loss: 0.00369, validation loss: 0.002709\n",
      "iteration 3983, train loss: 0.003935, validation loss: 0.00277\n",
      "iteration 3984, train loss: 0.004124, validation loss: 0.00269\n",
      "iteration 3985, train loss: 0.004167, validation loss: 0.002784\n",
      "iteration 3986, train loss: 0.003895, validation loss: 0.003134\n",
      "iteration 3987, train loss: 0.004093, validation loss: 0.003183\n",
      "iteration 3988, train loss: 0.00415, validation loss: 0.002909\n",
      "iteration 3989, train loss: 0.004279, validation loss: 0.002653\n",
      "iteration 3990, train loss: 0.004211, validation loss: 0.002811\n",
      "iteration 3991, train loss: 0.004106, validation loss: 0.002921\n",
      "iteration 3992, train loss: 0.003753, validation loss: 0.002744\n",
      "iteration 3993, train loss: 0.004366, validation loss: 0.002684\n",
      "iteration 3994, train loss: 0.003866, validation loss: 0.002858\n",
      "iteration 3995, train loss: 0.004101, validation loss: 0.00278\n",
      "iteration 3996, train loss: 0.004161, validation loss: 0.002799\n",
      "iteration 3997, train loss: 0.004181, validation loss: 0.002899\n",
      "iteration 3998, train loss: 0.004299, validation loss: 0.002792\n",
      "iteration 3999, train loss: 0.00404, validation loss: 0.002678\n",
      "iteration 4000, train loss: 0.003798, validation loss: 0.002551\n",
      "iteration 4001, train loss: 0.003856, validation loss: 0.002727\n",
      "iteration 4002, train loss: 0.004129, validation loss: 0.002943\n",
      "iteration 4003, train loss: 0.004015, validation loss: 0.0029\n",
      "iteration 4004, train loss: 0.003757, validation loss: 0.002714\n",
      "iteration 4005, train loss: 0.004271, validation loss: 0.002601\n",
      "iteration 4006, train loss: 0.003866, validation loss: 0.002621\n",
      "iteration 4007, train loss: 0.003982, validation loss: 0.002768\n",
      "iteration 4008, train loss: 0.004276, validation loss: 0.002691\n",
      "iteration 4009, train loss: 0.004221, validation loss: 0.002792\n",
      "iteration 4010, train loss: 0.004137, validation loss: 0.002943\n",
      "iteration 4011, train loss: 0.003982, validation loss: 0.003032\n",
      "iteration 4012, train loss: 0.004265, validation loss: 0.002988\n",
      "iteration 4013, train loss: 0.004225, validation loss: 0.002741\n",
      "iteration 4014, train loss: 0.00389, validation loss: 0.002941\n",
      "iteration 4015, train loss: 0.004383, validation loss: 0.002968\n",
      "iteration 4016, train loss: 0.004294, validation loss: 0.002759\n",
      "iteration 4017, train loss: 0.003973, validation loss: 0.002665\n",
      "iteration 4018, train loss: 0.003717, validation loss: 0.002763\n",
      "iteration 4019, train loss: 0.004245, validation loss: 0.002889\n",
      "iteration 4020, train loss: 0.004051, validation loss: 0.003\n",
      "iteration 4021, train loss: 0.003981, validation loss: 0.002885\n",
      "iteration 4022, train loss: 0.004051, validation loss: 0.002706\n",
      "iteration 4023, train loss: 0.003919, validation loss: 0.002733\n",
      "iteration 4024, train loss: 0.004433, validation loss: 0.002809\n",
      "iteration 4025, train loss: 0.003993, validation loss: 0.002915\n",
      "iteration 4026, train loss: 0.004084, validation loss: 0.003063\n",
      "iteration 4027, train loss: 0.004046, validation loss: 0.003395\n",
      "iteration 4028, train loss: 0.004795, validation loss: 0.003227\n",
      "iteration 4029, train loss: 0.004186, validation loss: 0.002731\n",
      "iteration 4030, train loss: 0.003748, validation loss: 0.00274\n",
      "iteration 4031, train loss: 0.003935, validation loss: 0.002968\n",
      "iteration 4032, train loss: 0.004368, validation loss: 0.00282\n",
      "iteration 4033, train loss: 0.004294, validation loss: 0.002796\n",
      "iteration 4034, train loss: 0.003832, validation loss: 0.0032\n",
      "iteration 4035, train loss: 0.004649, validation loss: 0.00321\n",
      "iteration 4036, train loss: 0.004253, validation loss: 0.002875\n",
      "iteration 4037, train loss: 0.004069, validation loss: 0.002727\n",
      "iteration 4038, train loss: 0.004298, validation loss: 0.00267\n",
      "iteration 4039, train loss: 0.003767, validation loss: 0.002724\n",
      "iteration 4040, train loss: 0.003981, validation loss: 0.002857\n",
      "iteration 4041, train loss: 0.003885, validation loss: 0.002819\n",
      "iteration 4042, train loss: 0.004032, validation loss: 0.002673\n",
      "iteration 4043, train loss: 0.004206, validation loss: 0.00262\n",
      "iteration 4044, train loss: 0.003675, validation loss: 0.002694\n",
      "iteration 4045, train loss: 0.003722, validation loss: 0.002924\n",
      "iteration 4046, train loss: 0.004321, validation loss: 0.002904\n",
      "iteration 4047, train loss: 0.003926, validation loss: 0.002748\n",
      "iteration 4048, train loss: 0.003692, validation loss: 0.002785\n",
      "iteration 4049, train loss: 0.004204, validation loss: 0.002676\n",
      "iteration 4050, train loss: 0.003818, validation loss: 0.002637\n",
      "iteration 4051, train loss: 0.004229, validation loss: 0.002663\n",
      "iteration 4052, train loss: 0.003833, validation loss: 0.002639\n",
      "iteration 4053, train loss: 0.003699, validation loss: 0.002687\n",
      "iteration 4054, train loss: 0.003862, validation loss: 0.00269\n",
      "iteration 4055, train loss: 0.00391, validation loss: 0.002727\n",
      "iteration 4056, train loss: 0.003915, validation loss: 0.002738\n",
      "iteration 4057, train loss: 0.003777, validation loss: 0.002704\n",
      "iteration 4058, train loss: 0.004133, validation loss: 0.002671\n",
      "iteration 4059, train loss: 0.003862, validation loss: 0.002709\n",
      "iteration 4060, train loss: 0.004, validation loss: 0.002828\n",
      "iteration 4061, train loss: 0.003757, validation loss: 0.002737\n",
      "iteration 4062, train loss: 0.003821, validation loss: 0.002608\n",
      "iteration 4063, train loss: 0.003847, validation loss: 0.002673\n",
      "iteration 4064, train loss: 0.003783, validation loss: 0.002666\n",
      "iteration 4065, train loss: 0.003916, validation loss: 0.002693\n",
      "iteration 4066, train loss: 0.003672, validation loss: 0.002669\n",
      "iteration 4067, train loss: 0.004153, validation loss: 0.002631\n",
      "iteration 4068, train loss: 0.003891, validation loss: 0.002712\n",
      "iteration 4069, train loss: 0.004428, validation loss: 0.00268\n",
      "iteration 4070, train loss: 0.00387, validation loss: 0.002655\n",
      "iteration 4071, train loss: 0.003793, validation loss: 0.002741\n",
      "iteration 4072, train loss: 0.00376, validation loss: 0.002823\n",
      "iteration 4073, train loss: 0.004063, validation loss: 0.00277\n",
      "iteration 4074, train loss: 0.003872, validation loss: 0.002593\n",
      "iteration 4075, train loss: 0.003849, validation loss: 0.002589\n",
      "iteration 4076, train loss: 0.00407, validation loss: 0.002695\n",
      "iteration 4077, train loss: 0.004123, validation loss: 0.002863\n",
      "iteration 4078, train loss: 0.003927, validation loss: 0.003062\n",
      "iteration 4079, train loss: 0.004293, validation loss: 0.002973\n",
      "iteration 4080, train loss: 0.004011, validation loss: 0.002818\n",
      "iteration 4081, train loss: 0.004203, validation loss: 0.002808\n",
      "iteration 4082, train loss: 0.004374, validation loss: 0.002606\n",
      "iteration 4083, train loss: 0.003707, validation loss: 0.002751\n",
      "iteration 4084, train loss: 0.003679, validation loss: 0.003108\n",
      "iteration 4085, train loss: 0.003992, validation loss: 0.003114\n",
      "iteration 4086, train loss: 0.004143, validation loss: 0.002656\n",
      "iteration 4087, train loss: 0.003884, validation loss: 0.002738\n",
      "iteration 4088, train loss: 0.003893, validation loss: 0.00293\n",
      "iteration 4089, train loss: 0.004182, validation loss: 0.002736\n",
      "iteration 4090, train loss: 0.003849, validation loss: 0.002722\n",
      "iteration 4091, train loss: 0.003775, validation loss: 0.002934\n",
      "iteration 4092, train loss: 0.003843, validation loss: 0.002928\n",
      "iteration 4093, train loss: 0.00415, validation loss: 0.002584\n",
      "iteration 4094, train loss: 0.003727, validation loss: 0.002646\n",
      "iteration 4095, train loss: 0.003797, validation loss: 0.00283\n",
      "iteration 4096, train loss: 0.004056, validation loss: 0.002685\n",
      "iteration 4097, train loss: 0.003977, validation loss: 0.002777\n",
      "iteration 4098, train loss: 0.004225, validation loss: 0.002659\n",
      "iteration 4099, train loss: 0.003845, validation loss: 0.002717\n",
      "iteration 4100, train loss: 0.00403, validation loss: 0.002765\n",
      "iteration 4101, train loss: 0.00414, validation loss: 0.002737\n",
      "iteration 4102, train loss: 0.003919, validation loss: 0.0028\n",
      "iteration 4103, train loss: \u001b[92m0.003613\u001b[0m, validation loss: 0.002823\n",
      "iteration 4104, train loss: 0.004311, validation loss: 0.002595\n",
      "iteration 4105, train loss: 0.003919, validation loss: 0.002594\n",
      "iteration 4106, train loss: 0.004221, validation loss: 0.002617\n",
      "iteration 4107, train loss: 0.003969, validation loss: 0.002612\n",
      "iteration 4108, train loss: 0.003849, validation loss: 0.002712\n",
      "iteration 4109, train loss: 0.004077, validation loss: 0.002711\n",
      "iteration 4110, train loss: 0.003902, validation loss: 0.002654\n",
      "iteration 4111, train loss: 0.00417, validation loss: 0.002673\n",
      "iteration 4112, train loss: 0.003809, validation loss: 0.002788\n",
      "iteration 4113, train loss: 0.003918, validation loss: 0.002796\n",
      "iteration 4114, train loss: 0.004255, validation loss: 0.002668\n",
      "iteration 4115, train loss: 0.004148, validation loss: 0.002658\n",
      "iteration 4116, train loss: 0.004262, validation loss: 0.002607\n",
      "iteration 4117, train loss: 0.003911, validation loss: 0.002701\n",
      "iteration 4118, train loss: 0.004458, validation loss: 0.002744\n",
      "iteration 4119, train loss: 0.004407, validation loss: 0.002731\n",
      "iteration 4120, train loss: 0.003973, validation loss: 0.002734\n",
      "iteration 4121, train loss: 0.003879, validation loss: 0.002593\n",
      "iteration 4122, train loss: 0.003927, validation loss: 0.002631\n",
      "iteration 4123, train loss: 0.003771, validation loss: 0.00269\n",
      "iteration 4124, train loss: 0.004029, validation loss: 0.002779\n",
      "iteration 4125, train loss: 0.004017, validation loss: 0.002678\n",
      "iteration 4126, train loss: 0.003689, validation loss: 0.002746\n",
      "iteration 4127, train loss: 0.003872, validation loss: 0.002677\n",
      "iteration 4128, train loss: 0.004082, validation loss: 0.00268\n",
      "iteration 4129, train loss: 0.003694, validation loss: 0.002759\n",
      "iteration 4130, train loss: 0.003913, validation loss: 0.002675\n",
      "iteration 4131, train loss: 0.003824, validation loss: 0.002651\n",
      "iteration 4132, train loss: 0.004009, validation loss: 0.002674\n",
      "iteration 4133, train loss: 0.003688, validation loss: 0.002723\n",
      "iteration 4134, train loss: 0.003654, validation loss: 0.002827\n",
      "iteration 4135, train loss: \u001b[92m0.003601\u001b[0m, validation loss: 0.003052\n",
      "iteration 4136, train loss: 0.00381, validation loss: 0.003097\n",
      "iteration 4137, train loss: 0.004371, validation loss: 0.002813\n",
      "iteration 4138, train loss: 0.004077, validation loss: 0.002611\n",
      "iteration 4139, train loss: 0.004186, validation loss: 0.002792\n",
      "iteration 4140, train loss: 0.003856, validation loss: 0.00298\n",
      "iteration 4141, train loss: 0.003986, validation loss: 0.002873\n",
      "iteration 4142, train loss: 0.004058, validation loss: 0.002772\n",
      "iteration 4143, train loss: 0.003737, validation loss: 0.002811\n",
      "iteration 4144, train loss: 0.003911, validation loss: 0.002778\n",
      "iteration 4145, train loss: 0.00408, validation loss: 0.002669\n",
      "iteration 4146, train loss: 0.003999, validation loss: 0.002768\n",
      "iteration 4147, train loss: 0.004314, validation loss: 0.002858\n",
      "iteration 4148, train loss: 0.004044, validation loss: 0.002782\n",
      "iteration 4149, train loss: 0.004489, validation loss: 0.002578\n",
      "iteration 4150, train loss: 0.003862, validation loss: 0.002642\n",
      "iteration 4151, train loss: 0.003888, validation loss: 0.002711\n",
      "iteration 4152, train loss: 0.003726, validation loss: 0.002778\n",
      "iteration 4153, train loss: 0.003788, validation loss: 0.002984\n",
      "iteration 4154, train loss: 0.003974, validation loss: 0.002925\n",
      "iteration 4155, train loss: 0.004042, validation loss: 0.002815\n",
      "iteration 4156, train loss: 0.003976, validation loss: 0.002642\n",
      "iteration 4157, train loss: 0.004041, validation loss: 0.002555\n",
      "iteration 4158, train loss: 0.003928, validation loss: 0.00272\n",
      "iteration 4159, train loss: 0.003778, validation loss: 0.002836\n",
      "iteration 4160, train loss: 0.003834, validation loss: 0.002994\n",
      "iteration 4161, train loss: 0.003994, validation loss: 0.003098\n",
      "iteration 4162, train loss: 0.003671, validation loss: 0.002988\n",
      "iteration 4163, train loss: 0.004083, validation loss: 0.002738\n",
      "iteration 4164, train loss: 0.003683, validation loss: 0.002644\n",
      "iteration 4165, train loss: 0.00395, validation loss: 0.002602\n",
      "iteration 4166, train loss: 0.004113, validation loss: 0.00264\n",
      "iteration 4167, train loss: 0.003964, validation loss: 0.002738\n",
      "iteration 4168, train loss: 0.004013, validation loss: 0.00289\n",
      "iteration 4169, train loss: 0.004002, validation loss: 0.002975\n",
      "iteration 4170, train loss: 0.004094, validation loss: 0.002747\n",
      "iteration 4171, train loss: 0.003743, validation loss: 0.002592\n",
      "iteration 4172, train loss: 0.003602, validation loss: 0.002644\n",
      "iteration 4173, train loss: 0.003996, validation loss: 0.002678\n",
      "iteration 4174, train loss: 0.003854, validation loss: 0.002668\n",
      "iteration 4175, train loss: 0.003927, validation loss: 0.002833\n",
      "iteration 4176, train loss: 0.004151, validation loss: 0.002788\n",
      "iteration 4177, train loss: 0.003678, validation loss: 0.002689\n",
      "iteration 4178, train loss: 0.0037, validation loss: 0.002574\n",
      "iteration 4179, train loss: 0.003833, validation loss: 0.002613\n",
      "iteration 4180, train loss: 0.003837, validation loss: 0.002581\n",
      "iteration 4181, train loss: 0.004331, validation loss: 0.002708\n",
      "iteration 4182, train loss: 0.004016, validation loss: 0.002874\n",
      "iteration 4183, train loss: 0.00401, validation loss: 0.002884\n",
      "iteration 4184, train loss: 0.003955, validation loss: 0.002621\n",
      "iteration 4185, train loss: 0.004111, validation loss: 0.002615\n",
      "iteration 4186, train loss: \u001b[92m0.003541\u001b[0m, validation loss: 0.002648\n",
      "iteration 4187, train loss: 0.00383, validation loss: 0.002561\n",
      "iteration 4188, train loss: 0.003845, validation loss: 0.00263\n",
      "iteration 4189, train loss: 0.003887, validation loss: 0.002834\n",
      "iteration 4190, train loss: 0.004525, validation loss: 0.002791\n",
      "iteration 4191, train loss: 0.003884, validation loss: 0.002732\n",
      "iteration 4192, train loss: 0.004435, validation loss: 0.002587\n",
      "iteration 4193, train loss: 0.003909, validation loss: 0.002613\n",
      "iteration 4194, train loss: 0.003737, validation loss: 0.002753\n",
      "iteration 4195, train loss: 0.004006, validation loss: 0.002742\n",
      "iteration 4196, train loss: 0.004172, validation loss: 0.002878\n",
      "iteration 4197, train loss: 0.004201, validation loss: 0.002744\n",
      "iteration 4198, train loss: 0.003873, validation loss: 0.002756\n",
      "iteration 4199, train loss: 0.004096, validation loss: 0.002852\n",
      "iteration 4200, train loss: 0.003995, validation loss: 0.00267\n",
      "iteration 4201, train loss: 0.003908, validation loss: 0.002639\n",
      "iteration 4202, train loss: 0.004069, validation loss: 0.002852\n",
      "iteration 4203, train loss: 0.004584, validation loss: 0.002691\n",
      "iteration 4204, train loss: 0.003808, validation loss: 0.002836\n",
      "iteration 4205, train loss: 0.003791, validation loss: 0.003049\n",
      "iteration 4206, train loss: 0.004072, validation loss: 0.00295\n",
      "iteration 4207, train loss: 0.003975, validation loss: 0.002639\n",
      "iteration 4208, train loss: 0.003982, validation loss: 0.002798\n",
      "iteration 4209, train loss: 0.004006, validation loss: 0.003045\n",
      "iteration 4210, train loss: 0.004175, validation loss: 0.002829\n",
      "iteration 4211, train loss: 0.003789, validation loss: 0.00291\n",
      "iteration 4212, train loss: 0.004078, validation loss: 0.00314\n",
      "iteration 4213, train loss: 0.003896, validation loss: 0.003292\n",
      "iteration 4214, train loss: 0.004307, validation loss: 0.002943\n",
      "iteration 4215, train loss: 0.004098, validation loss: 0.002546\n",
      "iteration 4216, train loss: 0.003953, validation loss: 0.0027\n",
      "iteration 4217, train loss: 0.003897, validation loss: 0.002711\n",
      "iteration 4218, train loss: 0.004155, validation loss: 0.002863\n",
      "iteration 4219, train loss: 0.003719, validation loss: 0.003246\n",
      "iteration 4220, train loss: 0.003805, validation loss: 0.003345\n",
      "iteration 4221, train loss: 0.003928, validation loss: 0.002856\n",
      "iteration 4222, train loss: 0.003944, validation loss: 0.002611\n",
      "iteration 4223, train loss: 0.00398, validation loss: 0.002663\n",
      "iteration 4224, train loss: 0.003802, validation loss: 0.002844\n",
      "iteration 4225, train loss: 0.004084, validation loss: 0.002755\n",
      "iteration 4226, train loss: 0.004045, validation loss: 0.002844\n",
      "iteration 4227, train loss: 0.003911, validation loss: 0.003614\n",
      "iteration 4228, train loss: 0.004413, validation loss: 0.00387\n",
      "iteration 4229, train loss: 0.004802, validation loss: 0.003096\n",
      "iteration 4230, train loss: 0.004033, validation loss: 0.002712\n",
      "iteration 4231, train loss: 0.004208, validation loss: 0.00283\n",
      "iteration 4232, train loss: 0.00451, validation loss: 0.002685\n",
      "iteration 4233, train loss: 0.00429, validation loss: 0.002976\n",
      "iteration 4234, train loss: 0.004079, validation loss: 0.003029\n",
      "iteration 4235, train loss: 0.003945, validation loss: 0.002968\n",
      "iteration 4236, train loss: 0.003558, validation loss: 0.003016\n",
      "iteration 4237, train loss: 0.003896, validation loss: 0.002761\n",
      "iteration 4238, train loss: 0.004126, validation loss: \u001b[92m0.00252\u001b[0m\n",
      "iteration 4239, train loss: 0.003709, validation loss: 0.002869\n",
      "iteration 4240, train loss: 0.003919, validation loss: 0.003234\n",
      "iteration 4241, train loss: 0.004381, validation loss: 0.002946\n",
      "iteration 4242, train loss: 0.003907, validation loss: 0.00277\n",
      "iteration 4243, train loss: 0.004046, validation loss: 0.002819\n",
      "iteration 4244, train loss: 0.004046, validation loss: 0.00268\n",
      "iteration 4245, train loss: 0.004033, validation loss: 0.002533\n",
      "iteration 4246, train loss: 0.003939, validation loss: 0.002707\n",
      "iteration 4247, train loss: 0.003849, validation loss: 0.00307\n",
      "iteration 4248, train loss: 0.004098, validation loss: 0.003027\n",
      "iteration 4249, train loss: 0.004253, validation loss: 0.002641\n",
      "iteration 4250, train loss: 0.003891, validation loss: 0.002738\n",
      "iteration 4251, train loss: 0.004093, validation loss: 0.002933\n",
      "iteration 4252, train loss: 0.004274, validation loss: 0.002644\n",
      "iteration 4253, train loss: 0.004315, validation loss: 0.002959\n",
      "iteration 4254, train loss: 0.004033, validation loss: 0.003414\n",
      "iteration 4255, train loss: 0.004828, validation loss: 0.003126\n",
      "iteration 4256, train loss: 0.004056, validation loss: 0.002717\n",
      "iteration 4257, train loss: 0.003823, validation loss: 0.002984\n",
      "iteration 4258, train loss: 0.004671, validation loss: 0.002906\n",
      "iteration 4259, train loss: 0.003719, validation loss: 0.003028\n",
      "iteration 4260, train loss: 0.004065, validation loss: 0.00324\n",
      "iteration 4261, train loss: 0.004251, validation loss: 0.002989\n",
      "iteration 4262, train loss: 0.004193, validation loss: 0.002708\n",
      "iteration 4263, train loss: 0.003735, validation loss: 0.002696\n",
      "iteration 4264, train loss: 0.003912, validation loss: 0.002787\n",
      "iteration 4265, train loss: 0.003993, validation loss: 0.002667\n",
      "iteration 4266, train loss: 0.004737, validation loss: 0.002571\n",
      "iteration 4267, train loss: 0.003874, validation loss: 0.002951\n",
      "iteration 4268, train loss: 0.004115, validation loss: 0.0031\n",
      "iteration 4269, train loss: 0.004555, validation loss: 0.002751\n",
      "iteration 4270, train loss: 0.004125, validation loss: 0.00269\n",
      "iteration 4271, train loss: 0.003735, validation loss: 0.002878\n",
      "iteration 4272, train loss: 0.004343, validation loss: 0.002776\n",
      "iteration 4273, train loss: 0.004147, validation loss: 0.002977\n",
      "iteration 4274, train loss: 0.004331, validation loss: 0.003097\n",
      "iteration 4275, train loss: 0.003585, validation loss: 0.003075\n",
      "iteration 4276, train loss: 0.004064, validation loss: 0.003082\n",
      "iteration 4277, train loss: 0.00404, validation loss: 0.003376\n",
      "iteration 4278, train loss: 0.004119, validation loss: 0.003158\n",
      "iteration 4279, train loss: 0.004318, validation loss: 0.002849\n",
      "iteration 4280, train loss: 0.004021, validation loss: 0.002879\n",
      "iteration 4281, train loss: 0.004267, validation loss: 0.002962\n",
      "iteration 4282, train loss: 0.004614, validation loss: 0.002797\n",
      "iteration 4283, train loss: 0.003905, validation loss: 0.002774\n",
      "iteration 4284, train loss: 0.004549, validation loss: 0.002862\n",
      "iteration 4285, train loss: 0.004164, validation loss: 0.003116\n",
      "iteration 4286, train loss: 0.00374, validation loss: 0.003188\n",
      "iteration 4287, train loss: 0.003988, validation loss: 0.00293\n",
      "iteration 4288, train loss: 0.00398, validation loss: 0.002561\n",
      "iteration 4289, train loss: 0.004047, validation loss: 0.002596\n",
      "iteration 4290, train loss: 0.004011, validation loss: 0.002904\n",
      "iteration 4291, train loss: 0.004131, validation loss: 0.002889\n",
      "iteration 4292, train loss: 0.003963, validation loss: 0.00269\n",
      "iteration 4293, train loss: 0.004225, validation loss: 0.002778\n",
      "iteration 4294, train loss: 0.004158, validation loss: 0.003019\n",
      "iteration 4295, train loss: 0.004071, validation loss: 0.002955\n",
      "iteration 4296, train loss: 0.004204, validation loss: 0.002617\n",
      "iteration 4297, train loss: 0.003877, validation loss: 0.002693\n",
      "iteration 4298, train loss: 0.00383, validation loss: 0.003065\n",
      "iteration 4299, train loss: 0.004934, validation loss: 0.002849\n",
      "iteration 4300, train loss: 0.003956, validation loss: 0.002638\n",
      "iteration 4301, train loss: 0.003828, validation loss: 0.002809\n",
      "iteration 4302, train loss: 0.004391, validation loss: 0.002905\n",
      "iteration 4303, train loss: 0.003676, validation loss: 0.002939\n",
      "iteration 4304, train loss: 0.004209, validation loss: 0.002863\n",
      "iteration 4305, train loss: 0.003669, validation loss: 0.002901\n",
      "iteration 4306, train loss: 0.00386, validation loss: 0.00294\n",
      "iteration 4307, train loss: 0.00388, validation loss: 0.002792\n",
      "iteration 4308, train loss: 0.004801, validation loss: 0.002561\n",
      "iteration 4309, train loss: 0.003937, validation loss: 0.002759\n",
      "iteration 4310, train loss: 0.003908, validation loss: 0.003101\n",
      "iteration 4311, train loss: 0.003996, validation loss: 0.003116\n",
      "iteration 4312, train loss: 0.004052, validation loss: 0.002934\n",
      "iteration 4313, train loss: 0.004022, validation loss: 0.002774\n",
      "iteration 4314, train loss: 0.003714, validation loss: 0.002719\n",
      "iteration 4315, train loss: 0.003924, validation loss: 0.002712\n",
      "iteration 4316, train loss: 0.004155, validation loss: 0.00272\n",
      "iteration 4317, train loss: 0.004384, validation loss: 0.002724\n",
      "iteration 4318, train loss: 0.004233, validation loss: 0.002734\n",
      "iteration 4319, train loss: 0.003982, validation loss: 0.002747\n",
      "iteration 4320, train loss: 0.003732, validation loss: 0.002906\n",
      "iteration 4321, train loss: 0.004067, validation loss: 0.002755\n",
      "iteration 4322, train loss: 0.004496, validation loss: 0.002555\n",
      "iteration 4323, train loss: 0.003855, validation loss: 0.002786\n",
      "iteration 4324, train loss: 0.004135, validation loss: 0.002847\n",
      "iteration 4325, train loss: 0.004135, validation loss: 0.00278\n",
      "iteration 4326, train loss: 0.003604, validation loss: 0.002814\n",
      "iteration 4327, train loss: 0.004004, validation loss: 0.002874\n",
      "iteration 4328, train loss: 0.004014, validation loss: 0.002732\n",
      "iteration 4329, train loss: 0.003933, validation loss: 0.002632\n",
      "iteration 4330, train loss: 0.003823, validation loss: 0.002582\n",
      "iteration 4331, train loss: 0.003966, validation loss: 0.002708\n",
      "iteration 4332, train loss: 0.003912, validation loss: 0.002814\n",
      "iteration 4333, train loss: 0.004034, validation loss: 0.002619\n",
      "iteration 4334, train loss: 0.003775, validation loss: 0.002742\n",
      "iteration 4335, train loss: 0.003656, validation loss: 0.002825\n",
      "iteration 4336, train loss: 0.004425, validation loss: 0.002543\n",
      "iteration 4337, train loss: 0.003861, validation loss: 0.002914\n",
      "iteration 4338, train loss: 0.004128, validation loss: 0.003113\n",
      "iteration 4339, train loss: 0.004672, validation loss: 0.002692\n",
      "iteration 4340, train loss: 0.003867, validation loss: 0.002605\n",
      "iteration 4341, train loss: 0.003765, validation loss: 0.002665\n",
      "iteration 4342, train loss: 0.003762, validation loss: 0.002596\n",
      "iteration 4343, train loss: \u001b[92m0.003381\u001b[0m, validation loss: 0.002529\n",
      "iteration 4344, train loss: 0.003475, validation loss: 0.002577\n",
      "iteration 4345, train loss: 0.003841, validation loss: 0.002689\n",
      "iteration 4346, train loss: 0.004095, validation loss: 0.002589\n",
      "iteration 4347, train loss: 0.003897, validation loss: \u001b[92m0.002486\u001b[0m\n",
      "iteration 4348, train loss: 0.003901, validation loss: 0.002581\n",
      "iteration 4349, train loss: 0.003749, validation loss: 0.002683\n",
      "iteration 4350, train loss: 0.00394, validation loss: 0.002602\n",
      "iteration 4351, train loss: 0.003732, validation loss: 0.00259\n",
      "iteration 4352, train loss: 0.003602, validation loss: 0.002574\n",
      "iteration 4353, train loss: 0.003578, validation loss: 0.002521\n",
      "iteration 4354, train loss: 0.00369, validation loss: 0.002662\n",
      "iteration 4355, train loss: 0.00427, validation loss: 0.002811\n",
      "iteration 4356, train loss: 0.003764, validation loss: 0.002801\n",
      "iteration 4357, train loss: 0.003554, validation loss: 0.002741\n",
      "iteration 4358, train loss: 0.003642, validation loss: 0.002619\n",
      "iteration 4359, train loss: 0.003902, validation loss: 0.002552\n",
      "iteration 4360, train loss: 0.00351, validation loss: 0.002571\n",
      "iteration 4361, train loss: 0.004064, validation loss: 0.002621\n",
      "iteration 4362, train loss: 0.003874, validation loss: 0.002653\n",
      "iteration 4363, train loss: 0.004254, validation loss: 0.002532\n",
      "iteration 4364, train loss: 0.003819, validation loss: 0.002492\n",
      "iteration 4365, train loss: 0.0043, validation loss: 0.00281\n",
      "iteration 4366, train loss: 0.003658, validation loss: 0.003255\n",
      "iteration 4367, train loss: 0.004541, validation loss: 0.002999\n",
      "iteration 4368, train loss: 0.003856, validation loss: 0.002775\n",
      "iteration 4369, train loss: 0.004081, validation loss: 0.00273\n",
      "iteration 4370, train loss: 0.004251, validation loss: 0.002548\n",
      "iteration 4371, train loss: 0.003663, validation loss: 0.0027\n",
      "iteration 4372, train loss: 0.004033, validation loss: 0.00274\n",
      "iteration 4373, train loss: 0.003734, validation loss: 0.002737\n",
      "iteration 4374, train loss: 0.003765, validation loss: 0.00268\n",
      "iteration 4375, train loss: 0.00369, validation loss: 0.002745\n",
      "iteration 4376, train loss: 0.003976, validation loss: 0.002614\n",
      "iteration 4377, train loss: 0.003663, validation loss: 0.002512\n",
      "iteration 4378, train loss: 0.00399, validation loss: 0.002533\n",
      "iteration 4379, train loss: 0.003686, validation loss: 0.00257\n",
      "iteration 4380, train loss: 0.003825, validation loss: 0.002697\n",
      "iteration 4381, train loss: 0.003963, validation loss: 0.002761\n",
      "iteration 4382, train loss: 0.004059, validation loss: 0.002657\n",
      "iteration 4383, train loss: 0.004261, validation loss: 0.002617\n",
      "iteration 4384, train loss: 0.003713, validation loss: 0.002553\n",
      "iteration 4385, train loss: 0.003451, validation loss: 0.002605\n",
      "iteration 4386, train loss: 0.00375, validation loss: 0.003084\n",
      "iteration 4387, train loss: 0.004274, validation loss: 0.003008\n",
      "iteration 4388, train loss: 0.004284, validation loss: 0.002618\n",
      "iteration 4389, train loss: 0.003994, validation loss: 0.002768\n",
      "iteration 4390, train loss: 0.003687, validation loss: 0.003016\n",
      "iteration 4391, train loss: 0.004289, validation loss: 0.002725\n",
      "iteration 4392, train loss: 0.003822, validation loss: 0.002595\n",
      "iteration 4393, train loss: 0.003857, validation loss: 0.003465\n",
      "iteration 4394, train loss: 0.004273, validation loss: 0.00371\n",
      "iteration 4395, train loss: 0.004729, validation loss: 0.002859\n",
      "iteration 4396, train loss: 0.004037, validation loss: 0.002858\n",
      "iteration 4397, train loss: 0.004167, validation loss: 0.003033\n",
      "iteration 4398, train loss: 0.004055, validation loss: 0.002901\n",
      "iteration 4399, train loss: 0.004052, validation loss: 0.002772\n",
      "iteration 4400, train loss: 0.004062, validation loss: 0.002803\n",
      "iteration 4401, train loss: 0.003822, validation loss: 0.002964\n",
      "iteration 4402, train loss: 0.003844, validation loss: 0.003003\n",
      "iteration 4403, train loss: 0.003835, validation loss: 0.002619\n",
      "iteration 4404, train loss: 0.003605, validation loss: 0.002569\n",
      "iteration 4405, train loss: 0.003693, validation loss: 0.002852\n",
      "iteration 4406, train loss: 0.00396, validation loss: 0.002903\n",
      "iteration 4407, train loss: 0.00406, validation loss: 0.00258\n",
      "iteration 4408, train loss: 0.00366, validation loss: 0.00255\n",
      "iteration 4409, train loss: 0.003711, validation loss: 0.002616\n",
      "iteration 4410, train loss: 0.003579, validation loss: 0.002557\n",
      "iteration 4411, train loss: 0.003595, validation loss: 0.002544\n",
      "iteration 4412, train loss: 0.003772, validation loss: 0.002613\n",
      "iteration 4413, train loss: 0.003652, validation loss: 0.002615\n",
      "iteration 4414, train loss: 0.003696, validation loss: 0.002553\n",
      "iteration 4415, train loss: 0.003655, validation loss: 0.002594\n",
      "iteration 4416, train loss: 0.003601, validation loss: 0.002677\n",
      "iteration 4417, train loss: 0.004034, validation loss: 0.002699\n",
      "iteration 4418, train loss: 0.003843, validation loss: 0.002564\n",
      "iteration 4419, train loss: 0.003628, validation loss: 0.002658\n",
      "iteration 4420, train loss: 0.003804, validation loss: 0.002795\n",
      "iteration 4421, train loss: 0.004028, validation loss: 0.002712\n",
      "iteration 4422, train loss: 0.004184, validation loss: 0.002649\n",
      "iteration 4423, train loss: 0.003531, validation loss: 0.002757\n",
      "iteration 4424, train loss: 0.003959, validation loss: 0.002759\n",
      "iteration 4425, train loss: 0.004042, validation loss: 0.002542\n",
      "iteration 4426, train loss: 0.003687, validation loss: 0.002712\n",
      "iteration 4427, train loss: 0.003874, validation loss: 0.003002\n",
      "iteration 4428, train loss: 0.004104, validation loss: 0.002735\n",
      "iteration 4429, train loss: 0.004403, validation loss: 0.002495\n",
      "iteration 4430, train loss: 0.003872, validation loss: 0.002868\n",
      "iteration 4431, train loss: 0.004018, validation loss: 0.002998\n",
      "iteration 4432, train loss: 0.004209, validation loss: 0.003245\n",
      "iteration 4433, train loss: 0.003894, validation loss: 0.003199\n",
      "iteration 4434, train loss: 0.004042, validation loss: 0.002816\n",
      "iteration 4435, train loss: 0.004069, validation loss: 0.002586\n",
      "iteration 4436, train loss: 0.004244, validation loss: 0.002691\n",
      "iteration 4437, train loss: 0.004205, validation loss: 0.002546\n",
      "iteration 4438, train loss: 0.003663, validation loss: 0.002727\n",
      "iteration 4439, train loss: 0.003686, validation loss: 0.003183\n",
      "iteration 4440, train loss: 0.004133, validation loss: 0.003219\n",
      "iteration 4441, train loss: 0.003648, validation loss: 0.002842\n",
      "iteration 4442, train loss: 0.003943, validation loss: 0.002531\n",
      "iteration 4443, train loss: 0.003938, validation loss: 0.002637\n",
      "iteration 4444, train loss: 0.0036, validation loss: 0.002891\n",
      "iteration 4445, train loss: 0.004344, validation loss: 0.002615\n",
      "iteration 4446, train loss: 0.004022, validation loss: 0.002738\n",
      "iteration 4447, train loss: 0.004188, validation loss: 0.003071\n",
      "iteration 4448, train loss: 0.003845, validation loss: 0.003055\n",
      "iteration 4449, train loss: 0.004119, validation loss: 0.002675\n",
      "iteration 4450, train loss: 0.003763, validation loss: 0.00269\n",
      "iteration 4451, train loss: 0.003713, validation loss: 0.002954\n",
      "iteration 4452, train loss: 0.004281, validation loss: 0.002689\n",
      "iteration 4453, train loss: 0.003829, validation loss: 0.002749\n",
      "iteration 4454, train loss: 0.003697, validation loss: 0.002903\n",
      "iteration 4455, train loss: 0.004085, validation loss: 0.002743\n",
      "iteration 4456, train loss: 0.003893, validation loss: 0.002523\n",
      "iteration 4457, train loss: 0.00363, validation loss: 0.002578\n",
      "iteration 4458, train loss: 0.003739, validation loss: 0.002619\n",
      "iteration 4459, train loss: 0.003798, validation loss: 0.002518\n",
      "iteration 4460, train loss: 0.003905, validation loss: 0.002681\n",
      "iteration 4461, train loss: 0.003964, validation loss: 0.0029\n",
      "iteration 4462, train loss: 0.003899, validation loss: 0.00287\n",
      "iteration 4463, train loss: 0.003754, validation loss: 0.002987\n",
      "iteration 4464, train loss: 0.004013, validation loss: 0.003037\n",
      "iteration 4465, train loss: 0.004431, validation loss: 0.002725\n",
      "iteration 4466, train loss: 0.004237, validation loss: 0.00258\n",
      "iteration 4467, train loss: 0.003708, validation loss: 0.002903\n",
      "iteration 4468, train loss: 0.003883, validation loss: 0.003038\n",
      "iteration 4469, train loss: 0.00396, validation loss: 0.002865\n",
      "iteration 4470, train loss: 0.004493, validation loss: 0.00295\n",
      "iteration 4471, train loss: 0.004115, validation loss: 0.00325\n",
      "iteration 4472, train loss: 0.004254, validation loss: 0.003075\n",
      "iteration 4473, train loss: 0.004362, validation loss: 0.002709\n",
      "iteration 4474, train loss: 0.004343, validation loss: 0.002961\n",
      "iteration 4475, train loss: 0.004514, validation loss: 0.003081\n",
      "iteration 4476, train loss: 0.003749, validation loss: 0.003136\n",
      "iteration 4477, train loss: 0.004789, validation loss: 0.002681\n",
      "iteration 4478, train loss: 0.003849, validation loss: 0.002748\n",
      "iteration 4479, train loss: 0.003913, validation loss: 0.003161\n",
      "iteration 4480, train loss: 0.004593, validation loss: 0.003108\n",
      "iteration 4481, train loss: 0.004974, validation loss: 0.002669\n",
      "iteration 4482, train loss: 0.003755, validation loss: 0.003162\n",
      "iteration 4483, train loss: 0.004409, validation loss: 0.003565\n",
      "iteration 4484, train loss: 0.004296, validation loss: 0.003248\n",
      "iteration 4485, train loss: 0.004416, validation loss: 0.002909\n",
      "iteration 4486, train loss: 0.004077, validation loss: 0.003551\n",
      "iteration 4487, train loss: 0.005146, validation loss: 0.003294\n",
      "iteration 4488, train loss: 0.004622, validation loss: 0.002706\n",
      "iteration 4489, train loss: 0.004403, validation loss: 0.003106\n",
      "iteration 4490, train loss: 0.004578, validation loss: 0.003852\n",
      "iteration 4491, train loss: 0.00465, validation loss: 0.004091\n",
      "iteration 4492, train loss: 0.004906, validation loss: 0.003246\n",
      "iteration 4493, train loss: 0.003935, validation loss: 0.002715\n",
      "iteration 4494, train loss: 0.003837, validation loss: 0.002798\n",
      "iteration 4495, train loss: 0.004124, validation loss: 0.002844\n",
      "iteration 4496, train loss: 0.004294, validation loss: 0.00254\n",
      "iteration 4497, train loss: 0.004483, validation loss: 0.002626\n",
      "iteration 4498, train loss: 0.003954, validation loss: 0.003174\n",
      "iteration 4499, train loss: 0.003801, validation loss: 0.003439\n",
      "iteration 4500, train loss: 0.004588, validation loss: 0.002973\n",
      "iteration 4501, train loss: 0.003869, validation loss: 0.002608\n",
      "iteration 4502, train loss: 0.003866, validation loss: 0.002519\n",
      "iteration 4503, train loss: 0.003833, validation loss: 0.002769\n",
      "iteration 4504, train loss: 0.004609, validation loss: 0.002604\n",
      "iteration 4505, train loss: 0.004206, validation loss: 0.002707\n",
      "iteration 4506, train loss: 0.004168, validation loss: 0.003164\n",
      "iteration 4507, train loss: 0.004349, validation loss: 0.003318\n",
      "iteration 4508, train loss: 0.004233, validation loss: 0.003018\n",
      "iteration 4509, train loss: 0.004341, validation loss: \u001b[92m0.002479\u001b[0m\n",
      "iteration 4510, train loss: 0.00366, validation loss: 0.002501\n",
      "iteration 4511, train loss: 0.003395, validation loss: 0.002728\n",
      "iteration 4512, train loss: 0.004039, validation loss: 0.002688\n",
      "iteration 4513, train loss: 0.004121, validation loss: 0.002641\n",
      "iteration 4514, train loss: 0.003641, validation loss: 0.00275\n",
      "iteration 4515, train loss: 0.004169, validation loss: 0.002831\n",
      "iteration 4516, train loss: 0.004382, validation loss: 0.002859\n",
      "iteration 4517, train loss: 0.003938, validation loss: 0.002635\n",
      "iteration 4518, train loss: 0.004085, validation loss: \u001b[92m0.002476\u001b[0m\n",
      "iteration 4519, train loss: 0.003889, validation loss: 0.002575\n",
      "iteration 4520, train loss: 0.003742, validation loss: 0.002639\n",
      "iteration 4521, train loss: 0.004072, validation loss: 0.002511\n",
      "iteration 4522, train loss: 0.003784, validation loss: 0.002512\n",
      "iteration 4523, train loss: 0.003509, validation loss: 0.002807\n",
      "iteration 4524, train loss: 0.004194, validation loss: 0.002943\n",
      "iteration 4525, train loss: 0.004201, validation loss: 0.002833\n",
      "iteration 4526, train loss: 0.004292, validation loss: 0.002714\n",
      "iteration 4527, train loss: 0.003885, validation loss: 0.002617\n",
      "iteration 4528, train loss: 0.003938, validation loss: 0.002499\n",
      "iteration 4529, train loss: 0.003786, validation loss: 0.002513\n",
      "iteration 4530, train loss: 0.00402, validation loss: 0.002535\n",
      "iteration 4531, train loss: 0.003885, validation loss: 0.00281\n",
      "iteration 4532, train loss: 0.003856, validation loss: 0.002917\n",
      "iteration 4533, train loss: 0.003884, validation loss: 0.002755\n",
      "iteration 4534, train loss: 0.003649, validation loss: 0.002549\n",
      "iteration 4535, train loss: 0.003777, validation loss: 0.002509\n",
      "iteration 4536, train loss: 0.003871, validation loss: 0.002497\n",
      "iteration 4537, train loss: 0.003691, validation loss: 0.002517\n",
      "iteration 4538, train loss: 0.003718, validation loss: 0.002714\n",
      "iteration 4539, train loss: 0.00389, validation loss: 0.002738\n",
      "iteration 4540, train loss: 0.003931, validation loss: 0.002657\n",
      "iteration 4541, train loss: 0.003743, validation loss: 0.00253\n",
      "iteration 4542, train loss: 0.003927, validation loss: 0.002487\n",
      "iteration 4543, train loss: 0.003838, validation loss: 0.002585\n",
      "iteration 4544, train loss: 0.003778, validation loss: 0.002619\n",
      "iteration 4545, train loss: 0.003852, validation loss: 0.002678\n",
      "iteration 4546, train loss: 0.003676, validation loss: 0.002708\n",
      "iteration 4547, train loss: 0.00364, validation loss: 0.00271\n",
      "iteration 4548, train loss: 0.003525, validation loss: 0.002604\n",
      "iteration 4549, train loss: 0.004145, validation loss: 0.002494\n",
      "iteration 4550, train loss: 0.004024, validation loss: 0.002513\n",
      "iteration 4551, train loss: 0.003639, validation loss: 0.002661\n",
      "iteration 4552, train loss: 0.003696, validation loss: 0.002731\n",
      "iteration 4553, train loss: 0.003803, validation loss: 0.002835\n",
      "iteration 4554, train loss: 0.00371, validation loss: 0.002765\n",
      "iteration 4555, train loss: 0.003682, validation loss: 0.00271\n",
      "iteration 4556, train loss: 0.003993, validation loss: 0.002542\n",
      "iteration 4557, train loss: 0.00365, validation loss: 0.002534\n",
      "iteration 4558, train loss: 0.003552, validation loss: 0.002596\n",
      "iteration 4559, train loss: 0.00394, validation loss: 0.002574\n",
      "iteration 4560, train loss: 0.0038, validation loss: 0.002571\n",
      "iteration 4561, train loss: 0.003811, validation loss: 0.002521\n",
      "iteration 4562, train loss: \u001b[92m0.003332\u001b[0m, validation loss: 0.002575\n",
      "iteration 4563, train loss: 0.00374, validation loss: 0.00274\n",
      "iteration 4564, train loss: 0.003972, validation loss: 0.002923\n",
      "iteration 4565, train loss: 0.00428, validation loss: 0.002647\n",
      "iteration 4566, train loss: 0.003791, validation loss: \u001b[92m0.002463\u001b[0m\n",
      "iteration 4567, train loss: 0.003718, validation loss: 0.002623\n",
      "iteration 4568, train loss: 0.003961, validation loss: 0.002706\n",
      "iteration 4569, train loss: 0.004008, validation loss: 0.002709\n",
      "iteration 4570, train loss: 0.003714, validation loss: 0.002792\n",
      "iteration 4571, train loss: 0.00351, validation loss: 0.002816\n",
      "iteration 4572, train loss: 0.003775, validation loss: 0.002648\n",
      "iteration 4573, train loss: 0.004635, validation loss: \u001b[92m0.002454\u001b[0m\n",
      "iteration 4574, train loss: 0.00369, validation loss: 0.002586\n",
      "iteration 4575, train loss: 0.003672, validation loss: 0.002787\n",
      "iteration 4576, train loss: 0.00372, validation loss: 0.002632\n",
      "iteration 4577, train loss: 0.004012, validation loss: 0.00251\n",
      "iteration 4578, train loss: 0.003674, validation loss: 0.00275\n",
      "iteration 4579, train loss: 0.00396, validation loss: 0.00278\n",
      "iteration 4580, train loss: 0.003654, validation loss: 0.002676\n",
      "iteration 4581, train loss: 0.003793, validation loss: 0.002669\n",
      "iteration 4582, train loss: 0.003531, validation loss: 0.002734\n",
      "iteration 4583, train loss: 0.003567, validation loss: 0.002794\n",
      "iteration 4584, train loss: 0.004261, validation loss: 0.002549\n",
      "iteration 4585, train loss: 0.003548, validation loss: \u001b[92m0.002452\u001b[0m\n",
      "iteration 4586, train loss: \u001b[92m0.003324\u001b[0m, validation loss: \u001b[92m0.002431\u001b[0m\n",
      "iteration 4587, train loss: 0.003952, validation loss: 0.002467\n",
      "iteration 4588, train loss: 0.003764, validation loss: 0.002552\n",
      "iteration 4589, train loss: 0.003608, validation loss: 0.002689\n",
      "iteration 4590, train loss: 0.003596, validation loss: 0.002828\n",
      "iteration 4591, train loss: 0.003815, validation loss: 0.00267\n",
      "iteration 4592, train loss: 0.003503, validation loss: 0.002532\n",
      "iteration 4593, train loss: 0.003991, validation loss: 0.00249\n",
      "iteration 4594, train loss: 0.0037, validation loss: 0.002556\n",
      "iteration 4595, train loss: 0.003784, validation loss: 0.002703\n",
      "iteration 4596, train loss: 0.003704, validation loss: 0.002715\n",
      "iteration 4597, train loss: 0.003684, validation loss: 0.002657\n",
      "iteration 4598, train loss: 0.003771, validation loss: 0.002729\n",
      "iteration 4599, train loss: 0.003408, validation loss: 0.002876\n",
      "iteration 4600, train loss: 0.003945, validation loss: 0.002848\n",
      "iteration 4601, train loss: 0.00443, validation loss: 0.00273\n",
      "iteration 4602, train loss: 0.003868, validation loss: 0.00278\n",
      "iteration 4603, train loss: 0.003989, validation loss: 0.002592\n",
      "iteration 4604, train loss: 0.004088, validation loss: 0.002456\n",
      "iteration 4605, train loss: 0.003747, validation loss: 0.002712\n",
      "iteration 4606, train loss: 0.004053, validation loss: 0.002905\n",
      "iteration 4607, train loss: 0.00372, validation loss: 0.002916\n",
      "iteration 4608, train loss: 0.003839, validation loss: 0.002763\n",
      "iteration 4609, train loss: 0.004131, validation loss: 0.002497\n",
      "iteration 4610, train loss: 0.003637, validation loss: 0.00245\n",
      "iteration 4611, train loss: 0.003908, validation loss: 0.002442\n",
      "iteration 4612, train loss: 0.003648, validation loss: 0.002636\n",
      "iteration 4613, train loss: 0.003672, validation loss: 0.002898\n",
      "iteration 4614, train loss: 0.003884, validation loss: 0.002792\n",
      "iteration 4615, train loss: 0.003796, validation loss: 0.002749\n",
      "iteration 4616, train loss: 0.0042, validation loss: 0.002522\n",
      "iteration 4617, train loss: 0.003915, validation loss: 0.002466\n",
      "iteration 4618, train loss: 0.003843, validation loss: 0.002682\n",
      "iteration 4619, train loss: 0.00353, validation loss: 0.002788\n",
      "iteration 4620, train loss: 0.003518, validation loss: 0.002705\n",
      "iteration 4621, train loss: 0.00366, validation loss: 0.0026\n",
      "iteration 4622, train loss: 0.003696, validation loss: 0.002567\n",
      "iteration 4623, train loss: 0.003731, validation loss: 0.002482\n",
      "iteration 4624, train loss: 0.003903, validation loss: 0.0026\n",
      "iteration 4625, train loss: 0.003902, validation loss: 0.002683\n",
      "iteration 4626, train loss: 0.004193, validation loss: 0.002492\n",
      "iteration 4627, train loss: 0.004127, validation loss: 0.002612\n",
      "iteration 4628, train loss: 0.0044, validation loss: 0.002647\n",
      "iteration 4629, train loss: 0.003629, validation loss: 0.002643\n",
      "iteration 4630, train loss: 0.003954, validation loss: 0.002575\n",
      "iteration 4631, train loss: 0.003779, validation loss: 0.00247\n",
      "iteration 4632, train loss: 0.003653, validation loss: 0.002472\n",
      "iteration 4633, train loss: 0.003758, validation loss: 0.002513\n",
      "iteration 4634, train loss: 0.00355, validation loss: 0.002551\n",
      "iteration 4635, train loss: 0.003761, validation loss: 0.00254\n",
      "iteration 4636, train loss: 0.003687, validation loss: 0.002624\n",
      "iteration 4637, train loss: 0.003884, validation loss: 0.00268\n",
      "iteration 4638, train loss: 0.003896, validation loss: 0.002572\n",
      "iteration 4639, train loss: 0.003844, validation loss: 0.002444\n",
      "iteration 4640, train loss: 0.004065, validation loss: 0.002616\n",
      "iteration 4641, train loss: 0.004053, validation loss: 0.002659\n",
      "iteration 4642, train loss: 0.004144, validation loss: 0.002615\n",
      "iteration 4643, train loss: 0.00372, validation loss: 0.002988\n",
      "iteration 4644, train loss: 0.00396, validation loss: 0.003081\n",
      "iteration 4645, train loss: 0.003942, validation loss: 0.003053\n",
      "iteration 4646, train loss: 0.003838, validation loss: 0.002786\n",
      "iteration 4647, train loss: 0.003641, validation loss: 0.002536\n",
      "iteration 4648, train loss: 0.003513, validation loss: \u001b[92m0.002426\u001b[0m\n",
      "iteration 4649, train loss: 0.003777, validation loss: 0.002537\n",
      "iteration 4650, train loss: 0.003561, validation loss: 0.002721\n",
      "iteration 4651, train loss: 0.00386, validation loss: 0.002641\n",
      "iteration 4652, train loss: 0.003622, validation loss: 0.00265\n",
      "iteration 4653, train loss: 0.003629, validation loss: 0.002822\n",
      "iteration 4654, train loss: 0.004162, validation loss: 0.002629\n",
      "iteration 4655, train loss: 0.003642, validation loss: 0.002478\n",
      "iteration 4656, train loss: 0.00369, validation loss: 0.002734\n",
      "iteration 4657, train loss: 0.003976, validation loss: 0.002958\n",
      "iteration 4658, train loss: 0.004134, validation loss: 0.002792\n",
      "iteration 4659, train loss: 0.004137, validation loss: 0.002727\n",
      "iteration 4660, train loss: 0.003748, validation loss: 0.002966\n",
      "iteration 4661, train loss: 0.004396, validation loss: 0.002809\n",
      "iteration 4662, train loss: 0.004269, validation loss: 0.00296\n",
      "iteration 4663, train loss: 0.003842, validation loss: 0.003672\n",
      "iteration 4664, train loss: 0.004395, validation loss: 0.003647\n",
      "iteration 4665, train loss: 0.004019, validation loss: 0.003132\n",
      "iteration 4666, train loss: 0.004232, validation loss: 0.00263\n",
      "iteration 4667, train loss: 0.003388, validation loss: 0.002863\n",
      "iteration 4668, train loss: 0.00422, validation loss: 0.002759\n",
      "iteration 4669, train loss: 0.003991, validation loss: 0.002707\n",
      "iteration 4670, train loss: 0.003644, validation loss: 0.003224\n",
      "iteration 4671, train loss: 0.004356, validation loss: 0.003197\n",
      "iteration 4672, train loss: 0.004084, validation loss: 0.002747\n",
      "iteration 4673, train loss: 0.003778, validation loss: 0.002513\n",
      "iteration 4674, train loss: 0.003694, validation loss: 0.002736\n",
      "iteration 4675, train loss: 0.004025, validation loss: 0.002793\n",
      "iteration 4676, train loss: 0.004362, validation loss: 0.002672\n",
      "iteration 4677, train loss: 0.004087, validation loss: 0.002947\n",
      "iteration 4678, train loss: 0.003966, validation loss: 0.003237\n",
      "iteration 4679, train loss: 0.004491, validation loss: 0.002753\n",
      "iteration 4680, train loss: 0.003835, validation loss: 0.002534\n",
      "iteration 4681, train loss: 0.003862, validation loss: 0.002655\n",
      "iteration 4682, train loss: 0.003943, validation loss: 0.00286\n",
      "iteration 4683, train loss: 0.003578, validation loss: 0.002877\n",
      "iteration 4684, train loss: 0.004222, validation loss: 0.00276\n",
      "iteration 4685, train loss: 0.003977, validation loss: 0.002724\n",
      "iteration 4686, train loss: 0.004034, validation loss: 0.002506\n",
      "iteration 4687, train loss: 0.004036, validation loss: \u001b[92m0.002416\u001b[0m\n",
      "iteration 4688, train loss: 0.003979, validation loss: 0.00264\n",
      "iteration 4689, train loss: 0.004036, validation loss: 0.003001\n",
      "iteration 4690, train loss: 0.004288, validation loss: 0.003013\n",
      "iteration 4691, train loss: 0.003865, validation loss: 0.002955\n",
      "iteration 4692, train loss: 0.003856, validation loss: 0.002782\n",
      "iteration 4693, train loss: 0.003737, validation loss: 0.002665\n",
      "iteration 4694, train loss: 0.004163, validation loss: 0.002541\n",
      "iteration 4695, train loss: 0.004379, validation loss: 0.002566\n",
      "iteration 4696, train loss: 0.003972, validation loss: 0.002781\n",
      "iteration 4697, train loss: 0.003811, validation loss: 0.002993\n",
      "iteration 4698, train loss: 0.003801, validation loss: 0.002891\n",
      "iteration 4699, train loss: 0.003609, validation loss: 0.002603\n",
      "iteration 4700, train loss: 0.003594, validation loss: \u001b[92m0.002402\u001b[0m\n",
      "iteration 4701, train loss: 0.003826, validation loss: 0.002463\n",
      "iteration 4702, train loss: 0.003663, validation loss: 0.002517\n",
      "iteration 4703, train loss: 0.003842, validation loss: 0.002448\n",
      "iteration 4704, train loss: 0.00379, validation loss: 0.00261\n",
      "iteration 4705, train loss: 0.003565, validation loss: 0.002932\n",
      "iteration 4706, train loss: 0.00377, validation loss: 0.002868\n",
      "iteration 4707, train loss: 0.003819, validation loss: 0.002635\n",
      "iteration 4708, train loss: 0.003816, validation loss: 0.002657\n",
      "iteration 4709, train loss: 0.003807, validation loss: 0.00267\n",
      "iteration 4710, train loss: 0.003476, validation loss: 0.002515\n",
      "iteration 4711, train loss: 0.003607, validation loss: 0.002509\n",
      "iteration 4712, train loss: 0.003796, validation loss: 0.002716\n",
      "iteration 4713, train loss: 0.004226, validation loss: 0.002585\n",
      "iteration 4714, train loss: 0.004135, validation loss: 0.002563\n",
      "iteration 4715, train loss: 0.003732, validation loss: 0.002788\n",
      "iteration 4716, train loss: 0.003726, validation loss: 0.002663\n",
      "iteration 4717, train loss: 0.003618, validation loss: 0.002621\n",
      "iteration 4718, train loss: 0.003875, validation loss: 0.002532\n",
      "iteration 4719, train loss: 0.003493, validation loss: 0.002492\n",
      "iteration 4720, train loss: 0.003732, validation loss: 0.002458\n",
      "iteration 4721, train loss: 0.003545, validation loss: 0.002736\n",
      "iteration 4722, train loss: 0.004484, validation loss: 0.002826\n",
      "iteration 4723, train loss: 0.003761, validation loss: 0.002832\n",
      "iteration 4724, train loss: 0.003614, validation loss: 0.002821\n",
      "iteration 4725, train loss: 0.003744, validation loss: 0.002576\n",
      "iteration 4726, train loss: 0.003776, validation loss: 0.002535\n",
      "iteration 4727, train loss: 0.003755, validation loss: 0.002644\n",
      "iteration 4728, train loss: 0.003557, validation loss: 0.002664\n",
      "iteration 4729, train loss: 0.004148, validation loss: 0.002547\n",
      "iteration 4730, train loss: 0.003746, validation loss: 0.002636\n",
      "iteration 4731, train loss: 0.003793, validation loss: 0.002797\n",
      "iteration 4732, train loss: 0.004044, validation loss: 0.002652\n",
      "iteration 4733, train loss: 0.003396, validation loss: 0.002971\n",
      "iteration 4734, train loss: 0.00408, validation loss: 0.002996\n",
      "iteration 4735, train loss: 0.00405, validation loss: 0.002639\n",
      "iteration 4736, train loss: 0.003973, validation loss: 0.002677\n",
      "iteration 4737, train loss: 0.003736, validation loss: 0.002873\n",
      "iteration 4738, train loss: 0.003592, validation loss: 0.002802\n",
      "iteration 4739, train loss: 0.004063, validation loss: 0.002521\n",
      "iteration 4740, train loss: 0.003487, validation loss: 0.002551\n",
      "iteration 4741, train loss: 0.003882, validation loss: 0.002698\n",
      "iteration 4742, train loss: 0.003867, validation loss: 0.002652\n",
      "iteration 4743, train loss: 0.003842, validation loss: 0.002565\n",
      "iteration 4744, train loss: 0.003708, validation loss: 0.002616\n",
      "iteration 4745, train loss: 0.003647, validation loss: 0.002722\n",
      "iteration 4746, train loss: 0.003766, validation loss: 0.002672\n",
      "iteration 4747, train loss: 0.003709, validation loss: 0.002645\n",
      "iteration 4748, train loss: 0.004003, validation loss: 0.002667\n",
      "iteration 4749, train loss: 0.004348, validation loss: 0.002486\n",
      "iteration 4750, train loss: 0.003922, validation loss: 0.002926\n",
      "iteration 4751, train loss: 0.004088, validation loss: 0.003327\n",
      "iteration 4752, train loss: 0.004071, validation loss: 0.003084\n",
      "iteration 4753, train loss: 0.003864, validation loss: 0.002542\n",
      "iteration 4754, train loss: 0.003807, validation loss: 0.002608\n",
      "iteration 4755, train loss: 0.003928, validation loss: 0.002805\n",
      "iteration 4756, train loss: 0.00406, validation loss: 0.002647\n",
      "iteration 4757, train loss: 0.00428, validation loss: 0.002542\n",
      "iteration 4758, train loss: 0.003779, validation loss: 0.002789\n",
      "iteration 4759, train loss: 0.003719, validation loss: 0.003052\n",
      "iteration 4760, train loss: 0.003773, validation loss: 0.002803\n",
      "iteration 4761, train loss: 0.003811, validation loss: 0.002462\n",
      "iteration 4762, train loss: 0.003921, validation loss: 0.002439\n",
      "iteration 4763, train loss: 0.003884, validation loss: 0.00258\n",
      "iteration 4764, train loss: 0.004089, validation loss: 0.002563\n",
      "iteration 4765, train loss: 0.003649, validation loss: 0.002741\n",
      "iteration 4766, train loss: 0.003807, validation loss: 0.002917\n",
      "iteration 4767, train loss: 0.004318, validation loss: 0.002661\n",
      "iteration 4768, train loss: 0.003441, validation loss: 0.002607\n",
      "iteration 4769, train loss: 0.003482, validation loss: 0.002804\n",
      "iteration 4770, train loss: 0.003928, validation loss: 0.002734\n",
      "iteration 4771, train loss: 0.004118, validation loss: 0.002543\n",
      "iteration 4772, train loss: 0.003551, validation loss: 0.002653\n",
      "iteration 4773, train loss: 0.003721, validation loss: 0.002746\n",
      "iteration 4774, train loss: 0.003934, validation loss: 0.002574\n",
      "iteration 4775, train loss: 0.003882, validation loss: 0.002495\n",
      "iteration 4776, train loss: 0.003629, validation loss: 0.002687\n",
      "iteration 4777, train loss: 0.003645, validation loss: 0.002735\n",
      "iteration 4778, train loss: 0.004133, validation loss: 0.002589\n",
      "iteration 4779, train loss: 0.003465, validation loss: 0.002812\n",
      "iteration 4780, train loss: 0.003942, validation loss: 0.002678\n",
      "iteration 4781, train loss: 0.003912, validation loss: 0.002468\n",
      "iteration 4782, train loss: 0.003635, validation loss: 0.00259\n",
      "iteration 4783, train loss: 0.003717, validation loss: 0.003111\n",
      "iteration 4784, train loss: 0.003769, validation loss: 0.00313\n",
      "iteration 4785, train loss: 0.004269, validation loss: 0.002621\n",
      "iteration 4786, train loss: 0.003765, validation loss: 0.002542\n",
      "iteration 4787, train loss: 0.004054, validation loss: 0.002705\n",
      "iteration 4788, train loss: 0.004193, validation loss: 0.002721\n",
      "iteration 4789, train loss: 0.003591, validation loss: 0.003048\n",
      "iteration 4790, train loss: 0.004094, validation loss: 0.002961\n",
      "iteration 4791, train loss: 0.003725, validation loss: 0.002771\n",
      "iteration 4792, train loss: 0.003938, validation loss: 0.00286\n",
      "iteration 4793, train loss: 0.004141, validation loss: 0.002665\n",
      "iteration 4794, train loss: 0.003682, validation loss: 0.002443\n",
      "iteration 4795, train loss: 0.003588, validation loss: 0.002646\n",
      "iteration 4796, train loss: 0.004225, validation loss: 0.002667\n",
      "iteration 4797, train loss: 0.003662, validation loss: 0.002622\n",
      "iteration 4798, train loss: 0.003959, validation loss: 0.002772\n",
      "iteration 4799, train loss: 0.003742, validation loss: 0.003197\n",
      "iteration 4800, train loss: 0.004238, validation loss: 0.002932\n",
      "iteration 4801, train loss: 0.004349, validation loss: 0.002719\n",
      "iteration 4802, train loss: 0.003937, validation loss: 0.002827\n",
      "iteration 4803, train loss: 0.004153, validation loss: 0.002737\n",
      "iteration 4804, train loss: 0.003571, validation loss: 0.002516\n",
      "iteration 4805, train loss: 0.003821, validation loss: 0.002456\n",
      "iteration 4806, train loss: 0.00387, validation loss: 0.002587\n",
      "iteration 4807, train loss: 0.003772, validation loss: 0.00267\n",
      "iteration 4808, train loss: 0.003899, validation loss: 0.002666\n",
      "iteration 4809, train loss: 0.003821, validation loss: 0.002556\n",
      "iteration 4810, train loss: 0.003723, validation loss: 0.002462\n",
      "iteration 4811, train loss: 0.003445, validation loss: 0.00258\n",
      "iteration 4812, train loss: 0.00392, validation loss: 0.002452\n",
      "iteration 4813, train loss: 0.003561, validation loss: 0.002576\n",
      "iteration 4814, train loss: 0.004273, validation loss: 0.002734\n",
      "iteration 4815, train loss: 0.003856, validation loss: 0.002853\n",
      "iteration 4816, train loss: 0.003532, validation loss: 0.002916\n",
      "iteration 4817, train loss: 0.003697, validation loss: 0.002772\n",
      "iteration 4818, train loss: 0.003757, validation loss: 0.002622\n",
      "iteration 4819, train loss: 0.003968, validation loss: 0.002466\n",
      "iteration 4820, train loss: 0.003828, validation loss: 0.002489\n",
      "iteration 4821, train loss: 0.00363, validation loss: 0.002735\n",
      "iteration 4822, train loss: 0.003874, validation loss: 0.002735\n",
      "iteration 4823, train loss: 0.00383, validation loss: 0.002604\n",
      "iteration 4824, train loss: 0.003619, validation loss: 0.002708\n",
      "iteration 4825, train loss: 0.003967, validation loss: 0.002625\n",
      "iteration 4826, train loss: 0.003534, validation loss: 0.0025\n",
      "iteration 4827, train loss: 0.003638, validation loss: 0.002415\n",
      "iteration 4828, train loss: 0.003551, validation loss: 0.002614\n",
      "iteration 4829, train loss: 0.003961, validation loss: 0.002729\n",
      "iteration 4830, train loss: 0.00376, validation loss: 0.002745\n",
      "iteration 4831, train loss: 0.003786, validation loss: 0.002562\n",
      "iteration 4832, train loss: 0.00365, validation loss: 0.002427\n",
      "iteration 4833, train loss: 0.003662, validation loss: 0.002443\n",
      "iteration 4834, train loss: 0.003731, validation loss: 0.002446\n",
      "iteration 4835, train loss: 0.00361, validation loss: 0.002473\n",
      "iteration 4836, train loss: 0.003634, validation loss: 0.002718\n",
      "iteration 4837, train loss: 0.00393, validation loss: 0.002888\n",
      "iteration 4838, train loss: 0.0041, validation loss: 0.002637\n",
      "iteration 4839, train loss: 0.003381, validation loss: 0.002519\n",
      "iteration 4840, train loss: 0.003607, validation loss: 0.002468\n",
      "iteration 4841, train loss: 0.00394, validation loss: 0.002436\n",
      "iteration 4842, train loss: 0.003704, validation loss: 0.002868\n",
      "iteration 4843, train loss: 0.003881, validation loss: 0.003051\n",
      "iteration 4844, train loss: 0.004172, validation loss: 0.00272\n",
      "iteration 4845, train loss: 0.003565, validation loss: 0.002604\n",
      "iteration 4846, train loss: 0.003768, validation loss: 0.002727\n",
      "iteration 4847, train loss: 0.003824, validation loss: 0.002524\n",
      "iteration 4848, train loss: 0.004053, validation loss: 0.002519\n",
      "iteration 4849, train loss: 0.003637, validation loss: 0.002873\n",
      "iteration 4850, train loss: 0.004245, validation loss: 0.002688\n",
      "iteration 4851, train loss: 0.003961, validation loss: 0.002479\n",
      "iteration 4852, train loss: 0.003359, validation loss: 0.002713\n",
      "iteration 4853, train loss: 0.003734, validation loss: 0.002846\n",
      "iteration 4854, train loss: 0.004193, validation loss: 0.002598\n",
      "iteration 4855, train loss: 0.003425, validation loss: 0.002642\n",
      "iteration 4856, train loss: 0.003899, validation loss: 0.002811\n",
      "iteration 4857, train loss: 0.003872, validation loss: 0.002591\n",
      "iteration 4858, train loss: 0.004001, validation loss: 0.002528\n",
      "iteration 4859, train loss: 0.003799, validation loss: 0.002749\n",
      "iteration 4860, train loss: 0.003912, validation loss: 0.002858\n",
      "iteration 4861, train loss: 0.004225, validation loss: 0.002857\n",
      "iteration 4862, train loss: 0.003689, validation loss: 0.00286\n",
      "iteration 4863, train loss: 0.003575, validation loss: 0.002737\n",
      "iteration 4864, train loss: 0.003801, validation loss: 0.002519\n",
      "iteration 4865, train loss: 0.00368, validation loss: 0.002596\n",
      "iteration 4866, train loss: 0.003703, validation loss: 0.002751\n",
      "iteration 4867, train loss: 0.004241, validation loss: 0.002522\n",
      "iteration 4868, train loss: 0.003324, validation loss: 0.00274\n",
      "iteration 4869, train loss: 0.00379, validation loss: 0.002689\n",
      "iteration 4870, train loss: 0.003736, validation loss: 0.002516\n",
      "iteration 4871, train loss: 0.003715, validation loss: 0.002626\n",
      "iteration 4872, train loss: 0.003544, validation loss: 0.002806\n",
      "iteration 4873, train loss: 0.003998, validation loss: 0.002631\n",
      "iteration 4874, train loss: 0.003777, validation loss: 0.002991\n",
      "iteration 4875, train loss: 0.003899, validation loss: 0.003592\n",
      "iteration 4876, train loss: 0.004275, validation loss: 0.003258\n",
      "iteration 4877, train loss: 0.004278, validation loss: 0.002602\n",
      "iteration 4878, train loss: 0.003417, validation loss: 0.00275\n",
      "iteration 4879, train loss: 0.003992, validation loss: 0.003079\n",
      "iteration 4880, train loss: 0.003843, validation loss: 0.003051\n",
      "iteration 4881, train loss: 0.004074, validation loss: 0.002788\n",
      "iteration 4882, train loss: 0.003658, validation loss: 0.00281\n",
      "iteration 4883, train loss: 0.004082, validation loss: 0.003028\n",
      "iteration 4884, train loss: 0.004208, validation loss: 0.0029\n",
      "iteration 4885, train loss: 0.004016, validation loss: 0.002557\n",
      "iteration 4886, train loss: 0.003601, validation loss: 0.002582\n",
      "iteration 4887, train loss: 0.003659, validation loss: 0.002756\n",
      "iteration 4888, train loss: 0.00376, validation loss: 0.002673\n",
      "iteration 4889, train loss: 0.003903, validation loss: 0.00255\n",
      "iteration 4890, train loss: \u001b[92m0.003276\u001b[0m, validation loss: 0.002752\n",
      "iteration 4891, train loss: 0.00418, validation loss: 0.002641\n",
      "iteration 4892, train loss: 0.003759, validation loss: 0.002529\n",
      "iteration 4893, train loss: 0.003648, validation loss: 0.002616\n",
      "iteration 4894, train loss: 0.003352, validation loss: 0.00274\n",
      "iteration 4895, train loss: 0.003434, validation loss: 0.002765\n",
      "iteration 4896, train loss: 0.004003, validation loss: 0.00259\n",
      "iteration 4897, train loss: 0.003804, validation loss: 0.002524\n",
      "iteration 4898, train loss: 0.003653, validation loss: 0.002944\n",
      "iteration 4899, train loss: 0.003924, validation loss: 0.003028\n",
      "iteration 4900, train loss: 0.004727, validation loss: 0.002645\n",
      "iteration 4901, train loss: 0.003574, validation loss: 0.00287\n",
      "iteration 4902, train loss: 0.003993, validation loss: 0.003194\n",
      "iteration 4903, train loss: 0.004121, validation loss: 0.00292\n",
      "iteration 4904, train loss: 0.003977, validation loss: 0.002622\n",
      "iteration 4905, train loss: 0.003957, validation loss: 0.002579\n",
      "iteration 4906, train loss: 0.004231, validation loss: 0.002657\n",
      "iteration 4907, train loss: 0.003891, validation loss: 0.002544\n",
      "iteration 4908, train loss: 0.003548, validation loss: 0.002696\n",
      "iteration 4909, train loss: 0.003613, validation loss: 0.002814\n",
      "iteration 4910, train loss: 0.003745, validation loss: 0.002718\n",
      "iteration 4911, train loss: 0.003571, validation loss: 0.002675\n",
      "iteration 4912, train loss: 0.003727, validation loss: 0.002694\n",
      "iteration 4913, train loss: 0.004018, validation loss: 0.002733\n",
      "iteration 4914, train loss: 0.004068, validation loss: 0.002578\n",
      "iteration 4915, train loss: 0.003651, validation loss: 0.002697\n",
      "iteration 4916, train loss: 0.003788, validation loss: 0.002804\n",
      "iteration 4917, train loss: 0.003693, validation loss: 0.002677\n",
      "iteration 4918, train loss: 0.003749, validation loss: 0.002652\n",
      "iteration 4919, train loss: 0.003583, validation loss: 0.002566\n",
      "iteration 4920, train loss: 0.003663, validation loss: 0.002441\n",
      "iteration 4921, train loss: 0.003667, validation loss: 0.002516\n",
      "iteration 4922, train loss: 0.003612, validation loss: 0.002483\n",
      "iteration 4923, train loss: 0.00348, validation loss: 0.002544\n",
      "iteration 4924, train loss: 0.003673, validation loss: 0.002944\n",
      "iteration 4925, train loss: 0.003642, validation loss: 0.00294\n",
      "iteration 4926, train loss: 0.003596, validation loss: 0.002643\n",
      "iteration 4927, train loss: 0.003487, validation loss: 0.00242\n",
      "iteration 4928, train loss: 0.003783, validation loss: \u001b[92m0.002384\u001b[0m\n",
      "iteration 4929, train loss: 0.003579, validation loss: 0.002441\n",
      "iteration 4930, train loss: 0.003511, validation loss: 0.002475\n",
      "iteration 4931, train loss: 0.003762, validation loss: 0.002607\n",
      "iteration 4932, train loss: 0.003596, validation loss: 0.00263\n",
      "iteration 4933, train loss: 0.00371, validation loss: 0.002599\n",
      "iteration 4934, train loss: 0.003666, validation loss: 0.002434\n",
      "iteration 4935, train loss: 0.003748, validation loss: 0.002493\n",
      "iteration 4936, train loss: 0.00364, validation loss: 0.002555\n",
      "iteration 4937, train loss: 0.003741, validation loss: 0.002429\n",
      "iteration 4938, train loss: 0.003736, validation loss: 0.002468\n",
      "iteration 4939, train loss: 0.003886, validation loss: 0.002573\n",
      "iteration 4940, train loss: 0.003849, validation loss: 0.002488\n",
      "iteration 4941, train loss: 0.003661, validation loss: 0.002517\n",
      "iteration 4942, train loss: 0.003507, validation loss: 0.002571\n",
      "iteration 4943, train loss: 0.003862, validation loss: 0.002507\n",
      "iteration 4944, train loss: 0.003544, validation loss: 0.002539\n",
      "iteration 4945, train loss: 0.003546, validation loss: 0.002562\n",
      "iteration 4946, train loss: 0.003601, validation loss: 0.002559\n",
      "iteration 4947, train loss: 0.003769, validation loss: 0.002511\n",
      "iteration 4948, train loss: 0.00339, validation loss: 0.002539\n",
      "iteration 4949, train loss: 0.003648, validation loss: 0.002542\n",
      "iteration 4950, train loss: 0.003392, validation loss: 0.002508\n",
      "iteration 4951, train loss: 0.003591, validation loss: 0.002447\n",
      "iteration 4952, train loss: 0.003517, validation loss: 0.002454\n",
      "iteration 4953, train loss: 0.003741, validation loss: 0.002442\n",
      "iteration 4954, train loss: 0.003533, validation loss: 0.002538\n",
      "iteration 4955, train loss: 0.004183, validation loss: 0.002568\n",
      "iteration 4956, train loss: 0.003471, validation loss: 0.00263\n",
      "iteration 4957, train loss: 0.003933, validation loss: 0.002779\n",
      "iteration 4958, train loss: 0.004073, validation loss: 0.002721\n",
      "iteration 4959, train loss: 0.004174, validation loss: 0.002454\n",
      "iteration 4960, train loss: 0.003367, validation loss: 0.002467\n",
      "iteration 4961, train loss: 0.003923, validation loss: 0.002575\n",
      "iteration 4962, train loss: 0.003849, validation loss: 0.002611\n",
      "iteration 4963, train loss: 0.003509, validation loss: 0.002619\n",
      "iteration 4964, train loss: 0.003482, validation loss: 0.00267\n",
      "iteration 4965, train loss: 0.003999, validation loss: 0.002709\n",
      "iteration 4966, train loss: 0.003356, validation loss: 0.002892\n",
      "iteration 4967, train loss: 0.003816, validation loss: 0.002899\n",
      "iteration 4968, train loss: 0.003747, validation loss: 0.002565\n",
      "iteration 4969, train loss: 0.003811, validation loss: \u001b[92m0.002372\u001b[0m\n",
      "iteration 4970, train loss: 0.003672, validation loss: 0.002752\n",
      "iteration 4971, train loss: 0.004018, validation loss: 0.002852\n",
      "iteration 4972, train loss: 0.004381, validation loss: 0.002509\n",
      "iteration 4973, train loss: 0.003706, validation loss: 0.002593\n",
      "iteration 4974, train loss: 0.003671, validation loss: 0.00258\n",
      "iteration 4975, train loss: 0.00385, validation loss: 0.002488\n",
      "iteration 4976, train loss: 0.00378, validation loss: 0.002507\n",
      "iteration 4977, train loss: 0.00365, validation loss: 0.002732\n",
      "iteration 4978, train loss: 0.003524, validation loss: 0.002702\n",
      "iteration 4979, train loss: 0.004001, validation loss: 0.002474\n",
      "iteration 4980, train loss: 0.003381, validation loss: 0.002479\n",
      "iteration 4981, train loss: 0.003575, validation loss: 0.00259\n",
      "iteration 4982, train loss: 0.003802, validation loss: 0.002702\n",
      "iteration 4983, train loss: 0.003993, validation loss: 0.002527\n",
      "iteration 4984, train loss: 0.003716, validation loss: 0.002591\n",
      "iteration 4985, train loss: 0.003747, validation loss: 0.002507\n",
      "iteration 4986, train loss: 0.003641, validation loss: 0.002388\n",
      "iteration 4987, train loss: 0.00365, validation loss: 0.002449\n",
      "iteration 4988, train loss: 0.003751, validation loss: 0.002688\n",
      "iteration 4989, train loss: 0.004039, validation loss: 0.002966\n",
      "iteration 4990, train loss: 0.004121, validation loss: 0.003106\n",
      "iteration 4991, train loss: 0.004175, validation loss: 0.002854\n",
      "iteration 4992, train loss: 0.003682, validation loss: 0.0027\n",
      "iteration 4993, train loss: 0.004061, validation loss: 0.002448\n",
      "iteration 4994, train loss: 0.003415, validation loss: 0.002442\n",
      "iteration 4995, train loss: 0.003658, validation loss: 0.002565\n",
      "iteration 4996, train loss: 0.003719, validation loss: 0.002594\n",
      "iteration 4997, train loss: 0.003937, validation loss: 0.002744\n",
      "iteration 4998, train loss: 0.004108, validation loss: 0.002826\n",
      "iteration 4999, train loss: 0.003615, validation loss: 0.002772\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_models):\n",
    "    # creating instances:\n",
    "    rng = torch.Generator()\n",
    "    if not (seed is None):\n",
    "        rng.manual_seed(seed)\n",
    "\n",
    "    rnn_torch = RNN_torch(N=N, dt=dt, tau=tau, input_size=input_size,   \n",
    "                        output_size=output_size,\n",
    "                        activation=activation, constrained=constrained,\n",
    "                        sigma_inp=sigma_inp, sigma_rec=sigma_rec,\n",
    "                        connectivity_density_rec=connectivity_density_rec,\n",
    "                        spectral_rad=spectral_rad,\n",
    "                        random_generator=rng, device=\"cpu\")\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(rnn_torch.parameters(),\n",
    "                                lr=lr,\n",
    "                                weight_decay=weight_decay)\n",
    "    trainer = Trainer(RNN=rnn_torch, Task=task,\n",
    "                    max_iter=max_iter, tol=tol,\n",
    "                    optimizer=optimizer, criterion=criterion,\n",
    "                    lambda_orth=lambda_orth, lambda_r=lambda_r,\n",
    "                    orth_input_only=orth_input_only)\n",
    "    datasaver = None\n",
    "\n",
    "    # inspect start eigenvalue distribution for the RNN\n",
    "    w, v = torch.linalg.eig(rnn_torch.W_rec)\n",
    "    rad = torch.absolute(w).detach().numpy()\n",
    "    plt.scatter(range(len(rad)), rad)\n",
    "    plt.savefig(model_path+f\"/spectral_{i}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # run training\n",
    "    rnn_trained, train_losses, val_losses, net_params = trainer.run_training(train_mask=mask, same_batch=same_batch)\n",
    "\n",
    "    # visualize loss\n",
    "    fig_trainloss = plt.figure(figsize=(10, 3))\n",
    "    plt.plot(train_losses, color='r', label='train loss (log scale)')\n",
    "    plt.plot(val_losses, color='b', label='valid loss (log scale)')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"# epoch\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig(model_path+f\"/loss_{i}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # validate\n",
    "    RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=np.zeros(N),\n",
    "                      y_init=net_params[\"y_init\"])\n",
    "    input_batch_valid, target_batch_valid = task.get_batch()\n",
    "\n",
    "    RNN_valid.clear_history()\n",
    "    RNN_valid.run(input_timeseries=input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "    output = RNN_valid.get_output()\n",
    "\n",
    "    # visualize RNN output\n",
    "    n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "    n_c = task_params[\"n_catches\"]\n",
    "    ch_names = [\"Left\", \"Right\"]\n",
    "\n",
    "    dirs = [\"L\", \"R\"]\n",
    "    for j in range(output_size): # left, right\n",
    "        fig = plt.figure(figsize = (10, 2))\n",
    "        plt.plot(np.average(output[j,:,:n_r], axis=1), c='tab:blue', label=\"right\")\n",
    "        plt.plot(np.average(output[j,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left\")\n",
    "        plt.plot(np.average(output[j,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch\")\n",
    "        plt.title(f\"Output Channle: {ch_names[i]}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(model_path+f\"/out_validation_avg_{i}{dirs[j]}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # generate data for more trials\n",
    "    n_r = 150\n",
    "    n_l = 150\n",
    "    n_c = 10 # can't set it to 0\n",
    "    n_trials = n_r + n_l + n_c\n",
    "    directions = np.zeros(n_trials)\n",
    "\n",
    "    directions[:n_r] = 1 # right: channel 1\n",
    "    directions[-n_c:] = -1 # catch: -1\n",
    "    assert(np.sum(directions) == n_r - n_c)\n",
    "    print(directions)\n",
    "    directions = directions.astype(int).tolist()\n",
    "    # reset the relevant parameters\n",
    "    task_params['directions'] = directions\n",
    "    task_params['n_rights'] = n_r\n",
    "    task_params['n_lefts'] = n_l\n",
    "    task_params['n_catches'] = n_c\n",
    "\n",
    "    # generate data for more trials + save data\n",
    "    RNN_valid.clear_history()\n",
    "    task = eval(\"Task\" + task_name)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "\n",
    "    inputs, targets = task.get_batch()\n",
    "    RNN_valid.run(inputs, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "    neural_traces = RNN_valid.get_history()\n",
    "    traces_data = {}\n",
    "    traces_data[\"inputs\"] = inputs\n",
    "    traces_data[\"targets\"] = targets\n",
    "    traces_data[\"traces\"] = neural_traces\n",
    "    traces_data[\"outputs\"] = RNN_valid.get_output()\n",
    "    traces_data[\"net_params\"] = net_params\n",
    "\n",
    "    output = traces_data[\"outputs\"]\n",
    "    for j in range(output_size): # left, right\n",
    "        fig = plt.figure(figsize = (10, 2))\n",
    "        plt.plot(output[j,:,:n_r], c='tab:blue', alpha=0.1)\n",
    "        plt.plot(output[j,:,n_r:n_r+n_l], c='tab:red', ls=\"-.\", alpha=0.1)\n",
    "        plt.plot(output[j,:,n_r+n_l:n_c+n_r+n_l], c='tab:purple', ls=\"--\", alpha=0.1)\n",
    "        plt.savefig(model_path+f\"/out_generated_{i}{dirs[j]}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    path_to_traces = os.path.join(model_path, \n",
    "                                  f\"trained_RNN_lambda_orth=0_{i}.pkl\")\n",
    "    pickle.dump(traces_data, open(path_to_traces, \"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for more trials\n",
    "n_r = 150\n",
    "n_l = 150\n",
    "n_c = 10 # can't set it to 0\n",
    "n_trials = n_r + n_l + n_c\n",
    "directions = np.zeros(n_trials)\n",
    "\n",
    "directions[:n_r] = 1 # right: channel 1\n",
    "directions[-n_c:] = -1 # catch: -1\n",
    "assert(np.sum(directions) == n_r - n_c)\n",
    "directions = directions.astype(int).tolist()\n",
    "# reset the relevant parameters\n",
    "task_params['directions'] = directions\n",
    "task_params['n_rights'] = n_r\n",
    "task_params['n_lefts'] = n_l\n",
    "task_params['n_catches'] = n_c\n",
    "\n",
    "task = eval(\"Task\" + task_name)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "input_batch_valid, target_batch_valid = task.get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 157, 310)\n"
     ]
    }
   ],
   "source": [
    "# validate\n",
    "RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=np.zeros(N),\n",
    "                      y_init=net_params[\"y_init\"])\n",
    "\n",
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_timeseries=input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "output = RNN_valid.get_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize RNN output\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "print(n_r, n_l, n_c)\n",
    "ch_names = [\"Left\", \"Right\"]\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(output[i,:,:n_r], c='tab:blue', alpha=0.1)\n",
    "    plt.plot(output[i,:,n_r:n_r+n_l], c='tab:red', ls=\"-.\", alpha=0.1)\n",
    "    plt.plot(output[i,:,n_r+n_l:n_c+n_r+n_l], c='tab:purple', ls=\"--\", alpha=0.1)\n",
    "    plt.title(f\"Output Channle: {ch_names[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_trained, train_losses, val_losses, net_params\n",
    "# save RNN data\n",
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "neural_traces = RNN_valid.get_history()\n",
    "traces_data = {}\n",
    "traces_data[\"inputs\"] = input_batch_valid\n",
    "traces_data[\"targets\"] = target_batch_valid\n",
    "traces_data[\"traces\"] = neural_traces\n",
    "traces_data[\"outputs\"] = RNN_valid.get_output()\n",
    "traces_data[\"net_params\"] = net_params\n",
    "\n",
    "path_to_traces = os.path.join(\"../data/trained_RNNs/ALM/\",\"test\",\"trained_RNN_lambda_orth=2_in_out_orth.pkl\")\n",
    "pickle.dump(traces_data, open(path_to_traces, \"wb+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_coach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
