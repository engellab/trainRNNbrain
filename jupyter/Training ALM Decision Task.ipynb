{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.DataSaver import DataSaver\n",
    "from src.DynamicSystemAnalyzer import *\n",
    "from src.PerformanceAnalyzer import *\n",
    "from src.RNN_numpy import RNN_numpy\n",
    "from src.utils import get_project_root, numpify, orthonormalize\n",
    "from src.Trainer import Trainer\n",
    "from src.RNN_torch import RNN_torch\n",
    "from src.Task import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file last updated:  2023-11-15 11:04:11.296195\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"/Users/jiayizhang/Documents/code_base/rnn-coach/\"\n",
    "task_name = \"ALM\"\n",
    "activation = \"relu\"\n",
    "config_dict = json.load(open(os.path.join(dict_path, \"data\", \"configs\", f'train_config_{task_name}_{activation}_lambda_orth=1_in_out_orth.json'), mode=\"r\"))\n",
    "print(\"config file last updated: \", config_dict[\"last_compiled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "n_steps = config_dict[\"n_steps\"]\n",
    "task_params = config_dict[\"task_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer:\n",
    "lambda_orth = config_dict[\"lambda_orth\"]\n",
    "orth_input_only = config_dict[\"orth_input_only\"]\n",
    "lambda_r = config_dict[\"lambda_r\"]\n",
    "max_iter = config_dict[\"max_iter\"]\n",
    "tol = config_dict[\"tol\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "same_batch = config_dict[\"same_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8000 False\n"
     ]
    }
   ],
   "source": [
    "print(lambda_orth, max_iter, orth_input_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN config:\n",
    "N = config_dict[\"N\"]\n",
    "dt = config_dict[\"dt\"]\n",
    "tau = config_dict[\"tau\"]\n",
    "mask = np.array(config_dict[\"mask\"])\n",
    "input_size = config_dict[\"num_inputs\"]\n",
    "output_size = config_dict[\"num_outputs\"]\n",
    "if (activation == \"relu\"):\n",
    "    activation = lambda x: torch.maximum(x, torch.tensor(0))\n",
    "constrained = config_dict[\"constrained\"]\n",
    "# constrained = False\n",
    "sigma_inp = config_dict[\"sigma_inp\"]\n",
    "sigma_rec = config_dict[\"sigma_rec\"]\n",
    "connectivity_density_rec = config_dict[\"connectivity_density_rec\"]\n",
    "spectral_rad = config_dict[\"sr\"]\n",
    "seed = config_dict[\"seed\"]\n",
    "\n",
    "rng = torch.Generator()\n",
    "if not (seed is None):\n",
    "    rng.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for RNN!\n"
     ]
    }
   ],
   "source": [
    "# creating instances:\n",
    "rnn_torch = RNN_torch(N=N, dt=dt, tau=tau, input_size=input_size,   \n",
    "                      output_size=output_size,\n",
    "                      activation=activation, constrained=constrained,\n",
    "                      sigma_inp=sigma_inp, sigma_rec=sigma_rec,\n",
    "                      connectivity_density_rec=connectivity_density_rec,\n",
    "                      spectral_rad=spectral_rad,\n",
    "                      random_generator=rng, device=\"cpu\")\n",
    "task = eval(\"Task\" + task_name)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_torch.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "trainer = Trainer(RNN=rnn_torch, Task=task,\n",
    "                  max_iter=max_iter, tol=tol,\n",
    "                  optimizer=optimizer, criterion=criterion,\n",
    "                  lambda_orth=lambda_orth, lambda_r=lambda_r,\n",
    "                  orth_input_only=orth_input_only)\n",
    "datasaver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28f580370>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect start eigenvalue distribution for the RNN\n",
    "w, v = torch.linalg.eig(rnn_torch.W_rec)\n",
    "rad = torch.absolute(w).detach().numpy()\n",
    "plt.scatter(range(len(rad)), rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAADaCAYAAABkQZn5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABIMUlEQVR4nO3deVxU5f4H8M/MMAs7KsqiCLjvguCGe+6WW5pkN5fqanb1KlJumaZ2jbS6qZma/Uw0S63cyw274UrmhmmuKYriEOHCgMis5/fH6MmRGWAQGfV83q8XLz3nfM8z3/PMMMx3nnOeIxMEQQAREREREZHEyF2dABERERERkSuwGCIiIiIiIkliMURERERERJLEYoiIiIiIiCSJxRAREREREUkSiyEiIiIiIpIkFkNERERERCRJLIaIiIiIiEiSWAwREREREZEksRgiInpCJCYmQiaT4fDhw65ORfT+++9j48aNTu2j0+kwe/ZsREdHw8fHB2q1GmFhYXj11Vdx9OhRMW7GjBmQyWTIzs4u46zL1vDhwxEWFlZuj/fDDz+gb9++CA4Ohkqlgre3NyIjI/Huu+8iPT293PIgInoasBgiIqJSc7YYunDhAiIjI/HBBx+gU6dOWL16NXbu3ImZM2fizz//RFRUFHJych5dwk8wi8WCYcOGoXfv3jAajUhISEBSUhK+++47PP/88/jqq6/Qpk0bV6dJRPREcXN1AkREJA1msxn9+/dHdnY2UlJS0KhRI3Fbhw4dMGzYMGzbtg1KpdKFWT6+5syZg5UrVyIhIQGTJ0+22dajRw9MmTIFn3/+uYuyIyJ6MnFkiIjoCTZ8+HB4eXnhjz/+QK9eveDl5YWQkBC8+eab0Ov1YtylS5cgk8kwd+5czJ49G9WrV4dGo0F0dDR++umnQm3aO+3r3mlr98hkMty+fRsrVqyATCaDTCZDx44dHea6ceNGnDhxAlOmTLEphO7Xs2dPeHh42Kz7888/MXjwYPj6+iIgIACvvvpqodGjzz77DO3bt0eVKlXg6emJxo0bY+7cuTAajTZxHTt2RKNGjXDo0CG0a9cOHh4eqFGjBj744ANYLBYxLjk5GTKZDKtXr8bUqVMRHBwMHx8fdOnSBWfPnnV4jPcIgoBFixYhIiIC7u7uqFChAgYOHIiLFy8Wu689BoMBc+fORaNGjQoVQve4ublh9OjRNussFgvmzp2LevXqQa1Wo0qVKhg6dCiuXr1aqjyIiJ42LIaIiJ5wRqMRffr0QefOnbFp0ya8+uqr+OSTTzBnzpxCsQsXLsT27dsxb948rFq1CnK5HD179kRKSorTj5uSkgJ3d3f06tULKSkpSElJwaJFixzG79y5EwDQr18/px5nwIABqFOnDtatW4fJkyfjm2++wfjx421iLly4gJdeeglfffUVfvjhB7z22mv48MMP8frrrxdqLzMzE//4xz/w8ssvY/PmzejZsyemTJmCVatWFYp9++23cfnyZfzf//0fli5divPnz6N3794wm81F5vz6668jLi4OXbp0wcaNG7Fo0SL8/vvviImJwZ9//inG3Su6ZsyYUWR7hw8fxq1bt9C7d+8i4x70xhtvYNKkSejatSs2b96M9957D9u3b0dMTMxjfy0WEVG5EIiI6ImwfPlyAYBw6NAhcd2wYcMEAMK3335rE9urVy+hbt264nJaWpoAQAgODhbu3LkjrtfpdELFihWFLl262LQZGhpa6PHfffdd4cE/G56ensKwYcNKlH+PHj0EAEJBQUGJ4u893ty5c23W/+tf/xI0Go1gsVjs7mc2mwWj0SisXLlSUCgUwo0bN8RtHTp0EAAIBw8etNmnQYMGQvfu3cXln3/+WQAg9OrVyybu22+/FQAIKSkp4roH+yslJUUAIHz88cc2+165ckVwd3cXJk6cKK5LTk4WFAqFMHPmzCL7Ys2aNQIAYcmSJYW2GY1Gm597Tp8+LQAQ/vWvf9nEHzx4UAAgvP3220U+JhGRFHBkiIjoCSeTyQqNGDRp0gSXL18uFPv8889Do9GIy97e3ujduzf27NlT7GiHq/Tp08dmuUmTJigoKEBWVpa47tixY+jTpw8qVaoEhUIBpVKJoUOHwmw249y5czb7BwYGokWLFoXatNdf9h4bgN3Ye3744QfIZDK8/PLLMJlM4k9gYCCaNm2K5ORkMbZDhw4wmUyYPn160Z3gwK1bt6BUKm1+7s02+PPPPwOwnvZ4vxYtWqB+/fqFTo8kIpIiFkNERE84Dw8PmwIHANRqNQoKCgrFBgYG2l1nMBiQl5f3yHIEgOrVqwMA0tLSnNqvUqVKNstqtRoAcOfOHQBAeno62rVrh4yMDMyfPx979+7FoUOH8Nlnn9nEOWrvXpsPxpXkse35888/IQgCAgICChUqv/zyS6lOT7vXdw8WYd7e3jh06BAOHTqEd99912bb9evXAQBBQUGF2gsODha3ExFJGWeTIyKSkMzMTLvrVCoVvLy8AAAajcZm8oV7HvYak+7du2Pp0qXYuHGjw0kASmPjxo24ffs21q9fj9DQUHF9ampqmT2GM/z9/SGTybB3716xeLqfvXXFiYqKQoUKFbBlyxa8//774nqFQoHo6GgAwMmTJ232uVfIabVaVKtWzWbbtWvX4O/v73QeRERPG44MERFJyPr1621GjHJzc7Flyxa0a9cOCoUCABAWFoasrCybC/0NBgN27NhRqD1HIyr29O3bF40bN0ZCQkKhD+737NixA/n5+c4ckjjD3f1FhiAI+OKLL5xqp6w899xzEAQBGRkZiI6OLvTTuHFjp9tUqVSYMGECTp48aXdiDHueeeYZACg0McShQ4dw+vRpdO7c2ek8iIieNhwZIiKSEIVCga5duyI+Ph4WiwVz5syBTqfDzJkzxZjY2FhMnz4dL774IiZMmICCggIsWLDA7jVFjRs3RnJyMrZs2YKgoCB4e3ujbt26Dh97w4YN6NatG1q3bo033ngDnTp1gqenJy5fvozvv/8eW7Zswc2bN506pq5du0KlUmHw4MGYOHEiCgoKsHjxYqfbKStt2rTByJEj8corr+Dw4cNo3749PD09odVqsW/fPjRu3BhvvPEGAGD37t3o3Lkzpk+fXux1Q5MmTcKZM2cwefJk7NmzB7GxsQgLC4Ner8fFixfxf//3f1AoFOLU5HXr1sXIkSPx6aefirMGXrp0CdOmTUNISEihGfmIiKSIxRARkYSMGTMGBQUFGDt2LLKystCwYUP8+OOPaNOmjRgTHh6OTZs24e2338bAgQMRFBSE+Ph4/PXXXzZFEwDMnz8fo0ePxosvvoj8/Hx06NDBZoKAB9WsWRNHjx7Fp59+ig0bNmDx4sXQ6/UICgpC+/btsW/fPvj6+jp1TPXq1cO6devwzjvv4Pnnn0elSpXw0ksvIT4+Hj179nSqrbLy+eefo1WrVvj888+xaNEiWCwWBAcHo02bNjaTNwiCALPZbHOPI0fkcjlWrFiBgQMH4osvvsDEiRNx/fp1uLu7o2bNmujcuTNWrVplU4wuXrwYNWvWxLJly/DZZ5/B19cXPXr0QEJCgt1rp4iIpEYmCILg6iSIiOjRunTpEsLDw/Hhhx/irbfecnU6REREjwVeM0RERERERJLEYoiIiIiIiCSJp8kREREREZEkcWSIiIiIiIgkicUQERERERFJEoshIiIiIiKSpKfmPkMWiwXXrl2Dt7e3eDdyIiIiIiKSHkEQkJubi+DgYMjljsd/nppi6Nq1awgJCXF1GkRERERE9Ji4cuUKqlWr5nD7U1MMeXt7A7AesI+Pj4uzISIiIiIiV9HpdAgJCRFrBEeemmLo3qlxPj4+LIaIiIiIiKjYy2ecnkBhz5496N27N4KDgyGTybBx48Zi99m9ezeioqKg0WhQo0YNLFmypFDMunXr0KBBA6jVajRo0AAbNmxwNjUiIiIiIqISc7oYun37Npo2bYqFCxeWKD4tLQ29evVCu3btcOzYMbz99tsYO3Ys1q1bJ8akpKQgNjYWQ4YMwfHjxzFkyBAMGjQIBw8edDY9IiIiIiKiEpEJgiCUemeZDBs2bEC/fv0cxkyaNAmbN2/G6dOnxXWjRo3C8ePHkZKSAgCIjY2FTqfDtm3bxJgePXqgQoUKWL16dYly0el08PX1RU5ODk+TIyIiIiKSsJLWBo/8mqGUlBR069bNZl337t2xbNkyGI1GKJVKpKSkYPz48YVi5s2b57BdvV4PvV4vLut0umJzsVgsMBgMzh0APbGUSiUUCoWr0yAioidEvsGESetOQHvrDgCg3eGtqHvxOA437oDU+jEAgEo3M9E/abnTbX/b83XovCsCAKJPJCPidApO1mmOXyK6AAA88nPx0g8lO+vmflueGYI//a0zZTU69ytapf6EC9Ub4OdWfQEAcrMJr6770Ol2k9oMwOWqdQAANS+fRKeDW5AREI5tHV4UY4Zu+C9URr2jJuzaF90TZ2pEAACqZl5Ezz1rcd0vABu6vSrGDNq2BD65N51q195zlO/uhW96/1uM6f3TSgRcz3CqXXvPkUWuwJcDJ4oxXfd9j9Br551q19FztLLfeBhUGgB/v/6c4eg5svf6c4aj5+jB15/KqEfIyy9iSKtQp9p3pUdeDGVmZiIgIMBmXUBAAEwmE7KzsxEUFOQwJjMz02G7CQkJmDlzZonzMBgMSEtLg8Vice4A6Inm5+eHwMBA3nuKiIiKlXImE0lH0mCQu8EiVyDm6lWEav/Az361cdjD+uG85q1shGr/cLrtM+nZuOZl/VvU8GoGQrV/4KhHEA5XsLZb6U5Oqdq9mP4XTt/2BABUu6JFqPYPXFR44/Bla7tuFhNmlqLdjCt/4rCpMgDA+8qfCNX+gesmudguAEy+dgGepgKn2t14RYvDCusHZUtWFkK1f8CUn2/T7qiMNFS9ne1Uu/aeo2yNj027g69dQuiNy061a+85MsoUNu32ykhHaKZzfezoOUpNv4F8pTsAiK8/Zzh6juy9/pzh6Dl68PXnYSrA1Zv5TrXtao/8NLk6derglVdewZQpU8R1+/fvR9u2baHVahEYGAiVSoUVK1Zg8ODBYszXX3+N1157DQUF9n/J7I0MhYSE2B0KEwQB6enpMBqNxd54iZ4OgiAgPz8fWVlZ8PPzQ1BQkKtTIiKix9zPi79G4Pz/4FxwXagXLIbq4jkosjJhDAmDqWp1AIDsdh40J4463XZB0+YQ3K0fcpVX0uCWcQWmwKowhtW0tqvXQ3PM+Wul9Q2bwuLtCwBw016F8vJFmP2rwFCrnjXAYoH7r/ucbtdQpwHMFf0BAIq//oTqwllYfCtAX7+xGON+aD9gNjvVrjG8NkwB1r/J8ls3oD5zEoKHFwqaNBNjNKmHICu441y7dp4jQaVCQbNWYoz6ZCrkecWfSXQ/u8+RTIY7LduJMaqzv0Nx87pT7Tp6ju5ExwBu1rGKe68/Zzh6juy9/pzh6Dl68PUHsxkhEQ1RN7Do6azLw2NzmlxgYGChEZ6srCy4ubmhUqVKRcY8OFp0P7VaDbVaXaIcTCYT8vPzERwcDA8PDyePgJ5U7nd/6bOyslClShWeMkdEREUy37F+AavQqNGjURDQyMEXaS1rP9wDOWo3KqwM2m1eeH2T2IdrF0FApwg7jzfw4dtt29BOu30esl3Yf44c9bsz7D1HZdGuveeoLNq19xyVSbt2nqOyaNcFHvkQSevWrZGUlGSzbufOnYiOjoZSqSwyJiYmpkxyMN/91kKlUpVJe/TkuFf8Go1GF2dCRESPO1OB9YwTixs/LxBJhdMjQ3l5efjjj7/PM0xLS0NqaioqVqyI6tWrY8qUKcjIyMDKlSsBWGeOW7hwIeLj4zFixAikpKRg2bJlNrPEjRs3Du3bt8ecOXPQt29fbNq0Cbt27cK+fc4P6xaF141ID59zIiIqKcu90+/55SmRZDg9MnT48GFERkYiMjISABAfH4/IyEhMnz4dAKDVapGeni7Gh4eHY+vWrUhOTkZERATee+89LFiwAAMGDBBjYmJisGbNGixfvhxNmjRBYmIi1q5di5YtWz7s8RERERGViEVvPU1OYDFEJBlOjwx17NgRRc25kJiYWGhdhw4dcPRo0RcbDhw4EAMHPuy5p9JQkokr7pecnIxOnTrh5s2b8PPze6S53S8xMRFxcXG4detWifcJCwtDXFwc4uLiHlleRERE9ggFHBkikhpOq/YE0mq16NmzZ5m2OWPGDERERBQbN3z48BIXYbGxsTh37tzDJUZERFROLIZ7xVDJJmgioiffI59NjsqOwWCASqVCYGCgq1MpltFohLu7uzijGxER0WNPb70xu0zNkSEiqeDI0GOsY8eOGDNmDOLj4+Hv74+uXbsCsJ4mt3HjRjHuwIEDiIiIgEajQXR0NDZu3AiZTIbU1FSb9o4cOYLo6Gh4eHggJiYGZ8+eBWA9nW3mzJk4fvw4ZDIZZDKZ3dMdZ8yYgRUrVmDTpk1iXHJyMi5dugSZTIZvv/0WHTt2hEajwapVq5CYmGhzWt6FCxfQt29fBAQEwMvLC82bN8euXbuK7IMZM2agevXqUKvVCA4OxtixY0vVl0RERMUy3CuGNC5OhIjKiyRHhgRBwB2jczcJKyvuSoVTM5ytWLECb7zxBvbv32/3Wq3c3Fz07t0bvXr1wjfffIPLly87vN5m6tSp+Pjjj1G5cmWMGjUKr776Kvbv34/Y2FicPHkS27dvF4sTX1/fQvu/9dZbOH36NHQ6HZYvXw4AqFixIq5duwYAmDRpEj7++GMsX74carUaO3futNk/Ly8PvXr1wn/+8x9oNBqsWLECvXv3xtmzZ1G9evVCj/f999/jk08+wZo1a9CwYUNkZmbi+PHjJe47IiIiZ8juniYnL+F9DInoySfJYuiO0YwG03e45LFPzeoOD1XJu71WrVqYO3euw+1ff/01ZDIZvvjiC2g0GjRo0AAZGRkYMWJEodjZs2ejQ4cOAIDJkyfj2WefRUFBAdzd3eHl5QU3N7ciT8Hz8vKCu7s79Hq93bi4uDg8//zzDvdv2rQpmjZtKi7/5z//wYYNG7B582aMGTOmUHx6ejoCAwPRpUsXKJVKVK9eHS1atHDYPhER0UO5OzLEYohIOnia3GMuOjq6yO1nz55FkyZNoNH8PaTvqGBo0qSJ+P+gIOtdgrOyssogS6vicr19+zYmTpyIBg0awM/PD15eXjhz5ozNVOz3e+GFF3Dnzh3UqFEDI0aMwIYNG2AymcosXyIiovvJjdZiSKFhMUQkFZIcGXJXKnBqVneXPbYzPD09i9wuCEKh0+4cTX2uVCrF/9/bx2KxOJVPUYrLdcKECdixYwc++ugj1KpVC+7u7hg4cCAMd7+Je1BISAjOnj2LpKQk7Nq1C//617/w4YcfYvfu3TbHQkREVBZk94ohd14zRCQVkiyGZDKZU6eqPc7q1auHr7/+Gnq9Huq7w/qHDx92uh2VSgWzufjrqEoaZ8/evXsxfPhw9O/fH4D1GqJLly4VuY+7uzv69OmDPn36YPTo0ahXrx5OnDiBZs2alSoHIiIiR3bGDMC1oJb4Z+NIV6dCROWEp8k94V566SVYLBaMHDkSp0+fFkdeADg1UUNYWBjS0tKQmpqK7Oxs6PV6h3G//fYbzp49i+zsbBiNxhI/Rq1atbB+/Xqkpqbi+PHjYu6OJCYmYtmyZTh58iQuXryIr776Cu7u7ggNDS3xYxIREZVUWqUQHApsALeAAFenQkTlhMXQE87HxwdbtmxBamoqIiIiMHXqVEyfPh0AbK4jKs6AAQPQo0cPdOrUCZUrV8bq1avtxo0YMQJ169ZFdHQ0KleujP3795f4MT755BNUqFABMTEx6N27N7p3717kCI+fnx+++OILtGnTBk2aNMFPP/2ELVu2oFKlSiV+TCIiopLSG61f0GmcPKWdiJ5cMsHRBSZPGJ1OB19fX+Tk5MDHx8dmW0FBAdLS0hAeHu5UgfCk+vrrr/HKK68gJydH8jc9ldpzT0REpffWqLm4lZOPUZOHIrppDVenQ0QPoaja4H5Px4UzErdy5UrUqFEDVatWxfHjxzFp0iQMGjRI8oUQERGRM/r/ugEV82/BdL0nABZDRFLAYugpkJmZienTpyMzMxNBQUF44YUXMHv2bFenRURE9EQ5EVAHHvk6tKro5+pUiKicsBh6CkycOBETJ050dRpERERPtAXNByPfYMaekBBXp0JE5YQTKBAREREB0JusEyiolfx4RCQV/G0nIiIiyTOZLTBbrHNKqd348YhIKniaHBEREUlefk4uftg0EQa5G1RTOwJQuTolIioH/OqDiIiIJE9/Ox8KwQJ3swFqd96KgUgqWAwRERGR5OnzCwAARrkCCjfedJVIKlgMERERkeTpb98BABgUShdnQkTlicXQY6xjx46Ii4tzap+NGzeiVq1aUCgUTu9bUjNmzEBERIRT+8hkMmzcuPGR5ENERPSwDPnWYsjIYohIUlgMPWVef/11DBw4EFeuXMF7772H4cOHo1+/fsXu50zh9dZbb+Gnn356uESJiIgeI4Y71tPkTCyGiCSFs8k9RfLy8pCVlYXu3bsjODi4zNsXBAFmsxleXl7w8vIq8/aJiIhcxZhfABUAkxuLISIp4cjQE8RgMGDixImoWrUqPD090bJlSyQnJwMAkpOT4e3tDQB45plnIJPJ0LFjR6xYsQKbNm2CTCaDTCYT4+83fPhw7N69G/PnzxfjLl26hOTkZMhkMuzYsQPR0dFQq9XYu3dvodPkDh06hK5du8Lf3x++vr7o0KEDjh49WuRxjBkzBkFBQdBoNAgLC0NCQkJZdhUREZFTjPdGhpScUptISkpVDC1atAjh4eHQaDSIiorC3r17HcYOHz5c/IB9/0/Dhg3FmMTERLsxBQUFpUmvxCz5+U7/CCaTuL9gMlnXP5Cno30f1iuvvIL9+/djzZo1+O233/DCCy+gR48eOH/+PGJiYnD27FkAwLp166DVarF582YMGjQIPXr0gFarhVarRUxMTKF258+fj9atW2PEiBFiXEhIiLh94sSJSEhIwOnTp9GkSZNC++fm5mLYsGHYu3cvfvnlF9SuXRu9evVCbm6u3eNYsGABNm/ejG+//RZnz57FqlWrEBYW9tD9Q0REVFrGO9ZrhiwcGSKSFKdPk1u7di3i4uKwaNEitGnTBp9//jl69uyJU6dOoXr16oXi58+fjw8++EBcNplMaNq0KV544QWbOB8fH/HD/D0azaOd5/9ssyin96k67xP49OgBAMjdtQsZcePh0bw5Qr9aKcb80bkLzDdvFtq3/pnTpc71woULWL16Na5evSqeAvfWW29h+/btWL58Od5//31UqVIFAFCxYkUEBgYCANzd3aHX68Vle3x9faFSqeDh4WE3btasWejatavD/Z955hmb5c8//xwVKlTA7t278dxzzxWKT09PR+3atdG2bVvIZDKEhoYW3wFERESPkLlAb/3XjSNDRFLi9MjQf//7X7z22mv45z//ifr162PevHkICQnB4sWL7cb7+voiMDBQ/Dl8+DBu3ryJV155xSZOJpPZxBX14V2Kjh49CkEQUKdOHfGaHS8vL+zevRsXLlx4pI8dHR1d5PasrCyMGjUKderUga+vL3x9fZGXl4f09HS78cOHD0dqairq1q2LsWPHYufOnY8ibSIiohIz3S2GLEqODBFJiVMjQwaDAUeOHMHkyZNt1nfr1g0HDhwoURvLli1Dly5dCo0G5OXlITQ0FGazGREREXjvvfcQGRnpsB29Xg+9Xi8u63Q6J47Equ7RI07vI1P9/Y2Rd5cu1jbktjVlrZ92Od1ucSwWCxQKBY4cOQKFwvZmcI96MgNPT88itw8fPhx//fUX5s2bh9DQUKjVarRu3RoGg8FufLNmzZCWloZt27Zh165dGDRoELp06YLvv//+UaRPRERULPPda4YsSrWLMyGi8uRUMZSdnQ2z2YyAgACb9QEBAcjMzCx2f61Wi23btuGbb76xWV+vXj0kJiaicePG0Ol0mD9/Ptq0aYPjx4+jdu3adttKSEjAzJkznUm/ELmHx0PtL3Nzg8ytcBc+bLv2REZGwmw2IysrC+3atSvxfiqVCmazuczi7Nm7dy8WLVqEXr16AQCuXLmC7OzsIvfx8fFBbGwsYmNjMXDgQPTo0QM3btxAxYoVS5UDERHRw7Dc+4JVxdPkiKSkVFNry2Qym2VBEAqtsycxMRF+fn6F7nvTqlUrtGrVSlxu06YNmjVrhk8//RQLFiyw29aUKVMQHx8vLut0OpuL/p82derUwT/+8Q8MHToUH3/8MSIjI5GdnY3//e9/aNy4sViIPCgsLAw7duzA2bNnUalSJfj6+kJp5xSAsLAwHDx4EJcuXYKXl5dTRUmtWrXw1VdfITo6GjqdDhMmTIC7u7vD+E8++QRBQUGIiIiAXC7Hd999h8DAQPj5+ZX4MYmIiMpSVv0IfBf1EhpG2P8SloieTk5dM+Tv7w+FQlFoFCgrK6vQaNGDBEHAl19+iSFDhkBVzLcucrkczZs3x/nz5x3GqNVq+Pj42Pw87ZYvX46hQ4fizTffRN26ddGnTx8cPHiwyCJwxIgRqFu3LqKjo1G5cmXs37/fbtxbb70FhUKBBg0aoHLlyg6v97Hnyy+/xM2bNxEZGYkhQ4Zg7Nix4mQO9nh5eWHOnDmIjo5G8+bNcenSJWzduhVyOWd6JyIi17hZqSp+DmmGW7UbFh9MRE8NmSAIgjM7tGzZElFRUVi0aJG4rkGDBujbt2+R94pJTk5Gp06dcOLECTRq1KjIxxAEAS1atEDjxo3x5ZdfligvnU4HX19f5OTkFCqMCgoKkJaWJk4HTtLB556IiEoiYdtpfL77Il5rG45pzzVwdTpE9JCKqg3u5/RpcvHx8RgyZAiio6PRunVrLF26FOnp6Rg1ahQA6+lrGRkZWLlypc1+y5YtQ8uWLe0WQjNnzkSrVq1Qu3Zt6HQ6LFiwAKmpqfjss8+cTY+IiIjIae6X/kAr7SlUuukOgMUQkVQ4XQzFxsbi+vXrmDVrFrRaLRo1aoStW7eKs8NptdpCp1jl5ORg3bp1mD9/vt02b926hZEjRyIzMxO+vr6IjIzEnj170KJFi1IcEhEREZFzwg/sRPdDu/BHZQMw5JnidyCip4LTp8k9rniaHNnD556IiEpixZgZ8D7yC2R9n0f/ySNdnQ4RPaRHdpocERER0dPm19Z98KNXc8xox1PkiKSE03cRERGR5OlN1nvtaZSKYiKJ6GnCYoiIiIgkT2+yAADUSn40IpIS/sYTERGR5A34/hN8tf09+J044upUiKgcsRgiIiIiyfO4nQP/ghyo5E/FvFJEVEIshoiIiEjyFCYDAMCNM48SSQqLIbKRmJgIPz+/R/oYMpkMGzduLHH8jBkzEBER8cjyISIicjMZAQBKdxZDRFLCYugpVl5FRHJyMmQyGW7dulWieK1Wi549ez7apIiIiJzwdzHk7uJMiKg88T5DVG4MBgNUKhUCAwNdnQoREZENN7O1GFJ7cmSISEo4MvQYs1gsmDNnDmrVqgW1Wo3q1atj9uzZ4vZJkyahTp068PDwQI0aNTBt2jQYjdY388TERMycORPHjx+HTCaDTCZDYmIiAODWrVsYOXIkAgICoNFo0KhRI/zwww82j71jxw7Ur18fXl5e6NGjB7Rard0cL126hE6dOgEAKlSoAJlMhuHDhwMAOnbsiDFjxiA+Ph7+/v7o2rUrgMKnyRV1HPYkJyejRYsW8PT0hJ+fH9q0aYPLly871bdERET3U94thlQcGSKSFEmPDBn1ZofbZHLA7b4brxUZKwPcVMXHKtXO3chtypQp+OKLL/DJJ5+gbdu20Gq1OHPmjLjd29sbiYmJCA4OxokTJzBixAh4e3tj4sSJiI2NxcmTJ7F9+3bs2rULAODr6wuLxYKePXsiNzcXq1atQs2aNXHq1CkoFH/nlp+fj48++ghfffUV5HI5Xn75Zbz11lv4+uuvC+UYEhKCdevWYcCAATh79ix8fHzgft8fkhUrVuCNN97A/v37IQj2Z+gp6jgeZDKZ0K9fP4wYMQKrV6+GwWDAr7/+CplM5lTfEhER3U9lNgHgyBCR1Ei6GFo6brfDbaGNKuG5MU3F5S8n7IXJYLEbG1zbD/3fbCYur5x6AAV5hUc2Ri95psS55ebmYv78+Vi4cCGGDRsGAKhZsybatm0rxrzzzjvi/8PCwvDmm29i7dq1mDhxItzd3eHl5QU3Nzeb09J27tyJX3/9FadPn0adOnUAADVq1LB5bKPRiCVLlqBmzZoAgDFjxmDWrFl281QoFKhYsSIAoEqVKoUmX6hVqxbmzp1b5LEWdRwP0ul0yMnJwXPPPSfmV79+/SLbJyIiKopRb4BCuHvTVQ+ODBFJiaSLocfZ6dOnodfr0blzZ4cx33//PebNm4c//vgDeXl5MJlM8PHxKbLd1NRUVKtWTSyE7PHw8BALDQAICgpCVlaW8wcBIDo6utgYZ46jYsWKGD58OLp3746uXbuiS5cuGDRoEIKCgkqVHxER0Z38AvH/Gi8PF2ZCROVN0sXQyPkdHG6TPXA11asftnMc+8AZWkNnxzxMWgBgc6qZPb/88gtefPFFzJw5E927d4evry/WrFmDjz/++KHaBQClUmmzLJPJHJ7iVhxPT88it5fmOJYvX46xY8di+/btWLt2Ld555x0kJSWhVatWpcqRiIikTZ93W/y/mlNrE0mKpIshZ67heVSxjtSuXRvu7u746aef8M9//rPQ9v379yM0NBRTp04V1z04iYBKpYLZbHv9UpMmTXD16lWcO3euyNEhZ6hUKgAo9FglUZLjsCcyMhKRkZGYMmUKWrdujW+++YbFEBERlYr+7siQUa6Awu3h/4YT0ZODs8k9pjQaDSZNmoSJEydi5cqVuHDhAn755RcsW7YMgPVanPT0dKxZswYXLlzAggULsGHDBps2wsLCkJaWhtTUVGRnZ0Ov16NDhw5o3749BgwYgKSkJKSlpWHbtm3Yvn17qXMNDQ2FTCbDDz/8gL/++gt5eXkl3rckx3G/tLQ0TJkyBSkpKbh8+TJ27tyJc+fO8bohIiIqtYLbdwAARoWymEgietqwGHqMTZs2DW+++SamT5+O+vXrIzY2Vrx2p2/fvhg/fjzGjBmDiIgIHDhwANOmTbPZf8CAAejRowc6deqEypUrY/Xq1QCAdevWoXnz5hg8eDAaNGiAiRMnlmpU556qVati5syZmDx5MgICAjBmzJgS71uS47ifh4cHzpw5gwEDBqBOnToYOXIkxowZg9dff73U+RMRkbQZPb3xWZP++Dait6tTIaJyJhNKezHIY0an08HX1xc5OTmFLr4vKChAWloawsPDodHwXGAp4XNPRETFSb1yC/0+24+qfu7YP7nkM78S0eOrqNrgfhwZIiIiIknTG61nR6iV/FhEJDWSnkCBiIiISH/jBhpnX0BlVWVXp0JE5YzFEBEREUnbieOYu28x0gNrABjs6myIqBxxPJiIiIgkzShXIt2rCnJ8/V2dChGVM0mNDD0lc0WQE/icExFRcW42isJbXSaiQ53KGOTqZIioXEliZEihsN5AzWAwuDgTKm/5+fkAAKWS944gIiL7Cu5NoOAmiY9FRHSfUo0MLVq0CB9++CG0Wi0aNmyIefPmoV27dnZjk5OT0alTp0LrT58+jXr16onL69atw7Rp03DhwgXUrFkTs2fPRv/+/UuTXiFubm7w8PDAX3/9BaVSCbmcb3ZPO0EQkJ+fj6ysLPj5+YkFMRER0YP0JgsAQKPk3woiqXG6GFq7di3i4uKwaNEitGnTBp9//jl69uyJU6dOoXr16g73O3v2rM0c35Ur/z1jS0pKCmJjY/Hee++hf//+2LBhAwYNGoR9+/ahZcuWzqZYiEwmQ1BQENLS0nD58uWHbo+eHH5+fggMDHR1GkRE9Bir+POPWPzT97iR0w4YHOnqdIioHDl909WWLVuiWbNmWLx4sbiufv366NevHxISEgrF3xsZunnzJvz8/Oy2GRsbC51Oh23btonrevTogQoVKmD16tUlyqskN1ayWCw8VU5ClEolR4SIiKhYW8bPRK1ta3CuVTf0TZzv6nSIqAyU9KarTo0MGQwGHDlyBJMnT7ZZ361bNxw4cKDIfSMjI1FQUIAGDRrgnXfesTl1LiUlBePHj7eJ7969O+bNm+ewPb1eD71eLy7rdLpi85fL5dBoNMXGERERkXRY7n2eUKlcmwgRlTunLp7Jzs6G2WxGQECAzfqAgABkZmba3ScoKAhLly7FunXrsH79etStWxedO3fGnj17xJjMzEyn2gSAhIQE+Pr6ij8hISHOHAoRERGR1d1iSKZWuzgRIipvpZpAQSaT2SwLglBo3T1169ZF3bp1xeXWrVvjypUr+Oijj9C+fftStQkAU6ZMQXx8vLis0+lYEBEREZHz7p5Cz2KISHqcGhny9/eHQqEoNGKTlZVVaGSnKK1atcL58+fF5cDAQKfbVKvV8PHxsfkhIiIicprROjIkV7EYIpIap4ohlUqFqKgoJCUl2axPSkpCTExMids5duwYgoKCxOXWrVsXanPnzp1OtUlERERUGjK9dWRIoWExRCQ1Tp8mFx8fjyFDhiA6OhqtW7fG0qVLkZ6ejlGjRgGwnr6WkZGBlStXAgDmzZuHsLAwNGzYEAaDAatWrcK6deuwbt06sc1x48ahffv2mDNnDvr27YtNmzZh165d2LdvXxkdJhEREZF9cqO1GJJzkiUiyXG6GIqNjcX169cxa9YsaLVaNGrUCFu3bkVoaCgAQKvVIj09XYw3GAx46623kJGRAXd3dzRs2BA//vgjevXqJcbExMRgzZo1eOeddzBt2jTUrFkTa9euLZN7DBEREREVRWbkyBCRVDl9n6HHVUnnEiciIiK63/Zn+iD02nn8Gf8uOo580dXpEFEZKGlt4NQ1Q0RERERPG7nJOjKkdOfIEJHUsBgiIiIiSVOYjAAAN3d3F2dCROWNxRARERFJmpvRWgwp3TmBApHUlOqmq0RERERPi531OkCRcxMvBAW7OhUiKmcshoiIiEjSttZuh5v5RgypGlR8MBE9VXiaHBEREUma3mQBAKjdFC7OhIjKG0eGiIiISNKCs6/ijkwBlfypuNsIETmBxRARERFJlkFvwKf/+xgAoJrWG4CnaxMionLF0+SIiIhIsgryC3BD7Y1cpTs0HpxNjkhqODJEREREkmVUqvGPnu8CAC56cVSISGo4MkRERESSdW/yBJVCDrlc5uJsiKi8sRgiIiIiySowmgEAajd+JCKSIp4mR0RERJKlv3QJH+1ZiDyvCgC6uzodIipnLIaIiIhIsgy3ctDwxiVk63WuToWIXIBjwkRERCRZxvwCAIBJqXJxJkTkCiyGiIiISLKMd+4WQ25KF2dCRK7AYoiIiIgky3jnDgDAwmKISJJYDBEREZFkmQv01n/deJockRSxGCIiIiLJMt0thiwqFkNEUsRiiIiIiCTLfPeaIYETKBBJEoshIiIikiyL3joyxGKISJpYDBEREZFkWfR3R4Z4mhyRJLEYIiIiIskS7l4zBBZDRJLEYoiIiIgky2K4VwypXZsIEblEqYqhRYsWITw8HBqNBlFRUdi7d6/D2PXr16Nr166oXLkyfHx80Lp1a+zYscMmJjExETKZrNBPQUFBadIjIiIiKhm9AQAgU3NkiEiKnC6G1q5di7i4OEydOhXHjh1Du3bt0LNnT6Snp9uN37NnD7p27YqtW7fiyJEj6NSpE3r37o1jx47ZxPn4+ECr1dr8aDSa0h0VERERUQlcq14Pm8NjkFezvqtTISIXkAmCIDizQ8uWLdGsWTMsXrxYXFe/fn3069cPCQkJJWqjYcOGiI2NxfTp0wFYR4bi4uJw69YtZ1KxodPp4Ovri5ycHPj4+JS6HSIiIpKO+G9Tsf5oBqb0rIfXO9R0dTpEVEZKWhs4NTJkMBhw5MgRdOvWzWZ9t27dcODAgRK1YbFYkJubi4oVK9qsz8vLQ2hoKKpVq4bnnnuu0MjRg/R6PXQ6nc0PERERkTP0RgsAQKNUuDgTInIFp4qh7OxsmM1mBAQE2KwPCAhAZmZmidr4+OOPcfv2bQwaNEhcV69ePSQmJmLz5s1YvXo1NBoN2rRpg/PnzztsJyEhAb6+vuJPSEiIM4dCREREBLnuJvwKcqERTK5OhYhcoFQTKMhkMptlQRAKrbNn9erVmDFjBtauXYsqVaqI61u1aoWXX34ZTZs2Rbt27fDtt9+iTp06+PTTTx22NWXKFOTk5Ig/V65cKc2hEBERkYT12LgYq7fPhP+Rfa5OhYhcwM2ZYH9/fygUikKjQFlZWYVGix60du1avPbaa/juu+/QpUuXImPlcjmaN29e5MiQWq2GWs1pMImIiKj0BIv1NDkFJ20ikiSnRoZUKhWioqKQlJRksz4pKQkxMTEO91u9ejWGDx+Ob775Bs8++2yxjyMIAlJTUxEUFORMekREREROWdo3Hj37fghL2w6uToWIXMCpkSEAiI+Px5AhQxAdHY3WrVtj6dKlSE9Px6hRowBYT1/LyMjAypUrAVgLoaFDh2L+/Plo1aqVOKrk7u4OX19fAMDMmTPRqlUr1K5dGzqdDgsWLEBqaio+++yzsjpOIiIiokL0Rgsgk0GtUro6FSJyAaeLodjYWFy/fh2zZs2CVqtFo0aNsHXrVoSGhgIAtFqtzT2HPv/8c5hMJowePRqjR48W1w8bNgyJiYkAgFu3bmHkyJHIzMyEr68vIiMjsWfPHrRo0eIhD4+IiIjIMb3JDABQu5XqMmoiesI5fZ+hxxXvM0RERETO+uLZ4RDu3EHruTPQOJo3XiV6WpS0NnB6ZIiIiIjoadH06kl4629DYTa4OhUicgGOCRMREZFkuZmNAACVu7uLMyEiV2AxRERERJKlMltvtqr25NTaRFLEYoiIiIgkyaA3QCFY7zOk9vRwcTZE5AoshoiIiEiSCm7fEf+v8eRpckRSxGKIiIiIJEl/O1/8v9qdp8kRSRGLISIiIpIkfX4BAMAoV0DhpnBxNkTkCiyGiIiISJLunSZnUChdnAkRuQqLISIiIpIkY761GDKyGCKSLBZDREREJEmGO9bT5Ewshogki8UQERERSZLx7jVDJjcWQ0RSxWKIiIiIJMlYYC2GzEqVizMhIldhMURERESSVODpi5+qNcP5sMauToWIXITFEBEREUlSXkgNfBT9Ena3H+jqVIjIRVgMERERkSTpTRYAgJr3GCKSLBZDREREJEn6OwVQmk3QuMlcnQoRuQiLISIiIpKkCklbsHnLZPTassTVqRCRi7AYIiIiIkmy6K2zyYGzyRFJlpurEyAiIiJyhT869sVEY20MalEdfV2dDBG5BEeGiIiISJLuCDLkK93h5unl6lSIyEVYDBEREZEk6Y13Z5NT8uMQkVTxt5+IiIgkKeSXXRh37FsEn011dSpE5CIshoiIiEiS/C/8jh6Xf4VfZrqrUyEiF2ExRERERJIkMxgAAHJ3jYszISJXKVUxtGjRIoSHh0Oj0SAqKgp79+4tMn737t2IioqCRqNBjRo1sGRJ4fn8161bhwYNGkCtVqNBgwbYsGFDaVIjIiIiKhG50VoMKdRqF2dCRK7idDG0du1axMXFYerUqTh27BjatWuHnj17Ij3d/hBzWloaevXqhXbt2uHYsWN4++23MXbsWKxbt06MSUlJQWxsLIYMGYLjx49jyJAhGDRoEA4ePFj6IyMiIiIqguxeMaRhMUQkVTJBEARndmjZsiWaNWuGxYsXi+vq16+Pfv36ISEhoVD8pEmTsHnzZpw+fVpcN2rUKBw/fhwpKSkAgNjYWOh0Omzbtk2M6dGjBypUqIDVq1eXKC+dTgdfX1/k5OTAx8fHmUN6JIx6s8NtMjngplSULFYGuKlKGWswA46eXRmgLGWsyWBGUa8apbqUsUYzBEvZxLqp5JDJZAAAs9ECi8VxEk7FKuWQye/GmiywmMsmVqGUQ16aWLMFFlMRsW4yyBVyp2MtZgvMRcTK3WRQlCbWIsBsdPzEyRUyKNycjxUsAkxlFSuXQXF3ZilBEGAylE2sU7/3fI+wH8v3COdj+R4BwPHvfVKvF1BdewFZY6fgmVEvFhkrtsv3iLvBfI8oVaxE3iMeByWtDZy66arBYMCRI0cwefJkm/XdunXDgQMH7O6TkpKCbt262azr3r07li1bBqPRCKVSiZSUFIwfP75QzLx58xzmotfrodfrxWWdTufMoTwyN28b8K9l+9H2lOMXi0feRQRlbBKXL9YeA0GutBsrM2Xjf1HVxOXOR2/ConC3G6u+k4lq6X8Xj5drvAqT0tdurNyUg5+iAsTlroe1MCkr2I11M+Yg9OKX4vLV6oOhdw+0n6+5AP9r5icudzl0GWZVgP1YixE1zi8Ul7VV+yLfq4bdWAD4uenf53R3PnwBFmVVh7Hh5z6FXDABALICuyHXt6HD2JS6chRorHcf73jkPGRuIQ5jq19YBqXJ+lrLrtwOORWjHcaeCDEgu6L1l6/t8XNQorrD2KqXv4Gm4E8AwM0KUbhRpb3D2IuVc3E5uDIAoMXv5+Fpcpxv4NWN8LydBgDQ+TTAX0HdHcZqfa7jTLi1T5ucvYBKBY77t7J2B3x0pwAAtz3DkVmtn8PYm5ospNa1HnudS+momlPFYWzFrD2ocPMIAKBAE4CM0JccxuYrMnGwURgAICQzE7X+9HMY63vjMPz/sp7Oa3TzQXrN1xzGGpCJ/U2t7VbIuYmIS/Z/3wDAO+d3VMncCQCwyNyQVuffDmMFy59IjgwFACiNer5H3MX3CL5HuPQ9ovYYpNUGKv/19+eJ3BsF+OqdFIftNupQFR0G17XmkGfElxP2OYyt1yoQnYc3AACYDBYsHbfbYWzNZpXRY2Rjcbmo2NBGlfDcmKbi8pcT9jostIJr+6H/m83E5ZVTD6Agz2g3tkqoN16Y0lxcXj3jIHJvFNiNrRDkiZfebSkuf5dwGDe1t+3GelfUYOj7MeLyho+PIutyrt1YjZcSr33UTlze8ulxXDt/y26sm0qO1xd0FJe3f34Sl09etxsLAKOXPCP+f9fyU7hw9C+HsSPndxCLp+Svz+DML5kOY1/9sC3cva3vEfu+P4+TuzMcxg75T2v4+Fvfo3/ZdBGpSY4n73hxegtUCrbeA+vItks49OMlh7EDJ0cjIMz6fnL8f1eQsv6Cw9h+4yNRta71/fzU3mvYs+acw9hnRzdBWGN/AMC5g3/ifytPO4ztPqIRakU5/v19XDlVDGVnZ8NsNiMgwPYPV0BAADIz7b9IMjMz7cabTCZkZ2cjKCjIYYyjNgEgISEBM2fOdCb9cmG0WJB6+Qbawv6HBgBw1+cjVPuHuJxWy+L4yxS9Hocv3xSXu5rNsCjsx6qMept2M0JNMNn//AS5yWjTbg+jwWGsm9lk025WkB56B58P5Razbbv6AphV9mNlgsWm3Vv++cgv4r53Nv1wJx8WB/kCQPXMi1BYrKc/3PZtjVz7n/cAAL9dzYFOaf1V6Hg7Dygitupfl+BecAMAYHRvjJyKjmMvaG/hfK71G7hWOTo4+MwJAAjKvgKf3LtviG5huFHEe0nGXzk4bLTm2+RmDjy9HX/QqXLjGvxvWPtYK/jjryDH7WZf1+Gw3AMAEH79Fip5Ov6g45+ThaBMa7vZFdXIrOYwFLk3deJzVzHrJqqqHR9chdzr4mtC521ARqjjdgt0uX+/Jm7cQC25n8NY37xbYrt3NBWRXtNxu+a8PLHd0LybiIDjYsjzTq7YrlmuQlodx+0K+XfEdt2NBXyPuNcu3yMA8D3C1e8RHn5FdD4RPdWcOk3u2rVrqFq1Kg4cOIDWrVuL62fPno2vvvoKZ86cKbRPnTp18Morr2DKlCniuv3796Nt27bQarUIDAyESqXCihUrMHjwYDHm66+/xmuvvYaCAvvfSNgbGQoJCXH5aXIFRjOSf78G9UH7I2UAABkgv28U0XJ3xFpfpyEsftYPSIq//oQq7TwsfhVhbNhIjFWn7AMsDsZ1HbRrqFEbZn9rsam4dQOqc6dg8fKGsWmkGKs6dBAygx6OyO/7cGWxABAAY/VwmAKtfwzluTqoT/8GQa2BIbrF3+2mHoXsdp5T7ZqCqsEYEmY9pII70Px2BFAooG/VRoxV/n4S8ls3HLYrk1uH++9v1+wfAEON2taVJhPcj/4CAChoGQOZm/WDg9u5M1D8leVUuxa/itDXaSDGuB8+AFgs0Ec1BzTWT4RuF/+AQnvNuXa9vKFv8Pc3f5rUQ5AZ9DA0joBw9zWuuHIZbumXS9SuYAEEARDUGhQ0/fvbas3vqZDdzoOhXkMIlSoBAOSZWigvnHfcrszaNnC3TQsAhQJ3ov5+X1CfOwX5rRsw1qoNS4D1E5b8ejaUZ045bPf+17DYLmBtV2F9sagunIPiehZMYeEwV7V+yJPpcqA6cdzpdguaRkNQW0cTlJcvwu3PazAFV4M5/O7ow518qI8edrpdfYOmsHh5AwDcrl2B8uplmCsHwFTH+m0yTCa+R9xrl+8R1nb5HmFt1wXvERXCQ9H4mRY8ldZeLE+TA8DT5J720+ScKoYMBgM8PDzw3XffoX///uL6cePGITU1Fbt3Fx7Sbd++PSIjIzF//nxx3YYNGzBo0CDk5+dDqVSievXqGD9+vM2pcp988gnmzZuHy5cdv4nf73G7ZoiIiIiIiFyjpLWBU+WbSqVCVFQUkpKSbNYnJSUhJibG7j6tW7cuFL9z505ER0dDqVQWGeOoTSIiIiIioofl1DVDABAfH48hQ4YgOjoarVu3xtKlS5Geno5Ro0YBAKZMmYKMjAysXLkSgHXmuIULFyI+Ph4jRoxASkoKli1bZjNL3Lhx49C+fXvMmTMHffv2xaZNm7Br1y7s2+f4okQiIiIiIqKH4XQxFBsbi+vXr2PWrFnQarVo1KgRtm7ditBQ61WMWq3W5p5D4eHh2Lp1K8aPH4/PPvsMwcHBWLBgAQYMGCDGxMTEYM2aNXjnnXcwbdo01KxZE2vXrkXLli0LPT4REREREVFZcPo+Q4+rnJwc+Pn54cqVK7xmiIiIiIhIwu5Nrnbr1i34+jqeMdLpkaHHVW6udb76kBDHU4gSEREREZF05ObmFlkMPTUjQxaLBdeuXYO3t7c4vaGr3KtEOUr1aLGfyw/7unywn8sH+7n8sK/LB/u5fLCfy09Z9LUgCMjNzUVwcDDkcsdzxj01I0NyuRzVqhVxVzcX8PHx4S9LOWA/lx/2dflgP5cP9nP5YV+XD/Zz+WA/l5+H7euiRoTueXzujERERERERFSOWAwREREREZEksRh6BNRqNd59912o1WpXp/JUYz+XH/Z1+WA/lw/2c/lhX5cP9nP5YD+Xn/Ls66dmAgUiIiIiIiJncGSIiIiIiIgkicUQERERERFJEoshIiIiIiKSJBZDREREREQkSSyGytiiRYsQHh4OjUaDqKgo7N2719UpPdESEhLQvHlzeHt7o0qVKujXrx/Onj1rEyMIAmbMmIHg4GC4u7ujY8eO+P33312U8dMhISEBMpkMcXFx4jr2c9nJyMjAyy+/jEqVKsHDwwMRERE4cuSIuJ19/fBMJhPeeecdhIeHw93dHTVq1MCsWbNgsVjEGPZz6ezZswe9e/dGcHAwZDIZNm7caLO9JP2q1+vx73//G/7+/vD09ESfPn1w9erVcjyKx19R/Ww0GjFp0iQ0btwYnp6eCA4OxtChQ3Ht2jWbNtjPxSvu9Xy/119/HTKZDPPmzbNZz34umZL09enTp9GnTx/4+vrC29sbrVq1Qnp6urj9UfQ1i6EytHbtWsTFxWHq1Kk4duwY2rVrh549e9o8ieSc3bt3Y/To0fjll1+QlJQEk8mEbt264fbt22LM3Llz8d///hcLFy7EoUOHEBgYiK5duyI3N9eFmT+5Dh06hKVLl6JJkyY269nPZePmzZto06YNlEoltm3bhlOnTuHjjz+Gn5+fGMO+fnhz5szBkiVLsHDhQpw+fRpz587Fhx9+iE8//VSMYT+Xzu3bt9G0aVMsXLjQ7vaS9GtcXBw2bNiANWvWYN++fcjLy8Nzzz0Hs9lcXofx2Cuqn/Pz83H06FFMmzYNR48exfr163Hu3Dn06dPHJo79XLziXs/3bNy4EQcPHkRwcHChbeznkimury9cuIC2bduiXr16SE5OxvHjxzFt2jRoNBox5pH0tUBlpkWLFsKoUaNs1tWrV0+YPHmyizJ6+mRlZQkAhN27dwuCIAgWi0UIDAwUPvjgAzGmoKBA8PX1FZYsWeKqNJ9Yubm5Qu3atYWkpCShQ4cOwrhx4wRBYD+XpUmTJglt27Z1uJ19XTaeffZZ4dVXX7VZ9/zzzwsvv/yyIAjs57ICQNiwYYO4XJJ+vXXrlqBUKoU1a9aIMRkZGYJcLhe2b99ebrk/SR7sZ3t+/fVXAYBw+fJlQRDYz6XhqJ+vXr0qVK1aVTh58qQQGhoqfPLJJ+I29nPp2Ovr2NhY8T3ankfV1xwZKiMGgwFHjhxBt27dbNZ369YNBw4ccFFWT5+cnBwAQMWKFQEAaWlpyMzMtOl3tVqNDh06sN9LYfTo0Xj22WfRpUsXm/Xs57KzefNmREdH44UXXkCVKlUQGRmJL774QtzOvi4bbdu2xU8//YRz584BAI4fP459+/ahV69eANjPj0pJ+vXIkSMwGo02McHBwWjUqBH7/iHk5ORAJpOJo8zs57JhsVgwZMgQTJgwAQ0bNiy0nf1cNiwWC3788UfUqVMH3bt3R5UqVdCyZUubU+keVV+zGCoj2dnZMJvNCAgIsFkfEBCAzMxMF2X1dBEEAfHx8Wjbti0aNWoEAGLfst8f3po1a3D06FEkJCQU2sZ+LjsXL17E4sWLUbt2bezYsQOjRo3C2LFjsXLlSgDs67IyadIkDB48GPXq1YNSqURkZCTi4uIwePBgAOznR6Uk/ZqZmQmVSoUKFSo4jCHnFBQUYPLkyXjppZfg4+MDgP1cVubMmQM3NzeMHTvW7nb2c9nIyspCXl4ePvjgA/To0QM7d+5E//798fzzz2P37t0AHl1fuz1U5lSITCazWRYEodA6Kp0xY8bgt99+w759+wptY78/nCtXrmDcuHHYuXOnzbm5D2I/PzyLxYLo6Gi8//77AIDIyEj8/vvvWLx4MYYOHSrGsa8fztq1a7Fq1Sp88803aNiwIVJTUxEXF4fg4GAMGzZMjGM/Pxql6Vf2fekYjUa8+OKLsFgsWLRoUbHx7OeSO3LkCObPn4+jR4863WfsZ+fcm9ymb9++GD9+PAAgIiICBw4cwJIlS9ChQweH+z5sX3NkqIz4+/tDoVAUqkyzsrIKfUNGzvv3v/+NzZs34+eff0a1atXE9YGBgQDAfn9IR44cQVZWFqKiouDm5gY3Nzfs3r0bCxYsgJubm9iX7OeHFxQUhAYNGtisq1+/vjjRCl/TZWPChAmYPHkyXnzxRTRu3BhDhgzB+PHjxZFP9vOjUZJ+DQwMhMFgwM2bNx3GUMkYjUYMGjQIaWlpSEpKEkeFAPZzWdi7dy+ysrJQvXp18W/j5cuX8eabbyIsLAwA+7ms+Pv7w83Nrdi/j4+ir1kMlRGVSoWoqCgkJSXZrE9KSkJMTIyLsnryCYKAMWPGYP369fjf//6H8PBwm+3h4eEIDAy06XeDwYDdu3ez353QuXNnnDhxAqmpqeJPdHQ0/vGPfyA1NRU1atRgP5eRNm3aFJoe/ty5cwgNDQXA13RZyc/Ph1xu+ydOoVCI3z6ynx+NkvRrVFQUlEqlTYxWq8XJkyfZ9064VwidP38eu3btQqVKlWy2s58f3pAhQ/Dbb7/Z/G0MDg7GhAkTsGPHDgDs57KiUqnQvHnzIv8+PrK+LvXUC1TImjVrBKVSKSxbtkw4deqUEBcXJ3h6egqXLl1ydWpPrDfeeEPw9fUVkpOTBa1WK/7k5+eLMR988IHg6+srrF+/Xjhx4oQwePBgISgoSNDpdC7M/Ml3/2xygsB+Liu//vqr4ObmJsyePVs4f/688PXXXwseHh7CqlWrxBj29cMbNmyYULVqVeGHH34Q0tLShPXr1wv+/v7CxIkTxRj2c+nk5uYKx44dE44dOyYAEP773/8Kx44dE2cxK0m/jho1SqhWrZqwa9cu4ejRo8IzzzwjNG3aVDCZTK46rMdOUf1sNBqFPn36CNWqVRNSU1Nt/j7q9XqxDfZz8Yp7PT/owdnkBIH9XFLF9fX69esFpVIpLF26VDh//rzw6aefCgqFQti7d6/YxqPoaxZDZeyzzz4TQkNDBZVKJTRr1kycAppKB4Ddn+XLl4sxFotFePfdd4XAwEBBrVYL7du3F06cOOG6pJ8SDxZD7Oeys2XLFqFRo0aCWq0W6tWrJyxdutRmO/v64el0OmHcuHFC9erVBY1GI9SoUUOYOnWqzQdF9nPp/Pzzz3bfl4cNGyYIQsn69c6dO8KYMWOEihUrCu7u7sJzzz0npKenu+BoHl9F9XNaWprDv48///yz2Ab7uXjFvZ4fZK8YYj+XTEn6etmyZUKtWrUEjUYjNG3aVNi4caNNG4+ir2WCIAilH1ciIiIiIiJ6MvGaISIiIiIikiQWQ0REREREJEkshoiIiIiISJJYDBERERERkSSxGCIiIiIiIkliMURERERERJLEYoiIiIiIiCSJxRAREREREUkSiyEiIiIiIpIkFkNERERERCRJLIaIiIiIiEiSWAwREREREZEk/T9R/nfIT8wOmwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAADaCAYAAABkQZn5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABHcklEQVR4nO3deVxUVf8H8M9lmI1tEFEWF8B9FwRTwbXcU8tcqCdxeUqz4lEk18cltV+PWVlohmUvE821wi1zwxJzQSsVWzQzQ3GBCFMWgRlgzu8P5OrIDDKIjDqf9+vF68W993vPPffcYZjvnHPPlYQQAkRERERERHbGwdYVICIiIiIisgUmQ0REREREZJeYDBERERERkV1iMkRERERERHaJyRAREREREdklJkNERERERGSXmAwREREREZFdYjJERERERER2ickQERERERHZJSZDREQ2duTIEQwbNgw+Pj5QqVTw9vbG0KFDkZSUdE/l/u9//8OWLVuqppJ3ceXKFcydOxfJyclW7ffnn38iMjISTZo0gVarhZOTE1q2bIlZs2bh8uXLclz37t3RqlWrKq511fP398fo0aOr5TgDBgyosvJmzZqF+vXrw9HREe7u7sjLy8PcuXORmJhYZccgInoQMRkiIrKhDz74AGFhYbh06RLefvtt7N27F++++y4uX76Mzp07Y+nSpZUuu7qToXnz5lmVDG3fvh1t2rTB9u3bMW7cOGzfvl3+/auvvqrSD/tk2datW/Hmm29i5MiR2L9/P/bu3Yu8vDzMmzePyRARPfIcbV0BIiJ7dejQIURFRaF///7YvHkzHB1vvSU/++yzGDx4MCZOnIigoCCEhYXZsKZVLyUlBc8++yyaNGmCffv2QafTydsef/xxTJgwAZs3b7ZhDe3HL7/8AgCYMGECateuDQDIzMy0ZZWIiKoNe4aIiGxkwYIFkCQJy5YtM0mEAMDR0RGxsbGQJAlvvfWWvH706NHw9/cvU9bcuXMhSZK8LEkSbty4gVWrVkGSJEiShO7duwMA4uLiIEkSEhISMGbMGHh4eMDZ2RkDBw7En3/+aVKupWFf3bt3l8tLTExE+/btAQBjxoyRjzd37lyL5/7ee+/hxo0biI2NNUmEbq//M888U2b9Dz/8gC5dusDJyQkNGjTAW2+9BaPRKG8vKCjAa6+9hsDAQOh0Onh4eKBTp07YunWr2WNERkbis88+Q/PmzeHk5IS2bdti+/btJnGlbfvrr7/iueeeg06ng5eXF/79738jKyvL4jmWys7OxuTJkxEQEACVSoU6deogKioKN27cuOu+90IIgdjYWAQGBkKr1aJGjRoYOnSoyTX29/fHrFmzAABeXl6QJAmjR49GrVq1AADz5s2Tr2d1DP8jIqpuTIaIiGyguLgY+/btQ0hICOrWrWs2pl69eggODsa3336L4uJiq8pPSkqCVqtF//79kZSUhKSkJMTGxprEvPDCC3BwcMC6desQExOD77//Ht27d8f169etOla7du2wcuVKACX3npQe78UXX7S4z549e+Dl5YWOHTtW+Djp6el4/vnnMWLECGzbtg39+vXDjBkzsGbNGjlGr9fjn3/+weTJk7FlyxasX78enTt3xjPPPIPVq1eXKfPrr7/G0qVLMX/+fMTHx8PDwwODBw8ukxQCwJAhQ9CkSRPEx8dj+vTpWLduHSZNmlRunfPy8tCtWzesWrUKEyZMwM6dOzFt2jTExcVh0KBBEELIsaVJV1UNTXvppZcQFRWFnj17YsuWLYiNjcWvv/6K0NBQ/PXXXwCAzZs344UXXgAA7Nq1C0lJSZg3bx527doFoOQ1Uno9Z8+eXSX1IiJ6kHCYHBGRDWRmZiIvLw8BAQHlxgUEBOD777/H1atX5SFMFdGxY0c4ODigVq1aFhOOkJAQrFixQl5u2bIlwsLC8OGHH2LmzJkVPpabm5s8uUHDhg0rlOCkpqYiMDCwwscAgKtXr2LHjh147LHHAAA9e/ZEYmIi1q1bh5EjRwIAdDqdnJgBJUnnE088gWvXriEmJkaOK5Wfn4+9e/fC1dUVQEli5+vri88//xzTp083iX3hhRcwZcoU+dh//PEHPv30U6xYscKkV+52S5YswU8//YSjR48iJCQEAPDEE0+gTp06GDp0KHbt2oV+/foBABwcHKBQKCyWZY0jR47gk08+waJFixAdHS2v79KlC5o0aYL33nsPCxcuRFBQkJyMBwcHw9PTEwDg7OwMAKhbt65VCSsR0cOGPUNERA+w0p6DqviAfKfnn3/eZDk0NBR+fn7Yt29flR+rKnh7e8uJUKk2bdrgwoULJuu++OILhIWFwcXFBY6OjlAqlVixYgVOnz5dpswePXrIiRBQMlSsdu3aZcoEgEGDBpU5dkFBATIyMizWefv27WjVqhUCAwNRVFQk//Tp06dML9CcOXNQVFSEbt26ldsOFbF9+3ZIkoQRI0aYHNfb2xtt27blxAhERDexZ4iIyAY8PT3h5OSElJSUcuPOnz8PJycneHh4VHkdvL29za67evVqlR/rTvXr17/rud+pZs2aZdap1Wrk5+fLy5s2bcLw4cMxbNgwTJkyBd7e3nB0dMSyZcvw6aefVqpMS7FqtRoAzMaW+uuvv/DHH39AqVSa3X6/Jir466+/IISAl5eX2e0NGjS4L8clInrYMBkiIrIBhUKBHj16YNeuXbh06ZLZ+4YuXbqEY8eOoV+/flAoFAAAjUYDvV5fJrYyH6rT09PNrmvUqJG8XN7xSodUVUafPn3wwQcf4MiRI1U6DGvNmjUICAjAxo0bTXrTzJ1DdfD09IRWqzWbiJVuv1/HlSQJBw4ckJO225lbR0RkjzhMjojIRmbMmAEhBF555ZUyEyQUFxfj5ZdfhhACM2bMkNf7+/sjIyNDvgEeAAwGA3bv3l2mfEs9HKXWrl1rsnz48GFcuHBBniWu9Hg//fSTSdzvv/+OM2fOlDkWUH4vye0mTZoEZ2dnvPLKK2ZnZBNCVGpqbUmSoFKpTBKh9PR0s7PJVYcBAwbg3LlzqFmzJkJCQsr8mJsZsKqOK4TA5cuXzR63devW5e5v7fUkInpYsWeIiMhGwsLCEBMTg6ioKHTu3BmRkZGoX78+UlNT8eGHH+Lo0aOIiYlBaGiovE94eDjmzJmDZ599FlOmTEFBQQGWLFlidra51q1bIzExEV999RV8fHzg6uqKpk2bytt//PFHvPjiixg2bBguXryImTNnok6dOnjllVfkmIiICIwYMQKvvPIKhgwZggsXLuDtt9+Wp14u1bBhQ2i1WqxduxbNmzeHi4sLfH194evra/bcAwICsGHDBoSHhyMwMBCRkZEICgoCAJw6dQqffvophBAYPHiwVW06YMAAbNq0Ca+88gqGDh2Kixcv4o033oCPjw/Onj1rVVlVISoqCvHx8ejatSsmTZqENm3awGg0IjU1FXv27MFrr72GDh06AADmz5+P+fPn45tvvqnQfUPp6en48ssvy6z39/dHWFgYxo0bhzFjxuDHH39E165d4ezsjLS0NBw8eBCtW7fGyy+/bLFsV1dX+Pn5YevWrXjiiSfg4eEBT0/P+5a8ERHZjCAiIptKSkoSQ4cOFV5eXsLR0VHUrl1bPPPMM+Lw4cNm43fs2CECAwOFVqsVDRo0EEuXLhWvv/66uPMtPTk5WYSFhQknJycBQHTr1k0IIcTKlSsFALFnzx4REREh3N3dhVarFf379xdnz541KcNoNIq3335bNGjQQGg0GhESEiK+/fZb0a1bN7m8UuvXrxfNmjUTSqVSABCvv/76Xc/93Llz4pVXXhGNGjUSarVaaLVa0aJFCxEdHS1SUlLkuG7duomWLVuW2X/UqFHCz8/PZN1bb70l/P39hVqtFs2bNxeffPKJ2fYBIF599dUyZfr5+YlRo0bJy6X7/v333yZxpe14ez3v3FcIIXJzc8WsWbNE06ZNhUqlEjqdTrRu3VpMmjRJpKenlznOvn37zDfWHXUEYPbn9uN/+umnokOHDsLZ2VlotVrRsGFDMXLkSPHjjz/e9fz27t0rgoKChFqtLlMuEdGjQhLitoccEBHRIy8uLg5jxozBDz/8IE/3TEREZI94zxAREREREdklJkNERERERGSXOEyOiIiIiIjsEnuGiIiIiIjILjEZIiIiIiIiu8RkiIiIiIiI7NIj89BVo9GIK1euwNXV1eTJ40REREREZF+EEMjJyYGvry8cHCz3/zwyydCVK1dQr149W1eDiIiIiIgeEBcvXkTdunUtbn9kkiFXV1cAJSfs5uZm49oQEREREZGtZGdno169enKOYMkjkwyVDo1zc3NjMkRERERERHe9fcbqCRS+++47DBw4EL6+vpAkCVu2bLnrPvv370dwcDA0Gg0aNGiAjz76qExMfHw8WrRoAbVajRYtWmDz5s3WVo2IiIiIiKjCrE6Gbty4gbZt22Lp0qUVik9JSUH//v3RpUsXnDhxAv/9738xYcIExMfHyzFJSUkIDw9HREQETp48iYiICAwfPhxHjx61tnpEREREREQVIgkhRKV3liRs3rwZTz/9tMWYadOmYdu2bTh9+rS8bvz48Th58iSSkpIAAOHh4cjOzsbOnTvlmL59+6JGjRpYv359heqSnZ0NnU6HrKwsDpMjIiIiIrJjFc0N7vs9Q0lJSejdu7fJuj59+mDFihUoLCyEUqlEUlISJk2aVCYmJibGYrl6vR56vV5ezs7OvmtdjEYjDAaDdSdADy2lUgmFQmHrahAR0UMq85NPkLsvEe7Dh8H95he/+pQUpM2cZXVZdRa9C6WPDwDg2oaNyNq2DW59+8JjZAQAoOiff3Ap8j9Wl+v9+uvQNG0CAMjesQP/rFkL59BQ1Ip8FQAgCgtxYdRoq8utHT0JTiEhAIDcg4eQGRsLbetW8JoxQ45JfeFFGPPzrSq35osvwPXxxwEA+T/9hL/eWgiVnx98F/xPjrkcHY3C9L+sKtfcNVLUqIF6H94ayZT2+lzoz561qlxz10hSKOD32Wo5JmPRIuQdO25VuZauUf1PlsPB2RnArdefNSxdI3OvP2tYukZ3vv6M+flwHzLEqrJt7b4nQ+np6fDy8jJZ5+XlhaKiImRmZsLHx8diTHp6usVyFyxYgHnz5lW4HgaDASkpKTAajdadAD3U3N3d4e3tzWdPERHRXQmDAaKwEJJaDcnREYWpqcg/fhwuXbveisnPR/5x6z74AoC47QvcwitXkH/8OLStW93aXlhYqXKNN27cKjcjA/nHj0Pp63vbgUWlyi3Oyrr1+7V/kH/8OBw0GpOY/JMnYczNtarcosynbpWbnYP848fLJFT5v/6KwgupVpVr7ho51q5tEqM/cwb5yclWlWv2GimVpuWe+9PqNrZ0jcRtn1NLX3/WsHSNzL3+rGHpGt35+jPm3rhz1wdetcwmd+cH0dKRebevNxdT3gfYGTNmIDo6Wl4unT7PHCEE0tLSoFAoUK9evXIfvESPBiEE8vLykJGRAQDwufltCBERkSU5e/ficvRrcOrQAX6r4uD+7LNw7tIF6kaN5Bhl3bqos2Sx1WU71qol/64bOACaVi2h8vOT1yl0ukqVqwrwl3937d4dSl9fuQeg5MCOlSpX06q1/LtTcDDqLFkMx5o1TWJ8314IUVRkXbktWtz6vVlT1FmyGIo7pj72njUbxvw8q8o1d40c1GqTmFqTJqE467pV5Zq9Rnd8Pq059kXonn7qzl3LZeka3Z7MlL7+rGHpGpl7/VnD0jW68/UnioutKvdBcN+TIW9v7zI9PBkZGXB0dETNmxfMUsydvUW3U6vVUN/xIrekqKgIeXl58PX1hZOTk5VnQA8rrVYLoOS1VLt2bQ6ZIyKichkLSr49l9QqAIC2ZUtoW5p+aFS4ucHtjuH/1lI3bgx148Ym6xw0mnsuV+XvD5W/v8k6ycHhnstV+vqa9mTcVDrcrbIcPT3N1s2lS+d7KtfSNXLu8Ng9lWvpGjkFBd1TuZaukbnXn7XMXSNzrz9rmbtGd772Hhb3vYukU6dOSEhIMFm3Z88ehISEQHmzm9FSTGhoaJXUofhmlqpSqaqkPHp4lCa/hYWFNq4JERE96IShJBm6s0eBiB5dVvcM5ebm4o8//pCXU1JSkJycDA8PD9SvXx8zZszA5cuXsXp1yY1l48ePx9KlSxEdHY2xY8ciKSkJK1asMJklbuLEiejatSsWLlyIp556Clu3bsXevXtx8ODBKjjFW3jfiP3hNScioooqva9CUjEZIrIXVvcM/fjjjwgKCkLQzS7B6OhoBAUFYc6cOQCAtLQ0pKbeuuktICAAO3bsQGJiIgIDA/HGG29gyZIlGHLbTBOhoaHYsGEDVq5ciTZt2iAuLg4bN25Ehw4d7vX8iIiIiCrEqC+ZcVbSMBkishdW9wx1794d5T2aKC4ursy6bt264fhdZq0YOnQohg4dam117FJFnu90u8TERPTo0QPXrl2Du7v7fa3b7eLi4hAVFYXr169XeB9/f39ERUUhKirqvtWLiIjIHFFQAIDD5IjsCadVewilpaWhX79+VVrm3LlzERgYeNe40aNHVzgJCw8Px++//35vFSMiIqompfcMcZgckf2olqm1qWoYDAaoVCp4e3vbuip3VVhYCK1WK8/oRkRE9KCTh8mxZ4jIbrBn6AHWvXt3REZGIjo6Gp6enujVqxeAkmFyW7ZskeMOHz6MwMBAaDQahISEYMuWLZAkCcl3PFjs2LFjCAkJgZOTE0JDQ3HmzBkAJcPZ5s2bh5MnT0KSJEiSZHa449y5c7Fq1Sps3bpVjktMTMT58+chSRI+//xzdO/eHRqNBmvWrEFcXJzJsLxz587hqaeegpeXF1xcXNC+fXvs3bu33DaYO3cu6tevD7VaDV9fX0yYMKFSbUlERHQ3pRMoOPCeISK7YZc9Q0II5Bfa5qFQWqXCqhnOVq1ahZdffhmHDh0ye69WTk4OBg4ciP79+2PdunW4cOGCxfttZs6ciUWLFqFWrVoYP348/v3vf+PQoUMIDw/HL7/8gl27dsnJiU6nK7P/5MmTcfr0aWRnZ2PlypUAAA8PD1y5cgUAMG3aNCxatAgrV66EWq3Gnj17TPbPzc1F//798X//93/QaDRYtWoVBg4ciDNnzqB+/fpljvfll1/i/fffx4YNG9CyZUukp6fj5MmTFW47IiIiawh9yT1DHCZHZD/sMhnKLyxGizm7bXLsU/P7wElV8WZv1KgR3n77bYvb165dC0mS8Mknn0Cj0aBFixa4fPkyxo4dWyb2zTffRLdu3QAA06dPx5NPPomCggJotVq4uLjA0dGx3CF4Li4u0Gq10Ov1ZuOioqLwzDPPWNy/bdu2aNu2rbz8f//3f9i8eTO2bduGyMjIMvGpqanw9vZGz549oVQqUb9+fTz22L09MI2IiMgSDpMjsj8cJveACwkJKXf7mTNn0KZNG2g0GnmdpYShTZs28u8+Pj4AgIyMjCqoZYm71fXGjRuYOnUqWrRoAXd3d7i4uOC3334zmYr9dsOGDUN+fj4aNGiAsWPHYvPmzSgqKqqy+hIREd1Ofs6Qmg9pJ7IXdtkzpFUqcGp+H5sd2xrOzs7lbhdClBl2Z2nqc6VSKf9euo/RaLSqPuW5W12nTJmC3bt3491330WjRo2g1WoxdOhQGAwGs/H16tXDmTNnkJCQgL179+KVV17BO++8g/3795ucCxERUVW4dc+Q5i6RRPSosMtkSJIkq4aqPciaNWuGtWvXQq/XQ32zW//HH3+0uhyVSoXi4rvfR1XROHMOHDiA0aNHY/DgwQBK7iE6f/58uftotVoMGjQIgwYNwquvvopmzZrh559/Rrt27SpVByIiIktqRUejxsgIaJo1s3VViKiacJjcQ+5f//oXjEYjxo0bh9OnT8s9LwCsmqjB398fKSkpSE5ORmZmJvQ3vx0zF/fTTz/hzJkzyMzMRGFhYYWP0ahRI2zatAnJyck4efKkXHdL4uLisGLFCvzyyy/4888/8dlnn0Gr1cLPz6/CxyQiIqoobauWcO3eHcqH4BEWRFQ1mAw95Nzc3PDVV18hOTkZgYGBmDlzJubMmQMAJvcR3c2QIUPQt29f9OjRA7Vq1cL69evNxo0dOxZNmzZFSEgIatWqhUOHDlX4GO+//z5q1KiB0NBQDBw4EH369Cm3h8fd3R2ffPIJwsLC0KZNG3zzzTf46quvULNmzQofk4iIiIjIEklYusHkIZOdnQ2dToesrCy4ubmZbCsoKEBKSgoCAgKsShAeVmvXrsWYMWOQlZVl9w89tbdrT0RElZe1/WsIgwEuPbrDsUYNW1eHiO5BebnB7R6NG2fs3OrVq9GgQQPUqVMHJ0+exLRp0zB8+HC7T4SIiIiskfHuuyhKT4f/l18yGSKyE0yGHgHp6emYM2cO0tPT4ePjg2HDhuHNN9+0dbWIiIgeKs4dO6Lon6tQuLvbuipEVE04TI4eabz2RERERPanosPkOIECERERERHZJSZDRERERERkl5gMERERkd0rzr2B0y1b4bd2wTAWFNi6OkRUTZgMERERkd0TBj1QXAyRlwdJpbJ1dYiomjAZIiIiIrsn9HoAgKRUQnLgxyMie8G/diIiIrJ7pUPjJM48SmRXmAw9wLp3746oqCir9tmyZQsaNWoEhUJh9b4VNXfuXAQGBlq1jyRJ2LJly32pDxER0b0SBgMAQFKrbVwTIqpOTIYeMS+99BKGDh2Kixcv4o033sDo0aPx9NNP33U/axKvyZMn45tvvrm3ihIRET1ASofJOfB+ISK74mjrClDVyc3NRUZGBvr06QNfX98qL18IgeLiYri4uMDFxaXKyyciIrIV+Z4h9gwR2RX2DD1EDAYDpk6dijp16sDZ2RkdOnRAYmIiACAxMRGurq4AgMcffxySJKF79+5YtWoVtm7dCkmSIEmSHH+70aNHY//+/Vi8eLEcd/78eSQmJkKSJOzevRshISFQq9U4cOBAmWFyP/zwA3r16gVPT0/odDp069YNx48fL/c8IiMj4ePjA41GA39/fyxYsKAqm4qIiMgqxoKbyRDvGSKyK5VKhmJjYxEQEACNRoPg4GAcOHDAYuzo0aPlD9i3/7Rs2VKOiYuLMxtTcJ/n+Tfm5Vn9I4qK5P1FUVHJ+jvqaWnfezVmzBgcOnQIGzZswE8//YRhw4ahb9++OHv2LEJDQ3HmzBkAQHx8PNLS0rBt2zYMHz4cffv2RVpaGtLS0hAaGlqm3MWLF6NTp04YO3asHFevXj15+9SpU7FgwQKcPn0abdq0KbN/Tk4ORo0ahQMHDuDIkSNo3Lgx+vfvj5ycHLPnsWTJEmzbtg2ff/45zpw5gzVr1sDf3/+e24eIiKiyhIHD5IjskdXD5DZu3IioqCjExsYiLCwMH3/8Mfr164dTp06hfv36ZeIXL16Mt956S14uKipC27ZtMWzYMJM4Nzc3+cN8Kc19/nbmTLtgq/epE/M+3Pr2BQDk7N2Ly1GT4NS+Pfw+Wy3H/PFETxRfu1Zm3+a/na50Xc+dO4f169fj0qVL8hC4yZMnY9euXVi5ciX+97//oXbt2gAADw8PeHt7AwC0Wi30er28bI5Op4NKpYKTk5PZuPnz56NXr14W93/88cdNlj/++GPUqFED+/fvx4ABA8rEp6amonHjxujcuTMkSYKfn9/dG4CIiOg+4jA5Ivtkdc/Qe++9hxdeeAEvvvgimjdvjpiYGNSrVw/Lli0zG6/T6eDt7S3//Pjjj7h27RrGjBljEidJkklceR/e7dHx48chhECTJk3ke3ZcXFywf/9+nDt37r4eOyQkpNztGRkZGD9+PJo0aQKdTgedTofc3FykpqaajR89ejSSk5PRtGlTTJgwAXv27Lkf1SYiIqowo56zyRHZI6t6hgwGA44dO4bp06ebrO/duzcOHz5coTJWrFiBnj17lukNyM3NhZ+fH4qLixEYGIg33ngDQUFBFsvR6/XQ3/wWBwCys7OtOJMSTY8fs3qf259K7dqzZ0kZdzycrdE3e60u926MRiMUCgWOHTsGhUJhsu1+T2bg7Oxc7vbRo0fj77//RkxMDPz8/KBWq9GpUycYbk5Teqd27dohJSUFO3fuxN69ezF8+HD07NkTX3755f2oPhER0V0JfcmQdwcNkyEie2JVMpSZmYni4mJ4eXmZrPfy8kJ6evpd909LS8POnTuxbt06k/XNmjVDXFwcWrdujezsbCxevBhhYWE4efIkGjdubLasBQsWYN68edZUvwwHJ6d72l9ydITkWLYJ77Vcc4KCglBcXIyMjAx06dKlwvupVCoUFxdXWZw5Bw4cQGxsLPr37w8AuHjxIjIzM8vdx83NDeHh4QgPD8fQoUPRt29f/PPPP/Dw8KhUHYiIiO6FPExOxWSIyJ5UamptSZJMloUQZdaZExcXB3d39zLPvenYsSM6duwoL4eFhaFdu3b44IMPsGTJErNlzZgxA9HR0fJydna2yU3/j5omTZrg+eefx8iRI7Fo0SIEBQUhMzMT3377LVq3bi0nInfy9/fH7t27cebMGdSsWRM6nQ5KpdJs3NGjR3H+/Hm4uLhYlZQ0atQIn332GUJCQpCdnY0pU6ZAq9VajH///ffh4+ODwMBAODg44IsvvoC3tzfc3d0rfEwiIqKq5NylC3w9PKDkMH0iu2LVPUOenp5QKBRleoEyMjLK9BbdSQiBTz/9FBEREVDdZaYWBwcHtG/fHmfPnrUYo1ar4ebmZvLzqFu5ciVGjhyJ1157DU2bNsWgQYNw9OjRcpPAsWPHomnTpggJCUGtWrVw6NAhs3GTJ0+GQqFAixYtUKtWLYv3+5jz6aef4tq1awgKCkJERAQmTJggT+ZgjouLCxYuXIiQkBC0b98e58+fx44dO+DgwJneiYjINtQNGkA3cCCc2re3dVWIqBpJQghhzQ4dOnRAcHAwYmNj5XUtWrTAU089Ve6zYhITE9GjRw/8/PPPaNWqVbnHEELgscceQ+vWrfHpp59WqF7Z2dnQ6XTIysoqkxgVFBQgJSVFng6c7AevPREREZH9KS83uJ3Vw+Sio6MRERGBkJAQdOrUCcuXL0dqairGjx8PoGT42uXLl7F69WqT/VasWIEOHTqYTYTmzZuHjh07onHjxsjOzsaSJUuQnJyMDz/80NrqEREREVmt4NQpFKalQd2wIVR89h2R3bA6GQoPD8fVq1cxf/58pKWloVWrVtixY4c8O1xaWlqZIVZZWVmIj4/H4sWLzZZ5/fp1jBs3Dunp6dDpdAgKCsJ3332Hxx57rBKnRERERGSdaxs24vrnn8Nzwn9Q65VXbF0dIqomVg+Te1BxmByZw2tPREQVkfnxcuR++y3cn3sW7ndM9ERED5/7NkyOiIiI6FHj+dI4eL40ztbVIKJqxum7iIiIiIjILjEZIiIiIiIiu8RkiIiIiOzexfEv42y37sg9cMDWVSGiasRkiIiIiOxe0dWrKPrrL4jiYltXhYiqEZMhIiIisnuioAAA4KBW27gmRFSdmAyRibi4OLi7u9/XY0iShC1btlQ4fu7cuQgMDLxv9SEiIjIa9AAAickQkV1hMvQIq64kIjExEZIk4fr16xWKT0tLQ79+/e5vpYiIiKwg9AYAgKRiMkRkT/icIao2BoMBKpUK3t7etq4KERGRCaEv6Rly0DAZIrIn7Bl6gBmNRixcuBCNGjWCWq1G/fr18eabb8rbp02bhiZNmsDJyQkNGjTA7NmzUVhYCKBkuNu8efNw8uRJSJIESZIQFxcHALh+/TrGjRsHLy8vaDQatGrVCtu3bzc59u7du9G8eXO4uLigb9++SEtLM1vH8+fPo0ePHgCAGjVqQJIkjB49GgDQvXt3REZGIjo6Gp6enujVqxeAssPkyjsPcxITE/HYY4/B2dkZ7u7uCAsLw4ULF6xqWyIiotsZ9RwmR2SP7LpnqFBvecYYyQFwVCoqFisBjqq7xyrVCrPrLZkxYwY++eQTvP/+++jcuTPS0tLw22+/ydtdXV0RFxcHX19f/Pzzzxg7dixcXV0xdepUhIeH45dffsGuXbuwd+9eAIBOp4PRaES/fv2Qk5ODNWvWoGHDhjh16hQUilt1y8vLw7vvvovPPvsMDg4OGDFiBCZPnoy1a9eWqWO9evUQHx+PIUOG4MyZM3Bzc4NWq5W3r1q1Ci+//DIOHToEIYTZ8yzvPO5UVFSEp59+GmPHjsX69ethMBjw/fffQ5Ikq9qWiIjodqU9QxwmR2Rf7DoZWj5xv8Vtfq1qYkBkW3n50ykHUGQwmo31beyOwa+1k5dXzzyMgtyyPRuvfvR4heuWk5ODxYsXY+nSpRg1ahQAoGHDhujcubMcM2vWLPl3f39/vPbaa9i4cSOmTp0KrVYLFxcXODo6mgxL27NnD77//nucPn0aTZo0AQA0aNDA5NiFhYX46KOP0LBhQwBAZGQk5s+fb7aeCoUCHh4eAIDatWuXmXyhUaNGePvtt8s91/LO407Z2dnIysrCgAED5Po1b9683PKJiIjKI4qKgJtTajuoVTauDRFVJ7tOhh5kp0+fhl6vxxNPPGEx5ssvv0RMTAz++OMP5ObmoqioCG5ubuWWm5ycjLp168qJkDlOTk5yogEAPj4+yMjIsP4kAISEhNw1xprz8PDwwOjRo9GnTx/06tULPXv2xPDhw+Hj41Op+hEREZX2CgGApNHYsCZEVN3sOhkat7ibxW3SHXdT/fudLpZj7xihNfLN0HupFgCYDDUz58iRI3j22Wcxb9489OnTBzqdDhs2bMCiRYvuqVwAUCqVJsuSJFkc4nY3zs7O5W6vzHmsXLkSEyZMwK5du7Bx40bMmjULCQkJ6NixY6XqSERE9s14ezKkYs8QkT2x62TImnt47lesJY0bN4ZWq8U333yDF198scz2Q4cOwc/PDzNnzpTX3TmJgEqlQvEdT9Ju06YNLl26hN9//73c3iFrqG7+47jzWBVRkfMwJygoCEFBQZgxYwY6deqEdevWMRkiIqJKke8XUiohOXBuKSJ7wr/4B5RGo8G0adMwdepUrF69GufOncORI0ewYsUKACX34qSmpmLDhg04d+4clixZgs2bN5uU4e/vj5SUFCQnJyMzMxN6vR7dunVD165dMWTIECQkJCAlJQU7d+7Erl27Kl1XPz8/SJKE7du34++//0Zubm6F963IedwuJSUFM2bMQFJSEi5cuIA9e/bg999/531DRERUaYIzyRHZLSZDD7DZs2fjtddew5w5c9C8eXOEh4fL9+489dRTmDRpEiIjIxEYGIjDhw9j9uzZJvsPGTIEffv2RY8ePVCrVi2sX78eABAfH4/27dvjueeeQ4sWLTB16tRK9eqUqlOnDubNm4fp06fDy8sLkZGRFd63IudxOycnJ/z2228YMmQImjRpgnHjxiEyMhIvvfRSpetPRET2TeHuDq/Zs1ArepKtq0JE1UwSlb0Z5AGTnZ0NnU6HrKysMjffFxQUICUlBQEBAdDwxki7wmtPREREZH/Kyw1ux54hIiIiIiKyS3Y9gQIRERFR0bVr0J89C4XOHZqmVTO5EBE9HNgzRERERHYt//hxpI4chfQ5c2xdFSKqZkyGiIiIyK5JKjVUDRpAWaeOratCRNXMrobJPSJzRZAVeM2JiOhuXLp0hkuXr21dDSKyAbvoGVIoSh6CajAYbFwTqm55eXkAAKVSaeOaEBEREdGDplI9Q7GxsXjnnXeQlpaGli1bIiYmBl26dDEbm5iYiB49epRZf/r0aTRr1kxejo+Px+zZs3Hu3Dk0bNgQb775JgYPHlyZ6pXh6OgIJycn/P3331AqlXDg06UfeUII5OXlISMjA+7u7nJCTERERERUyupkaOPGjYiKikJsbCzCwsLw8ccfo1+/fjh16hTq169vcb8zZ86YzPFdq1Yt+fekpCSEh4fjjTfewODBg7F582YMHz4cBw8eRIcOHaytYhmSJMHHxwcpKSm4cOHCPZdHDw93d3d4e3vbuhpERPQAu7bxc1xb8xlc+/RFrchXbV0dIqpGVj90tUOHDmjXrh2WLVsmr2vevDmefvppLFiwoEx8ac/QtWvX4O7ubrbM8PBwZGdnY+fOnfK6vn37okaNGli/fn2F6lWRBysZjUYOlbMjSqWSPUJERHRXf3+wFJkffgj3556Fz+uv27o6RFQFKvrQVat6hgwGA44dO4bp06ebrO/duzcOHz5c7r5BQUEoKChAixYtMGvWLJOhc0lJSZg0aZJJfJ8+fRATE2OxPL1eD71eLy9nZ2fftf4ODg7QaDR3jSMiIiL7IQwlnyccVGob14SIqptVN89kZmaiuLgYXl5eJuu9vLyQnp5udh8fHx8sX74c8fHx2LRpE5o2bYonnngC3333nRyTnp5uVZkAsGDBAuh0OvmnXr161pwKEREREQDAWFCSDEn8wpTI7lRqAgVJkkyWhRBl1pVq2rQpmjZtKi936tQJFy9exLvvvouuXbtWqkwAmDFjBqKjo+Xl7OxsJkRERERkNXFzpImkVtm4JkRU3azqGfL09IRCoSjTY5ORkVGmZ6c8HTt2xNmzZ+Vlb29vq8tUq9Vwc3Mz+SEiIiKyVmky5KDmMDkie2NVMqRSqRAcHIyEhAST9QkJCQgNDa1wOSdOnICPj4+83KlTpzJl7tmzx6oyiYiIiCrDWNozxHuGiOyO1cPkoqOjERERgZCQEHTq1AnLly9Hamoqxo8fD6Bk+Nrly5exevVqAEBMTAz8/f3RsmVLGAwGrFmzBvHx8YiPj5fLnDhxIrp27YqFCxfiqaeewtatW7F3714cPHiwik6TiIiIyDx5mJyGyRCRvbE6GQoPD8fVq1cxf/58pKWloVWrVtixYwf8/PwAAGlpaUhNTZXjDQYDJk+ejMuXL0Or1aJly5b4+uuv0b9/fzkmNDQUGzZswKxZszB79mw0bNgQGzdurJJnDBERERGVh8PkiOyX1c8ZelBVdC5xIiIiotudf34E8o8dQ52YGLj17WPr6hBRFahobmDVPUNEREREjxrOJkdkv5gMERERkV2Th8nxOUNEdofJEBEREdk1eTY53jNEZHcq9dBVIiIiokeFR0QEiq5mQnnbYz+IyD4wGSIiIiK75hExwtZVICIb4TA5IiIiIiKyS0yGiIiIyK4V/PYb9H+mQBQX27oqRFTNOEyOiIiI7JYoKkLK04MBAE2OHoFCp7NxjYioOjEZIiIiIrslDAYoanlC6A2QVHzOEJG9YTJEREREdsvByQlNDhywdTWIyEZ4zxAREREREdklJkNERERERGSXmAwRERGR3TKcP4/z/3oel6Im2boqRGQDvGeIiIiI7FZxdjbyjx+H0tfX1lUhIhtgzxARERHZLaHXAwAkjcbGNSEiW2AyRERERHbLWHAzGVKrbVwTIrIFJkNERERkt4ShJBly4DOGiOwSkyEiIiKyW/IwOfYMEdklJkNERERkt4x6AwBA0jAZIrJHTIaIiIjIbgl9AQDAgT1DRHaJyRARERHZLXmYnIrJEJE9YjJEREREdkseJseeISK7xGSIiIiI7JYouDlMjvcMEdklJkNERERkt0qn1uYwOSL7VKlkKDY2FgEBAdBoNAgODsaBAwcsxm7atAm9evVCrVq14Obmhk6dOmH37t0mMXFxcZAkqcxPwc1va4iIiIjuBw6TI7JvVidDGzduRFRUFGbOnIkTJ06gS5cu6NevH1JTU83Gf/fdd+jVqxd27NiBY8eOoUePHhg4cCBOnDhhEufm5oa0tDSTH41GU7mzIiIiIqoAp+Bg1PjXv6Bt29bWVSEiG5CEEMKaHTp06IB27dph2bJl8rrmzZvj6aefxoIFCypURsuWLREeHo45c+YAKOkZioqKwvXr162pions7GzodDpkZWXBzc2t0uUQEREREdHDraK5gVU9QwaDAceOHUPv3r1N1vfu3RuHDx+uUBlGoxE5OTnw8PAwWZ+bmws/Pz/UrVsXAwYMKNNzdCe9Xo/s7GyTHyIiIiIiooqyKhnKzMxEcXExvLy8TNZ7eXkhPT29QmUsWrQIN27cwPDhw+V1zZo1Q1xcHLZt24b169dDo9EgLCwMZ8+etVjOggULoNPp5J969epZcypEREREKPrnHxRlZsJoMNi6KkRkA5WaQEGSJJNlIUSZdeasX78ec+fOxcaNG1G7dm15fceOHTFixAi0bdsWXbp0weeff44mTZrggw8+sFjWjBkzkJWVJf9cvHixMqdCREREduzK5Ck427kLcnbtsnVViMgGHK0J9vT0hEKhKNMLlJGRUaa36E4bN27ECy+8gC+++AI9e/YsN9bBwQHt27cvt2dIrVZDzZlfiIiI6B4IYQTAqbWJ7JVVPUMqlQrBwcFISEgwWZ+QkIDQ0FCL+61fvx6jR4/GunXr8OSTT971OEIIJCcnw8fHx5rqEREREVnFb+VKNDt9Cq69e9m6KkRkA1b1DAFAdHQ0IiIiEBISgk6dOmH58uVITU3F+PHjAZQMX7t8+TJWr14NoCQRGjlyJBYvXoyOHTvKvUparRY6nQ4AMG/ePHTs2BGNGzdGdnY2lixZguTkZHz44YdVdZ5EREREZkmSBFRguD8RPXqsTobCw8Nx9epVzJ8/H2lpaWjVqhV27NgBPz8/AEBaWprJM4c+/vhjFBUV4dVXX8Wrr74qrx81ahTi4uIAANevX8e4ceOQnp4OnU6HoKAgfPfdd3jsscfu8fSIiIiIiIjMs/o5Qw8qPmeIiIiIrHVl+gwY8/NRe8oUqOrWsXV1iKiK3JfnDBERERE9SnITE5GzezdEQb6tq0JENsBkiIiIiOxW6fOFJM5QS2SXmAwRERGR3RJ6PQBOrU1kr5gMERERkV0SRUVAcTEAwEHDZIjIHjEZIiIiIrtkLNDLv3OYHJF9YjJEREREdkkYbkuGVCob1oSIbIXJEBEREdkl+X4hpRKSAz8SEdkj/uUTERGRXZKTIY3GxjUhIlthMkRERER2yViaDPF+ISK7xWSIiIiI7FJpz5AD7xcisltMhoiIiMguCfYMEdk9JkNERERkl4x6AwDeM0RkzxxtXQEiIiIiW3Cs6QG3QQOh9PaxdVWIyEYkIYSwdSWqQnZ2NnQ6HbKysuDm5mbr6hARERERkY1UNDfgMDkiIiIiIrJLHCZHREREdkkYDBC4+dBVSbJ1dYjIBtgzRERERHbp2vr1ONOmLa5MmWrrqhCRjTAZIiIiIrskzyan5nOGiOwVh8kRERGRXao5ZjRqPPcsIPG7YSJ7xWSIiIiI7JKkVEKhVNq6GkRkQ/wqhIiIiIiI7BJ7hoiIiMguXY+PR96JE3Dr3RsuXbvaujpEZAPsGSIiIiK7lPf9D8j6Mh76s2dtXRUishEmQ0RERGSXjHo9AEBSa2xcEyKylUolQ7GxsQgICIBGo0FwcDAOHDhQbvz+/fsRHBwMjUaDBg0a4KOPPioTEx8fjxYtWkCtVqNFixbYvHlzZapGREREVCFCToY4tTaRvbI6Gdq4cSOioqIwc+ZMnDhxAl26dEG/fv2QmppqNj4lJQX9+/dHly5dcOLECfz3v//FhAkTEB8fL8ckJSUhPDwcEREROHnyJCIiIjB8+HAcPXq08mdGREREVI7SZMhBrbZxTYjIViQhhLBmhw4dOqBdu3ZYtmyZvK558+Z4+umnsWDBgjLx06ZNw7Zt23D69Gl53fjx43Hy5EkkJSUBAMLDw5GdnY2dO3fKMX379kWNGjWwfv36CtUrOzsbOp0OWVlZcHNzs+aUqpQQAvmFxSjUF1uMkRwkOCpv5aHlxkqAo0pRqdgiQzEsXd37FQsASnUlYwuNEEbLwdbEOqocIEkSAKC40AhjVcUqHSA53IwtMsJYXDWxCqUDHCoRayw2orionFhHBzgoKhMrUFxkLCdWgoPCwfpYo0BxoeVYB4UEhaP1scIoUFRVsQ4SFDf/PoUQKDJUTaxVf/d8jzAfy/cIq2P5HlH+3/1f48bBcDIZnm8tgFvf3uXGyuXyPeK+xgJ8j6hM7IP0HqFVKuT2sKWK5gZWzSZnMBhw7NgxTJ8+3WR97969cfjwYbP7JCUloXfv3ibr+vTpgxUrVqCwsBBKpRJJSUmYNGlSmZiYmBiLddHr9dDf/EYHKDnhB0F+YTGC/vsVJuTqLMaoss+hxvlN8vJfrSYCDua76Av1GYjxcpWXp2YKCEcns7GOeWmo+ccaefnvZuNgVJmvh9FwHYtq3/ombOpf+RBqD7OxDoYs1Pptubx8tdEIFDn5mI1FUT7e8by1+FpaFhy03uZjjQZ4/bJYXrzm/wwMbg3NxwJ4xz1f/n3ilUyonOpZjK39cwwkUQgAyKrbDwUerSzGfuyci2xlyRvky5fT4eIcYDHW8/THUBSWvNZyfLohr9ZjFmO/VF1FilPJtRqTdgWeWsvn5nH2Myjz0wEAN2q1R65Pd4ux3zik47hbyXUdnn4FfhrL5bqnxEOd8ycAIL9GS2TX628x9kfjZezzKHkN9M+4gpYqy+W6XdwB7bVfAQB61wa4HjDEYuyZwovYVqvkRdH56l/opPC3GOuSlgjnv38AABRqvfFP4wiLsZcNF7Gudkm5rbKuop+oazHW6e/v4Zq2HwBQrHRDZvOXLMb+U3AJK7xrAgDq5GXjXwYvi7Gaf36B7lLJlzhCUiKjdZTF2BsFVxDrXQMAoC4y8D3iJr5H8D3Cpu8RulFA11FI3ZaOjYd3l9S9WMJLOZbvITqhKsJep5LXjtYIRGZrLcb+oizCTueSWKUAorIsx55RFmObs0FennLdcuw5x2JscrkVO/G6BiqY//CZqijGRtdbsa9maeAkzMemKYxY43rrs9W4LDV0wvwgokwHI1a63Yodk62Gp9F8bJZkxHLdrdgROWr4FJuPzZMEPtQVyMvhOSrUL1aYjTVAYLH7rdhnclVoWGQ+FjB9jxh0Q4WmhZZjY3T5KLzZTP1uKNGq0PJH5qVu+ci/eTo985QIMliO/di1ANmKkoSiW74jHtNbfs7Vp64FuHozNjTfEWHlxH7mUoB0x5LY9gWO6F5gOXaDsx4XlSWJeZBegZ75loeJxjvr8efN2JZ6BfqXE7vVSY/fVUacmt8HTqqHZ8Jqq2qamZmJ4uJieHmZfjjw8vJCenq62X3S09PNxhcVFSEzMxM+Pj4WYyyVCQALFizAvHnzrKl+tXEQlr/5AQDXwny0/Oe8vPy3ELC0h7qo0GRZYTSiyEKstshgUu5hYxEKLMSqjEUAbn3QURUXQl9O7O3l/lBkQI6FWIUw4vbRl9oivcVyHYQwKfdknXxctRB7J6eiAovtAADNr12Awljy5n+qdi4sv5IA4Na3HM5FllqsROOsS9AW/AMA+MM9COYHh5ZwNN769s3ZkA9Y/r+GBtlX4JZTUtoF50bILafckmtXwqUwHyjnvt/6OX/B82Ybp6m8Ud5XBuriW/8snQsLgHKG0Ne5cRU+N8vNhDOul1OutujWK8CpSA9Y/t8Dr7xr8LtZbrarEf+UV27hrWulKTaUO+jXMz8bjW6Wm6/xQGa59b1V7u3X0Bx3Qy5a3Cy32EGFjHLLvdW+fI/gewTA94hSD8J7RJ4jJ1AgsldWDZO7cuUK6tSpg8OHD6NTp07y+jfffBOfffYZfvvttzL7NGnSBGPGjMGMGTPkdYcOHULnzp2RlpYGb29vqFQqrFq1Cs8995wcs3btWrzwwgsoKDD/j8dcz1C9evUeiGFyeXkFyPlmv8UYSQIUt73ZF938v6Vq0waKmiXfSBdduYLCM2fg4FkTLiFBcmz2nm8AS92vEuBoplxls2Zw9Cn5lrYoMxOFP/8MBzdXuIR2kGNz9n0HoTfAEsfb0uaiYgACUDZqCMd69QEAxdlZMBw7Dkmjhmv3LrfKPXQEIsfyv+3byy0uBoQAHOvXg7JhIwCAMT8P+iNHAYUCbr16yLE3fjiO4quW/w0qFCXtfHu5Ch9vqJo1BwCIwkIUHDwIAHDp2R0ONyuS99MvKLpi+WORSblGQBgBh5o1oW7TRo7J358IGAWcu4RC4VzyrW/+6d9QeOGSxXIdFIDDneW6uUIdHCLHFBw+BKE3QNsxBEp3dwCA/o8/of/jT8vlOpT8AIDRWPIjadTQdAq9Ve6PP0Dk5ELbri2UtWsBAAwXL6Hg17J/z+WVC4UC2tue06E/eRLGf/6BpmVzqOrVKSn3rwwUnPjJYrmSA6AoLVcApZ8VNV27Qrr5R2M4fRrF6elQNWkETQN/AEDRtevIO/pjhcoVouQ1AQDqTh3hoCn5BFr4x1kUXbwEpX99aJs1AQAU37iBGweSLJd729/y7eWqQtpB4VryzXzRhfMo/DMFjr7ecGpT0vsgCgv5HlFaLt8jStbxPaKkXBu8Ryh8vKFp2YJDac3Ecpgch8nZwzA5q5Ihg8EAJycnfPHFFxg8eLC8fuLEiUhOTsb+/WX/uXft2hVBQUFYvPjWUIfNmzdj+PDhyMvLg1KpRP369TFp0iSToXLvv/8+YmJicOHChQrV7UG5Z4iIiIiIiGyrormBVbPJqVQqBAcHIyEhwWR9QkICQkNDze7TqVOnMvF79uxBSEgIlEpluTGWyiQiIiIiIrpXVt/dFB0djYiICISEhKBTp05Yvnw5UlNTMX78eADAjBkzcPnyZaxevRpAycxxS5cuRXR0NMaOHYukpCSsWLHCZJa4iRMnomvXrli4cCGeeuopbN26FXv37sXBm0MUiIiIiIiIqprVyVB4eDiuXr2K+fPnIy0tDa1atcKOHTvg5+cHAEhLSzN55lBAQAB27NiBSZMm4cMPP4Svry+WLFmCIUNuzSwTGhqKDRs2YNasWZg9ezYaNmyIjRs3okOHDmWOT0REREREVBWsfs7QgyorKwvu7u64ePEi7xkiIiIiIrJjpZOrXb9+HTqd5cdZPDyTgN9FTk7JRK716ll+rgQREREREdmPnJyccpOhR6ZnyGg04sqVK3B1dbX5dH6lmSh7qe4vtnP1YVtXD7Zz9WA7Vx+2dfVgO1cPtnP1qYq2FkIgJycHvr6+cHCwPGfcI9Mz5ODggLp1LT+B3hbc3Nz4x1IN2M7Vh21dPdjO1YPtXH3Y1tWD7Vw92M7V517burweoVJWTa1NRERERET0qGAyREREREREdonJ0H2gVqvx+uuvQ61W27oqjzS2c/VhW1cPtnP1YDtXH7Z19WA7Vw+2c/WpzrZ+ZCZQICIiIiIisgZ7hoiIiIiIyC4xGSIiIiIiIrvEZIiIiIiIiOwSkyEiIiIiIrJLTIaqWGxsLAICAqDRaBAcHIwDBw7YukoPtQULFqB9+/ZwdXVF7dq18fTTT+PMmTMmMUIIzJ07F76+vtBqtejevTt+/fVXG9X40bBgwQJIkoSoqCh5Hdu56ly+fBkjRoxAzZo14eTkhMDAQBw7dkzezra+d0VFRZg1axYCAgKg1WrRoEEDzJ8/H0ajUY5hO1fOd999h4EDB8LX1xeSJGHLli0m2yvSrnq9Hv/5z3/g6ekJZ2dnDBo0CJcuXarGs3jwldfOhYWFmDZtGlq3bg1nZ2f4+vpi5MiRuHLlikkZbOe7u9vr+XYvvfQSJElCTEyMyXq2c8VUpK1Pnz6NQYMGQafTwdXVFR07dkRqaqq8/X60NZOhKrRx40ZERUVh5syZOHHiBLp06YJ+/fqZXESyzv79+/Hqq6/iyJEjSEhIQFFREXr37o0bN27IMW+//Tbee+89LF26FD/88AO8vb3Rq1cv5OTk2LDmD68ffvgBy5cvR5s2bUzWs52rxrVr1xAWFgalUomdO3fi1KlTWLRoEdzd3eUYtvW9W7hwIT766CMsXboUp0+fxttvv4133nkHH3zwgRzDdq6cGzduoG3btli6dKnZ7RVp16ioKGzevBkbNmzAwYMHkZubiwEDBqC4uLi6TuOBV1475+Xl4fjx45g9ezaOHz+OTZs24ffff8egQYNM4tjOd3e313OpLVu24OjRo/D19S2zje1cMXdr63PnzqFz585o1qwZEhMTcfLkScyePRsajUaOuS9tLajKPPbYY2L8+PEm65o1ayamT59uoxo9ejIyMgQAsX//fiGEEEajUXh7e4u33npLjikoKBA6nU589NFHtqrmQysnJ0c0btxYJCQkiG7duomJEycKIdjOVWnatGmic+fOFrezravGk08+Kf7973+brHvmmWfEiBEjhBBs56oCQGzevFlerki7Xr9+XSiVSrFhwwY55vLly8LBwUHs2rWr2ur+MLmznc35/vvvBQBx4cIFIQTbuTIstfOlS5dEnTp1xC+//CL8/PzE+++/L29jO1eOubYODw+X36PNuV9tzZ6hKmIwGHDs2DH07t3bZH3v3r1x+PBhG9Xq0ZOVlQUA8PDwAACkpKQgPT3dpN3VajW6devGdq+EV199FU8++SR69uxpsp7tXHW2bduGkJAQDBs2DLVr10ZQUBA++eQTeTvbump07twZ33zzDX7//XcAwMmTJ3Hw4EH0798fANv5fqlIux47dgyFhYUmMb6+vmjVqhXb/h5kZWVBkiS5l5ntXDWMRiMiIiIwZcoUtGzZssx2tnPVMBqN+Prrr9GkSRP06dMHtWvXRocOHUyG0t2vtmYyVEUyMzNRXFwMLy8vk/VeXl5IT0+3Ua0eLUIIREdHo3PnzmjVqhUAyG3Ldr93GzZswPHjx7FgwYIy29jOVefPP//EsmXL0LhxY+zevRvjx4/HhAkTsHr1agBs66oybdo0PPfcc2jWrBmUSiWCgoIQFRWF5557DgDb+X6pSLump6dDpVKhRo0aFmPIOgUFBZg+fTr+9a9/wc3NDQDbuaosXLgQjo6OmDBhgtntbOeqkZGRgdzcXLz11lvo27cv9uzZg8GDB+OZZ57B/v37Ady/tna8p5pTGZIkmSwLIcqso8qJjIzETz/9hIMHD5bZxna/NxcvXsTEiROxZ88ek7G5d2I73zuj0YiQkBD873//AwAEBQXh119/xbJlyzBy5Eg5jm19bzZu3Ig1a9Zg3bp1aNmyJZKTkxEVFQVfX1+MGjVKjmM73x+VaVe2feUUFhbi2WefhdFoRGxs7F3j2c4Vd+zYMSxevBjHjx+3us3YztYpndzmqaeewqRJkwAAgYGBOHz4MD766CN069bN4r732tbsGaoinp6eUCgUZTLTjIyMMt+QkfX+85//YNu2bdi3bx/q1q0rr/f29gYAtvs9OnbsGDIyMhAcHAxHR0c4Ojpi//79WLJkCRwdHeW2ZDvfOx8fH7Ro0cJkXfPmzeWJVviarhpTpkzB9OnT8eyzz6J169aIiIjApEmT5J5PtvP9UZF29fb2hsFgwLVr1yzGUMUUFhZi+PDhSElJQUJCgtwrBLCdq8KBAweQkZGB+vXry/8bL1y4gNdeew3+/v4A2M5VxdPTE46Ojnf9/3g/2prJUBVRqVQIDg5GQkKCyfqEhASEhobaqFYPPyEEIiMjsWnTJnz77bcICAgw2R4QEABvb2+TdjcYDNi/fz/b3QpPPPEEfv75ZyQnJ8s/ISEheP7555GcnIwGDRqwnatIWFhYmenhf//9d/j5+QHga7qq5OXlwcHB9F+cQqGQv31kO98fFWnX4OBgKJVKk5i0tDT88ssvbHsrlCZCZ8+exd69e1GzZk2T7WznexcREYGffvrJ5H+jr68vpkyZgt27dwNgO1cVlUqF9u3bl/v/8b61daWnXqAyNmzYIJRKpVixYoU4deqUiIqKEs7OzuL8+fO2rtpD6+WXXxY6nU4kJiaKtLQ0+ScvL0+Oeeutt4ROpxObNm0SP//8s3juueeEj4+PyM7OtmHNH363zyYnBNu5qnz//ffC0dFRvPnmm+Ls2bNi7dq1wsnJSaxZs0aOYVvfu1GjRok6deqI7du3i5SUFLFp0ybh6ekppk6dKsewnSsnJydHnDhxQpw4cUIAEO+99544ceKEPItZRdp1/Pjxom7dumLv3r3i+PHj4vHHHxdt27YVRUVFtjqtB0557VxYWCgGDRok6tatK5KTk03+P+r1erkMtvPd3e31fKc7Z5MTgu1cUXdr602bNgmlUimWL18uzp49Kz744AOhUCjEgQMH5DLuR1szGapiH374ofDz8xMqlUq0a9dOngKaKgeA2Z+VK1fKMUajUbz++uvC29tbqNVq0bVrV/Hzzz/brtKPiDuTIbZz1fnqq69Eq1athFqtFs2aNRPLly832c62vnfZ2dli4sSJon79+kKj0YgGDRqImTNnmnxQZDtXzr59+8y+L48aNUoIUbF2zc/PF5GRkcLDw0NotVoxYMAAkZqaaoOzeXCV184pKSkW/z/u27dPLoPtfHd3ez3fyVwyxHaumIq09YoVK0SjRo2ERqMRbdu2FVu2bDEp4360tSSEEJXvVyIiIiIiIno48Z4hIiIiIiKyS0yGiIiIiIjILjEZIiIiIiIiu8RkiIiIiIiI7BKTISIiIiIisktMhoiIiIiIyC4xGSIiIiIiIrvEZIiIiIiIiOwSkyEiIiIiIrJLTIaIiIiIiMguMRkiIiIiIiK7xGSIiIiIiIjs0v8D7JqkZSLvghkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize inputs and targets\n",
    "\n",
    "inputs, targets = task.get_batch()\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "channel_names = [\"Left\", \"Right\", \"Go\"]\n",
    "\n",
    "for i in range(input_size):  # left, right, go\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(inputs[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(inputs[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Input Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(targets[i,:,:n_r], axis=1), c='tab:blue', label=\"right trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left trials\")\n",
    "    plt.plot(np.average(targets[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch trials\")\n",
    "    plt.title(f\"Output Channle: {channel_names[i]}\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: \u001b[92m3.950294\u001b[0m, validation loss: \u001b[92m3.299102\u001b[0m\n",
      "iteration 1, train loss: \u001b[92m3.309696\u001b[0m, validation loss: \u001b[92m3.109713\u001b[0m\n",
      "iteration 2, train loss: \u001b[92m3.114107\u001b[0m, validation loss: \u001b[92m2.904701\u001b[0m\n",
      "iteration 3, train loss: \u001b[92m2.908978\u001b[0m, validation loss: \u001b[92m2.694427\u001b[0m\n",
      "iteration 4, train loss: \u001b[92m2.699446\u001b[0m, validation loss: \u001b[92m2.489055\u001b[0m\n",
      "iteration 5, train loss: \u001b[92m2.492042\u001b[0m, validation loss: \u001b[92m2.287998\u001b[0m\n",
      "iteration 6, train loss: \u001b[92m2.289707\u001b[0m, validation loss: \u001b[92m2.08718\u001b[0m\n",
      "iteration 7, train loss: \u001b[92m2.091123\u001b[0m, validation loss: \u001b[92m1.895389\u001b[0m\n",
      "iteration 8, train loss: \u001b[92m1.897499\u001b[0m, validation loss: \u001b[92m1.713503\u001b[0m\n",
      "iteration 9, train loss: \u001b[92m1.7165\u001b[0m, validation loss: \u001b[92m1.543508\u001b[0m\n",
      "iteration 10, train loss: \u001b[92m1.545153\u001b[0m, validation loss: \u001b[92m1.387069\u001b[0m\n",
      "iteration 11, train loss: \u001b[92m1.389339\u001b[0m, validation loss: \u001b[92m1.2387\u001b[0m\n",
      "iteration 12, train loss: \u001b[92m1.240703\u001b[0m, validation loss: \u001b[92m1.099665\u001b[0m\n",
      "iteration 13, train loss: \u001b[92m1.103115\u001b[0m, validation loss: \u001b[92m0.971426\u001b[0m\n",
      "iteration 14, train loss: \u001b[92m0.973235\u001b[0m, validation loss: \u001b[92m0.853014\u001b[0m\n",
      "iteration 15, train loss: \u001b[92m0.857147\u001b[0m, validation loss: \u001b[92m0.745236\u001b[0m\n",
      "iteration 16, train loss: \u001b[92m0.748278\u001b[0m, validation loss: \u001b[92m0.649395\u001b[0m\n",
      "iteration 17, train loss: \u001b[92m0.651626\u001b[0m, validation loss: \u001b[92m0.564725\u001b[0m\n",
      "iteration 18, train loss: \u001b[92m0.566357\u001b[0m, validation loss: \u001b[92m0.488793\u001b[0m\n",
      "iteration 19, train loss: \u001b[92m0.491051\u001b[0m, validation loss: \u001b[92m0.421138\u001b[0m\n",
      "iteration 20, train loss: \u001b[92m0.422256\u001b[0m, validation loss: \u001b[92m0.362842\u001b[0m\n",
      "iteration 21, train loss: \u001b[92m0.364176\u001b[0m, validation loss: \u001b[92m0.313593\u001b[0m\n",
      "iteration 22, train loss: \u001b[92m0.31714\u001b[0m, validation loss: \u001b[92m0.272381\u001b[0m\n",
      "iteration 23, train loss: \u001b[92m0.274736\u001b[0m, validation loss: \u001b[92m0.23784\u001b[0m\n",
      "iteration 24, train loss: \u001b[92m0.239035\u001b[0m, validation loss: \u001b[92m0.211558\u001b[0m\n",
      "iteration 25, train loss: \u001b[92m0.214689\u001b[0m, validation loss: \u001b[92m0.18992\u001b[0m\n",
      "iteration 26, train loss: \u001b[92m0.193122\u001b[0m, validation loss: \u001b[92m0.171626\u001b[0m\n",
      "iteration 27, train loss: \u001b[92m0.173865\u001b[0m, validation loss: \u001b[92m0.155997\u001b[0m\n",
      "iteration 28, train loss: \u001b[92m0.159425\u001b[0m, validation loss: \u001b[92m0.14225\u001b[0m\n",
      "iteration 29, train loss: \u001b[92m0.146592\u001b[0m, validation loss: \u001b[92m0.131423\u001b[0m\n",
      "iteration 30, train loss: \u001b[92m0.132673\u001b[0m, validation loss: \u001b[92m0.123607\u001b[0m\n",
      "iteration 31, train loss: \u001b[92m0.126909\u001b[0m, validation loss: \u001b[92m0.116686\u001b[0m\n",
      "iteration 32, train loss: \u001b[92m0.121641\u001b[0m, validation loss: \u001b[92m0.110548\u001b[0m\n",
      "iteration 33, train loss: \u001b[92m0.113625\u001b[0m, validation loss: \u001b[92m0.105216\u001b[0m\n",
      "iteration 34, train loss: \u001b[92m0.109744\u001b[0m, validation loss: \u001b[92m0.100324\u001b[0m\n",
      "iteration 35, train loss: \u001b[92m0.103799\u001b[0m, validation loss: \u001b[92m0.095793\u001b[0m\n",
      "iteration 36, train loss: \u001b[92m0.099906\u001b[0m, validation loss: \u001b[92m0.09148\u001b[0m\n",
      "iteration 37, train loss: \u001b[92m0.095679\u001b[0m, validation loss: \u001b[92m0.087392\u001b[0m\n",
      "iteration 38, train loss: \u001b[92m0.092851\u001b[0m, validation loss: \u001b[92m0.083616\u001b[0m\n",
      "iteration 39, train loss: \u001b[92m0.089633\u001b[0m, validation loss: \u001b[92m0.080024\u001b[0m\n",
      "iteration 40, train loss: \u001b[92m0.085123\u001b[0m, validation loss: \u001b[92m0.076599\u001b[0m\n",
      "iteration 41, train loss: \u001b[92m0.082817\u001b[0m, validation loss: \u001b[92m0.073404\u001b[0m\n",
      "iteration 42, train loss: \u001b[92m0.077468\u001b[0m, validation loss: \u001b[92m0.070457\u001b[0m\n",
      "iteration 43, train loss: \u001b[92m0.0774\u001b[0m, validation loss: \u001b[92m0.067711\u001b[0m\n",
      "iteration 44, train loss: \u001b[92m0.073759\u001b[0m, validation loss: \u001b[92m0.065261\u001b[0m\n",
      "iteration 45, train loss: \u001b[92m0.070721\u001b[0m, validation loss: \u001b[92m0.063063\u001b[0m\n",
      "iteration 46, train loss: \u001b[92m0.067938\u001b[0m, validation loss: \u001b[92m0.061122\u001b[0m\n",
      "iteration 47, train loss: \u001b[92m0.067919\u001b[0m, validation loss: \u001b[92m0.059476\u001b[0m\n",
      "iteration 48, train loss: \u001b[92m0.065245\u001b[0m, validation loss: \u001b[92m0.058142\u001b[0m\n",
      "iteration 49, train loss: \u001b[92m0.063297\u001b[0m, validation loss: \u001b[92m0.057134\u001b[0m\n",
      "iteration 50, train loss: 0.064126, validation loss: \u001b[92m0.056447\u001b[0m\n",
      "iteration 51, train loss: \u001b[92m0.062785\u001b[0m, validation loss: \u001b[92m0.056061\u001b[0m\n",
      "iteration 52, train loss: \u001b[92m0.062424\u001b[0m, validation loss: \u001b[92m0.055923\u001b[0m\n",
      "iteration 53, train loss: 0.063623, validation loss: \u001b[92m0.055905\u001b[0m\n",
      "iteration 54, train loss: 0.063297, validation loss: \u001b[92m0.055895\u001b[0m\n",
      "iteration 55, train loss: \u001b[92m0.061603\u001b[0m, validation loss: \u001b[92m0.055803\u001b[0m\n",
      "iteration 56, train loss: 0.06228, validation loss: \u001b[92m0.055605\u001b[0m\n",
      "iteration 57, train loss: \u001b[92m0.061079\u001b[0m, validation loss: \u001b[92m0.055292\u001b[0m\n",
      "iteration 58, train loss: \u001b[92m0.060923\u001b[0m, validation loss: \u001b[92m0.054887\u001b[0m\n",
      "iteration 59, train loss: \u001b[92m0.059856\u001b[0m, validation loss: \u001b[92m0.054466\u001b[0m\n",
      "iteration 60, train loss: 0.06172, validation loss: \u001b[92m0.054124\u001b[0m\n",
      "iteration 61, train loss: \u001b[92m0.059608\u001b[0m, validation loss: \u001b[92m0.053842\u001b[0m\n",
      "iteration 62, train loss: \u001b[92m0.059577\u001b[0m, validation loss: \u001b[92m0.053662\u001b[0m\n",
      "iteration 63, train loss: \u001b[92m0.058792\u001b[0m, validation loss: \u001b[92m0.053528\u001b[0m\n",
      "iteration 64, train loss: 0.059898, validation loss: \u001b[92m0.053442\u001b[0m\n",
      "iteration 65, train loss: 0.059669, validation loss: \u001b[92m0.053395\u001b[0m\n",
      "iteration 66, train loss: 0.05884, validation loss: \u001b[92m0.053376\u001b[0m\n",
      "iteration 67, train loss: 0.058985, validation loss: \u001b[92m0.053374\u001b[0m\n",
      "iteration 68, train loss: 0.059819, validation loss: 0.05338\n",
      "iteration 69, train loss: 0.059458, validation loss: 0.053388\n",
      "iteration 70, train loss: 0.059365, validation loss: 0.053393\n",
      "iteration 71, train loss: 0.059891, validation loss: 0.053386\n",
      "iteration 72, train loss: \u001b[92m0.058621\u001b[0m, validation loss: \u001b[92m0.05337\u001b[0m\n",
      "iteration 73, train loss: 0.059765, validation loss: \u001b[92m0.053349\u001b[0m\n",
      "iteration 74, train loss: 0.05969, validation loss: \u001b[92m0.053318\u001b[0m\n",
      "iteration 75, train loss: 0.058661, validation loss: \u001b[92m0.053283\u001b[0m\n",
      "iteration 76, train loss: \u001b[92m0.058198\u001b[0m, validation loss: \u001b[92m0.053244\u001b[0m\n",
      "iteration 77, train loss: 0.058502, validation loss: \u001b[92m0.053208\u001b[0m\n",
      "iteration 78, train loss: 0.058654, validation loss: \u001b[92m0.053174\u001b[0m\n",
      "iteration 79, train loss: 0.05994, validation loss: \u001b[92m0.053147\u001b[0m\n",
      "iteration 80, train loss: 0.058834, validation loss: \u001b[92m0.053127\u001b[0m\n",
      "iteration 81, train loss: 0.059268, validation loss: \u001b[92m0.053113\u001b[0m\n",
      "iteration 82, train loss: 0.058571, validation loss: \u001b[92m0.053108\u001b[0m\n",
      "iteration 83, train loss: 0.058738, validation loss: \u001b[92m0.053108\u001b[0m\n",
      "iteration 84, train loss: 0.058504, validation loss: 0.053113\n",
      "iteration 85, train loss: 0.058286, validation loss: 0.053116\n",
      "iteration 86, train loss: 0.058216, validation loss: 0.053118\n",
      "iteration 87, train loss: \u001b[92m0.057813\u001b[0m, validation loss: 0.053119\n",
      "iteration 88, train loss: 0.058066, validation loss: 0.053114\n",
      "iteration 89, train loss: 0.059041, validation loss: \u001b[92m0.053107\u001b[0m\n",
      "iteration 90, train loss: 0.059278, validation loss: \u001b[92m0.053094\u001b[0m\n",
      "iteration 91, train loss: 0.059073, validation loss: \u001b[92m0.053079\u001b[0m\n",
      "iteration 92, train loss: \u001b[92m0.057765\u001b[0m, validation loss: \u001b[92m0.053059\u001b[0m\n",
      "iteration 93, train loss: \u001b[92m0.057475\u001b[0m, validation loss: \u001b[92m0.053039\u001b[0m\n",
      "iteration 94, train loss: 0.058426, validation loss: \u001b[92m0.053024\u001b[0m\n",
      "iteration 95, train loss: 0.059206, validation loss: \u001b[92m0.053004\u001b[0m\n",
      "iteration 96, train loss: \u001b[92m0.057331\u001b[0m, validation loss: \u001b[92m0.052986\u001b[0m\n",
      "iteration 97, train loss: 0.058689, validation loss: \u001b[92m0.052969\u001b[0m\n",
      "iteration 98, train loss: 0.058598, validation loss: \u001b[92m0.052955\u001b[0m\n",
      "iteration 99, train loss: 0.05804, validation loss: \u001b[92m0.052943\u001b[0m\n",
      "iteration 100, train loss: 0.058528, validation loss: \u001b[92m0.052932\u001b[0m\n",
      "iteration 101, train loss: 0.058202, validation loss: \u001b[92m0.052924\u001b[0m\n",
      "iteration 102, train loss: 0.057676, validation loss: \u001b[92m0.052917\u001b[0m\n",
      "iteration 103, train loss: \u001b[92m0.057331\u001b[0m, validation loss: \u001b[92m0.052911\u001b[0m\n",
      "iteration 104, train loss: \u001b[92m0.057265\u001b[0m, validation loss: \u001b[92m0.052906\u001b[0m\n",
      "iteration 105, train loss: 0.058184, validation loss: \u001b[92m0.0529\u001b[0m\n",
      "iteration 106, train loss: 0.058213, validation loss: \u001b[92m0.052893\u001b[0m\n",
      "iteration 107, train loss: 0.05786, validation loss: \u001b[92m0.052887\u001b[0m\n",
      "iteration 108, train loss: 0.057346, validation loss: \u001b[92m0.05288\u001b[0m\n",
      "iteration 109, train loss: 0.058057, validation loss: \u001b[92m0.052874\u001b[0m\n",
      "iteration 110, train loss: 0.058781, validation loss: \u001b[92m0.052866\u001b[0m\n",
      "iteration 111, train loss: 0.057716, validation loss: \u001b[92m0.052859\u001b[0m\n",
      "iteration 112, train loss: 0.05921, validation loss: \u001b[92m0.052852\u001b[0m\n",
      "iteration 113, train loss: 0.057841, validation loss: \u001b[92m0.052845\u001b[0m\n",
      "iteration 114, train loss: \u001b[92m0.056947\u001b[0m, validation loss: \u001b[92m0.052839\u001b[0m\n",
      "iteration 115, train loss: 0.058033, validation loss: \u001b[92m0.052833\u001b[0m\n",
      "iteration 116, train loss: 0.058608, validation loss: \u001b[92m0.052828\u001b[0m\n",
      "iteration 117, train loss: 0.057958, validation loss: \u001b[92m0.052823\u001b[0m\n",
      "iteration 118, train loss: 0.057041, validation loss: \u001b[92m0.05282\u001b[0m\n",
      "iteration 119, train loss: 0.057465, validation loss: \u001b[92m0.052818\u001b[0m\n",
      "iteration 120, train loss: 0.057798, validation loss: \u001b[92m0.052816\u001b[0m\n",
      "iteration 121, train loss: 0.057634, validation loss: \u001b[92m0.052813\u001b[0m\n",
      "iteration 122, train loss: 0.057411, validation loss: \u001b[92m0.052806\u001b[0m\n",
      "iteration 123, train loss: 0.05737, validation loss: \u001b[92m0.052797\u001b[0m\n",
      "iteration 124, train loss: 0.057164, validation loss: \u001b[92m0.052786\u001b[0m\n",
      "iteration 125, train loss: 0.057814, validation loss: \u001b[92m0.052771\u001b[0m\n",
      "iteration 126, train loss: 0.057674, validation loss: \u001b[92m0.052755\u001b[0m\n",
      "iteration 127, train loss: \u001b[92m0.056466\u001b[0m, validation loss: \u001b[92m0.052733\u001b[0m\n",
      "iteration 128, train loss: 0.058299, validation loss: \u001b[92m0.052709\u001b[0m\n",
      "iteration 129, train loss: 0.057146, validation loss: \u001b[92m0.052689\u001b[0m\n",
      "iteration 130, train loss: 0.057185, validation loss: \u001b[92m0.052671\u001b[0m\n",
      "iteration 131, train loss: 0.056868, validation loss: \u001b[92m0.052655\u001b[0m\n",
      "iteration 132, train loss: \u001b[92m0.056411\u001b[0m, validation loss: \u001b[92m0.052642\u001b[0m\n",
      "iteration 133, train loss: 0.057545, validation loss: \u001b[92m0.05263\u001b[0m\n",
      "iteration 134, train loss: 0.05691, validation loss: \u001b[92m0.052618\u001b[0m\n",
      "iteration 135, train loss: 0.057038, validation loss: \u001b[92m0.052607\u001b[0m\n",
      "iteration 136, train loss: 0.057277, validation loss: \u001b[92m0.052597\u001b[0m\n",
      "iteration 137, train loss: 0.056515, validation loss: \u001b[92m0.052586\u001b[0m\n",
      "iteration 138, train loss: 0.058482, validation loss: \u001b[92m0.052576\u001b[0m\n",
      "iteration 139, train loss: 0.056757, validation loss: \u001b[92m0.052567\u001b[0m\n",
      "iteration 140, train loss: 0.056803, validation loss: \u001b[92m0.052558\u001b[0m\n",
      "iteration 141, train loss: 0.057427, validation loss: \u001b[92m0.052549\u001b[0m\n",
      "iteration 142, train loss: 0.05668, validation loss: \u001b[92m0.05254\u001b[0m\n",
      "iteration 143, train loss: 0.057783, validation loss: \u001b[92m0.052531\u001b[0m\n",
      "iteration 144, train loss: 0.056701, validation loss: \u001b[92m0.052522\u001b[0m\n",
      "iteration 145, train loss: \u001b[92m0.056236\u001b[0m, validation loss: \u001b[92m0.052514\u001b[0m\n",
      "iteration 146, train loss: 0.057021, validation loss: \u001b[92m0.052508\u001b[0m\n",
      "iteration 147, train loss: \u001b[92m0.055914\u001b[0m, validation loss: \u001b[92m0.052502\u001b[0m\n",
      "iteration 148, train loss: 0.057128, validation loss: \u001b[92m0.052497\u001b[0m\n",
      "iteration 149, train loss: 0.057066, validation loss: \u001b[92m0.052489\u001b[0m\n",
      "iteration 150, train loss: 0.057199, validation loss: \u001b[92m0.052479\u001b[0m\n",
      "iteration 151, train loss: 0.05659, validation loss: \u001b[92m0.052472\u001b[0m\n",
      "iteration 152, train loss: 0.056535, validation loss: \u001b[92m0.052466\u001b[0m\n",
      "iteration 153, train loss: 0.057245, validation loss: \u001b[92m0.052461\u001b[0m\n",
      "iteration 154, train loss: \u001b[92m0.05585\u001b[0m, validation loss: \u001b[92m0.052457\u001b[0m\n",
      "iteration 155, train loss: 0.056473, validation loss: \u001b[92m0.052453\u001b[0m\n",
      "iteration 156, train loss: 0.056943, validation loss: \u001b[92m0.05245\u001b[0m\n",
      "iteration 157, train loss: 0.056125, validation loss: \u001b[92m0.052447\u001b[0m\n",
      "iteration 158, train loss: 0.056737, validation loss: \u001b[92m0.052444\u001b[0m\n",
      "iteration 159, train loss: 0.056264, validation loss: \u001b[92m0.052442\u001b[0m\n",
      "iteration 160, train loss: 0.056264, validation loss: \u001b[92m0.052441\u001b[0m\n",
      "iteration 161, train loss: 0.057293, validation loss: \u001b[92m0.052441\u001b[0m\n",
      "iteration 162, train loss: 0.056581, validation loss: \u001b[92m0.05244\u001b[0m\n",
      "iteration 163, train loss: 0.057138, validation loss: \u001b[92m0.052439\u001b[0m\n",
      "iteration 164, train loss: 0.056662, validation loss: \u001b[92m0.052439\u001b[0m\n",
      "iteration 165, train loss: 0.057044, validation loss: 0.052441\n",
      "iteration 166, train loss: \u001b[92m0.05582\u001b[0m, validation loss: 0.052447\n",
      "iteration 167, train loss: 0.056213, validation loss: 0.052453\n",
      "iteration 168, train loss: 0.056175, validation loss: 0.052461\n",
      "iteration 169, train loss: 0.056325, validation loss: 0.052469\n",
      "iteration 170, train loss: 0.056577, validation loss: 0.052476\n",
      "iteration 171, train loss: 0.056385, validation loss: 0.052483\n",
      "iteration 172, train loss: 0.05613, validation loss: 0.052489\n",
      "iteration 173, train loss: 0.056743, validation loss: 0.05248\n",
      "iteration 174, train loss: 0.056078, validation loss: 0.052469\n",
      "iteration 175, train loss: 0.056018, validation loss: 0.052457\n",
      "iteration 176, train loss: 0.056223, validation loss: 0.052449\n",
      "iteration 177, train loss: 0.055925, validation loss: 0.052451\n",
      "iteration 178, train loss: 0.05595, validation loss: 0.052462\n",
      "iteration 179, train loss: 0.05648, validation loss: 0.052476\n",
      "iteration 180, train loss: 0.055928, validation loss: 0.052478\n",
      "iteration 181, train loss: \u001b[92m0.055682\u001b[0m, validation loss: 0.052471\n",
      "iteration 182, train loss: 0.056576, validation loss: 0.052463\n",
      "iteration 183, train loss: 0.05695, validation loss: 0.052459\n",
      "iteration 184, train loss: 0.055897, validation loss: 0.052463\n",
      "iteration 185, train loss: 0.055904, validation loss: 0.052471\n",
      "iteration 186, train loss: 0.056585, validation loss: 0.05248\n",
      "iteration 187, train loss: 0.055807, validation loss: 0.052487\n",
      "iteration 188, train loss: 0.055835, validation loss: 0.05249\n",
      "iteration 189, train loss: 0.056622, validation loss: 0.052493\n",
      "iteration 190, train loss: 0.056619, validation loss: 0.052491\n",
      "iteration 191, train loss: 0.056345, validation loss: 0.052492\n",
      "iteration 192, train loss: 0.055982, validation loss: 0.052492\n",
      "iteration 193, train loss: 0.055704, validation loss: 0.052489\n",
      "iteration 194, train loss: 0.056104, validation loss: 0.052488\n",
      "iteration 195, train loss: 0.055843, validation loss: 0.052489\n",
      "iteration 196, train loss: 0.056046, validation loss: 0.052491\n",
      "iteration 197, train loss: \u001b[92m0.05566\u001b[0m, validation loss: 0.052493\n",
      "iteration 198, train loss: 0.056377, validation loss: 0.052495\n",
      "iteration 199, train loss: 0.056089, validation loss: 0.052496\n",
      "iteration 200, train loss: 0.05572, validation loss: 0.052499\n",
      "iteration 201, train loss: 0.055929, validation loss: 0.052502\n",
      "iteration 202, train loss: 0.056188, validation loss: 0.052504\n",
      "iteration 203, train loss: 0.056704, validation loss: 0.052504\n",
      "iteration 204, train loss: 0.056232, validation loss: 0.052502\n",
      "iteration 205, train loss: 0.05607, validation loss: 0.052503\n",
      "iteration 206, train loss: 0.055821, validation loss: 0.052503\n",
      "iteration 207, train loss: 0.055901, validation loss: 0.052506\n",
      "iteration 208, train loss: 0.056168, validation loss: 0.052508\n",
      "iteration 209, train loss: 0.05646, validation loss: 0.05251\n",
      "iteration 210, train loss: 0.05629, validation loss: 0.052516\n",
      "iteration 211, train loss: 0.055777, validation loss: 0.05252\n",
      "iteration 212, train loss: 0.056067, validation loss: 0.052518\n",
      "iteration 213, train loss: 0.056066, validation loss: 0.052513\n",
      "iteration 214, train loss: \u001b[92m0.055504\u001b[0m, validation loss: 0.052504\n",
      "iteration 215, train loss: 0.056185, validation loss: 0.052498\n",
      "iteration 216, train loss: 0.055901, validation loss: 0.0525\n",
      "iteration 217, train loss: 0.056201, validation loss: 0.05251\n",
      "iteration 218, train loss: 0.05563, validation loss: 0.052514\n",
      "iteration 219, train loss: 0.055791, validation loss: 0.052508\n",
      "iteration 220, train loss: 0.055863, validation loss: 0.052501\n",
      "iteration 221, train loss: 0.05602, validation loss: 0.052495\n",
      "iteration 222, train loss: 0.055945, validation loss: 0.052492\n",
      "iteration 223, train loss: 0.056179, validation loss: 0.052496\n",
      "iteration 224, train loss: 0.056181, validation loss: 0.052505\n",
      "iteration 225, train loss: 0.055776, validation loss: 0.052511\n",
      "iteration 226, train loss: 0.056078, validation loss: 0.052514\n",
      "iteration 227, train loss: 0.056435, validation loss: 0.052515\n",
      "iteration 228, train loss: 0.055599, validation loss: 0.052515\n",
      "iteration 229, train loss: 0.055709, validation loss: 0.052511\n",
      "iteration 230, train loss: 0.05573, validation loss: 0.0525\n",
      "iteration 231, train loss: 0.05599, validation loss: 0.052492\n",
      "iteration 232, train loss: 0.055534, validation loss: 0.052488\n",
      "iteration 233, train loss: \u001b[92m0.055417\u001b[0m, validation loss: 0.052488\n",
      "iteration 234, train loss: 0.056106, validation loss: 0.052486\n",
      "iteration 235, train loss: \u001b[92m0.054962\u001b[0m, validation loss: 0.052485\n",
      "iteration 236, train loss: 0.055628, validation loss: 0.052485\n",
      "iteration 237, train loss: 0.055844, validation loss: 0.052491\n",
      "iteration 238, train loss: 0.055614, validation loss: 0.052497\n",
      "iteration 239, train loss: 0.055889, validation loss: 0.052502\n",
      "iteration 240, train loss: 0.055622, validation loss: 0.052501\n",
      "iteration 241, train loss: 0.055286, validation loss: 0.052499\n",
      "iteration 242, train loss: 0.055688, validation loss: 0.052496\n",
      "iteration 243, train loss: 0.055427, validation loss: 0.052492\n",
      "iteration 244, train loss: 0.055547, validation loss: 0.052489\n",
      "iteration 245, train loss: 0.055565, validation loss: 0.052488\n",
      "iteration 246, train loss: 0.055686, validation loss: 0.052487\n",
      "iteration 247, train loss: 0.055653, validation loss: 0.052487\n",
      "iteration 248, train loss: 0.0561, validation loss: 0.052486\n",
      "iteration 249, train loss: 0.055478, validation loss: 0.052487\n",
      "iteration 250, train loss: 0.055481, validation loss: 0.052487\n",
      "iteration 251, train loss: 0.055866, validation loss: 0.052486\n",
      "iteration 252, train loss: 0.055909, validation loss: 0.052486\n",
      "iteration 253, train loss: \u001b[92m0.05491\u001b[0m, validation loss: 0.052486\n",
      "iteration 254, train loss: 0.055801, validation loss: 0.052485\n",
      "iteration 255, train loss: 0.056137, validation loss: 0.052484\n",
      "iteration 256, train loss: 0.055335, validation loss: 0.052485\n",
      "iteration 257, train loss: 0.055181, validation loss: 0.052489\n",
      "iteration 258, train loss: 0.055486, validation loss: 0.052495\n",
      "iteration 259, train loss: 0.055591, validation loss: 0.052496\n",
      "iteration 260, train loss: 0.055426, validation loss: 0.052493\n",
      "iteration 261, train loss: 0.055295, validation loss: 0.05249\n",
      "iteration 262, train loss: 0.055244, validation loss: 0.052493\n",
      "iteration 263, train loss: 0.056339, validation loss: 0.052493\n",
      "iteration 264, train loss: 0.055958, validation loss: 0.05249\n",
      "iteration 265, train loss: 0.055454, validation loss: 0.05249\n",
      "iteration 266, train loss: 0.055885, validation loss: 0.052491\n",
      "iteration 267, train loss: 0.055323, validation loss: 0.052493\n",
      "iteration 268, train loss: 0.055007, validation loss: 0.052494\n",
      "iteration 269, train loss: 0.055472, validation loss: 0.052497\n",
      "iteration 270, train loss: 0.055978, validation loss: 0.052501\n",
      "iteration 271, train loss: 0.055468, validation loss: 0.052507\n",
      "iteration 272, train loss: 0.055489, validation loss: 0.052515\n",
      "iteration 273, train loss: 0.055111, validation loss: 0.052512\n",
      "iteration 274, train loss: 0.055355, validation loss: 0.052506\n",
      "iteration 275, train loss: 0.055699, validation loss: 0.052498\n",
      "iteration 276, train loss: 0.055861, validation loss: 0.052492\n",
      "iteration 277, train loss: 0.055462, validation loss: 0.052488\n",
      "iteration 278, train loss: \u001b[92m0.054865\u001b[0m, validation loss: 0.052482\n",
      "iteration 279, train loss: 0.055667, validation loss: 0.052479\n",
      "iteration 280, train loss: 0.055647, validation loss: 0.052477\n",
      "iteration 281, train loss: 0.055648, validation loss: 0.052472\n",
      "iteration 282, train loss: 0.055277, validation loss: 0.052471\n",
      "iteration 283, train loss: 0.055261, validation loss: 0.052471\n",
      "iteration 284, train loss: \u001b[92m0.054812\u001b[0m, validation loss: 0.05247\n",
      "iteration 285, train loss: 0.055423, validation loss: 0.05247\n",
      "iteration 286, train loss: 0.055001, validation loss: 0.052472\n",
      "iteration 287, train loss: 0.055536, validation loss: 0.052476\n",
      "iteration 288, train loss: 0.056071, validation loss: 0.052478\n",
      "iteration 289, train loss: 0.055463, validation loss: 0.052483\n",
      "iteration 290, train loss: 0.055167, validation loss: 0.052484\n",
      "iteration 291, train loss: 0.055742, validation loss: 0.05248\n",
      "iteration 292, train loss: 0.055346, validation loss: 0.052476\n",
      "iteration 293, train loss: 0.055375, validation loss: 0.052466\n",
      "iteration 294, train loss: 0.055582, validation loss: 0.052458\n",
      "iteration 295, train loss: 0.055249, validation loss: 0.052455\n",
      "iteration 296, train loss: 0.05602, validation loss: 0.052455\n",
      "iteration 297, train loss: 0.055367, validation loss: 0.052455\n",
      "iteration 298, train loss: 0.055336, validation loss: 0.052454\n",
      "iteration 299, train loss: 0.055733, validation loss: 0.052453\n",
      "iteration 300, train loss: 0.055521, validation loss: 0.052455\n",
      "iteration 301, train loss: 0.055505, validation loss: 0.052462\n",
      "iteration 302, train loss: 0.05483, validation loss: 0.052469\n",
      "iteration 303, train loss: 0.055394, validation loss: 0.052473\n",
      "iteration 304, train loss: 0.054875, validation loss: 0.052471\n",
      "iteration 305, train loss: 0.055518, validation loss: 0.052471\n",
      "iteration 306, train loss: 0.055168, validation loss: 0.052468\n",
      "iteration 307, train loss: 0.055554, validation loss: 0.052468\n",
      "iteration 308, train loss: 0.055282, validation loss: 0.05247\n",
      "iteration 309, train loss: 0.054953, validation loss: 0.052476\n",
      "iteration 310, train loss: 0.055768, validation loss: 0.052476\n",
      "iteration 311, train loss: 0.055098, validation loss: 0.052471\n",
      "iteration 312, train loss: 0.055881, validation loss: 0.052466\n",
      "iteration 313, train loss: \u001b[92m0.054697\u001b[0m, validation loss: 0.052467\n",
      "iteration 314, train loss: 0.054825, validation loss: 0.052468\n",
      "iteration 315, train loss: 0.055002, validation loss: 0.052468\n",
      "iteration 316, train loss: 0.055024, validation loss: 0.052468\n",
      "iteration 317, train loss: 0.055365, validation loss: 0.052469\n",
      "iteration 318, train loss: 0.055132, validation loss: 0.052469\n",
      "iteration 319, train loss: 0.054971, validation loss: 0.05247\n",
      "iteration 320, train loss: 0.05513, validation loss: 0.052478\n",
      "iteration 321, train loss: 0.055433, validation loss: 0.052486\n",
      "iteration 322, train loss: 0.055433, validation loss: 0.052485\n",
      "iteration 323, train loss: 0.055868, validation loss: 0.052478\n",
      "iteration 324, train loss: 0.054731, validation loss: 0.052467\n",
      "iteration 325, train loss: 0.055656, validation loss: 0.052461\n",
      "iteration 326, train loss: 0.055116, validation loss: 0.052461\n",
      "iteration 327, train loss: 0.055784, validation loss: 0.052469\n",
      "iteration 328, train loss: 0.055527, validation loss: 0.052464\n",
      "iteration 329, train loss: 0.055262, validation loss: 0.052457\n",
      "iteration 330, train loss: 0.055198, validation loss: 0.052451\n",
      "iteration 331, train loss: 0.055407, validation loss: 0.052446\n",
      "iteration 332, train loss: 0.054996, validation loss: 0.052449\n",
      "iteration 333, train loss: 0.054831, validation loss: 0.052454\n",
      "iteration 334, train loss: 0.054919, validation loss: 0.052465\n",
      "iteration 335, train loss: 0.055209, validation loss: 0.052474\n",
      "iteration 336, train loss: 0.055271, validation loss: 0.052476\n",
      "iteration 337, train loss: 0.054835, validation loss: 0.052466\n",
      "iteration 338, train loss: 0.055139, validation loss: 0.052456\n",
      "iteration 339, train loss: 0.055488, validation loss: 0.052441\n",
      "iteration 340, train loss: 0.055073, validation loss: \u001b[92m0.052434\u001b[0m\n",
      "iteration 341, train loss: 0.055186, validation loss: 0.05244\n",
      "iteration 342, train loss: \u001b[92m0.054582\u001b[0m, validation loss: 0.052451\n",
      "iteration 343, train loss: 0.05511, validation loss: 0.052451\n",
      "iteration 344, train loss: 0.055533, validation loss: 0.052444\n",
      "iteration 345, train loss: 0.055104, validation loss: \u001b[92m0.052432\u001b[0m\n",
      "iteration 346, train loss: 0.055507, validation loss: \u001b[92m0.052426\u001b[0m\n",
      "iteration 347, train loss: 0.054867, validation loss: 0.052435\n",
      "iteration 348, train loss: 0.055083, validation loss: 0.052452\n",
      "iteration 349, train loss: 0.055607, validation loss: 0.052463\n",
      "iteration 350, train loss: 0.055165, validation loss: 0.052469\n",
      "iteration 351, train loss: 0.054971, validation loss: 0.052469\n",
      "iteration 352, train loss: 0.055151, validation loss: 0.05246\n",
      "iteration 353, train loss: 0.054979, validation loss: 0.052451\n",
      "iteration 354, train loss: 0.054735, validation loss: 0.052444\n",
      "iteration 355, train loss: 0.055208, validation loss: 0.052437\n",
      "iteration 356, train loss: 0.055026, validation loss: 0.052434\n",
      "iteration 357, train loss: 0.055064, validation loss: 0.052434\n",
      "iteration 358, train loss: 0.054962, validation loss: 0.052433\n",
      "iteration 359, train loss: 0.055364, validation loss: 0.052431\n",
      "iteration 360, train loss: 0.055257, validation loss: 0.052432\n",
      "iteration 361, train loss: 0.054997, validation loss: 0.052436\n",
      "iteration 362, train loss: 0.055148, validation loss: 0.052443\n",
      "iteration 363, train loss: 0.054837, validation loss: 0.05245\n",
      "iteration 364, train loss: 0.055141, validation loss: 0.052462\n",
      "iteration 365, train loss: 0.055058, validation loss: 0.052466\n",
      "iteration 366, train loss: 0.055301, validation loss: 0.05246\n",
      "iteration 367, train loss: 0.055002, validation loss: 0.052447\n",
      "iteration 368, train loss: 0.054824, validation loss: 0.052433\n",
      "iteration 369, train loss: 0.054793, validation loss: \u001b[92m0.052422\u001b[0m\n",
      "iteration 370, train loss: 0.055264, validation loss: 0.052423\n",
      "iteration 371, train loss: 0.055068, validation loss: 0.052426\n",
      "iteration 372, train loss: 0.054863, validation loss: 0.052425\n",
      "iteration 373, train loss: 0.054785, validation loss: \u001b[92m0.05242\u001b[0m\n",
      "iteration 374, train loss: 0.055344, validation loss: 0.052421\n",
      "iteration 375, train loss: 0.054699, validation loss: 0.052428\n",
      "iteration 376, train loss: 0.054915, validation loss: 0.052445\n",
      "iteration 377, train loss: 0.055347, validation loss: 0.052452\n",
      "iteration 378, train loss: 0.055222, validation loss: 0.052462\n",
      "iteration 379, train loss: 0.054934, validation loss: 0.052466\n",
      "iteration 380, train loss: 0.055388, validation loss: 0.052468\n",
      "iteration 381, train loss: 0.054925, validation loss: 0.05246\n",
      "iteration 382, train loss: 0.055019, validation loss: 0.052456\n",
      "iteration 383, train loss: 0.055178, validation loss: 0.052428\n",
      "iteration 384, train loss: 0.055401, validation loss: \u001b[92m0.052415\u001b[0m\n",
      "iteration 385, train loss: 0.054948, validation loss: \u001b[92m0.052413\u001b[0m\n",
      "iteration 386, train loss: 0.055234, validation loss: \u001b[92m0.052413\u001b[0m\n",
      "iteration 387, train loss: 0.054739, validation loss: 0.052414\n",
      "iteration 388, train loss: 0.055017, validation loss: \u001b[92m0.052412\u001b[0m\n",
      "iteration 389, train loss: 0.055278, validation loss: 0.052413\n",
      "iteration 390, train loss: \u001b[92m0.054472\u001b[0m, validation loss: 0.052421\n",
      "iteration 391, train loss: 0.055082, validation loss: 0.05243\n",
      "iteration 392, train loss: 0.05523, validation loss: 0.052432\n",
      "iteration 393, train loss: 0.055499, validation loss: 0.052443\n",
      "iteration 394, train loss: 0.05528, validation loss: 0.052458\n",
      "iteration 395, train loss: 0.055265, validation loss: 0.05246\n",
      "iteration 396, train loss: 0.055442, validation loss: 0.052448\n",
      "iteration 397, train loss: 0.055197, validation loss: 0.052435\n",
      "iteration 398, train loss: 0.055342, validation loss: 0.052424\n",
      "iteration 399, train loss: 0.055353, validation loss: 0.052427\n",
      "iteration 400, train loss: 0.055229, validation loss: 0.052429\n",
      "iteration 401, train loss: 0.054742, validation loss: 0.052427\n",
      "iteration 402, train loss: 0.055299, validation loss: 0.052425\n",
      "iteration 403, train loss: 0.054899, validation loss: 0.052436\n",
      "iteration 404, train loss: 0.055665, validation loss: 0.052452\n",
      "iteration 405, train loss: 0.054571, validation loss: 0.052465\n",
      "iteration 406, train loss: 0.055098, validation loss: 0.052468\n",
      "iteration 407, train loss: 0.05486, validation loss: 0.052474\n",
      "iteration 408, train loss: 0.055677, validation loss: 0.05247\n",
      "iteration 409, train loss: 0.054593, validation loss: 0.052457\n",
      "iteration 410, train loss: 0.054713, validation loss: 0.052447\n",
      "iteration 411, train loss: 0.054915, validation loss: 0.052442\n",
      "iteration 412, train loss: \u001b[92m0.054438\u001b[0m, validation loss: 0.05244\n",
      "iteration 413, train loss: 0.055127, validation loss: 0.052441\n",
      "iteration 414, train loss: 0.054816, validation loss: 0.052442\n",
      "iteration 415, train loss: 0.054587, validation loss: 0.05244\n",
      "iteration 416, train loss: 0.055176, validation loss: 0.05243\n",
      "iteration 417, train loss: 0.05449, validation loss: 0.052423\n",
      "iteration 418, train loss: 0.054452, validation loss: 0.052424\n",
      "iteration 419, train loss: 0.054824, validation loss: 0.052434\n",
      "iteration 420, train loss: 0.054933, validation loss: 0.052445\n",
      "iteration 421, train loss: 0.054923, validation loss: 0.052451\n",
      "iteration 422, train loss: 0.055129, validation loss: 0.052456\n",
      "iteration 423, train loss: 0.055124, validation loss: 0.052443\n",
      "iteration 424, train loss: 0.055058, validation loss: 0.052427\n",
      "iteration 425, train loss: 0.055207, validation loss: 0.052416\n",
      "iteration 426, train loss: 0.054754, validation loss: \u001b[92m0.052408\u001b[0m\n",
      "iteration 427, train loss: 0.055759, validation loss: \u001b[92m0.052402\u001b[0m\n",
      "iteration 428, train loss: 0.055016, validation loss: \u001b[92m0.052399\u001b[0m\n",
      "iteration 429, train loss: 0.054898, validation loss: 0.052402\n",
      "iteration 430, train loss: 0.055249, validation loss: 0.052404\n",
      "iteration 431, train loss: 0.055144, validation loss: 0.052406\n",
      "iteration 432, train loss: 0.055514, validation loss: 0.052403\n",
      "iteration 433, train loss: 0.055148, validation loss: 0.052404\n",
      "iteration 434, train loss: 0.05497, validation loss: 0.052406\n",
      "iteration 435, train loss: 0.054906, validation loss: 0.052405\n",
      "iteration 436, train loss: 0.055455, validation loss: 0.052404\n",
      "iteration 437, train loss: 0.054924, validation loss: 0.052403\n",
      "iteration 438, train loss: 0.055041, validation loss: 0.052403\n",
      "iteration 439, train loss: 0.055179, validation loss: 0.052406\n",
      "iteration 440, train loss: 0.054934, validation loss: 0.052408\n",
      "iteration 441, train loss: 0.054713, validation loss: 0.052409\n",
      "iteration 442, train loss: 0.055257, validation loss: 0.052408\n",
      "iteration 443, train loss: 0.054832, validation loss: 0.052416\n",
      "iteration 444, train loss: 0.055114, validation loss: 0.052428\n",
      "iteration 445, train loss: 0.055494, validation loss: 0.052437\n",
      "iteration 446, train loss: 0.054868, validation loss: 0.052433\n",
      "iteration 447, train loss: 0.055378, validation loss: 0.052419\n",
      "iteration 448, train loss: 0.05477, validation loss: 0.052412\n",
      "iteration 449, train loss: 0.054532, validation loss: 0.052402\n",
      "iteration 450, train loss: 0.055037, validation loss: 0.052401\n",
      "iteration 451, train loss: 0.055153, validation loss: 0.052401\n",
      "iteration 452, train loss: \u001b[92m0.054434\u001b[0m, validation loss: 0.052402\n",
      "iteration 453, train loss: 0.054941, validation loss: 0.052404\n",
      "iteration 454, train loss: 0.054614, validation loss: 0.052405\n",
      "iteration 455, train loss: 0.054693, validation loss: 0.052404\n",
      "iteration 456, train loss: 0.054668, validation loss: 0.052404\n",
      "iteration 457, train loss: 0.054923, validation loss: 0.05241\n",
      "iteration 458, train loss: 0.054618, validation loss: 0.052423\n",
      "iteration 459, train loss: 0.054757, validation loss: 0.052442\n",
      "iteration 460, train loss: 0.054998, validation loss: 0.052454\n",
      "iteration 461, train loss: 0.054598, validation loss: 0.052459\n",
      "iteration 462, train loss: 0.055104, validation loss: 0.052446\n",
      "iteration 463, train loss: \u001b[92m0.054342\u001b[0m, validation loss: 0.052431\n",
      "iteration 464, train loss: 0.054815, validation loss: 0.05242\n",
      "iteration 465, train loss: 0.055179, validation loss: 0.05242\n",
      "iteration 466, train loss: 0.055147, validation loss: 0.052424\n",
      "iteration 467, train loss: 0.054958, validation loss: 0.052427\n",
      "iteration 468, train loss: 0.054495, validation loss: 0.052419\n",
      "iteration 469, train loss: 0.054917, validation loss: 0.052412\n",
      "iteration 470, train loss: 0.054714, validation loss: 0.052414\n",
      "iteration 471, train loss: 0.054971, validation loss: 0.052431\n",
      "iteration 472, train loss: 0.054879, validation loss: 0.052439\n",
      "iteration 473, train loss: 0.055053, validation loss: 0.052429\n",
      "iteration 474, train loss: 0.054876, validation loss: 0.052418\n",
      "iteration 475, train loss: 0.055286, validation loss: 0.052402\n",
      "iteration 476, train loss: 0.055055, validation loss: \u001b[92m0.052394\u001b[0m\n",
      "iteration 477, train loss: 0.055369, validation loss: 0.052395\n",
      "iteration 478, train loss: 0.054582, validation loss: 0.052409\n",
      "iteration 479, train loss: 0.055297, validation loss: 0.052416\n",
      "iteration 480, train loss: 0.0548, validation loss: 0.052404\n",
      "iteration 481, train loss: 0.054758, validation loss: 0.052397\n",
      "iteration 482, train loss: 0.055086, validation loss: \u001b[92m0.05239\u001b[0m\n",
      "iteration 483, train loss: 0.054885, validation loss: 0.052394\n",
      "iteration 484, train loss: 0.055305, validation loss: 0.052399\n",
      "iteration 485, train loss: 0.055118, validation loss: 0.05241\n",
      "iteration 486, train loss: 0.054783, validation loss: 0.052415\n",
      "iteration 487, train loss: 0.05483, validation loss: 0.052411\n",
      "iteration 488, train loss: 0.05479, validation loss: 0.052404\n",
      "iteration 489, train loss: 0.054572, validation loss: 0.052396\n",
      "iteration 490, train loss: \u001b[92m0.054176\u001b[0m, validation loss: 0.052391\n",
      "iteration 491, train loss: 0.055173, validation loss: \u001b[92m0.052388\u001b[0m\n",
      "iteration 492, train loss: 0.054444, validation loss: 0.052388\n",
      "iteration 493, train loss: 0.054709, validation loss: \u001b[92m0.052387\u001b[0m\n",
      "iteration 494, train loss: 0.054676, validation loss: \u001b[92m0.052385\u001b[0m\n",
      "iteration 495, train loss: 0.054887, validation loss: \u001b[92m0.052383\u001b[0m\n",
      "iteration 496, train loss: 0.05456, validation loss: \u001b[92m0.052381\u001b[0m\n",
      "iteration 497, train loss: 0.055141, validation loss: 0.052382\n",
      "iteration 498, train loss: 0.054782, validation loss: 0.052383\n",
      "iteration 499, train loss: 0.054548, validation loss: 0.052387\n",
      "iteration 500, train loss: 0.055158, validation loss: 0.05239\n",
      "iteration 501, train loss: \u001b[92m0.054134\u001b[0m, validation loss: 0.052388\n",
      "iteration 502, train loss: 0.054692, validation loss: 0.052386\n",
      "iteration 503, train loss: 0.054758, validation loss: 0.052381\n",
      "iteration 504, train loss: 0.055162, validation loss: \u001b[92m0.052377\u001b[0m\n",
      "iteration 505, train loss: 0.054304, validation loss: \u001b[92m0.052376\u001b[0m\n",
      "iteration 506, train loss: 0.054673, validation loss: \u001b[92m0.052373\u001b[0m\n",
      "iteration 507, train loss: 0.054941, validation loss: \u001b[92m0.052371\u001b[0m\n",
      "iteration 508, train loss: 0.054649, validation loss: 0.052376\n",
      "iteration 509, train loss: 0.054623, validation loss: 0.052383\n",
      "iteration 510, train loss: 0.054593, validation loss: 0.052384\n",
      "iteration 511, train loss: 0.054831, validation loss: 0.052379\n",
      "iteration 512, train loss: 0.054927, validation loss: \u001b[92m0.052371\u001b[0m\n",
      "iteration 513, train loss: 0.054373, validation loss: \u001b[92m0.052365\u001b[0m\n",
      "iteration 514, train loss: 0.055041, validation loss: 0.052368\n",
      "iteration 515, train loss: 0.05451, validation loss: 0.052372\n",
      "iteration 516, train loss: 0.054534, validation loss: 0.052374\n",
      "iteration 517, train loss: 0.055053, validation loss: 0.052376\n",
      "iteration 518, train loss: 0.054714, validation loss: 0.052373\n",
      "iteration 519, train loss: 0.054391, validation loss: 0.052372\n",
      "iteration 520, train loss: 0.054944, validation loss: 0.052374\n",
      "iteration 521, train loss: 0.054565, validation loss: 0.052385\n",
      "iteration 522, train loss: 0.054743, validation loss: 0.052403\n",
      "iteration 523, train loss: 0.054786, validation loss: 0.052424\n",
      "iteration 524, train loss: 0.054508, validation loss: 0.052427\n",
      "iteration 525, train loss: 0.054993, validation loss: 0.052423\n",
      "iteration 526, train loss: 0.054533, validation loss: 0.052408\n",
      "iteration 527, train loss: 0.054669, validation loss: 0.05239\n",
      "iteration 528, train loss: 0.055336, validation loss: 0.052385\n",
      "iteration 529, train loss: 0.054482, validation loss: 0.052396\n",
      "iteration 530, train loss: 0.055198, validation loss: 0.052399\n",
      "iteration 531, train loss: 0.054538, validation loss: 0.052395\n",
      "iteration 532, train loss: 0.054789, validation loss: 0.052391\n",
      "iteration 533, train loss: 0.054757, validation loss: 0.052378\n",
      "iteration 534, train loss: 0.05483, validation loss: 0.052374\n",
      "iteration 535, train loss: 0.054578, validation loss: 0.052384\n",
      "iteration 536, train loss: 0.055038, validation loss: 0.052396\n",
      "iteration 537, train loss: 0.054887, validation loss: 0.052395\n",
      "iteration 538, train loss: 0.054869, validation loss: 0.05239\n",
      "iteration 539, train loss: 0.054546, validation loss: 0.052382\n",
      "iteration 540, train loss: 0.055203, validation loss: 0.052382\n",
      "iteration 541, train loss: 0.055289, validation loss: 0.052385\n",
      "iteration 542, train loss: 0.054389, validation loss: 0.052386\n",
      "iteration 543, train loss: 0.054544, validation loss: 0.052383\n",
      "iteration 544, train loss: 0.055211, validation loss: 0.052382\n",
      "iteration 545, train loss: 0.054979, validation loss: 0.052383\n",
      "iteration 546, train loss: 0.054541, validation loss: 0.052388\n",
      "iteration 547, train loss: 0.054702, validation loss: 0.052395\n",
      "iteration 548, train loss: 0.054461, validation loss: 0.052398\n",
      "iteration 549, train loss: 0.054676, validation loss: 0.052395\n",
      "iteration 550, train loss: 0.054819, validation loss: 0.052394\n",
      "iteration 551, train loss: 0.055237, validation loss: 0.052392\n",
      "iteration 552, train loss: 0.05425, validation loss: 0.052391\n",
      "iteration 553, train loss: 0.054929, validation loss: 0.052391\n",
      "iteration 554, train loss: 0.054564, validation loss: 0.052391\n",
      "iteration 555, train loss: 0.054632, validation loss: 0.052392\n",
      "iteration 556, train loss: 0.05462, validation loss: 0.052394\n",
      "iteration 557, train loss: 0.054537, validation loss: 0.052399\n",
      "iteration 558, train loss: 0.054608, validation loss: 0.052405\n",
      "iteration 559, train loss: 0.054683, validation loss: 0.052407\n",
      "iteration 560, train loss: 0.054311, validation loss: 0.052415\n",
      "iteration 561, train loss: 0.054676, validation loss: 0.052421\n",
      "iteration 562, train loss: 0.055191, validation loss: 0.052413\n",
      "iteration 563, train loss: 0.054849, validation loss: 0.052406\n",
      "iteration 564, train loss: 0.054427, validation loss: 0.052402\n",
      "iteration 565, train loss: 0.054426, validation loss: 0.05241\n",
      "iteration 566, train loss: 0.054755, validation loss: 0.05243\n",
      "iteration 567, train loss: 0.05458, validation loss: 0.052443\n",
      "iteration 568, train loss: 0.054528, validation loss: 0.052414\n",
      "iteration 569, train loss: 0.054922, validation loss: 0.052401\n",
      "iteration 570, train loss: 0.055112, validation loss: 0.05242\n",
      "iteration 571, train loss: 0.054811, validation loss: 0.05245\n",
      "iteration 572, train loss: 0.054659, validation loss: 0.052475\n",
      "iteration 573, train loss: 0.054797, validation loss: 0.052471\n",
      "iteration 574, train loss: 0.0547, validation loss: 0.052427\n",
      "iteration 575, train loss: 0.055255, validation loss: 0.052387\n",
      "iteration 576, train loss: 0.054869, validation loss: 0.052392\n",
      "iteration 577, train loss: 0.054934, validation loss: 0.05242\n",
      "iteration 578, train loss: 0.054955, validation loss: 0.052413\n",
      "iteration 579, train loss: 0.054477, validation loss: 0.052392\n",
      "iteration 580, train loss: 0.054548, validation loss: 0.052375\n",
      "iteration 581, train loss: 0.054371, validation loss: 0.052383\n",
      "iteration 582, train loss: 0.054555, validation loss: 0.052397\n",
      "iteration 583, train loss: 0.055005, validation loss: 0.052404\n",
      "iteration 584, train loss: 0.055068, validation loss: 0.052408\n",
      "iteration 585, train loss: 0.054881, validation loss: 0.052398\n",
      "iteration 586, train loss: 0.054453, validation loss: 0.052392\n",
      "iteration 587, train loss: 0.054524, validation loss: 0.052386\n",
      "iteration 588, train loss: 0.054408, validation loss: 0.052387\n",
      "iteration 589, train loss: 0.054856, validation loss: 0.052385\n",
      "iteration 590, train loss: 0.054648, validation loss: 0.052383\n",
      "iteration 591, train loss: 0.054853, validation loss: 0.052383\n",
      "iteration 592, train loss: 0.054168, validation loss: 0.052387\n",
      "iteration 593, train loss: 0.05493, validation loss: 0.052387\n",
      "iteration 594, train loss: 0.054797, validation loss: 0.052382\n",
      "iteration 595, train loss: 0.055195, validation loss: 0.052379\n",
      "iteration 596, train loss: 0.054582, validation loss: 0.052379\n",
      "iteration 597, train loss: 0.054643, validation loss: 0.052383\n",
      "iteration 598, train loss: 0.05533, validation loss: 0.052385\n",
      "iteration 599, train loss: 0.0548, validation loss: 0.052391\n",
      "iteration 600, train loss: 0.05476, validation loss: 0.052403\n",
      "iteration 601, train loss: 0.054734, validation loss: 0.05241\n",
      "iteration 602, train loss: 0.054517, validation loss: 0.052417\n",
      "iteration 603, train loss: 0.054585, validation loss: 0.052413\n",
      "iteration 604, train loss: 0.05439, validation loss: 0.052394\n",
      "iteration 605, train loss: 0.054603, validation loss: 0.05238\n",
      "iteration 606, train loss: 0.054888, validation loss: 0.052375\n",
      "iteration 607, train loss: 0.055453, validation loss: 0.052372\n",
      "iteration 608, train loss: 0.054695, validation loss: 0.052371\n",
      "iteration 609, train loss: 0.054503, validation loss: 0.052375\n",
      "iteration 610, train loss: 0.054392, validation loss: 0.052377\n",
      "iteration 611, train loss: 0.054763, validation loss: 0.052381\n",
      "iteration 612, train loss: 0.05474, validation loss: 0.052385\n",
      "iteration 613, train loss: 0.054601, validation loss: 0.052384\n",
      "iteration 614, train loss: 0.054691, validation loss: 0.052382\n",
      "iteration 615, train loss: 0.054661, validation loss: 0.05238\n",
      "iteration 616, train loss: 0.054467, validation loss: 0.05238\n",
      "iteration 617, train loss: 0.054544, validation loss: 0.052378\n",
      "iteration 618, train loss: 0.054627, validation loss: 0.052378\n",
      "iteration 619, train loss: 0.05435, validation loss: 0.052379\n",
      "iteration 620, train loss: 0.054465, validation loss: 0.052381\n",
      "iteration 621, train loss: 0.054969, validation loss: 0.052377\n",
      "iteration 622, train loss: 0.05461, validation loss: 0.05237\n",
      "iteration 623, train loss: 0.054809, validation loss: \u001b[92m0.05236\u001b[0m\n",
      "iteration 624, train loss: 0.054441, validation loss: \u001b[92m0.052358\u001b[0m\n",
      "iteration 625, train loss: 0.055496, validation loss: 0.052358\n",
      "iteration 626, train loss: 0.054707, validation loss: 0.052359\n",
      "iteration 627, train loss: 0.054744, validation loss: 0.052363\n",
      "iteration 628, train loss: 0.054304, validation loss: 0.052368\n",
      "iteration 629, train loss: \u001b[92m0.054021\u001b[0m, validation loss: 0.052368\n",
      "iteration 630, train loss: 0.054623, validation loss: 0.052366\n",
      "iteration 631, train loss: 0.054553, validation loss: 0.05236\n",
      "iteration 632, train loss: 0.054936, validation loss: \u001b[92m0.052356\u001b[0m\n",
      "iteration 633, train loss: 0.054788, validation loss: \u001b[92m0.052353\u001b[0m\n",
      "iteration 634, train loss: 0.054806, validation loss: \u001b[92m0.052351\u001b[0m\n",
      "iteration 635, train loss: 0.054423, validation loss: 0.052359\n",
      "iteration 636, train loss: 0.054325, validation loss: 0.05238\n",
      "iteration 637, train loss: 0.054439, validation loss: 0.052396\n",
      "iteration 638, train loss: 0.054765, validation loss: 0.052396\n",
      "iteration 639, train loss: 0.054657, validation loss: 0.052408\n",
      "iteration 640, train loss: 0.054415, validation loss: 0.052402\n",
      "iteration 641, train loss: 0.054744, validation loss: 0.05239\n",
      "iteration 642, train loss: 0.054575, validation loss: 0.052373\n",
      "iteration 643, train loss: \u001b[92m0.053981\u001b[0m, validation loss: 0.052353\n",
      "iteration 644, train loss: 0.05449, validation loss: \u001b[92m0.052343\u001b[0m\n",
      "iteration 645, train loss: 0.054294, validation loss: \u001b[92m0.052337\u001b[0m\n",
      "iteration 646, train loss: 0.054366, validation loss: 0.052337\n",
      "iteration 647, train loss: 0.05463, validation loss: 0.052337\n",
      "iteration 648, train loss: 0.054971, validation loss: 0.052343\n",
      "iteration 649, train loss: 0.054949, validation loss: 0.052352\n",
      "iteration 650, train loss: 0.054716, validation loss: 0.052359\n",
      "iteration 651, train loss: 0.055053, validation loss: 0.052351\n",
      "iteration 652, train loss: 0.055157, validation loss: 0.052351\n",
      "iteration 653, train loss: 0.054887, validation loss: 0.052339\n",
      "iteration 654, train loss: 0.054339, validation loss: \u001b[92m0.052331\u001b[0m\n",
      "iteration 655, train loss: 0.054347, validation loss: \u001b[92m0.052329\u001b[0m\n",
      "iteration 656, train loss: 0.054295, validation loss: 0.052331\n",
      "iteration 657, train loss: 0.054697, validation loss: 0.052331\n",
      "iteration 658, train loss: 0.054962, validation loss: 0.052331\n",
      "iteration 659, train loss: 0.054719, validation loss: 0.052335\n",
      "iteration 660, train loss: 0.054616, validation loss: 0.052347\n",
      "iteration 661, train loss: 0.054555, validation loss: 0.052353\n",
      "iteration 662, train loss: 0.054899, validation loss: 0.052362\n",
      "iteration 663, train loss: 0.054323, validation loss: 0.052357\n",
      "iteration 664, train loss: 0.054855, validation loss: 0.052361\n",
      "iteration 665, train loss: 0.05418, validation loss: 0.052362\n",
      "iteration 666, train loss: 0.054254, validation loss: 0.052352\n",
      "iteration 667, train loss: 0.054948, validation loss: 0.052355\n",
      "iteration 668, train loss: 0.054722, validation loss: 0.052365\n",
      "iteration 669, train loss: 0.054565, validation loss: 0.052364\n",
      "iteration 670, train loss: 0.054256, validation loss: 0.052363\n",
      "iteration 671, train loss: 0.054502, validation loss: 0.052363\n",
      "iteration 672, train loss: 0.054387, validation loss: 0.052352\n",
      "iteration 673, train loss: 0.054553, validation loss: 0.052333\n",
      "iteration 674, train loss: 0.054226, validation loss: 0.052335\n",
      "iteration 675, train loss: 0.054855, validation loss: 0.052335\n",
      "iteration 676, train loss: 0.054628, validation loss: \u001b[92m0.052295\u001b[0m\n",
      "iteration 677, train loss: \u001b[92m0.053879\u001b[0m, validation loss: \u001b[92m0.052199\u001b[0m\n",
      "iteration 678, train loss: 0.05398, validation loss: \u001b[92m0.052052\u001b[0m\n",
      "iteration 679, train loss: 0.054666, validation loss: \u001b[92m0.051861\u001b[0m\n",
      "iteration 680, train loss: 0.054331, validation loss: \u001b[92m0.051434\u001b[0m\n",
      "iteration 681, train loss: \u001b[92m0.053719\u001b[0m, validation loss: \u001b[92m0.05029\u001b[0m\n",
      "iteration 682, train loss: \u001b[92m0.052861\u001b[0m, validation loss: \u001b[92m0.047312\u001b[0m\n",
      "iteration 683, train loss: \u001b[92m0.049937\u001b[0m, validation loss: \u001b[92m0.042746\u001b[0m\n",
      "iteration 684, train loss: \u001b[92m0.046717\u001b[0m, validation loss: \u001b[92m0.042114\u001b[0m\n",
      "iteration 685, train loss: \u001b[92m0.042404\u001b[0m, validation loss: \u001b[92m0.035652\u001b[0m\n",
      "iteration 686, train loss: \u001b[92m0.038013\u001b[0m, validation loss: \u001b[92m0.030076\u001b[0m\n",
      "iteration 687, train loss: \u001b[92m0.035533\u001b[0m, validation loss: 0.048451\n",
      "iteration 688, train loss: 0.040383, validation loss: \u001b[92m0.026458\u001b[0m\n",
      "iteration 689, train loss: \u001b[92m0.032983\u001b[0m, validation loss: 0.033021\n",
      "iteration 690, train loss: 0.038103, validation loss: 0.032919\n",
      "iteration 691, train loss: 0.041172, validation loss: 0.02779\n",
      "iteration 692, train loss: 0.036278, validation loss: \u001b[92m0.018415\u001b[0m\n",
      "iteration 693, train loss: \u001b[92m0.023991\u001b[0m, validation loss: 0.021674\n",
      "iteration 694, train loss: \u001b[92m0.021873\u001b[0m, validation loss: 0.036623\n",
      "iteration 695, train loss: 0.027842, validation loss: \u001b[92m0.015607\u001b[0m\n",
      "iteration 696, train loss: \u001b[92m0.019832\u001b[0m, validation loss: 0.016365\n",
      "iteration 697, train loss: 0.02329, validation loss: 0.019014\n",
      "iteration 698, train loss: 0.02791, validation loss: 0.018148\n",
      "iteration 699, train loss: 0.023699, validation loss: \u001b[92m0.015089\u001b[0m\n",
      "iteration 700, train loss: 0.019838, validation loss: 0.015316\n",
      "iteration 701, train loss: \u001b[92m0.018177\u001b[0m, validation loss: 0.027476\n",
      "iteration 702, train loss: 0.031714, validation loss: 0.015417\n",
      "iteration 703, train loss: 0.019398, validation loss: 0.015484\n",
      "iteration 704, train loss: 0.020511, validation loss: 0.018492\n",
      "iteration 705, train loss: 0.024899, validation loss: 0.018805\n",
      "iteration 706, train loss: 0.02547, validation loss: 0.016389\n",
      "iteration 707, train loss: 0.023245, validation loss: \u001b[92m0.013463\u001b[0m\n",
      "iteration 708, train loss: 0.018684, validation loss: 0.014775\n",
      "iteration 709, train loss: 0.019095, validation loss: 0.017309\n",
      "iteration 710, train loss: 0.021353, validation loss: 0.013979\n",
      "iteration 711, train loss: 0.019136, validation loss: \u001b[92m0.013164\u001b[0m\n",
      "iteration 712, train loss: \u001b[92m0.017645\u001b[0m, validation loss: 0.014168\n",
      "iteration 713, train loss: 0.018558, validation loss: 0.014585\n",
      "iteration 714, train loss: 0.01985, validation loss: 0.013624\n",
      "iteration 715, train loss: 0.018117, validation loss: \u001b[92m0.012675\u001b[0m\n",
      "iteration 716, train loss: 0.018336, validation loss: 0.01418\n",
      "iteration 717, train loss: 0.020744, validation loss: 0.013635\n",
      "iteration 718, train loss: \u001b[92m0.017286\u001b[0m, validation loss: \u001b[92m0.012477\u001b[0m\n",
      "iteration 719, train loss: 0.01753, validation loss: 0.012826\n",
      "iteration 720, train loss: 0.017295, validation loss: 0.014155\n",
      "iteration 721, train loss: 0.018126, validation loss: 0.014269\n",
      "iteration 722, train loss: 0.021048, validation loss: 0.012896\n",
      "iteration 723, train loss: 0.017707, validation loss: \u001b[92m0.012209\u001b[0m\n",
      "iteration 724, train loss: 0.017475, validation loss: 0.01296\n",
      "iteration 725, train loss: 0.018512, validation loss: 0.01574\n",
      "iteration 726, train loss: 0.022738, validation loss: \u001b[92m0.012139\u001b[0m\n",
      "iteration 727, train loss: \u001b[92m0.017239\u001b[0m, validation loss: 0.01252\n",
      "iteration 728, train loss: 0.018011, validation loss: 0.013541\n",
      "iteration 729, train loss: 0.018248, validation loss: 0.013192\n",
      "iteration 730, train loss: 0.019263, validation loss: \u001b[92m0.01189\u001b[0m\n",
      "iteration 731, train loss: \u001b[92m0.016995\u001b[0m, validation loss: 0.01313\n",
      "iteration 732, train loss: 0.017466, validation loss: 0.016881\n",
      "iteration 733, train loss: 0.018436, validation loss: 0.014431\n",
      "iteration 734, train loss: 0.020264, validation loss: \u001b[92m0.011691\u001b[0m\n",
      "iteration 735, train loss: \u001b[92m0.016667\u001b[0m, validation loss: 0.012915\n",
      "iteration 736, train loss: 0.018595, validation loss: 0.013827\n",
      "iteration 737, train loss: 0.019798, validation loss: 0.012828\n",
      "iteration 738, train loss: 0.017787, validation loss: \u001b[92m0.011628\u001b[0m\n",
      "iteration 739, train loss: \u001b[92m0.015135\u001b[0m, validation loss: 0.012582\n",
      "iteration 740, train loss: \u001b[92m0.014834\u001b[0m, validation loss: 0.015068\n",
      "iteration 741, train loss: 0.01714, validation loss: 0.013389\n",
      "iteration 742, train loss: 0.017542, validation loss: \u001b[92m0.011564\u001b[0m\n",
      "iteration 743, train loss: 0.015385, validation loss: 0.011964\n",
      "iteration 744, train loss: 0.017061, validation loss: 0.012125\n",
      "iteration 745, train loss: 0.016108, validation loss: 0.011687\n",
      "iteration 746, train loss: 0.017542, validation loss: \u001b[92m0.011456\u001b[0m\n",
      "iteration 747, train loss: 0.015348, validation loss: 0.012181\n",
      "iteration 748, train loss: 0.015908, validation loss: 0.011726\n",
      "iteration 749, train loss: 0.015967, validation loss: \u001b[92m0.011272\u001b[0m\n",
      "iteration 750, train loss: 0.015266, validation loss: \u001b[92m0.011247\u001b[0m\n",
      "iteration 751, train loss: 0.015688, validation loss: \u001b[92m0.011232\u001b[0m\n",
      "iteration 752, train loss: \u001b[92m0.014361\u001b[0m, validation loss: \u001b[92m0.011127\u001b[0m\n",
      "iteration 753, train loss: 0.014952, validation loss: \u001b[92m0.011099\u001b[0m\n",
      "iteration 754, train loss: 0.015607, validation loss: 0.011534\n",
      "iteration 755, train loss: 0.01484, validation loss: 0.011445\n",
      "iteration 756, train loss: 0.014557, validation loss: 0.011219\n",
      "iteration 757, train loss: 0.015034, validation loss: \u001b[92m0.0109\u001b[0m\n",
      "iteration 758, train loss: 0.014842, validation loss: \u001b[92m0.010883\u001b[0m\n",
      "iteration 759, train loss: 0.014991, validation loss: 0.010954\n",
      "iteration 760, train loss: \u001b[92m0.01344\u001b[0m, validation loss: 0.011465\n",
      "iteration 761, train loss: 0.015711, validation loss: 0.011032\n",
      "iteration 762, train loss: 0.015074, validation loss: \u001b[92m0.010742\u001b[0m\n",
      "iteration 763, train loss: 0.014947, validation loss: 0.010956\n",
      "iteration 764, train loss: 0.015582, validation loss: 0.010766\n",
      "iteration 765, train loss: 0.01518, validation loss: \u001b[92m0.010646\u001b[0m\n",
      "iteration 766, train loss: 0.014857, validation loss: 0.011263\n",
      "iteration 767, train loss: 0.014931, validation loss: 0.012002\n",
      "iteration 768, train loss: 0.014912, validation loss: 0.011186\n",
      "iteration 769, train loss: 0.014278, validation loss: \u001b[92m0.010515\u001b[0m\n",
      "iteration 770, train loss: 0.014568, validation loss: 0.010841\n",
      "iteration 771, train loss: 0.015462, validation loss: 0.011004\n",
      "iteration 772, train loss: 0.014869, validation loss: 0.010555\n",
      "iteration 773, train loss: 0.013845, validation loss: 0.010701\n",
      "iteration 774, train loss: 0.014308, validation loss: 0.011829\n",
      "iteration 775, train loss: 0.015347, validation loss: 0.011281\n",
      "iteration 776, train loss: 0.013666, validation loss: 0.010516\n",
      "iteration 777, train loss: 0.014644, validation loss: 0.01052\n",
      "iteration 778, train loss: 0.013991, validation loss: 0.010553\n",
      "iteration 779, train loss: 0.01427, validation loss: \u001b[92m0.010324\u001b[0m\n",
      "iteration 780, train loss: 0.014953, validation loss: 0.010397\n",
      "iteration 781, train loss: 0.013943, validation loss: 0.010803\n",
      "iteration 782, train loss: 0.01394, validation loss: 0.010841\n",
      "iteration 783, train loss: 0.014534, validation loss: 0.010677\n",
      "iteration 784, train loss: \u001b[92m0.012994\u001b[0m, validation loss: 0.01045\n",
      "iteration 785, train loss: 0.013577, validation loss: 0.010358\n",
      "iteration 786, train loss: 0.014137, validation loss: \u001b[92m0.010247\u001b[0m\n",
      "iteration 787, train loss: 0.013482, validation loss: \u001b[92m0.010194\u001b[0m\n",
      "iteration 788, train loss: 0.01307, validation loss: 0.010205\n",
      "iteration 789, train loss: 0.013588, validation loss: 0.010307\n",
      "iteration 790, train loss: 0.01421, validation loss: 0.010698\n",
      "iteration 791, train loss: 0.013135, validation loss: 0.011254\n",
      "iteration 792, train loss: 0.014408, validation loss: 0.010682\n",
      "iteration 793, train loss: 0.013997, validation loss: \u001b[92m0.01006\u001b[0m\n",
      "iteration 794, train loss: 0.013131, validation loss: \u001b[92m0.010045\u001b[0m\n",
      "iteration 795, train loss: 0.013427, validation loss: \u001b[92m0.010042\u001b[0m\n",
      "iteration 796, train loss: 0.013385, validation loss: \u001b[92m0.009986\u001b[0m\n",
      "iteration 797, train loss: 0.013326, validation loss: 0.009993\n",
      "iteration 798, train loss: 0.013549, validation loss: 0.010192\n",
      "iteration 799, train loss: \u001b[92m0.012931\u001b[0m, validation loss: 0.010575\n",
      "iteration 800, train loss: 0.013374, validation loss: 0.010664\n",
      "iteration 801, train loss: 0.014111, validation loss: 0.010361\n",
      "iteration 802, train loss: 0.013164, validation loss: 0.010036\n",
      "iteration 803, train loss: 0.013848, validation loss: \u001b[92m0.00997\u001b[0m\n",
      "iteration 804, train loss: 0.01335, validation loss: \u001b[92m0.009929\u001b[0m\n",
      "iteration 805, train loss: 0.013757, validation loss: 0.009939\n",
      "iteration 806, train loss: 0.013668, validation loss: 0.009948\n",
      "iteration 807, train loss: 0.014335, validation loss: 0.010046\n",
      "iteration 808, train loss: 0.013871, validation loss: 0.010135\n",
      "iteration 809, train loss: 0.013367, validation loss: 0.010081\n",
      "iteration 810, train loss: 0.013267, validation loss: 0.010175\n",
      "iteration 811, train loss: 0.013045, validation loss: \u001b[92m0.00991\u001b[0m\n",
      "iteration 812, train loss: 0.013007, validation loss: \u001b[92m0.009778\u001b[0m\n",
      "iteration 813, train loss: 0.013174, validation loss: \u001b[92m0.009743\u001b[0m\n",
      "iteration 814, train loss: 0.01357, validation loss: \u001b[92m0.00974\u001b[0m\n",
      "iteration 815, train loss: 0.014343, validation loss: \u001b[92m0.009723\u001b[0m\n",
      "iteration 816, train loss: \u001b[92m0.012587\u001b[0m, validation loss: 0.009974\n",
      "iteration 817, train loss: 0.01299, validation loss: 0.010288\n",
      "iteration 818, train loss: 0.013115, validation loss: 0.010343\n",
      "iteration 819, train loss: 0.012967, validation loss: 0.009908\n",
      "iteration 820, train loss: \u001b[92m0.01243\u001b[0m, validation loss: \u001b[92m0.009676\u001b[0m\n",
      "iteration 821, train loss: 0.013899, validation loss: \u001b[92m0.009672\u001b[0m\n",
      "iteration 822, train loss: 0.013305, validation loss: \u001b[92m0.009662\u001b[0m\n",
      "iteration 823, train loss: 0.013451, validation loss: 0.009806\n",
      "iteration 824, train loss: 0.013554, validation loss: 0.009821\n",
      "iteration 825, train loss: 0.013088, validation loss: 0.009745\n",
      "iteration 826, train loss: 0.01324, validation loss: 0.009772\n",
      "iteration 827, train loss: 0.013267, validation loss: 0.009808\n",
      "iteration 828, train loss: 0.013688, validation loss: 0.009685\n",
      "iteration 829, train loss: 0.013018, validation loss: \u001b[92m0.009641\u001b[0m\n",
      "iteration 830, train loss: 0.012525, validation loss: \u001b[92m0.009605\u001b[0m\n",
      "iteration 831, train loss: 0.013013, validation loss: \u001b[92m0.009578\u001b[0m\n",
      "iteration 832, train loss: 0.012578, validation loss: 0.009584\n",
      "iteration 833, train loss: 0.01351, validation loss: \u001b[92m0.009547\u001b[0m\n",
      "iteration 834, train loss: 0.013018, validation loss: 0.009559\n",
      "iteration 835, train loss: 0.012631, validation loss: 0.009597\n",
      "iteration 836, train loss: 0.012737, validation loss: 0.009609\n",
      "iteration 837, train loss: 0.013186, validation loss: 0.00978\n",
      "iteration 838, train loss: 0.012836, validation loss: 0.009848\n",
      "iteration 839, train loss: 0.012486, validation loss: 0.009781\n",
      "iteration 840, train loss: 0.013653, validation loss: 0.00957\n",
      "iteration 841, train loss: 0.013598, validation loss: \u001b[92m0.009473\u001b[0m\n",
      "iteration 842, train loss: 0.012752, validation loss: \u001b[92m0.009453\u001b[0m\n",
      "iteration 843, train loss: 0.012858, validation loss: 0.009521\n",
      "iteration 844, train loss: \u001b[92m0.012194\u001b[0m, validation loss: 0.009676\n",
      "iteration 845, train loss: 0.012662, validation loss: 0.010091\n",
      "iteration 846, train loss: 0.013237, validation loss: 0.010086\n",
      "iteration 847, train loss: 0.0132, validation loss: 0.009619\n",
      "iteration 848, train loss: \u001b[92m0.012148\u001b[0m, validation loss: \u001b[92m0.009374\u001b[0m\n",
      "iteration 849, train loss: 0.012411, validation loss: 0.009401\n",
      "iteration 850, train loss: 0.01234, validation loss: 0.009419\n",
      "iteration 851, train loss: 0.012725, validation loss: \u001b[92m0.009339\u001b[0m\n",
      "iteration 852, train loss: 0.012799, validation loss: 0.009359\n",
      "iteration 853, train loss: 0.013456, validation loss: 0.009676\n",
      "iteration 854, train loss: 0.012912, validation loss: 0.010098\n",
      "iteration 855, train loss: 0.013577, validation loss: 0.010648\n",
      "iteration 856, train loss: 0.01358, validation loss: 0.010101\n",
      "iteration 857, train loss: 0.012927, validation loss: 0.009397\n",
      "iteration 858, train loss: 0.012474, validation loss: \u001b[92m0.009249\u001b[0m\n",
      "iteration 859, train loss: 0.013206, validation loss: 0.009406\n",
      "iteration 860, train loss: 0.013781, validation loss: 0.009316\n",
      "iteration 861, train loss: 0.012844, validation loss: \u001b[92m0.009202\u001b[0m\n",
      "iteration 862, train loss: \u001b[92m0.012005\u001b[0m, validation loss: 0.00932\n",
      "iteration 863, train loss: 0.012784, validation loss: 0.009999\n",
      "iteration 864, train loss: 0.013068, validation loss: 0.010741\n",
      "iteration 865, train loss: 0.013288, validation loss: 0.010457\n",
      "iteration 866, train loss: 0.012933, validation loss: 0.009779\n",
      "iteration 867, train loss: 0.012777, validation loss: 0.009408\n",
      "iteration 868, train loss: 0.01217, validation loss: 0.009224\n",
      "iteration 869, train loss: 0.013644, validation loss: 0.009288\n",
      "iteration 870, train loss: 0.012616, validation loss: 0.009209\n",
      "iteration 871, train loss: 0.012777, validation loss: \u001b[92m0.009135\u001b[0m\n",
      "iteration 872, train loss: 0.012228, validation loss: 0.009231\n",
      "iteration 873, train loss: 0.012676, validation loss: 0.009687\n",
      "iteration 874, train loss: 0.012327, validation loss: 0.009738\n",
      "iteration 875, train loss: 0.01255, validation loss: 0.009349\n",
      "iteration 876, train loss: 0.012467, validation loss: \u001b[92m0.009043\u001b[0m\n",
      "iteration 877, train loss: 0.012865, validation loss: \u001b[92m0.008926\u001b[0m\n",
      "iteration 878, train loss: 0.013063, validation loss: 0.008956\n",
      "iteration 879, train loss: 0.012562, validation loss: 0.009003\n",
      "iteration 880, train loss: 0.012768, validation loss: 0.009178\n",
      "iteration 881, train loss: \u001b[92m0.011788\u001b[0m, validation loss: 0.009446\n",
      "iteration 882, train loss: 0.0121, validation loss: 0.009623\n",
      "iteration 883, train loss: 0.013042, validation loss: 0.00935\n",
      "iteration 884, train loss: 0.012207, validation loss: 0.009072\n",
      "iteration 885, train loss: 0.013094, validation loss: 0.008942\n",
      "iteration 886, train loss: 0.011886, validation loss: \u001b[92m0.008906\u001b[0m\n",
      "iteration 887, train loss: 0.012216, validation loss: \u001b[92m0.008873\u001b[0m\n",
      "iteration 888, train loss: 0.012411, validation loss: \u001b[92m0.008822\u001b[0m\n",
      "iteration 889, train loss: 0.01184, validation loss: \u001b[92m0.008802\u001b[0m\n",
      "iteration 890, train loss: 0.011806, validation loss: \u001b[92m0.008793\u001b[0m\n",
      "iteration 891, train loss: 0.011902, validation loss: 0.008795\n",
      "iteration 892, train loss: 0.013078, validation loss: 0.008922\n",
      "iteration 893, train loss: 0.011898, validation loss: 0.009156\n",
      "iteration 894, train loss: 0.011802, validation loss: 0.009493\n",
      "iteration 895, train loss: 0.01272, validation loss: 0.009214\n",
      "iteration 896, train loss: 0.012245, validation loss: \u001b[92m0.008765\u001b[0m\n",
      "iteration 897, train loss: \u001b[92m0.0114\u001b[0m, validation loss: \u001b[92m0.008568\u001b[0m\n",
      "iteration 898, train loss: 0.012178, validation loss: \u001b[92m0.008545\u001b[0m\n",
      "iteration 899, train loss: 0.012563, validation loss: \u001b[92m0.008507\u001b[0m\n",
      "iteration 900, train loss: 0.011729, validation loss: 0.008616\n",
      "iteration 901, train loss: 0.012611, validation loss: 0.009089\n",
      "iteration 902, train loss: 0.01246, validation loss: 0.009296\n",
      "iteration 903, train loss: 0.012485, validation loss: 0.009008\n",
      "iteration 904, train loss: 0.011947, validation loss: 0.008884\n",
      "iteration 905, train loss: 0.012726, validation loss: 0.008741\n",
      "iteration 906, train loss: 0.012557, validation loss: 0.008624\n",
      "iteration 907, train loss: 0.011577, validation loss: 0.008624\n",
      "iteration 908, train loss: 0.012512, validation loss: 0.008575\n",
      "iteration 909, train loss: 0.011896, validation loss: 0.008552\n",
      "iteration 910, train loss: 0.011767, validation loss: 0.008556\n",
      "iteration 911, train loss: 0.012693, validation loss: 0.008541\n",
      "iteration 912, train loss: 0.011761, validation loss: 0.008542\n",
      "iteration 913, train loss: 0.011609, validation loss: 0.008528\n",
      "iteration 914, train loss: 0.012269, validation loss: \u001b[92m0.008401\u001b[0m\n",
      "iteration 915, train loss: 0.012094, validation loss: \u001b[92m0.008319\u001b[0m\n",
      "iteration 916, train loss: 0.011843, validation loss: \u001b[92m0.008243\u001b[0m\n",
      "iteration 917, train loss: \u001b[92m0.011251\u001b[0m, validation loss: \u001b[92m0.008215\u001b[0m\n",
      "iteration 918, train loss: 0.011779, validation loss: 0.008228\n",
      "iteration 919, train loss: 0.012342, validation loss: 0.008449\n",
      "iteration 920, train loss: \u001b[92m0.010828\u001b[0m, validation loss: 0.008596\n",
      "iteration 921, train loss: 0.011544, validation loss: 0.008695\n",
      "iteration 922, train loss: 0.011971, validation loss: 0.008644\n",
      "iteration 923, train loss: 0.012322, validation loss: 0.008551\n",
      "iteration 924, train loss: 0.01225, validation loss: 0.008417\n",
      "iteration 925, train loss: 0.012212, validation loss: 0.008347\n",
      "iteration 926, train loss: 0.011828, validation loss: 0.008271\n",
      "iteration 927, train loss: 0.011825, validation loss: 0.008316\n",
      "iteration 928, train loss: 0.012782, validation loss: \u001b[92m0.008156\u001b[0m\n",
      "iteration 929, train loss: 0.011992, validation loss: \u001b[92m0.007971\u001b[0m\n",
      "iteration 930, train loss: 0.012516, validation loss: 0.008216\n",
      "iteration 931, train loss: 0.0114, validation loss: 0.008403\n",
      "iteration 932, train loss: 0.011769, validation loss: 0.008539\n",
      "iteration 933, train loss: 0.011702, validation loss: 0.008621\n",
      "iteration 934, train loss: 0.011101, validation loss: 0.008643\n",
      "iteration 935, train loss: 0.012219, validation loss: 0.008562\n",
      "iteration 936, train loss: 0.012139, validation loss: 0.008549\n",
      "iteration 937, train loss: 0.01157, validation loss: 0.008382\n",
      "iteration 938, train loss: 0.0121, validation loss: 0.008006\n",
      "iteration 939, train loss: 0.011296, validation loss: \u001b[92m0.007769\u001b[0m\n",
      "iteration 940, train loss: 0.011633, validation loss: \u001b[92m0.007719\u001b[0m\n",
      "iteration 941, train loss: 0.011619, validation loss: \u001b[92m0.007591\u001b[0m\n",
      "iteration 942, train loss: 0.011331, validation loss: 0.008109\n",
      "iteration 943, train loss: 0.011539, validation loss: 0.008379\n",
      "iteration 944, train loss: 0.011691, validation loss: 0.008349\n",
      "iteration 945, train loss: 0.011362, validation loss: 0.008133\n",
      "iteration 946, train loss: 0.011023, validation loss: 0.007986\n",
      "iteration 947, train loss: 0.011578, validation loss: 0.007815\n",
      "iteration 948, train loss: 0.011846, validation loss: 0.007623\n",
      "iteration 949, train loss: 0.01167, validation loss: \u001b[92m0.007434\u001b[0m\n",
      "iteration 950, train loss: 0.01149, validation loss: \u001b[92m0.007421\u001b[0m\n",
      "iteration 951, train loss: \u001b[92m0.010613\u001b[0m, validation loss: 0.007535\n",
      "iteration 952, train loss: 0.010811, validation loss: 0.007756\n",
      "iteration 953, train loss: 0.010645, validation loss: 0.007707\n",
      "iteration 954, train loss: 0.011244, validation loss: 0.007539\n",
      "iteration 955, train loss: \u001b[92m0.010406\u001b[0m, validation loss: \u001b[92m0.007411\u001b[0m\n",
      "iteration 956, train loss: 0.011651, validation loss: \u001b[92m0.007261\u001b[0m\n",
      "iteration 957, train loss: 0.011041, validation loss: 0.00729\n",
      "iteration 958, train loss: 0.010953, validation loss: \u001b[92m0.007166\u001b[0m\n",
      "iteration 959, train loss: 0.011017, validation loss: \u001b[92m0.007059\u001b[0m\n",
      "iteration 960, train loss: 0.011046, validation loss: 0.007345\n",
      "iteration 961, train loss: 0.01128, validation loss: 0.007407\n",
      "iteration 962, train loss: 0.011233, validation loss: 0.007275\n",
      "iteration 963, train loss: 0.01044, validation loss: \u001b[92m0.006934\u001b[0m\n",
      "iteration 964, train loss: 0.011303, validation loss: 0.007181\n",
      "iteration 965, train loss: 0.010462, validation loss: 0.007278\n",
      "iteration 966, train loss: 0.011704, validation loss: 0.006964\n",
      "iteration 967, train loss: \u001b[92m0.0104\u001b[0m, validation loss: 0.007348\n",
      "iteration 968, train loss: 0.011493, validation loss: 0.007392\n",
      "iteration 969, train loss: \u001b[92m0.009882\u001b[0m, validation loss: 0.006993\n",
      "iteration 970, train loss: 0.010067, validation loss: \u001b[92m0.006592\u001b[0m\n",
      "iteration 971, train loss: \u001b[92m0.009762\u001b[0m, validation loss: 0.006657\n",
      "iteration 972, train loss: 0.010941, validation loss: \u001b[92m0.006473\u001b[0m\n",
      "iteration 973, train loss: 0.010513, validation loss: 0.006619\n",
      "iteration 974, train loss: 0.009799, validation loss: 0.006982\n",
      "iteration 975, train loss: 0.010633, validation loss: 0.007319\n",
      "iteration 976, train loss: 0.010592, validation loss: 0.006996\n",
      "iteration 977, train loss: 0.010958, validation loss: 0.006648\n",
      "iteration 978, train loss: 0.010319, validation loss: 0.007108\n",
      "iteration 979, train loss: 0.010634, validation loss: 0.006977\n",
      "iteration 980, train loss: 0.010562, validation loss: \u001b[92m0.006264\u001b[0m\n",
      "iteration 981, train loss: 0.010193, validation loss: 0.006859\n",
      "iteration 982, train loss: 0.010268, validation loss: 0.006936\n",
      "iteration 983, train loss: 0.011486, validation loss: 0.007387\n",
      "iteration 984, train loss: 0.009841, validation loss: 0.007514\n",
      "iteration 985, train loss: 0.010865, validation loss: 0.006903\n",
      "iteration 986, train loss: 0.009988, validation loss: 0.006403\n",
      "iteration 987, train loss: 0.010047, validation loss: \u001b[92m0.006091\u001b[0m\n",
      "iteration 988, train loss: \u001b[92m0.009631\u001b[0m, validation loss: 0.006165\n",
      "iteration 989, train loss: 0.010495, validation loss: \u001b[92m0.00601\u001b[0m\n",
      "iteration 990, train loss: \u001b[92m0.009482\u001b[0m, validation loss: 0.006247\n",
      "iteration 991, train loss: 0.009696, validation loss: 0.006416\n",
      "iteration 992, train loss: \u001b[92m0.009365\u001b[0m, validation loss: 0.006284\n",
      "iteration 993, train loss: \u001b[92m0.009277\u001b[0m, validation loss: 0.006304\n",
      "iteration 994, train loss: 0.009584, validation loss: \u001b[92m0.005896\u001b[0m\n",
      "iteration 995, train loss: 0.009538, validation loss: \u001b[92m0.005875\u001b[0m\n",
      "iteration 996, train loss: \u001b[92m0.009196\u001b[0m, validation loss: 0.006305\n",
      "iteration 997, train loss: 0.010619, validation loss: 0.00608\n",
      "iteration 998, train loss: 0.010544, validation loss: 0.00602\n",
      "iteration 999, train loss: \u001b[92m0.009062\u001b[0m, validation loss: 0.006847\n",
      "iteration 1000, train loss: 0.009152, validation loss: 0.00594\n",
      "iteration 1001, train loss: 0.009148, validation loss: 0.006043\n",
      "iteration 1002, train loss: 0.009696, validation loss: 0.00611\n",
      "iteration 1003, train loss: 0.010128, validation loss: \u001b[92m0.005804\u001b[0m\n",
      "iteration 1004, train loss: \u001b[92m0.009019\u001b[0m, validation loss: 0.006087\n",
      "iteration 1005, train loss: 0.009999, validation loss: 0.006395\n",
      "iteration 1006, train loss: 0.00911, validation loss: 0.007405\n",
      "iteration 1007, train loss: 0.010717, validation loss: 0.007254\n",
      "iteration 1008, train loss: 0.009626, validation loss: 0.006389\n",
      "iteration 1009, train loss: 0.009486, validation loss: 0.005822\n",
      "iteration 1010, train loss: \u001b[92m0.008918\u001b[0m, validation loss: 0.005824\n",
      "iteration 1011, train loss: 0.009023, validation loss: \u001b[92m0.005779\u001b[0m\n",
      "iteration 1012, train loss: 0.009529, validation loss: 0.00581\n",
      "iteration 1013, train loss: 0.009713, validation loss: 0.005895\n",
      "iteration 1014, train loss: 0.009434, validation loss: \u001b[92m0.00573\u001b[0m\n",
      "iteration 1015, train loss: 0.009514, validation loss: 0.006005\n",
      "iteration 1016, train loss: 0.009328, validation loss: 0.006184\n",
      "iteration 1017, train loss: 0.009727, validation loss: 0.006233\n",
      "iteration 1018, train loss: 0.00955, validation loss: 0.005829\n",
      "iteration 1019, train loss: 0.009453, validation loss: \u001b[92m0.005665\u001b[0m\n",
      "iteration 1020, train loss: 0.009824, validation loss: 0.006284\n",
      "iteration 1021, train loss: 0.009276, validation loss: 0.005711\n",
      "iteration 1022, train loss: 0.009565, validation loss: 0.006142\n",
      "iteration 1023, train loss: 0.008983, validation loss: 0.006234\n",
      "iteration 1024, train loss: 0.01014, validation loss: 0.005909\n",
      "iteration 1025, train loss: \u001b[92m0.008872\u001b[0m, validation loss: 0.007168\n",
      "iteration 1026, train loss: 0.010742, validation loss: 0.005778\n",
      "iteration 1027, train loss: \u001b[92m0.008714\u001b[0m, validation loss: 0.006426\n",
      "iteration 1028, train loss: 0.009837, validation loss: 0.006486\n",
      "iteration 1029, train loss: 0.010581, validation loss: 0.00572\n",
      "iteration 1030, train loss: 0.008747, validation loss: 0.00674\n",
      "iteration 1031, train loss: 0.010044, validation loss: 0.006103\n",
      "iteration 1032, train loss: 0.009964, validation loss: 0.00607\n",
      "iteration 1033, train loss: 0.009621, validation loss: 0.006273\n",
      "iteration 1034, train loss: 0.010089, validation loss: 0.005955\n",
      "iteration 1035, train loss: 0.009235, validation loss: 0.005705\n",
      "iteration 1036, train loss: 0.009578, validation loss: 0.005848\n",
      "iteration 1037, train loss: 0.009196, validation loss: 0.005734\n",
      "iteration 1038, train loss: 0.008764, validation loss: 0.005785\n",
      "iteration 1039, train loss: 0.009127, validation loss: 0.005925\n",
      "iteration 1040, train loss: 0.008939, validation loss: 0.005875\n",
      "iteration 1041, train loss: 0.009173, validation loss: 0.005866\n",
      "iteration 1042, train loss: 0.009681, validation loss: 0.00567\n",
      "iteration 1043, train loss: 0.009091, validation loss: 0.006133\n",
      "iteration 1044, train loss: 0.009811, validation loss: 0.006129\n",
      "iteration 1045, train loss: 0.008876, validation loss: 0.006209\n",
      "iteration 1046, train loss: 0.009017, validation loss: 0.006607\n",
      "iteration 1047, train loss: 0.00919, validation loss: 0.006442\n",
      "iteration 1048, train loss: 0.010449, validation loss: 0.00573\n",
      "iteration 1049, train loss: 0.009261, validation loss: 0.005842\n",
      "iteration 1050, train loss: 0.008804, validation loss: 0.005749\n",
      "iteration 1051, train loss: \u001b[92m0.008553\u001b[0m, validation loss: 0.005979\n",
      "iteration 1052, train loss: 0.009266, validation loss: 0.005967\n",
      "iteration 1053, train loss: 0.009427, validation loss: 0.005773\n",
      "iteration 1054, train loss: 0.00874, validation loss: 0.006096\n",
      "iteration 1055, train loss: 0.008918, validation loss: 0.006259\n",
      "iteration 1056, train loss: 0.009307, validation loss: 0.005904\n",
      "iteration 1057, train loss: 0.008758, validation loss: \u001b[92m0.005605\u001b[0m\n",
      "iteration 1058, train loss: 0.009324, validation loss: \u001b[92m0.005566\u001b[0m\n",
      "iteration 1059, train loss: 0.009769, validation loss: 0.005593\n",
      "iteration 1060, train loss: 0.009608, validation loss: \u001b[92m0.005547\u001b[0m\n",
      "iteration 1061, train loss: 0.009214, validation loss: \u001b[92m0.005532\u001b[0m\n",
      "iteration 1062, train loss: 0.009089, validation loss: 0.005555\n",
      "iteration 1063, train loss: 0.009048, validation loss: 0.005702\n",
      "iteration 1064, train loss: 0.00883, validation loss: 0.005757\n",
      "iteration 1065, train loss: \u001b[92m0.008545\u001b[0m, validation loss: 0.005948\n",
      "iteration 1066, train loss: 0.009746, validation loss: 0.005978\n",
      "iteration 1067, train loss: 0.009701, validation loss: 0.005821\n",
      "iteration 1068, train loss: 0.009137, validation loss: 0.005615\n",
      "iteration 1069, train loss: \u001b[92m0.008468\u001b[0m, validation loss: 0.005689\n",
      "iteration 1070, train loss: 0.009344, validation loss: 0.005556\n",
      "iteration 1071, train loss: 0.008974, validation loss: 0.005985\n",
      "iteration 1072, train loss: 0.009345, validation loss: 0.005788\n",
      "iteration 1073, train loss: 0.009299, validation loss: 0.006028\n",
      "iteration 1074, train loss: 0.009774, validation loss: 0.005863\n",
      "iteration 1075, train loss: 0.008837, validation loss: 0.00572\n",
      "iteration 1076, train loss: 0.008798, validation loss: 0.005728\n",
      "iteration 1077, train loss: 0.00917, validation loss: 0.00554\n",
      "iteration 1078, train loss: 0.009324, validation loss: \u001b[92m0.005512\u001b[0m\n",
      "iteration 1079, train loss: 0.009075, validation loss: 0.00562\n",
      "iteration 1080, train loss: 0.008795, validation loss: 0.005741\n",
      "iteration 1081, train loss: 0.009132, validation loss: 0.006196\n",
      "iteration 1082, train loss: 0.009004, validation loss: 0.006509\n",
      "iteration 1083, train loss: 0.009526, validation loss: 0.006128\n",
      "iteration 1084, train loss: 0.009562, validation loss: 0.005732\n",
      "iteration 1085, train loss: 0.008948, validation loss: 0.00566\n",
      "iteration 1086, train loss: 0.008961, validation loss: 0.005724\n",
      "iteration 1087, train loss: 0.008946, validation loss: 0.005582\n",
      "iteration 1088, train loss: 0.008895, validation loss: 0.005596\n",
      "iteration 1089, train loss: 0.008695, validation loss: 0.005693\n",
      "iteration 1090, train loss: 0.00885, validation loss: 0.005634\n",
      "iteration 1091, train loss: 0.009229, validation loss: 0.005693\n",
      "iteration 1092, train loss: 0.008946, validation loss: 0.005786\n",
      "iteration 1093, train loss: 0.009324, validation loss: 0.005703\n",
      "iteration 1094, train loss: \u001b[92m0.008359\u001b[0m, validation loss: \u001b[92m0.005482\u001b[0m\n",
      "iteration 1095, train loss: 0.009178, validation loss: \u001b[92m0.005474\u001b[0m\n",
      "iteration 1096, train loss: 0.009186, validation loss: \u001b[92m0.005415\u001b[0m\n",
      "iteration 1097, train loss: 0.008401, validation loss: \u001b[92m0.005402\u001b[0m\n",
      "iteration 1098, train loss: 0.008702, validation loss: 0.005643\n",
      "iteration 1099, train loss: 0.008584, validation loss: 0.005993\n",
      "iteration 1100, train loss: \u001b[92m0.008243\u001b[0m, validation loss: 0.005735\n",
      "iteration 1101, train loss: 0.009879, validation loss: 0.005637\n",
      "iteration 1102, train loss: 0.009385, validation loss: 0.005765\n",
      "iteration 1103, train loss: 0.009298, validation loss: 0.005765\n",
      "iteration 1104, train loss: 0.008972, validation loss: 0.006202\n",
      "iteration 1105, train loss: 0.008992, validation loss: 0.005834\n",
      "iteration 1106, train loss: 0.008764, validation loss: 0.005629\n",
      "iteration 1107, train loss: 0.008729, validation loss: 0.00602\n",
      "iteration 1108, train loss: 0.010299, validation loss: 0.005612\n",
      "iteration 1109, train loss: 0.008952, validation loss: 0.006286\n",
      "iteration 1110, train loss: 0.0086, validation loss: 0.006517\n",
      "iteration 1111, train loss: 0.01023, validation loss: 0.005628\n",
      "iteration 1112, train loss: 0.00881, validation loss: 0.006193\n",
      "iteration 1113, train loss: 0.009416, validation loss: 0.006079\n",
      "iteration 1114, train loss: 0.009771, validation loss: 0.005448\n",
      "iteration 1115, train loss: 0.008937, validation loss: 0.006074\n",
      "iteration 1116, train loss: 0.009516, validation loss: 0.00618\n",
      "iteration 1117, train loss: 0.009876, validation loss: 0.005437\n",
      "iteration 1118, train loss: 0.009005, validation loss: 0.005977\n",
      "iteration 1119, train loss: 0.0095, validation loss: 0.006216\n",
      "iteration 1120, train loss: 0.009597, validation loss: 0.005525\n",
      "iteration 1121, train loss: \u001b[92m0.008076\u001b[0m, validation loss: 0.006037\n",
      "iteration 1122, train loss: 0.009501, validation loss: 0.006038\n",
      "iteration 1123, train loss: 0.00969, validation loss: 0.005531\n",
      "iteration 1124, train loss: 0.008304, validation loss: 0.005536\n",
      "iteration 1125, train loss: 0.008582, validation loss: 0.005685\n",
      "iteration 1126, train loss: 0.008436, validation loss: 0.005489\n",
      "iteration 1127, train loss: 0.009141, validation loss: 0.005874\n",
      "iteration 1128, train loss: 0.008473, validation loss: 0.006315\n",
      "iteration 1129, train loss: 0.009964, validation loss: 0.006106\n",
      "iteration 1130, train loss: 0.009235, validation loss: 0.005599\n",
      "iteration 1131, train loss: 0.008912, validation loss: 0.005475\n",
      "iteration 1132, train loss: 0.008534, validation loss: 0.005894\n",
      "iteration 1133, train loss: 0.00967, validation loss: 0.005755\n",
      "iteration 1134, train loss: 0.008776, validation loss: 0.00558\n",
      "iteration 1135, train loss: 0.008282, validation loss: 0.005845\n",
      "iteration 1136, train loss: 0.008728, validation loss: 0.005925\n",
      "iteration 1137, train loss: 0.008617, validation loss: 0.005925\n",
      "iteration 1138, train loss: 0.008902, validation loss: 0.005542\n",
      "iteration 1139, train loss: 0.008641, validation loss: 0.005584\n",
      "iteration 1140, train loss: 0.008852, validation loss: 0.005534\n",
      "iteration 1141, train loss: 0.009334, validation loss: 0.005494\n",
      "iteration 1142, train loss: 0.008514, validation loss: 0.005668\n",
      "iteration 1143, train loss: 0.008476, validation loss: 0.005709\n",
      "iteration 1144, train loss: 0.009178, validation loss: 0.005765\n",
      "iteration 1145, train loss: 0.009045, validation loss: 0.005559\n",
      "iteration 1146, train loss: 0.008696, validation loss: 0.005573\n",
      "iteration 1147, train loss: 0.008731, validation loss: 0.005484\n",
      "iteration 1148, train loss: 0.008615, validation loss: 0.005488\n",
      "iteration 1149, train loss: 0.009438, validation loss: 0.005506\n",
      "iteration 1150, train loss: 0.009049, validation loss: 0.005765\n",
      "iteration 1151, train loss: 0.00829, validation loss: 0.005947\n",
      "iteration 1152, train loss: \u001b[92m0.007926\u001b[0m, validation loss: 0.005774\n",
      "iteration 1153, train loss: 0.008934, validation loss: 0.005598\n",
      "iteration 1154, train loss: 0.008908, validation loss: 0.005637\n",
      "iteration 1155, train loss: 0.008876, validation loss: 0.005763\n",
      "iteration 1156, train loss: 0.008492, validation loss: 0.005488\n",
      "iteration 1157, train loss: 0.00933, validation loss: 0.005782\n",
      "iteration 1158, train loss: 0.009257, validation loss: 0.006005\n",
      "iteration 1159, train loss: 0.00885, validation loss: 0.005838\n",
      "iteration 1160, train loss: 0.008913, validation loss: 0.005539\n",
      "iteration 1161, train loss: 0.009176, validation loss: 0.005687\n",
      "iteration 1162, train loss: 0.010024, validation loss: 0.005407\n",
      "iteration 1163, train loss: 0.008601, validation loss: 0.00567\n",
      "iteration 1164, train loss: 0.00865, validation loss: 0.005781\n",
      "iteration 1165, train loss: 0.008944, validation loss: 0.005502\n",
      "iteration 1166, train loss: 0.008871, validation loss: \u001b[92m0.005397\u001b[0m\n",
      "iteration 1167, train loss: 0.009195, validation loss: 0.005419\n",
      "iteration 1168, train loss: 0.00824, validation loss: 0.005505\n",
      "iteration 1169, train loss: 0.008179, validation loss: \u001b[92m0.005394\u001b[0m\n",
      "iteration 1170, train loss: 0.009489, validation loss: 0.005401\n",
      "iteration 1171, train loss: 0.008565, validation loss: 0.005484\n",
      "iteration 1172, train loss: 0.008945, validation loss: 0.005457\n",
      "iteration 1173, train loss: 0.008569, validation loss: 0.005467\n",
      "iteration 1174, train loss: 0.008825, validation loss: 0.005497\n",
      "iteration 1175, train loss: 0.008194, validation loss: 0.005421\n",
      "iteration 1176, train loss: 0.008578, validation loss: \u001b[92m0.005372\u001b[0m\n",
      "iteration 1177, train loss: 0.008445, validation loss: \u001b[92m0.005363\u001b[0m\n",
      "iteration 1178, train loss: 0.008274, validation loss: \u001b[92m0.005344\u001b[0m\n",
      "iteration 1179, train loss: 0.009189, validation loss: 0.005651\n",
      "iteration 1180, train loss: 0.008309, validation loss: 0.006163\n",
      "iteration 1181, train loss: 0.009083, validation loss: 0.005823\n",
      "iteration 1182, train loss: 0.009367, validation loss: 0.005381\n",
      "iteration 1183, train loss: 0.008581, validation loss: 0.005473\n",
      "iteration 1184, train loss: 0.009092, validation loss: 0.00553\n",
      "iteration 1185, train loss: 0.008912, validation loss: 0.005486\n",
      "iteration 1186, train loss: 0.008206, validation loss: 0.00573\n",
      "iteration 1187, train loss: 0.00859, validation loss: 0.0059\n",
      "iteration 1188, train loss: 0.008261, validation loss: 0.0057\n",
      "iteration 1189, train loss: 0.0085, validation loss: \u001b[92m0.005321\u001b[0m\n",
      "iteration 1190, train loss: 0.00822, validation loss: 0.005345\n",
      "iteration 1191, train loss: 0.008654, validation loss: 0.005408\n",
      "iteration 1192, train loss: 0.008488, validation loss: 0.005571\n",
      "iteration 1193, train loss: 0.009195, validation loss: 0.005535\n",
      "iteration 1194, train loss: 0.008335, validation loss: 0.005485\n",
      "iteration 1195, train loss: 0.008299, validation loss: 0.005625\n",
      "iteration 1196, train loss: 0.008385, validation loss: 0.005489\n",
      "iteration 1197, train loss: 0.008542, validation loss: 0.005402\n",
      "iteration 1198, train loss: 0.008501, validation loss: \u001b[92m0.005294\u001b[0m\n",
      "iteration 1199, train loss: 0.009282, validation loss: 0.005316\n",
      "iteration 1200, train loss: 0.008867, validation loss: 0.005368\n",
      "iteration 1201, train loss: 0.008147, validation loss: 0.005575\n",
      "iteration 1202, train loss: 0.00815, validation loss: 0.005956\n",
      "iteration 1203, train loss: 0.008707, validation loss: 0.005619\n",
      "iteration 1204, train loss: 0.008745, validation loss: 0.005608\n",
      "iteration 1205, train loss: 0.008085, validation loss: 0.005828\n",
      "iteration 1206, train loss: 0.00852, validation loss: 0.005617\n",
      "iteration 1207, train loss: 0.008683, validation loss: 0.005435\n",
      "iteration 1208, train loss: 0.008754, validation loss: 0.005546\n",
      "iteration 1209, train loss: 0.008782, validation loss: 0.005675\n",
      "iteration 1210, train loss: 0.009091, validation loss: 0.005328\n",
      "iteration 1211, train loss: 0.008583, validation loss: 0.005519\n",
      "iteration 1212, train loss: 0.008777, validation loss: 0.005565\n",
      "iteration 1213, train loss: 0.008367, validation loss: 0.005558\n",
      "iteration 1214, train loss: 0.008878, validation loss: 0.00532\n",
      "iteration 1215, train loss: 0.008573, validation loss: 0.005342\n",
      "iteration 1216, train loss: 0.008295, validation loss: \u001b[92m0.005261\u001b[0m\n",
      "iteration 1217, train loss: 0.008441, validation loss: 0.005394\n",
      "iteration 1218, train loss: \u001b[92m0.007923\u001b[0m, validation loss: 0.005363\n",
      "iteration 1219, train loss: 0.008593, validation loss: 0.005305\n",
      "iteration 1220, train loss: 0.008628, validation loss: 0.005325\n",
      "iteration 1221, train loss: 0.008257, validation loss: 0.005456\n",
      "iteration 1222, train loss: 0.009065, validation loss: 0.005523\n",
      "iteration 1223, train loss: 0.008138, validation loss: 0.005289\n",
      "iteration 1224, train loss: 0.009289, validation loss: 0.005508\n",
      "iteration 1225, train loss: 0.00873, validation loss: 0.005802\n",
      "iteration 1226, train loss: 0.008894, validation loss: 0.005729\n",
      "iteration 1227, train loss: 0.008506, validation loss: 0.005335\n",
      "iteration 1228, train loss: 0.008319, validation loss: 0.005648\n",
      "iteration 1229, train loss: 0.009116, validation loss: 0.005641\n",
      "iteration 1230, train loss: 0.009441, validation loss: 0.005273\n",
      "iteration 1231, train loss: 0.008447, validation loss: 0.005706\n",
      "iteration 1232, train loss: 0.009071, validation loss: 0.005766\n",
      "iteration 1233, train loss: 0.008726, validation loss: 0.005483\n",
      "iteration 1234, train loss: 0.009617, validation loss: 0.005425\n",
      "iteration 1235, train loss: 0.008848, validation loss: 0.005685\n",
      "iteration 1236, train loss: 0.009195, validation loss: 0.005293\n",
      "iteration 1237, train loss: 0.008034, validation loss: 0.005913\n",
      "iteration 1238, train loss: 0.009275, validation loss: 0.006394\n",
      "iteration 1239, train loss: 0.00956, validation loss: 0.006015\n",
      "iteration 1240, train loss: 0.008808, validation loss: \u001b[92m0.005257\u001b[0m\n",
      "iteration 1241, train loss: 0.008321, validation loss: 0.005831\n",
      "iteration 1242, train loss: 0.008661, validation loss: 0.006139\n",
      "iteration 1243, train loss: 0.008765, validation loss: 0.00578\n",
      "iteration 1244, train loss: 0.008671, validation loss: 0.005841\n",
      "iteration 1245, train loss: 0.008893, validation loss: 0.00567\n",
      "iteration 1246, train loss: 0.00906, validation loss: 0.005465\n",
      "iteration 1247, train loss: 0.008096, validation loss: 0.005424\n",
      "iteration 1248, train loss: 0.008897, validation loss: 0.00558\n",
      "iteration 1249, train loss: 0.00905, validation loss: 0.005358\n",
      "iteration 1250, train loss: 0.009076, validation loss: 0.005754\n",
      "iteration 1251, train loss: 0.008606, validation loss: 0.005819\n",
      "iteration 1252, train loss: 0.009277, validation loss: 0.005472\n",
      "iteration 1253, train loss: 0.008426, validation loss: 0.005459\n",
      "iteration 1254, train loss: 0.008293, validation loss: 0.005513\n",
      "iteration 1255, train loss: 0.00882, validation loss: 0.005451\n",
      "iteration 1256, train loss: 0.009535, validation loss: \u001b[92m0.005236\u001b[0m\n",
      "iteration 1257, train loss: 0.008057, validation loss: 0.005512\n",
      "iteration 1258, train loss: 0.008579, validation loss: 0.005836\n",
      "iteration 1259, train loss: 0.008122, validation loss: 0.005799\n",
      "iteration 1260, train loss: 0.008649, validation loss: 0.005319\n",
      "iteration 1261, train loss: 0.00813, validation loss: 0.005348\n",
      "iteration 1262, train loss: 0.008159, validation loss: 0.005699\n",
      "iteration 1263, train loss: 0.00913, validation loss: 0.005551\n",
      "iteration 1264, train loss: 0.008954, validation loss: 0.005355\n",
      "iteration 1265, train loss: 0.008545, validation loss: 0.005707\n",
      "iteration 1266, train loss: 0.008501, validation loss: 0.00597\n",
      "iteration 1267, train loss: 0.008805, validation loss: 0.005514\n",
      "iteration 1268, train loss: 0.008248, validation loss: 0.005304\n",
      "iteration 1269, train loss: 0.008026, validation loss: 0.005293\n",
      "iteration 1270, train loss: \u001b[92m0.00776\u001b[0m, validation loss: 0.005392\n",
      "iteration 1271, train loss: 0.008945, validation loss: 0.005308\n",
      "iteration 1272, train loss: 0.008899, validation loss: \u001b[92m0.005236\u001b[0m\n",
      "iteration 1273, train loss: 0.008053, validation loss: \u001b[92m0.005226\u001b[0m\n",
      "iteration 1274, train loss: \u001b[92m0.007703\u001b[0m, validation loss: 0.00532\n",
      "iteration 1275, train loss: 0.008335, validation loss: 0.00544\n",
      "iteration 1276, train loss: 0.00851, validation loss: 0.005443\n",
      "iteration 1277, train loss: 0.008277, validation loss: 0.005518\n",
      "iteration 1278, train loss: 0.008809, validation loss: 0.005372\n",
      "iteration 1279, train loss: 0.008174, validation loss: 0.005311\n",
      "iteration 1280, train loss: 0.008612, validation loss: 0.005381\n",
      "iteration 1281, train loss: 0.008988, validation loss: 0.005299\n",
      "iteration 1282, train loss: 0.008039, validation loss: 0.005299\n",
      "iteration 1283, train loss: 0.008502, validation loss: 0.005652\n",
      "iteration 1284, train loss: 0.008674, validation loss: 0.005667\n",
      "iteration 1285, train loss: 0.008292, validation loss: 0.005427\n",
      "iteration 1286, train loss: 0.00828, validation loss: \u001b[92m0.005223\u001b[0m\n",
      "iteration 1287, train loss: 0.008314, validation loss: \u001b[92m0.005182\u001b[0m\n",
      "iteration 1288, train loss: \u001b[92m0.007699\u001b[0m, validation loss: 0.005194\n",
      "iteration 1289, train loss: \u001b[92m0.007304\u001b[0m, validation loss: 0.005208\n",
      "iteration 1290, train loss: 0.007856, validation loss: 0.005329\n",
      "iteration 1291, train loss: 0.008605, validation loss: 0.005364\n",
      "iteration 1292, train loss: 0.008693, validation loss: 0.005325\n",
      "iteration 1293, train loss: 0.008572, validation loss: 0.005218\n",
      "iteration 1294, train loss: 0.00825, validation loss: 0.005231\n",
      "iteration 1295, train loss: 0.00824, validation loss: \u001b[92m0.005164\u001b[0m\n",
      "iteration 1296, train loss: 0.008428, validation loss: 0.005465\n",
      "iteration 1297, train loss: 0.00836, validation loss: 0.005582\n",
      "iteration 1298, train loss: 0.008935, validation loss: 0.005296\n",
      "iteration 1299, train loss: 0.008316, validation loss: 0.005177\n",
      "iteration 1300, train loss: 0.00816, validation loss: 0.005225\n",
      "iteration 1301, train loss: 0.008315, validation loss: 0.005289\n",
      "iteration 1302, train loss: 0.008481, validation loss: 0.005315\n",
      "iteration 1303, train loss: 0.00839, validation loss: 0.005326\n",
      "iteration 1304, train loss: 0.007596, validation loss: 0.005254\n",
      "iteration 1305, train loss: 0.008189, validation loss: \u001b[92m0.005159\u001b[0m\n",
      "iteration 1306, train loss: 0.007749, validation loss: \u001b[92m0.005157\u001b[0m\n",
      "iteration 1307, train loss: 0.008289, validation loss: 0.005188\n",
      "iteration 1308, train loss: 0.007861, validation loss: 0.005182\n",
      "iteration 1309, train loss: 0.00775, validation loss: \u001b[92m0.005153\u001b[0m\n",
      "iteration 1310, train loss: 0.007978, validation loss: 0.005156\n",
      "iteration 1311, train loss: 0.008424, validation loss: 0.005394\n",
      "iteration 1312, train loss: 0.007796, validation loss: 0.00571\n",
      "iteration 1313, train loss: 0.009029, validation loss: 0.005332\n",
      "iteration 1314, train loss: 0.008082, validation loss: 0.005206\n",
      "iteration 1315, train loss: 0.007719, validation loss: 0.00537\n",
      "iteration 1316, train loss: 0.008737, validation loss: 0.005163\n",
      "iteration 1317, train loss: 0.007486, validation loss: 0.005425\n",
      "iteration 1318, train loss: 0.008299, validation loss: 0.005416\n",
      "iteration 1319, train loss: 0.008786, validation loss: 0.005358\n",
      "iteration 1320, train loss: 0.008324, validation loss: \u001b[92m0.005139\u001b[0m\n",
      "iteration 1321, train loss: 0.008246, validation loss: 0.005269\n",
      "iteration 1322, train loss: 0.008124, validation loss: 0.005368\n",
      "iteration 1323, train loss: 0.008638, validation loss: 0.005257\n",
      "iteration 1324, train loss: 0.008847, validation loss: 0.005413\n",
      "iteration 1325, train loss: 0.008123, validation loss: 0.005501\n",
      "iteration 1326, train loss: 0.008629, validation loss: 0.005269\n",
      "iteration 1327, train loss: 0.008067, validation loss: 0.005283\n",
      "iteration 1328, train loss: 0.008469, validation loss: 0.005219\n",
      "iteration 1329, train loss: 0.008475, validation loss: 0.00522\n",
      "iteration 1330, train loss: 0.008365, validation loss: 0.005538\n",
      "iteration 1331, train loss: 0.007678, validation loss: 0.005769\n",
      "iteration 1332, train loss: 0.009011, validation loss: 0.005405\n",
      "iteration 1333, train loss: 0.008757, validation loss: 0.005321\n",
      "iteration 1334, train loss: 0.007681, validation loss: 0.005532\n",
      "iteration 1335, train loss: 0.007862, validation loss: 0.005264\n",
      "iteration 1336, train loss: 0.008498, validation loss: 0.005199\n",
      "iteration 1337, train loss: 0.008222, validation loss: 0.005376\n",
      "iteration 1338, train loss: 0.008347, validation loss: 0.005243\n",
      "iteration 1339, train loss: 0.007945, validation loss: \u001b[92m0.005116\u001b[0m\n",
      "iteration 1340, train loss: 0.008229, validation loss: 0.005174\n",
      "iteration 1341, train loss: 0.008402, validation loss: 0.005192\n",
      "iteration 1342, train loss: 0.009019, validation loss: 0.005354\n",
      "iteration 1343, train loss: 0.008783, validation loss: 0.005807\n",
      "iteration 1344, train loss: 0.008568, validation loss: 0.005618\n",
      "iteration 1345, train loss: 0.008097, validation loss: 0.005188\n",
      "iteration 1346, train loss: 0.008615, validation loss: 0.00528\n",
      "iteration 1347, train loss: 0.008064, validation loss: 0.005597\n",
      "iteration 1348, train loss: 0.008405, validation loss: 0.005197\n",
      "iteration 1349, train loss: 0.008078, validation loss: 0.005301\n",
      "iteration 1350, train loss: 0.009166, validation loss: 0.005543\n",
      "iteration 1351, train loss: 0.00834, validation loss: 0.005447\n",
      "iteration 1352, train loss: 0.008361, validation loss: 0.005196\n",
      "iteration 1353, train loss: 0.00768, validation loss: 0.005191\n",
      "iteration 1354, train loss: 0.008496, validation loss: 0.005402\n",
      "iteration 1355, train loss: 0.008299, validation loss: 0.005332\n",
      "iteration 1356, train loss: 0.00783, validation loss: 0.005384\n",
      "iteration 1357, train loss: 0.007968, validation loss: 0.00574\n",
      "iteration 1358, train loss: 0.007916, validation loss: 0.005773\n",
      "iteration 1359, train loss: 0.008117, validation loss: 0.005363\n",
      "iteration 1360, train loss: 0.008451, validation loss: 0.005339\n",
      "iteration 1361, train loss: 0.008159, validation loss: 0.005728\n",
      "iteration 1362, train loss: 0.008694, validation loss: 0.005311\n",
      "iteration 1363, train loss: 0.008717, validation loss: 0.005207\n",
      "iteration 1364, train loss: 0.007662, validation loss: 0.00566\n",
      "iteration 1365, train loss: 0.008395, validation loss: 0.005672\n",
      "iteration 1366, train loss: 0.008192, validation loss: 0.005256\n",
      "iteration 1367, train loss: 0.008172, validation loss: 0.005243\n",
      "iteration 1368, train loss: 0.007942, validation loss: 0.005332\n",
      "iteration 1369, train loss: 0.008154, validation loss: 0.00526\n",
      "iteration 1370, train loss: 0.00881, validation loss: 0.005271\n",
      "iteration 1371, train loss: 0.007662, validation loss: 0.005401\n",
      "iteration 1372, train loss: 0.008156, validation loss: 0.005497\n",
      "iteration 1373, train loss: 0.007855, validation loss: 0.005418\n",
      "iteration 1374, train loss: 0.00803, validation loss: 0.00525\n",
      "iteration 1375, train loss: 0.007672, validation loss: 0.005256\n",
      "iteration 1376, train loss: 0.008477, validation loss: 0.005339\n",
      "iteration 1377, train loss: 0.00807, validation loss: 0.005438\n",
      "iteration 1378, train loss: 0.007873, validation loss: 0.005606\n",
      "iteration 1379, train loss: 0.007854, validation loss: 0.005706\n",
      "iteration 1380, train loss: 0.008226, validation loss: 0.005354\n",
      "iteration 1381, train loss: 0.008502, validation loss: \u001b[92m0.005072\u001b[0m\n",
      "iteration 1382, train loss: 0.008366, validation loss: 0.005346\n",
      "iteration 1383, train loss: 0.008714, validation loss: 0.005529\n",
      "iteration 1384, train loss: 0.008457, validation loss: 0.005164\n",
      "iteration 1385, train loss: 0.007624, validation loss: 0.00534\n",
      "iteration 1386, train loss: 0.007977, validation loss: 0.00588\n",
      "iteration 1387, train loss: 0.008107, validation loss: 0.005788\n",
      "iteration 1388, train loss: 0.008465, validation loss: 0.005168\n",
      "iteration 1389, train loss: 0.007318, validation loss: 0.005197\n",
      "iteration 1390, train loss: 0.008429, validation loss: 0.005362\n",
      "iteration 1391, train loss: 0.008643, validation loss: 0.005201\n",
      "iteration 1392, train loss: 0.008147, validation loss: 0.005401\n",
      "iteration 1393, train loss: 0.008718, validation loss: 0.00566\n",
      "iteration 1394, train loss: 0.007686, validation loss: 0.005588\n",
      "iteration 1395, train loss: 0.008032, validation loss: 0.005275\n",
      "iteration 1396, train loss: 0.007951, validation loss: 0.005232\n",
      "iteration 1397, train loss: 0.008177, validation loss: 0.005276\n",
      "iteration 1398, train loss: 0.008126, validation loss: 0.00525\n",
      "iteration 1399, train loss: 0.00797, validation loss: 0.005189\n",
      "iteration 1400, train loss: 0.007898, validation loss: 0.005562\n",
      "iteration 1401, train loss: 0.008129, validation loss: 0.00568\n",
      "iteration 1402, train loss: 0.008485, validation loss: 0.005098\n",
      "iteration 1403, train loss: 0.007522, validation loss: 0.005155\n",
      "iteration 1404, train loss: 0.007773, validation loss: 0.005231\n",
      "iteration 1405, train loss: 0.008204, validation loss: 0.005108\n",
      "iteration 1406, train loss: 0.008319, validation loss: 0.005107\n",
      "iteration 1407, train loss: 0.008258, validation loss: 0.005301\n",
      "iteration 1408, train loss: 0.008226, validation loss: 0.005212\n",
      "iteration 1409, train loss: 0.007773, validation loss: 0.005122\n",
      "iteration 1410, train loss: 0.00818, validation loss: 0.005099\n",
      "iteration 1411, train loss: 0.007837, validation loss: 0.005154\n",
      "iteration 1412, train loss: 0.007486, validation loss: 0.005198\n",
      "iteration 1413, train loss: \u001b[92m0.007079\u001b[0m, validation loss: 0.005096\n",
      "iteration 1414, train loss: 0.007748, validation loss: 0.005131\n",
      "iteration 1415, train loss: 0.008194, validation loss: 0.005198\n",
      "iteration 1416, train loss: 0.007972, validation loss: 0.005214\n",
      "iteration 1417, train loss: 0.007864, validation loss: 0.005242\n",
      "iteration 1418, train loss: 0.007733, validation loss: 0.005185\n",
      "iteration 1419, train loss: 0.008029, validation loss: 0.005093\n",
      "iteration 1420, train loss: 0.007473, validation loss: \u001b[92m0.005068\u001b[0m\n",
      "iteration 1421, train loss: 0.008403, validation loss: 0.005089\n",
      "iteration 1422, train loss: 0.007821, validation loss: 0.005141\n",
      "iteration 1423, train loss: 0.008262, validation loss: 0.005335\n",
      "iteration 1424, train loss: 0.0081, validation loss: 0.005187\n",
      "iteration 1425, train loss: 0.007733, validation loss: 0.005199\n",
      "iteration 1426, train loss: 0.007833, validation loss: 0.005283\n",
      "iteration 1427, train loss: 0.008131, validation loss: 0.005279\n",
      "iteration 1428, train loss: 0.007958, validation loss: 0.005331\n",
      "iteration 1429, train loss: 0.007638, validation loss: 0.005205\n",
      "iteration 1430, train loss: 0.007563, validation loss: 0.005108\n",
      "iteration 1431, train loss: 0.007626, validation loss: 0.005129\n",
      "iteration 1432, train loss: 0.00812, validation loss: 0.005229\n",
      "iteration 1433, train loss: 0.00784, validation loss: 0.005169\n",
      "iteration 1434, train loss: 0.007699, validation loss: \u001b[92m0.005062\u001b[0m\n",
      "iteration 1435, train loss: 0.00785, validation loss: 0.005118\n",
      "iteration 1436, train loss: 0.007266, validation loss: 0.005372\n",
      "iteration 1437, train loss: 0.008175, validation loss: 0.005382\n",
      "iteration 1438, train loss: 0.00799, validation loss: 0.005202\n",
      "iteration 1439, train loss: 0.008161, validation loss: 0.005089\n",
      "iteration 1440, train loss: 0.007265, validation loss: 0.005084\n",
      "iteration 1441, train loss: 0.007937, validation loss: 0.005098\n",
      "iteration 1442, train loss: 0.007888, validation loss: 0.005072\n",
      "iteration 1443, train loss: 0.007786, validation loss: 0.005074\n",
      "iteration 1444, train loss: \u001b[92m0.007008\u001b[0m, validation loss: \u001b[92m0.005061\u001b[0m\n",
      "iteration 1445, train loss: 0.007725, validation loss: 0.005088\n",
      "iteration 1446, train loss: 0.008138, validation loss: 0.005139\n",
      "iteration 1447, train loss: 0.008126, validation loss: 0.005221\n",
      "iteration 1448, train loss: 0.007604, validation loss: 0.005186\n",
      "iteration 1449, train loss: 0.008137, validation loss: \u001b[92m0.005012\u001b[0m\n",
      "iteration 1450, train loss: 0.007634, validation loss: 0.005071\n",
      "iteration 1451, train loss: 0.007928, validation loss: 0.005058\n",
      "iteration 1452, train loss: 0.007784, validation loss: 0.00505\n",
      "iteration 1453, train loss: 0.007876, validation loss: 0.005019\n",
      "iteration 1454, train loss: 0.0075, validation loss: 0.00507\n",
      "iteration 1455, train loss: 0.007192, validation loss: 0.005187\n",
      "iteration 1456, train loss: 0.007746, validation loss: 0.005125\n",
      "iteration 1457, train loss: 0.008032, validation loss: 0.005299\n",
      "iteration 1458, train loss: 0.008167, validation loss: 0.005278\n",
      "iteration 1459, train loss: 0.007933, validation loss: 0.005065\n",
      "iteration 1460, train loss: 0.00815, validation loss: 0.005254\n",
      "iteration 1461, train loss: 0.008321, validation loss: 0.005407\n",
      "iteration 1462, train loss: 0.007782, validation loss: 0.00591\n",
      "iteration 1463, train loss: 0.008209, validation loss: 0.00556\n",
      "iteration 1464, train loss: 0.007939, validation loss: 0.005366\n",
      "iteration 1465, train loss: 0.007723, validation loss: 0.005327\n",
      "iteration 1466, train loss: 0.008831, validation loss: 0.005289\n",
      "iteration 1467, train loss: 0.007858, validation loss: 0.005134\n",
      "iteration 1468, train loss: 0.008081, validation loss: 0.005637\n",
      "iteration 1469, train loss: 0.007759, validation loss: 0.005553\n",
      "iteration 1470, train loss: 0.008573, validation loss: 0.00518\n",
      "iteration 1471, train loss: 0.007701, validation loss: 0.00525\n",
      "iteration 1472, train loss: 0.008716, validation loss: 0.005323\n",
      "iteration 1473, train loss: 0.007691, validation loss: 0.005135\n",
      "iteration 1474, train loss: 0.007468, validation loss: 0.005201\n",
      "iteration 1475, train loss: 0.008378, validation loss: 0.005246\n",
      "iteration 1476, train loss: 0.007601, validation loss: 0.005103\n",
      "iteration 1477, train loss: 0.007849, validation loss: 0.005205\n",
      "iteration 1478, train loss: 0.007789, validation loss: 0.005259\n",
      "iteration 1479, train loss: 0.008153, validation loss: 0.005086\n",
      "iteration 1480, train loss: 0.007643, validation loss: 0.005305\n",
      "iteration 1481, train loss: 0.007535, validation loss: 0.005507\n",
      "iteration 1482, train loss: 0.007525, validation loss: 0.005273\n",
      "iteration 1483, train loss: 0.007727, validation loss: 0.005053\n",
      "iteration 1484, train loss: 0.00783, validation loss: 0.005149\n",
      "iteration 1485, train loss: 0.007779, validation loss: 0.0052\n",
      "iteration 1486, train loss: 0.008168, validation loss: 0.005042\n",
      "iteration 1487, train loss: 0.007845, validation loss: 0.005039\n",
      "iteration 1488, train loss: 0.007582, validation loss: 0.00591\n",
      "iteration 1489, train loss: 0.008021, validation loss: 0.005805\n",
      "iteration 1490, train loss: 0.008139, validation loss: 0.005132\n",
      "iteration 1491, train loss: 0.007702, validation loss: 0.005202\n",
      "iteration 1492, train loss: 0.007594, validation loss: 0.005494\n",
      "iteration 1493, train loss: 0.008543, validation loss: 0.005156\n",
      "iteration 1494, train loss: 0.007745, validation loss: 0.005158\n",
      "iteration 1495, train loss: 0.007313, validation loss: 0.005687\n",
      "iteration 1496, train loss: 0.007777, validation loss: 0.005564\n",
      "iteration 1497, train loss: 0.008494, validation loss: 0.005045\n",
      "iteration 1498, train loss: 0.0074, validation loss: 0.005297\n",
      "iteration 1499, train loss: 0.00805, validation loss: 0.00548\n",
      "iteration 1500, train loss: 0.008277, validation loss: 0.005086\n",
      "iteration 1501, train loss: 0.00742, validation loss: 0.005525\n",
      "iteration 1502, train loss: 0.007707, validation loss: 0.005881\n",
      "iteration 1503, train loss: 0.008253, validation loss: 0.005555\n",
      "iteration 1504, train loss: 0.007347, validation loss: 0.005166\n",
      "iteration 1505, train loss: 0.007727, validation loss: 0.005178\n",
      "iteration 1506, train loss: 0.007742, validation loss: 0.005304\n",
      "iteration 1507, train loss: 0.007946, validation loss: 0.005255\n",
      "iteration 1508, train loss: 0.007655, validation loss: 0.005367\n",
      "iteration 1509, train loss: 0.007534, validation loss: 0.005396\n",
      "iteration 1510, train loss: 0.007639, validation loss: 0.005227\n",
      "iteration 1511, train loss: 0.007546, validation loss: 0.005346\n",
      "iteration 1512, train loss: 0.00837, validation loss: 0.005157\n",
      "iteration 1513, train loss: 0.007571, validation loss: 0.005166\n",
      "iteration 1514, train loss: 0.007962, validation loss: 0.005194\n",
      "iteration 1515, train loss: 0.008163, validation loss: 0.005188\n",
      "iteration 1516, train loss: 0.007669, validation loss: 0.005192\n",
      "iteration 1517, train loss: 0.007203, validation loss: 0.005258\n",
      "iteration 1518, train loss: 0.007463, validation loss: 0.005162\n",
      "iteration 1519, train loss: 0.007514, validation loss: 0.005024\n",
      "iteration 1520, train loss: 0.007305, validation loss: 0.005095\n",
      "iteration 1521, train loss: 0.008171, validation loss: 0.005148\n",
      "iteration 1522, train loss: 0.007836, validation loss: 0.005037\n",
      "iteration 1523, train loss: 0.007566, validation loss: \u001b[92m0.005003\u001b[0m\n",
      "iteration 1524, train loss: \u001b[92m0.006981\u001b[0m, validation loss: 0.005109\n",
      "iteration 1525, train loss: 0.007461, validation loss: 0.005125\n",
      "iteration 1526, train loss: 0.007762, validation loss: 0.005164\n",
      "iteration 1527, train loss: 0.008008, validation loss: 0.005075\n",
      "iteration 1528, train loss: 0.007656, validation loss: 0.005263\n",
      "iteration 1529, train loss: 0.007709, validation loss: 0.005207\n",
      "iteration 1530, train loss: 0.007989, validation loss: 0.005339\n",
      "iteration 1531, train loss: 0.007745, validation loss: 0.005279\n",
      "iteration 1532, train loss: 0.007966, validation loss: 0.005075\n",
      "iteration 1533, train loss: 0.008151, validation loss: 0.005114\n",
      "iteration 1534, train loss: 0.007296, validation loss: 0.005313\n",
      "iteration 1535, train loss: 0.0075, validation loss: 0.005297\n",
      "iteration 1536, train loss: 0.007813, validation loss: 0.005084\n",
      "iteration 1537, train loss: \u001b[92m0.006942\u001b[0m, validation loss: 0.005031\n",
      "iteration 1538, train loss: 0.007384, validation loss: 0.005141\n",
      "iteration 1539, train loss: 0.007352, validation loss: 0.005086\n",
      "iteration 1540, train loss: 0.00734, validation loss: 0.005091\n",
      "iteration 1541, train loss: 0.008018, validation loss: 0.005097\n",
      "iteration 1542, train loss: 0.007178, validation loss: 0.005256\n",
      "iteration 1543, train loss: 0.008783, validation loss: 0.005048\n",
      "iteration 1544, train loss: 0.007691, validation loss: 0.005196\n",
      "iteration 1545, train loss: 0.007597, validation loss: 0.005499\n",
      "iteration 1546, train loss: 0.007363, validation loss: 0.005032\n",
      "iteration 1547, train loss: 0.007759, validation loss: 0.005048\n",
      "iteration 1548, train loss: 0.007232, validation loss: 0.005236\n",
      "iteration 1549, train loss: 0.00788, validation loss: 0.005016\n",
      "iteration 1550, train loss: 0.007334, validation loss: \u001b[92m0.004996\u001b[0m\n",
      "iteration 1551, train loss: 0.007473, validation loss: 0.005268\n",
      "iteration 1552, train loss: 0.007703, validation loss: 0.005069\n",
      "iteration 1553, train loss: 0.007243, validation loss: 0.004997\n",
      "iteration 1554, train loss: 0.007186, validation loss: 0.005044\n",
      "iteration 1555, train loss: 0.007429, validation loss: 0.005073\n",
      "iteration 1556, train loss: 0.007719, validation loss: 0.005151\n",
      "iteration 1557, train loss: 0.007043, validation loss: 0.00518\n",
      "iteration 1558, train loss: 0.007042, validation loss: 0.005019\n",
      "iteration 1559, train loss: 0.007059, validation loss: 0.005067\n",
      "iteration 1560, train loss: 0.007525, validation loss: 0.005183\n",
      "iteration 1561, train loss: 0.007231, validation loss: 0.005063\n",
      "iteration 1562, train loss: 0.007316, validation loss: \u001b[92m0.004935\u001b[0m\n",
      "iteration 1563, train loss: 0.007014, validation loss: 0.005204\n",
      "iteration 1564, train loss: 0.008017, validation loss: 0.005266\n",
      "iteration 1565, train loss: 0.007655, validation loss: 0.005086\n",
      "iteration 1566, train loss: 0.007707, validation loss: 0.005481\n",
      "iteration 1567, train loss: 0.007791, validation loss: 0.005444\n",
      "iteration 1568, train loss: 0.00727, validation loss: 0.005078\n",
      "iteration 1569, train loss: 0.007492, validation loss: 0.004968\n",
      "iteration 1570, train loss: 0.007443, validation loss: 0.005054\n",
      "iteration 1571, train loss: 0.008096, validation loss: 0.005049\n",
      "iteration 1572, train loss: 0.007245, validation loss: 0.005174\n",
      "iteration 1573, train loss: 0.007521, validation loss: 0.005081\n",
      "iteration 1574, train loss: 0.00731, validation loss: 0.004981\n",
      "iteration 1575, train loss: 0.007414, validation loss: 0.004984\n",
      "iteration 1576, train loss: 0.007458, validation loss: 0.005022\n",
      "iteration 1577, train loss: 0.007032, validation loss: 0.004978\n",
      "iteration 1578, train loss: 0.007093, validation loss: 0.005039\n",
      "iteration 1579, train loss: 0.007421, validation loss: 0.005124\n",
      "iteration 1580, train loss: 0.007643, validation loss: 0.005121\n",
      "iteration 1581, train loss: 0.007565, validation loss: 0.00525\n",
      "iteration 1582, train loss: 0.007475, validation loss: 0.00528\n",
      "iteration 1583, train loss: 0.006969, validation loss: 0.005306\n",
      "iteration 1584, train loss: 0.006993, validation loss: 0.005062\n",
      "iteration 1585, train loss: 0.007181, validation loss: 0.005056\n",
      "iteration 1586, train loss: 0.00736, validation loss: 0.005042\n",
      "iteration 1587, train loss: 0.00724, validation loss: 0.005002\n",
      "iteration 1588, train loss: 0.008012, validation loss: 0.005062\n",
      "iteration 1589, train loss: 0.007462, validation loss: 0.005078\n",
      "iteration 1590, train loss: 0.007098, validation loss: 0.004999\n",
      "iteration 1591, train loss: 0.0073, validation loss: \u001b[92m0.004877\u001b[0m\n",
      "iteration 1592, train loss: 0.007682, validation loss: 0.004984\n",
      "iteration 1593, train loss: 0.007919, validation loss: 0.004917\n",
      "iteration 1594, train loss: 0.007707, validation loss: 0.004919\n",
      "iteration 1595, train loss: 0.007933, validation loss: 0.005056\n",
      "iteration 1596, train loss: 0.007862, validation loss: 0.005023\n",
      "iteration 1597, train loss: 0.007225, validation loss: 0.004971\n",
      "iteration 1598, train loss: 0.007303, validation loss: 0.004953\n",
      "iteration 1599, train loss: 0.007432, validation loss: 0.004994\n",
      "iteration 1600, train loss: 0.007281, validation loss: 0.00503\n",
      "iteration 1601, train loss: 0.0074, validation loss: 0.0051\n",
      "iteration 1602, train loss: 0.00711, validation loss: 0.005154\n",
      "iteration 1603, train loss: 0.00737, validation loss: 0.005106\n",
      "iteration 1604, train loss: 0.007711, validation loss: 0.005061\n",
      "iteration 1605, train loss: 0.007051, validation loss: 0.005091\n",
      "iteration 1606, train loss: \u001b[92m0.006695\u001b[0m, validation loss: 0.005064\n",
      "iteration 1607, train loss: 0.007474, validation loss: 0.004984\n",
      "iteration 1608, train loss: 0.007477, validation loss: 0.004994\n",
      "iteration 1609, train loss: 0.007287, validation loss: 0.005229\n",
      "iteration 1610, train loss: 0.007459, validation loss: 0.005061\n",
      "iteration 1611, train loss: 0.006734, validation loss: 0.00492\n",
      "iteration 1612, train loss: 0.00737, validation loss: 0.004991\n",
      "iteration 1613, train loss: 0.007268, validation loss: 0.00502\n",
      "iteration 1614, train loss: 0.007401, validation loss: 0.005042\n",
      "iteration 1615, train loss: 0.007284, validation loss: 0.005041\n",
      "iteration 1616, train loss: 0.007441, validation loss: 0.004991\n",
      "iteration 1617, train loss: 0.007331, validation loss: 0.005017\n",
      "iteration 1618, train loss: 0.007111, validation loss: 0.005008\n",
      "iteration 1619, train loss: 0.006936, validation loss: 0.005004\n",
      "iteration 1620, train loss: 0.007011, validation loss: 0.005073\n",
      "iteration 1621, train loss: 0.007418, validation loss: 0.005079\n",
      "iteration 1622, train loss: 0.007024, validation loss: 0.005161\n",
      "iteration 1623, train loss: 0.007672, validation loss: 0.005014\n",
      "iteration 1624, train loss: 0.007131, validation loss: 0.005118\n",
      "iteration 1625, train loss: 0.007263, validation loss: 0.00542\n",
      "iteration 1626, train loss: 0.006962, validation loss: 0.005365\n",
      "iteration 1627, train loss: 0.007766, validation loss: 0.004928\n",
      "iteration 1628, train loss: 0.007016, validation loss: 0.005147\n",
      "iteration 1629, train loss: 0.007238, validation loss: 0.004939\n",
      "iteration 1630, train loss: 0.0073, validation loss: 0.005013\n",
      "iteration 1631, train loss: 0.007057, validation loss: 0.005359\n",
      "iteration 1632, train loss: 0.007693, validation loss: 0.005343\n",
      "iteration 1633, train loss: 0.007592, validation loss: 0.004981\n",
      "iteration 1634, train loss: 0.007011, validation loss: 0.004982\n",
      "iteration 1635, train loss: 0.00794, validation loss: 0.004977\n",
      "iteration 1636, train loss: 0.007216, validation loss: 0.004979\n",
      "iteration 1637, train loss: 0.007006, validation loss: 0.004943\n",
      "iteration 1638, train loss: 0.006821, validation loss: 0.004996\n",
      "iteration 1639, train loss: 0.006995, validation loss: 0.005007\n",
      "iteration 1640, train loss: 0.007064, validation loss: 0.004974\n",
      "iteration 1641, train loss: 0.007015, validation loss: 0.005237\n",
      "iteration 1642, train loss: 0.007642, validation loss: 0.005027\n",
      "iteration 1643, train loss: 0.00769, validation loss: 0.004948\n",
      "iteration 1644, train loss: 0.007415, validation loss: 0.005318\n",
      "iteration 1645, train loss: 0.007405, validation loss: 0.005046\n",
      "iteration 1646, train loss: 0.007662, validation loss: 0.005005\n",
      "iteration 1647, train loss: 0.007075, validation loss: 0.005271\n",
      "iteration 1648, train loss: 0.007759, validation loss: 0.005107\n",
      "iteration 1649, train loss: 0.00686, validation loss: 0.005016\n",
      "iteration 1650, train loss: 0.006931, validation loss: 0.00534\n",
      "iteration 1651, train loss: 0.007179, validation loss: 0.005251\n",
      "iteration 1652, train loss: 0.00768, validation loss: 0.005003\n",
      "iteration 1653, train loss: 0.00717, validation loss: 0.005196\n",
      "iteration 1654, train loss: 0.007594, validation loss: 0.005405\n",
      "iteration 1655, train loss: 0.00767, validation loss: 0.005302\n",
      "iteration 1656, train loss: 0.007453, validation loss: 0.00503\n",
      "iteration 1657, train loss: 0.007187, validation loss: 0.005341\n",
      "iteration 1658, train loss: 0.007473, validation loss: 0.005362\n",
      "iteration 1659, train loss: 0.007161, validation loss: 0.004925\n",
      "iteration 1660, train loss: 0.007244, validation loss: 0.005084\n",
      "iteration 1661, train loss: 0.006845, validation loss: 0.005361\n",
      "iteration 1662, train loss: 0.006906, validation loss: 0.005281\n",
      "iteration 1663, train loss: 0.006786, validation loss: 0.004971\n",
      "iteration 1664, train loss: 0.006773, validation loss: 0.005185\n",
      "iteration 1665, train loss: 0.007539, validation loss: 0.005363\n",
      "iteration 1666, train loss: 0.007341, validation loss: 0.005291\n",
      "iteration 1667, train loss: 0.007812, validation loss: 0.005174\n",
      "iteration 1668, train loss: 0.007714, validation loss: 0.005016\n",
      "iteration 1669, train loss: 0.007071, validation loss: 0.005136\n",
      "iteration 1670, train loss: 0.006972, validation loss: 0.005302\n",
      "iteration 1671, train loss: 0.007817, validation loss: 0.005007\n",
      "iteration 1672, train loss: 0.006862, validation loss: 0.005254\n",
      "iteration 1673, train loss: 0.00718, validation loss: 0.005388\n",
      "iteration 1674, train loss: 0.007435, validation loss: 0.005221\n",
      "iteration 1675, train loss: 0.007336, validation loss: 0.005003\n",
      "iteration 1676, train loss: 0.007019, validation loss: 0.005075\n",
      "iteration 1677, train loss: 0.007332, validation loss: \u001b[92m0.004873\u001b[0m\n",
      "iteration 1678, train loss: 0.007177, validation loss: 0.005048\n",
      "iteration 1679, train loss: 0.006897, validation loss: 0.00542\n",
      "iteration 1680, train loss: 0.006884, validation loss: 0.005384\n",
      "iteration 1681, train loss: 0.007115, validation loss: 0.005045\n",
      "iteration 1682, train loss: 0.007348, validation loss: 0.004891\n",
      "iteration 1683, train loss: 0.006995, validation loss: 0.005069\n",
      "iteration 1684, train loss: 0.006952, validation loss: 0.004928\n",
      "iteration 1685, train loss: 0.007009, validation loss: \u001b[92m0.00486\u001b[0m\n",
      "iteration 1686, train loss: 0.007173, validation loss: 0.005027\n",
      "iteration 1687, train loss: 0.006871, validation loss: 0.005245\n",
      "iteration 1688, train loss: 0.007364, validation loss: 0.005166\n",
      "iteration 1689, train loss: \u001b[92m0.006658\u001b[0m, validation loss: 0.00503\n",
      "iteration 1690, train loss: 0.006947, validation loss: 0.004941\n",
      "iteration 1691, train loss: 0.006999, validation loss: 0.00501\n",
      "iteration 1692, train loss: \u001b[92m0.006646\u001b[0m, validation loss: 0.004929\n",
      "iteration 1693, train loss: 0.007093, validation loss: 0.004882\n",
      "iteration 1694, train loss: 0.007084, validation loss: 0.00506\n",
      "iteration 1695, train loss: 0.007051, validation loss: 0.005146\n",
      "iteration 1696, train loss: 0.007049, validation loss: 0.0049\n",
      "iteration 1697, train loss: 0.006882, validation loss: \u001b[92m0.004769\u001b[0m\n",
      "iteration 1698, train loss: 0.007285, validation loss: 0.004836\n",
      "iteration 1699, train loss: 0.007242, validation loss: 0.004817\n",
      "iteration 1700, train loss: 0.007366, validation loss: 0.005432\n",
      "iteration 1701, train loss: 0.007213, validation loss: 0.005546\n",
      "iteration 1702, train loss: 0.006914, validation loss: 0.005063\n",
      "iteration 1703, train loss: 0.007058, validation loss: 0.004786\n",
      "iteration 1704, train loss: 0.006793, validation loss: 0.004836\n",
      "iteration 1705, train loss: 0.006668, validation loss: \u001b[92m0.004749\u001b[0m\n",
      "iteration 1706, train loss: \u001b[92m0.006187\u001b[0m, validation loss: \u001b[92m0.004705\u001b[0m\n",
      "iteration 1707, train loss: 0.00676, validation loss: 0.004754\n",
      "iteration 1708, train loss: 0.006835, validation loss: 0.00483\n",
      "iteration 1709, train loss: 0.006754, validation loss: 0.00478\n",
      "iteration 1710, train loss: 0.006534, validation loss: 0.004819\n",
      "iteration 1711, train loss: 0.006674, validation loss: 0.004942\n",
      "iteration 1712, train loss: 0.006593, validation loss: 0.004824\n",
      "iteration 1713, train loss: 0.006796, validation loss: 0.004773\n",
      "iteration 1714, train loss: 0.006667, validation loss: \u001b[92m0.004701\u001b[0m\n",
      "iteration 1715, train loss: \u001b[92m0.006132\u001b[0m, validation loss: \u001b[92m0.00463\u001b[0m\n",
      "iteration 1716, train loss: 0.00707, validation loss: \u001b[92m0.004589\u001b[0m\n",
      "iteration 1717, train loss: 0.00741, validation loss: 0.004653\n",
      "iteration 1718, train loss: 0.006969, validation loss: 0.004603\n",
      "iteration 1719, train loss: 0.006667, validation loss: 0.004701\n",
      "iteration 1720, train loss: 0.006392, validation loss: 0.005083\n",
      "iteration 1721, train loss: 0.006968, validation loss: 0.004908\n",
      "iteration 1722, train loss: 0.006767, validation loss: 0.00487\n",
      "iteration 1723, train loss: 0.006341, validation loss: 0.004938\n",
      "iteration 1724, train loss: 0.006739, validation loss: 0.004817\n",
      "iteration 1725, train loss: 0.007079, validation loss: \u001b[92m0.004494\u001b[0m\n",
      "iteration 1726, train loss: 0.006531, validation loss: \u001b[92m0.004464\u001b[0m\n",
      "iteration 1727, train loss: 0.006311, validation loss: 0.00467\n",
      "iteration 1728, train loss: 0.006866, validation loss: 0.00475\n",
      "iteration 1729, train loss: 0.006506, validation loss: 0.004617\n",
      "iteration 1730, train loss: 0.007089, validation loss: 0.004547\n",
      "iteration 1731, train loss: 0.006468, validation loss: 0.004864\n",
      "iteration 1732, train loss: 0.006532, validation loss: 0.004871\n",
      "iteration 1733, train loss: 0.006538, validation loss: 0.004787\n",
      "iteration 1734, train loss: 0.006717, validation loss: 0.00474\n",
      "iteration 1735, train loss: 0.006512, validation loss: 0.004715\n",
      "iteration 1736, train loss: 0.006557, validation loss: 0.004465\n",
      "iteration 1737, train loss: 0.006828, validation loss: \u001b[92m0.004312\u001b[0m\n",
      "iteration 1738, train loss: 0.0062, validation loss: 0.004616\n",
      "iteration 1739, train loss: 0.006676, validation loss: 0.005137\n",
      "iteration 1740, train loss: 0.007003, validation loss: 0.004413\n",
      "iteration 1741, train loss: 0.00673, validation loss: 0.004502\n",
      "iteration 1742, train loss: 0.006915, validation loss: 0.004518\n",
      "iteration 1743, train loss: 0.006724, validation loss: \u001b[92m0.004299\u001b[0m\n",
      "iteration 1744, train loss: 0.00639, validation loss: 0.004397\n",
      "iteration 1745, train loss: 0.006226, validation loss: 0.004512\n",
      "iteration 1746, train loss: 0.006386, validation loss: 0.004431\n",
      "iteration 1747, train loss: 0.006298, validation loss: 0.00441\n",
      "iteration 1748, train loss: 0.006425, validation loss: \u001b[92m0.004221\u001b[0m\n",
      "iteration 1749, train loss: 0.006259, validation loss: 0.004366\n",
      "iteration 1750, train loss: 0.006392, validation loss: 0.0044\n",
      "iteration 1751, train loss: 0.006501, validation loss: 0.004759\n",
      "iteration 1752, train loss: 0.00673, validation loss: 0.00496\n",
      "iteration 1753, train loss: 0.0077, validation loss: \u001b[92m0.004103\u001b[0m\n",
      "iteration 1754, train loss: 0.006228, validation loss: 0.004531\n",
      "iteration 1755, train loss: 0.006479, validation loss: 0.005149\n",
      "iteration 1756, train loss: 0.006516, validation loss: 0.004288\n",
      "iteration 1757, train loss: \u001b[92m0.006035\u001b[0m, validation loss: 0.004509\n",
      "iteration 1758, train loss: 0.006386, validation loss: 0.004416\n",
      "iteration 1759, train loss: 0.006833, validation loss: 0.004453\n",
      "iteration 1760, train loss: 0.006645, validation loss: 0.004543\n",
      "iteration 1761, train loss: \u001b[92m0.005921\u001b[0m, validation loss: 0.004248\n",
      "iteration 1762, train loss: 0.006298, validation loss: 0.004166\n",
      "iteration 1763, train loss: 0.006409, validation loss: 0.004152\n",
      "iteration 1764, train loss: 0.006143, validation loss: 0.004191\n",
      "iteration 1765, train loss: 0.00598, validation loss: \u001b[92m0.00405\u001b[0m\n",
      "iteration 1766, train loss: 0.006643, validation loss: \u001b[92m0.004028\u001b[0m\n",
      "iteration 1767, train loss: 0.006002, validation loss: 0.004271\n",
      "iteration 1768, train loss: 0.006019, validation loss: 0.004295\n",
      "iteration 1769, train loss: \u001b[92m0.005841\u001b[0m, validation loss: 0.004403\n",
      "iteration 1770, train loss: 0.00587, validation loss: \u001b[92m0.003986\u001b[0m\n",
      "iteration 1771, train loss: \u001b[92m0.005703\u001b[0m, validation loss: 0.004341\n",
      "iteration 1772, train loss: 0.006393, validation loss: 0.004137\n",
      "iteration 1773, train loss: 0.006072, validation loss: 0.004318\n",
      "iteration 1774, train loss: 0.006193, validation loss: 0.004169\n",
      "iteration 1775, train loss: 0.005912, validation loss: \u001b[92m0.003953\u001b[0m\n",
      "iteration 1776, train loss: 0.005883, validation loss: 0.004063\n",
      "iteration 1777, train loss: 0.006138, validation loss: 0.004451\n",
      "iteration 1778, train loss: 0.006087, validation loss: 0.004022\n",
      "iteration 1779, train loss: \u001b[92m0.005629\u001b[0m, validation loss: 0.003974\n",
      "iteration 1780, train loss: 0.00567, validation loss: 0.004191\n",
      "iteration 1781, train loss: 0.005742, validation loss: 0.004387\n",
      "iteration 1782, train loss: 0.006272, validation loss: 0.005043\n",
      "iteration 1783, train loss: 0.006967, validation loss: 0.005217\n",
      "iteration 1784, train loss: 0.006927, validation loss: 0.004152\n",
      "iteration 1785, train loss: 0.006119, validation loss: 0.005219\n",
      "iteration 1786, train loss: 0.006204, validation loss: 0.006198\n",
      "iteration 1787, train loss: 0.006742, validation loss: 0.00493\n",
      "iteration 1788, train loss: 0.006132, validation loss: 0.005167\n",
      "iteration 1789, train loss: 0.006451, validation loss: 0.004481\n",
      "iteration 1790, train loss: 0.006998, validation loss: 0.004427\n",
      "iteration 1791, train loss: 0.006615, validation loss: 0.005166\n",
      "iteration 1792, train loss: 0.007868, validation loss: 0.004385\n",
      "iteration 1793, train loss: 0.006657, validation loss: 0.005745\n",
      "iteration 1794, train loss: 0.007332, validation loss: 0.005757\n",
      "iteration 1795, train loss: 0.006848, validation loss: 0.005278\n",
      "iteration 1796, train loss: 0.006752, validation loss: 0.005289\n",
      "iteration 1797, train loss: 0.006945, validation loss: 0.005308\n",
      "iteration 1798, train loss: 0.006761, validation loss: 0.004875\n",
      "iteration 1799, train loss: 0.006158, validation loss: 0.004628\n",
      "iteration 1800, train loss: 0.006427, validation loss: 0.004905\n",
      "iteration 1801, train loss: 0.006175, validation loss: 0.004836\n",
      "iteration 1802, train loss: 0.006292, validation loss: 0.004788\n",
      "iteration 1803, train loss: 0.006406, validation loss: 0.0048\n",
      "iteration 1804, train loss: 0.007741, validation loss: 0.004295\n",
      "iteration 1805, train loss: 0.006597, validation loss: 0.004214\n",
      "iteration 1806, train loss: 0.006841, validation loss: 0.004833\n",
      "iteration 1807, train loss: 0.006824, validation loss: 0.004599\n",
      "iteration 1808, train loss: 0.006956, validation loss: 0.004482\n",
      "iteration 1809, train loss: 0.006056, validation loss: 0.005062\n",
      "iteration 1810, train loss: 0.006355, validation loss: 0.005214\n",
      "iteration 1811, train loss: 0.006909, validation loss: 0.004948\n",
      "iteration 1812, train loss: 0.006619, validation loss: 0.005389\n",
      "iteration 1813, train loss: 0.007785, validation loss: 0.005448\n",
      "iteration 1814, train loss: 0.006468, validation loss: 0.004844\n",
      "iteration 1815, train loss: 0.005898, validation loss: 0.004785\n",
      "iteration 1816, train loss: 0.00645, validation loss: 0.005134\n",
      "iteration 1817, train loss: 0.006608, validation loss: 0.004788\n",
      "iteration 1818, train loss: 0.006357, validation loss: 0.00426\n",
      "iteration 1819, train loss: 0.006406, validation loss: 0.004177\n",
      "iteration 1820, train loss: 0.006765, validation loss: 0.004124\n",
      "iteration 1821, train loss: 0.006134, validation loss: 0.00409\n",
      "iteration 1822, train loss: 0.006489, validation loss: 0.003982\n",
      "iteration 1823, train loss: 0.006339, validation loss: 0.004204\n",
      "iteration 1824, train loss: 0.006194, validation loss: 0.004403\n",
      "iteration 1825, train loss: 0.006849, validation loss: 0.004387\n",
      "iteration 1826, train loss: 0.00591, validation loss: 0.004477\n",
      "iteration 1827, train loss: 0.005811, validation loss: 0.004524\n",
      "iteration 1828, train loss: 0.005931, validation loss: 0.004255\n",
      "iteration 1829, train loss: 0.006317, validation loss: 0.003956\n",
      "iteration 1830, train loss: 0.00614, validation loss: \u001b[92m0.003839\u001b[0m\n",
      "iteration 1831, train loss: \u001b[92m0.005607\u001b[0m, validation loss: 0.00397\n",
      "iteration 1832, train loss: 0.006114, validation loss: 0.004068\n",
      "iteration 1833, train loss: 0.006274, validation loss: 0.004019\n",
      "iteration 1834, train loss: 0.005938, validation loss: 0.004188\n",
      "iteration 1835, train loss: 0.005982, validation loss: 0.004223\n",
      "iteration 1836, train loss: 0.0058, validation loss: 0.003971\n",
      "iteration 1837, train loss: \u001b[92m0.005365\u001b[0m, validation loss: 0.003997\n",
      "iteration 1838, train loss: 0.00601, validation loss: 0.004001\n",
      "iteration 1839, train loss: 0.00547, validation loss: 0.003968\n",
      "iteration 1840, train loss: 0.005835, validation loss: 0.003981\n",
      "iteration 1841, train loss: 0.005621, validation loss: 0.003881\n",
      "iteration 1842, train loss: 0.005946, validation loss: 0.004315\n",
      "iteration 1843, train loss: 0.006247, validation loss: 0.00412\n",
      "iteration 1844, train loss: 0.005687, validation loss: 0.004132\n",
      "iteration 1845, train loss: 0.006104, validation loss: 0.004329\n",
      "iteration 1846, train loss: 0.006023, validation loss: 0.004682\n",
      "iteration 1847, train loss: 0.006135, validation loss: \u001b[92m0.003789\u001b[0m\n",
      "iteration 1848, train loss: 0.006018, validation loss: 0.003835\n",
      "iteration 1849, train loss: 0.005833, validation loss: \u001b[92m0.003645\u001b[0m\n",
      "iteration 1850, train loss: 0.005392, validation loss: 0.003936\n",
      "iteration 1851, train loss: 0.005918, validation loss: 0.004127\n",
      "iteration 1852, train loss: 0.005841, validation loss: 0.003865\n",
      "iteration 1853, train loss: \u001b[92m0.005104\u001b[0m, validation loss: 0.004577\n",
      "iteration 1854, train loss: 0.006263, validation loss: 0.004092\n",
      "iteration 1855, train loss: 0.005826, validation loss: 0.003792\n",
      "iteration 1856, train loss: 0.005692, validation loss: 0.004631\n",
      "iteration 1857, train loss: 0.006247, validation loss: 0.003881\n",
      "iteration 1858, train loss: 0.005724, validation loss: 0.004037\n",
      "iteration 1859, train loss: 0.005783, validation loss: 0.003895\n",
      "iteration 1860, train loss: 0.00513, validation loss: 0.003646\n",
      "iteration 1861, train loss: 0.006297, validation loss: \u001b[92m0.003568\u001b[0m\n",
      "iteration 1862, train loss: 0.005545, validation loss: 0.003681\n",
      "iteration 1863, train loss: 0.005566, validation loss: 0.003917\n",
      "iteration 1864, train loss: 0.005117, validation loss: 0.003656\n",
      "iteration 1865, train loss: 0.005124, validation loss: 0.003781\n",
      "iteration 1866, train loss: 0.005553, validation loss: 0.00381\n",
      "iteration 1867, train loss: 0.00619, validation loss: 0.003591\n",
      "iteration 1868, train loss: 0.005548, validation loss: 0.004032\n",
      "iteration 1869, train loss: 0.005811, validation loss: 0.003736\n",
      "iteration 1870, train loss: 0.005474, validation loss: 0.004114\n",
      "iteration 1871, train loss: 0.005637, validation loss: 0.00422\n",
      "iteration 1872, train loss: 0.005618, validation loss: 0.003823\n",
      "iteration 1873, train loss: 0.005433, validation loss: 0.005097\n",
      "iteration 1874, train loss: 0.006386, validation loss: 0.003956\n",
      "iteration 1875, train loss: 0.005968, validation loss: 0.004889\n",
      "iteration 1876, train loss: 0.006311, validation loss: 0.005456\n",
      "iteration 1877, train loss: 0.006127, validation loss: 0.004499\n",
      "iteration 1878, train loss: 0.006011, validation loss: 0.004004\n",
      "iteration 1879, train loss: 0.005717, validation loss: 0.005703\n",
      "iteration 1880, train loss: 0.00701, validation loss: 0.004159\n",
      "iteration 1881, train loss: 0.005649, validation loss: 0.003875\n",
      "iteration 1882, train loss: 0.005802, validation loss: 0.004477\n",
      "iteration 1883, train loss: 0.006037, validation loss: 0.00446\n",
      "iteration 1884, train loss: 0.006136, validation loss: 0.003794\n",
      "iteration 1885, train loss: 0.005434, validation loss: 0.003868\n",
      "iteration 1886, train loss: 0.005448, validation loss: 0.004517\n",
      "iteration 1887, train loss: 0.006774, validation loss: 0.00415\n",
      "iteration 1888, train loss: 0.006279, validation loss: 0.003639\n",
      "iteration 1889, train loss: 0.005708, validation loss: 0.004021\n",
      "iteration 1890, train loss: 0.005899, validation loss: 0.004233\n",
      "iteration 1891, train loss: 0.005685, validation loss: 0.003986\n",
      "iteration 1892, train loss: 0.005769, validation loss: 0.003992\n",
      "iteration 1893, train loss: 0.005649, validation loss: 0.003796\n",
      "iteration 1894, train loss: 0.00603, validation loss: 0.003622\n",
      "iteration 1895, train loss: 0.005187, validation loss: 0.003803\n",
      "iteration 1896, train loss: 0.005765, validation loss: 0.00394\n",
      "iteration 1897, train loss: 0.005659, validation loss: 0.003685\n",
      "iteration 1898, train loss: 0.005382, validation loss: 0.003943\n",
      "iteration 1899, train loss: 0.005768, validation loss: 0.004107\n",
      "iteration 1900, train loss: 0.005337, validation loss: 0.004041\n",
      "iteration 1901, train loss: 0.005309, validation loss: 0.003888\n",
      "iteration 1902, train loss: 0.005538, validation loss: 0.003593\n",
      "iteration 1903, train loss: 0.005164, validation loss: 0.00364\n",
      "iteration 1904, train loss: 0.00595, validation loss: \u001b[92m0.003522\u001b[0m\n",
      "iteration 1905, train loss: 0.005216, validation loss: 0.003589\n",
      "iteration 1906, train loss: 0.00516, validation loss: 0.003887\n",
      "iteration 1907, train loss: 0.005556, validation loss: 0.003828\n",
      "iteration 1908, train loss: 0.005367, validation loss: 0.003645\n",
      "iteration 1909, train loss: 0.005448, validation loss: 0.00381\n",
      "iteration 1910, train loss: 0.005459, validation loss: 0.003989\n",
      "iteration 1911, train loss: 0.006103, validation loss: 0.003604\n",
      "iteration 1912, train loss: 0.005534, validation loss: 0.004043\n",
      "iteration 1913, train loss: 0.006497, validation loss: 0.003943\n",
      "iteration 1914, train loss: 0.005648, validation loss: 0.003856\n",
      "iteration 1915, train loss: 0.005446, validation loss: 0.003946\n",
      "iteration 1916, train loss: 0.00611, validation loss: 0.00468\n",
      "iteration 1917, train loss: 0.005941, validation loss: 0.004102\n",
      "iteration 1918, train loss: 0.005562, validation loss: 0.003589\n",
      "iteration 1919, train loss: \u001b[92m0.005028\u001b[0m, validation loss: 0.004026\n",
      "iteration 1920, train loss: 0.00586, validation loss: 0.003576\n",
      "iteration 1921, train loss: 0.005344, validation loss: 0.003622\n",
      "iteration 1922, train loss: 0.005371, validation loss: 0.004253\n",
      "iteration 1923, train loss: 0.006093, validation loss: 0.00387\n",
      "iteration 1924, train loss: 0.005727, validation loss: 0.003857\n",
      "iteration 1925, train loss: 0.005467, validation loss: 0.004106\n",
      "iteration 1926, train loss: 0.005331, validation loss: 0.003791\n",
      "iteration 1927, train loss: 0.005468, validation loss: 0.003763\n",
      "iteration 1928, train loss: 0.00521, validation loss: 0.003819\n",
      "iteration 1929, train loss: 0.00531, validation loss: 0.00378\n",
      "iteration 1930, train loss: 0.006069, validation loss: 0.004012\n",
      "iteration 1931, train loss: 0.006159, validation loss: 0.003564\n",
      "iteration 1932, train loss: 0.005357, validation loss: 0.004193\n",
      "iteration 1933, train loss: 0.005576, validation loss: 0.004186\n",
      "iteration 1934, train loss: 0.00568, validation loss: \u001b[92m0.003509\u001b[0m\n",
      "iteration 1935, train loss: 0.005326, validation loss: 0.00418\n",
      "iteration 1936, train loss: 0.005654, validation loss: 0.004136\n",
      "iteration 1937, train loss: 0.005959, validation loss: \u001b[92m0.003481\u001b[0m\n",
      "iteration 1938, train loss: 0.005092, validation loss: 0.003951\n",
      "iteration 1939, train loss: 0.005957, validation loss: 0.003866\n",
      "iteration 1940, train loss: 0.005612, validation loss: 0.003545\n",
      "iteration 1941, train loss: 0.006072, validation loss: \u001b[92m0.003392\u001b[0m\n",
      "iteration 1942, train loss: 0.005099, validation loss: 0.003997\n",
      "iteration 1943, train loss: 0.005853, validation loss: 0.003858\n",
      "iteration 1944, train loss: 0.005228, validation loss: 0.0035\n",
      "iteration 1945, train loss: 0.005259, validation loss: 0.003592\n",
      "iteration 1946, train loss: 0.005183, validation loss: 0.003581\n",
      "iteration 1947, train loss: 0.005345, validation loss: 0.003395\n",
      "iteration 1948, train loss: \u001b[92m0.00493\u001b[0m, validation loss: 0.003718\n",
      "iteration 1949, train loss: 0.005492, validation loss: 0.003475\n",
      "iteration 1950, train loss: 0.005173, validation loss: 0.00397\n",
      "iteration 1951, train loss: 0.005708, validation loss: 0.003519\n",
      "iteration 1952, train loss: 0.005369, validation loss: 0.004347\n",
      "iteration 1953, train loss: 0.005661, validation loss: 0.004207\n",
      "iteration 1954, train loss: 0.005874, validation loss: 0.004009\n",
      "iteration 1955, train loss: 0.005578, validation loss: 0.004425\n",
      "iteration 1956, train loss: 0.005837, validation loss: 0.003509\n",
      "iteration 1957, train loss: 0.005488, validation loss: 0.00456\n",
      "iteration 1958, train loss: 0.00579, validation loss: 0.004454\n",
      "iteration 1959, train loss: 0.006065, validation loss: 0.003888\n",
      "iteration 1960, train loss: 0.005318, validation loss: 0.004576\n",
      "iteration 1961, train loss: 0.006203, validation loss: 0.004432\n",
      "iteration 1962, train loss: 0.005991, validation loss: 0.003527\n",
      "iteration 1963, train loss: 0.00525, validation loss: 0.00389\n",
      "iteration 1964, train loss: 0.005842, validation loss: 0.003973\n",
      "iteration 1965, train loss: 0.005707, validation loss: 0.003784\n",
      "iteration 1966, train loss: 0.005673, validation loss: 0.004255\n",
      "iteration 1967, train loss: 0.00583, validation loss: 0.00431\n",
      "iteration 1968, train loss: 0.005599, validation loss: 0.004033\n",
      "iteration 1969, train loss: 0.005395, validation loss: 0.004257\n",
      "iteration 1970, train loss: 0.005782, validation loss: 0.003979\n",
      "iteration 1971, train loss: 0.005693, validation loss: 0.003458\n",
      "iteration 1972, train loss: 0.005293, validation loss: 0.003781\n",
      "iteration 1973, train loss: 0.005399, validation loss: 0.004033\n",
      "iteration 1974, train loss: 0.005729, validation loss: 0.003564\n",
      "iteration 1975, train loss: 0.005321, validation loss: 0.004182\n",
      "iteration 1976, train loss: 0.006072, validation loss: 0.004202\n",
      "iteration 1977, train loss: 0.005876, validation loss: 0.00375\n",
      "iteration 1978, train loss: 0.005644, validation loss: 0.00352\n",
      "iteration 1979, train loss: \u001b[92m0.004797\u001b[0m, validation loss: 0.00397\n",
      "iteration 1980, train loss: 0.005387, validation loss: 0.003829\n",
      "iteration 1981, train loss: 0.005766, validation loss: 0.003401\n",
      "iteration 1982, train loss: 0.005044, validation loss: 0.004208\n",
      "iteration 1983, train loss: 0.006057, validation loss: 0.004594\n",
      "iteration 1984, train loss: 0.005804, validation loss: 0.003964\n",
      "iteration 1985, train loss: 0.005314, validation loss: 0.00343\n",
      "iteration 1986, train loss: 0.005681, validation loss: 0.003989\n",
      "iteration 1987, train loss: 0.005403, validation loss: 0.004214\n",
      "iteration 1988, train loss: 0.006613, validation loss: 0.003608\n",
      "iteration 1989, train loss: 0.00549, validation loss: 0.004402\n",
      "iteration 1990, train loss: 0.006373, validation loss: 0.004652\n",
      "iteration 1991, train loss: 0.006602, validation loss: 0.004037\n",
      "iteration 1992, train loss: 0.005961, validation loss: 0.004559\n",
      "iteration 1993, train loss: 0.006516, validation loss: 0.004342\n",
      "iteration 1994, train loss: 0.005894, validation loss: 0.003721\n",
      "iteration 1995, train loss: 0.006302, validation loss: 0.00409\n",
      "iteration 1996, train loss: 0.005942, validation loss: 0.004083\n",
      "iteration 1997, train loss: 0.006241, validation loss: 0.003613\n",
      "iteration 1998, train loss: 0.005106, validation loss: 0.003659\n",
      "iteration 1999, train loss: 0.005502, validation loss: 0.004456\n",
      "iteration 2000, train loss: 0.006397, validation loss: 0.003677\n",
      "iteration 2001, train loss: 0.005898, validation loss: 0.003427\n",
      "iteration 2002, train loss: 0.005476, validation loss: 0.003974\n",
      "iteration 2003, train loss: 0.005834, validation loss: 0.003779\n",
      "iteration 2004, train loss: 0.005732, validation loss: \u001b[92m0.003336\u001b[0m\n",
      "iteration 2005, train loss: 0.005054, validation loss: 0.003451\n",
      "iteration 2006, train loss: 0.00528, validation loss: 0.003713\n",
      "iteration 2007, train loss: 0.005484, validation loss: 0.003946\n",
      "iteration 2008, train loss: 0.005828, validation loss: 0.003863\n",
      "iteration 2009, train loss: 0.005734, validation loss: 0.003697\n",
      "iteration 2010, train loss: 0.005216, validation loss: 0.003539\n",
      "iteration 2011, train loss: 0.005605, validation loss: 0.003367\n",
      "iteration 2012, train loss: 0.005191, validation loss: 0.003544\n",
      "iteration 2013, train loss: 0.005524, validation loss: 0.003408\n",
      "iteration 2014, train loss: 0.00484, validation loss: 0.003552\n",
      "iteration 2015, train loss: 0.004917, validation loss: 0.003789\n",
      "iteration 2016, train loss: 0.005577, validation loss: 0.003475\n",
      "iteration 2017, train loss: 0.005178, validation loss: 0.003491\n",
      "iteration 2018, train loss: 0.005171, validation loss: 0.003534\n",
      "iteration 2019, train loss: 0.005216, validation loss: 0.003481\n",
      "iteration 2020, train loss: 0.005277, validation loss: 0.003536\n",
      "iteration 2021, train loss: 0.005423, validation loss: 0.00355\n",
      "iteration 2022, train loss: 0.005251, validation loss: 0.003624\n",
      "iteration 2023, train loss: 0.005552, validation loss: 0.003878\n",
      "iteration 2024, train loss: 0.005237, validation loss: 0.00389\n",
      "iteration 2025, train loss: 0.005347, validation loss: \u001b[92m0.003327\u001b[0m\n",
      "iteration 2026, train loss: 0.004952, validation loss: 0.00366\n",
      "iteration 2027, train loss: 0.005305, validation loss: 0.003347\n",
      "iteration 2028, train loss: 0.004802, validation loss: 0.003999\n",
      "iteration 2029, train loss: 0.005401, validation loss: 0.003731\n",
      "iteration 2030, train loss: 0.005193, validation loss: 0.003409\n",
      "iteration 2031, train loss: 0.004939, validation loss: \u001b[92m0.003313\u001b[0m\n",
      "iteration 2032, train loss: 0.00494, validation loss: 0.003488\n",
      "iteration 2033, train loss: 0.004994, validation loss: 0.003716\n",
      "iteration 2034, train loss: 0.005363, validation loss: 0.003552\n",
      "iteration 2035, train loss: 0.005162, validation loss: 0.003364\n",
      "iteration 2036, train loss: 0.004863, validation loss: 0.003753\n",
      "iteration 2037, train loss: 0.005084, validation loss: 0.003499\n",
      "iteration 2038, train loss: 0.005327, validation loss: 0.003399\n",
      "iteration 2039, train loss: 0.005, validation loss: 0.00336\n",
      "iteration 2040, train loss: 0.00495, validation loss: 0.003705\n",
      "iteration 2041, train loss: 0.005174, validation loss: 0.003716\n",
      "iteration 2042, train loss: 0.005274, validation loss: \u001b[92m0.003305\u001b[0m\n",
      "iteration 2043, train loss: 0.004903, validation loss: 0.003817\n",
      "iteration 2044, train loss: 0.004808, validation loss: 0.003936\n",
      "iteration 2045, train loss: 0.005237, validation loss: \u001b[92m0.003243\u001b[0m\n",
      "iteration 2046, train loss: 0.00542, validation loss: 0.003397\n",
      "iteration 2047, train loss: 0.005279, validation loss: 0.003286\n",
      "iteration 2048, train loss: 0.005323, validation loss: 0.003506\n",
      "iteration 2049, train loss: \u001b[92m0.004718\u001b[0m, validation loss: 0.003619\n",
      "iteration 2050, train loss: 0.0053, validation loss: 0.003281\n",
      "iteration 2051, train loss: 0.004836, validation loss: 0.003336\n",
      "iteration 2052, train loss: 0.004817, validation loss: 0.003255\n",
      "iteration 2053, train loss: 0.005529, validation loss: 0.003725\n",
      "iteration 2054, train loss: 0.005671, validation loss: \u001b[92m0.003205\u001b[0m\n",
      "iteration 2055, train loss: \u001b[92m0.004705\u001b[0m, validation loss: 0.003457\n",
      "iteration 2056, train loss: 0.005179, validation loss: 0.003727\n",
      "iteration 2057, train loss: 0.005126, validation loss: 0.003414\n",
      "iteration 2058, train loss: 0.005281, validation loss: 0.003482\n",
      "iteration 2059, train loss: 0.005275, validation loss: 0.003247\n",
      "iteration 2060, train loss: \u001b[92m0.004669\u001b[0m, validation loss: 0.003552\n",
      "iteration 2061, train loss: 0.005671, validation loss: 0.003918\n",
      "iteration 2062, train loss: 0.005141, validation loss: 0.003575\n",
      "iteration 2063, train loss: 0.005227, validation loss: 0.00327\n",
      "iteration 2064, train loss: 0.004922, validation loss: 0.003226\n",
      "iteration 2065, train loss: 0.005027, validation loss: 0.003224\n",
      "iteration 2066, train loss: 0.005073, validation loss: 0.003524\n",
      "iteration 2067, train loss: 0.005075, validation loss: 0.00344\n",
      "iteration 2068, train loss: 0.005221, validation loss: 0.00323\n",
      "iteration 2069, train loss: 0.004757, validation loss: 0.00353\n",
      "iteration 2070, train loss: 0.005003, validation loss: 0.004324\n",
      "iteration 2071, train loss: 0.005864, validation loss: 0.003512\n",
      "iteration 2072, train loss: 0.005294, validation loss: 0.003858\n",
      "iteration 2073, train loss: 0.00517, validation loss: 0.003744\n",
      "iteration 2074, train loss: 0.005003, validation loss: 0.00343\n",
      "iteration 2075, train loss: 0.004946, validation loss: 0.003967\n",
      "iteration 2076, train loss: 0.004985, validation loss: 0.003968\n",
      "iteration 2077, train loss: 0.005356, validation loss: 0.003568\n",
      "iteration 2078, train loss: 0.004803, validation loss: 0.003436\n",
      "iteration 2079, train loss: 0.006061, validation loss: 0.003511\n",
      "iteration 2080, train loss: 0.005207, validation loss: 0.003631\n",
      "iteration 2081, train loss: 0.005266, validation loss: 0.00348\n",
      "iteration 2082, train loss: 0.005386, validation loss: 0.003715\n",
      "iteration 2083, train loss: 0.005147, validation loss: 0.004032\n",
      "iteration 2084, train loss: 0.005509, validation loss: 0.003388\n",
      "iteration 2085, train loss: 0.005279, validation loss: 0.003515\n",
      "iteration 2086, train loss: 0.00518, validation loss: 0.003793\n",
      "iteration 2087, train loss: 0.005668, validation loss: 0.003433\n",
      "iteration 2088, train loss: 0.004812, validation loss: 0.003542\n",
      "iteration 2089, train loss: 0.005257, validation loss: 0.003509\n",
      "iteration 2090, train loss: 0.005304, validation loss: 0.003456\n",
      "iteration 2091, train loss: 0.004874, validation loss: 0.003761\n",
      "iteration 2092, train loss: 0.004973, validation loss: 0.00355\n",
      "iteration 2093, train loss: 0.005052, validation loss: 0.003282\n",
      "iteration 2094, train loss: 0.005023, validation loss: 0.00342\n",
      "iteration 2095, train loss: 0.004777, validation loss: 0.003907\n",
      "iteration 2096, train loss: 0.004849, validation loss: 0.003313\n",
      "iteration 2097, train loss: 0.004903, validation loss: 0.003579\n",
      "iteration 2098, train loss: 0.005126, validation loss: 0.00378\n",
      "iteration 2099, train loss: 0.0054, validation loss: \u001b[92m0.003138\u001b[0m\n",
      "iteration 2100, train loss: 0.005109, validation loss: 0.003562\n",
      "iteration 2101, train loss: 0.005037, validation loss: 0.003582\n",
      "iteration 2102, train loss: 0.004845, validation loss: 0.003178\n",
      "iteration 2103, train loss: 0.004813, validation loss: 0.003239\n",
      "iteration 2104, train loss: 0.005123, validation loss: 0.003188\n",
      "iteration 2105, train loss: \u001b[92m0.004501\u001b[0m, validation loss: 0.00371\n",
      "iteration 2106, train loss: 0.005208, validation loss: 0.003339\n",
      "iteration 2107, train loss: 0.00509, validation loss: \u001b[92m0.003137\u001b[0m\n",
      "iteration 2108, train loss: 0.004743, validation loss: 0.003393\n",
      "iteration 2109, train loss: 0.00489, validation loss: 0.003386\n",
      "iteration 2110, train loss: 0.004964, validation loss: 0.00329\n",
      "iteration 2111, train loss: 0.004968, validation loss: 0.003978\n",
      "iteration 2112, train loss: 0.005564, validation loss: 0.003233\n",
      "iteration 2113, train loss: 0.005186, validation loss: 0.003556\n",
      "iteration 2114, train loss: 0.004721, validation loss: 0.003986\n",
      "iteration 2115, train loss: 0.005386, validation loss: 0.003356\n",
      "iteration 2116, train loss: 0.004955, validation loss: 0.004253\n",
      "iteration 2117, train loss: 0.005414, validation loss: 0.004038\n",
      "iteration 2118, train loss: 0.005288, validation loss: 0.00328\n",
      "iteration 2119, train loss: 0.004993, validation loss: 0.003532\n",
      "iteration 2120, train loss: 0.004902, validation loss: 0.003537\n",
      "iteration 2121, train loss: 0.005153, validation loss: 0.003994\n",
      "iteration 2122, train loss: 0.005422, validation loss: 0.003715\n",
      "iteration 2123, train loss: 0.00456, validation loss: 0.003849\n",
      "iteration 2124, train loss: 0.004946, validation loss: 0.00359\n",
      "iteration 2125, train loss: 0.005105, validation loss: 0.003488\n",
      "iteration 2126, train loss: 0.005002, validation loss: 0.00415\n",
      "iteration 2127, train loss: 0.004767, validation loss: 0.003923\n",
      "iteration 2128, train loss: 0.005265, validation loss: 0.003209\n",
      "iteration 2129, train loss: 0.004932, validation loss: 0.003779\n",
      "iteration 2130, train loss: 0.004876, validation loss: 0.003662\n",
      "iteration 2131, train loss: 0.005225, validation loss: 0.003284\n",
      "iteration 2132, train loss: \u001b[92m0.004459\u001b[0m, validation loss: 0.004605\n",
      "iteration 2133, train loss: 0.005557, validation loss: 0.004974\n",
      "iteration 2134, train loss: 0.006109, validation loss: 0.004444\n",
      "iteration 2135, train loss: 0.005377, validation loss: 0.003283\n",
      "iteration 2136, train loss: 0.00532, validation loss: 0.003683\n",
      "iteration 2137, train loss: 0.005191, validation loss: 0.004056\n",
      "iteration 2138, train loss: 0.005789, validation loss: 0.003454\n",
      "iteration 2139, train loss: 0.004976, validation loss: 0.004052\n",
      "iteration 2140, train loss: 0.005675, validation loss: 0.003978\n",
      "iteration 2141, train loss: 0.005632, validation loss: 0.00322\n",
      "iteration 2142, train loss: 0.004937, validation loss: 0.003736\n",
      "iteration 2143, train loss: 0.004854, validation loss: 0.004015\n",
      "iteration 2144, train loss: 0.006111, validation loss: 0.003183\n",
      "iteration 2145, train loss: 0.00476, validation loss: 0.003548\n",
      "iteration 2146, train loss: 0.005592, validation loss: 0.003481\n",
      "iteration 2147, train loss: 0.005257, validation loss: 0.003442\n",
      "iteration 2148, train loss: 0.004727, validation loss: 0.003611\n",
      "iteration 2149, train loss: 0.005013, validation loss: 0.003549\n",
      "iteration 2150, train loss: 0.005045, validation loss: 0.003441\n",
      "iteration 2151, train loss: 0.004892, validation loss: 0.003572\n",
      "iteration 2152, train loss: 0.005076, validation loss: 0.003735\n",
      "iteration 2153, train loss: 0.005003, validation loss: 0.003737\n",
      "iteration 2154, train loss: 0.005442, validation loss: 0.003606\n",
      "iteration 2155, train loss: 0.004902, validation loss: 0.003557\n",
      "iteration 2156, train loss: 0.005101, validation loss: 0.003248\n",
      "iteration 2157, train loss: 0.004544, validation loss: 0.003766\n",
      "iteration 2158, train loss: 0.005493, validation loss: 0.003433\n",
      "iteration 2159, train loss: 0.00503, validation loss: 0.003249\n",
      "iteration 2160, train loss: 0.004883, validation loss: 0.003572\n",
      "iteration 2161, train loss: 0.005209, validation loss: 0.003334\n",
      "iteration 2162, train loss: 0.005217, validation loss: 0.003413\n",
      "iteration 2163, train loss: 0.004876, validation loss: 0.003213\n",
      "iteration 2164, train loss: 0.004872, validation loss: 0.003363\n",
      "iteration 2165, train loss: 0.004973, validation loss: 0.003346\n",
      "iteration 2166, train loss: 0.004933, validation loss: \u001b[92m0.003054\u001b[0m\n",
      "iteration 2167, train loss: 0.004777, validation loss: 0.003246\n",
      "iteration 2168, train loss: \u001b[92m0.004311\u001b[0m, validation loss: 0.003377\n",
      "iteration 2169, train loss: 0.004992, validation loss: 0.003057\n",
      "iteration 2170, train loss: 0.004497, validation loss: 0.003545\n",
      "iteration 2171, train loss: 0.004813, validation loss: 0.003438\n",
      "iteration 2172, train loss: 0.005289, validation loss: 0.003444\n",
      "iteration 2173, train loss: 0.004824, validation loss: 0.003156\n",
      "iteration 2174, train loss: 0.004524, validation loss: 0.003555\n",
      "iteration 2175, train loss: 0.005094, validation loss: 0.003151\n",
      "iteration 2176, train loss: 0.005199, validation loss: 0.003997\n",
      "iteration 2177, train loss: 0.005019, validation loss: 0.004615\n",
      "iteration 2178, train loss: 0.005453, validation loss: 0.003922\n",
      "iteration 2179, train loss: 0.005503, validation loss: 0.004174\n",
      "iteration 2180, train loss: 0.00536, validation loss: 0.003977\n",
      "iteration 2181, train loss: 0.005088, validation loss: 0.003405\n",
      "iteration 2182, train loss: 0.00556, validation loss: 0.003719\n",
      "iteration 2183, train loss: 0.00511, validation loss: 0.003999\n",
      "iteration 2184, train loss: 0.005304, validation loss: 0.003595\n",
      "iteration 2185, train loss: 0.005307, validation loss: 0.004061\n",
      "iteration 2186, train loss: 0.005202, validation loss: 0.004292\n",
      "iteration 2187, train loss: 0.005494, validation loss: 0.003542\n",
      "iteration 2188, train loss: 0.004995, validation loss: 0.003627\n",
      "iteration 2189, train loss: 0.004985, validation loss: 0.003953\n",
      "iteration 2190, train loss: 0.005677, validation loss: 0.003537\n",
      "iteration 2191, train loss: 0.004899, validation loss: 0.003382\n",
      "iteration 2192, train loss: 0.004767, validation loss: 0.004009\n",
      "iteration 2193, train loss: 0.005361, validation loss: 0.004018\n",
      "iteration 2194, train loss: 0.005826, validation loss: 0.003439\n",
      "iteration 2195, train loss: 0.005426, validation loss: 0.003926\n",
      "iteration 2196, train loss: 0.005289, validation loss: 0.004273\n",
      "iteration 2197, train loss: 0.005834, validation loss: 0.003807\n",
      "iteration 2198, train loss: 0.005161, validation loss: 0.003498\n",
      "iteration 2199, train loss: 0.004904, validation loss: 0.003837\n",
      "iteration 2200, train loss: 0.005088, validation loss: 0.004024\n",
      "iteration 2201, train loss: 0.005755, validation loss: 0.003374\n",
      "iteration 2202, train loss: 0.005019, validation loss: 0.00327\n",
      "iteration 2203, train loss: 0.005425, validation loss: 0.003746\n",
      "iteration 2204, train loss: 0.004964, validation loss: 0.003486\n",
      "iteration 2205, train loss: 0.005316, validation loss: 0.003079\n",
      "iteration 2206, train loss: 0.004473, validation loss: 0.003651\n",
      "iteration 2207, train loss: 0.00497, validation loss: 0.003884\n",
      "iteration 2208, train loss: 0.005149, validation loss: 0.003555\n",
      "iteration 2209, train loss: 0.005151, validation loss: 0.003493\n",
      "iteration 2210, train loss: 0.005603, validation loss: 0.003274\n",
      "iteration 2211, train loss: 0.005462, validation loss: 0.003357\n",
      "iteration 2212, train loss: 0.005119, validation loss: 0.003626\n",
      "iteration 2213, train loss: 0.005141, validation loss: 0.00354\n",
      "iteration 2214, train loss: 0.005252, validation loss: 0.003284\n",
      "iteration 2215, train loss: 0.004779, validation loss: 0.003672\n",
      "iteration 2216, train loss: 0.004969, validation loss: 0.003533\n",
      "iteration 2217, train loss: 0.004915, validation loss: \u001b[92m0.003018\u001b[0m\n",
      "iteration 2218, train loss: 0.004598, validation loss: 0.003462\n",
      "iteration 2219, train loss: 0.005033, validation loss: 0.003701\n",
      "iteration 2220, train loss: 0.00553, validation loss: 0.003078\n",
      "iteration 2221, train loss: 0.004507, validation loss: 0.003283\n",
      "iteration 2222, train loss: 0.004654, validation loss: 0.003558\n",
      "iteration 2223, train loss: 0.004836, validation loss: 0.003451\n",
      "iteration 2224, train loss: 0.005401, validation loss: 0.003468\n",
      "iteration 2225, train loss: 0.004821, validation loss: 0.003374\n",
      "iteration 2226, train loss: 0.004763, validation loss: 0.003349\n",
      "iteration 2227, train loss: 0.005064, validation loss: 0.003595\n",
      "iteration 2228, train loss: 0.005191, validation loss: 0.003277\n",
      "iteration 2229, train loss: 0.004633, validation loss: 0.004019\n",
      "iteration 2230, train loss: 0.004972, validation loss: 0.003952\n",
      "iteration 2231, train loss: 0.00506, validation loss: 0.003358\n",
      "iteration 2232, train loss: 0.004849, validation loss: 0.003946\n",
      "iteration 2233, train loss: 0.005613, validation loss: 0.003113\n",
      "iteration 2234, train loss: 0.00509, validation loss: 0.003451\n",
      "iteration 2235, train loss: 0.004687, validation loss: 0.003941\n",
      "iteration 2236, train loss: 0.005216, validation loss: 0.003297\n",
      "iteration 2237, train loss: 0.004926, validation loss: 0.003057\n",
      "iteration 2238, train loss: 0.004528, validation loss: 0.003618\n",
      "iteration 2239, train loss: 0.005252, validation loss: 0.003375\n",
      "iteration 2240, train loss: 0.004895, validation loss: 0.003223\n",
      "iteration 2241, train loss: 0.004591, validation loss: 0.003375\n",
      "iteration 2242, train loss: 0.005088, validation loss: 0.003347\n",
      "iteration 2243, train loss: 0.004939, validation loss: 0.003232\n",
      "iteration 2244, train loss: 0.005166, validation loss: 0.003514\n",
      "iteration 2245, train loss: 0.005315, validation loss: 0.003186\n",
      "iteration 2246, train loss: 0.004574, validation loss: 0.003845\n",
      "iteration 2247, train loss: 0.00545, validation loss: 0.003833\n",
      "iteration 2248, train loss: 0.005364, validation loss: 0.003829\n",
      "iteration 2249, train loss: 0.005796, validation loss: 0.003611\n",
      "iteration 2250, train loss: 0.005531, validation loss: 0.003642\n",
      "iteration 2251, train loss: 0.005352, validation loss: 0.003578\n",
      "iteration 2252, train loss: 0.005081, validation loss: 0.00365\n",
      "iteration 2253, train loss: 0.005392, validation loss: 0.003586\n",
      "iteration 2254, train loss: 0.005437, validation loss: 0.003467\n",
      "iteration 2255, train loss: 0.004669, validation loss: 0.00377\n",
      "iteration 2256, train loss: 0.004962, validation loss: 0.00368\n",
      "iteration 2257, train loss: 0.005327, validation loss: 0.003062\n",
      "iteration 2258, train loss: 0.004808, validation loss: 0.003567\n",
      "iteration 2259, train loss: 0.005653, validation loss: 0.003347\n",
      "iteration 2260, train loss: 0.004668, validation loss: 0.003094\n",
      "iteration 2261, train loss: 0.004547, validation loss: 0.0035\n",
      "iteration 2262, train loss: 0.005166, validation loss: 0.003634\n",
      "iteration 2263, train loss: 0.005892, validation loss: 0.003232\n",
      "iteration 2264, train loss: 0.004891, validation loss: 0.003793\n",
      "iteration 2265, train loss: 0.005341, validation loss: 0.003626\n",
      "iteration 2266, train loss: 0.005319, validation loss: 0.003169\n",
      "iteration 2267, train loss: 0.004638, validation loss: 0.003276\n",
      "iteration 2268, train loss: 0.004816, validation loss: 0.003205\n",
      "iteration 2269, train loss: 0.005579, validation loss: 0.003083\n",
      "iteration 2270, train loss: 0.004867, validation loss: 0.003424\n",
      "iteration 2271, train loss: 0.00477, validation loss: 0.003495\n",
      "iteration 2272, train loss: 0.004771, validation loss: 0.003865\n",
      "iteration 2273, train loss: 0.00523, validation loss: 0.003602\n",
      "iteration 2274, train loss: 0.005439, validation loss: 0.003111\n",
      "iteration 2275, train loss: 0.005076, validation loss: 0.003698\n",
      "iteration 2276, train loss: 0.005007, validation loss: 0.003753\n",
      "iteration 2277, train loss: 0.005187, validation loss: 0.003085\n",
      "iteration 2278, train loss: 0.004476, validation loss: 0.00342\n",
      "iteration 2279, train loss: 0.004706, validation loss: 0.004068\n",
      "iteration 2280, train loss: 0.005698, validation loss: 0.003338\n",
      "iteration 2281, train loss: 0.004577, validation loss: 0.003311\n",
      "iteration 2282, train loss: 0.004685, validation loss: 0.004052\n",
      "iteration 2283, train loss: 0.005095, validation loss: 0.004234\n",
      "iteration 2284, train loss: 0.005549, validation loss: 0.003072\n",
      "iteration 2285, train loss: 0.004421, validation loss: 0.003365\n",
      "iteration 2286, train loss: 0.004787, validation loss: 0.003814\n",
      "iteration 2287, train loss: 0.005168, validation loss: 0.003421\n",
      "iteration 2288, train loss: 0.00498, validation loss: 0.003296\n",
      "iteration 2289, train loss: 0.004677, validation loss: 0.003801\n",
      "iteration 2290, train loss: 0.005265, validation loss: 0.00338\n",
      "iteration 2291, train loss: 0.005063, validation loss: 0.003363\n",
      "iteration 2292, train loss: 0.005096, validation loss: 0.00395\n",
      "iteration 2293, train loss: 0.005226, validation loss: 0.003992\n",
      "iteration 2294, train loss: 0.00497, validation loss: 0.003539\n",
      "iteration 2295, train loss: 0.00494, validation loss: 0.00379\n",
      "iteration 2296, train loss: 0.005225, validation loss: 0.003933\n",
      "iteration 2297, train loss: 0.005784, validation loss: 0.003541\n",
      "iteration 2298, train loss: 0.004857, validation loss: 0.003816\n",
      "iteration 2299, train loss: 0.005395, validation loss: 0.00444\n",
      "iteration 2300, train loss: 0.005935, validation loss: 0.004054\n",
      "iteration 2301, train loss: 0.006084, validation loss: 0.003474\n",
      "iteration 2302, train loss: 0.00538, validation loss: 0.003707\n",
      "iteration 2303, train loss: 0.005353, validation loss: 0.004035\n",
      "iteration 2304, train loss: 0.005375, validation loss: 0.003719\n",
      "iteration 2305, train loss: 0.005039, validation loss: 0.003558\n",
      "iteration 2306, train loss: 0.004815, validation loss: 0.004089\n",
      "iteration 2307, train loss: 0.005283, validation loss: 0.003369\n",
      "iteration 2308, train loss: 0.00536, validation loss: 0.003283\n",
      "iteration 2309, train loss: 0.005449, validation loss: 0.003509\n",
      "iteration 2310, train loss: 0.005005, validation loss: 0.00326\n",
      "iteration 2311, train loss: 0.004632, validation loss: 0.003134\n",
      "iteration 2312, train loss: 0.004521, validation loss: 0.00329\n",
      "iteration 2313, train loss: 0.004929, validation loss: 0.003097\n",
      "iteration 2314, train loss: \u001b[92m0.004271\u001b[0m, validation loss: \u001b[92m0.002958\u001b[0m\n",
      "iteration 2315, train loss: 0.004624, validation loss: 0.002967\n",
      "iteration 2316, train loss: 0.004484, validation loss: 0.003029\n",
      "iteration 2317, train loss: 0.004649, validation loss: 0.00297\n",
      "iteration 2318, train loss: 0.004863, validation loss: \u001b[92m0.002922\u001b[0m\n",
      "iteration 2319, train loss: 0.004381, validation loss: 0.003202\n",
      "iteration 2320, train loss: 0.004374, validation loss: 0.003212\n",
      "iteration 2321, train loss: 0.004684, validation loss: 0.003294\n",
      "iteration 2322, train loss: \u001b[92m0.004249\u001b[0m, validation loss: 0.003246\n",
      "iteration 2323, train loss: 0.00473, validation loss: 0.002986\n",
      "iteration 2324, train loss: \u001b[92m0.004229\u001b[0m, validation loss: 0.003287\n",
      "iteration 2325, train loss: 0.004882, validation loss: 0.003288\n",
      "iteration 2326, train loss: 0.00463, validation loss: 0.00355\n",
      "iteration 2327, train loss: 0.005028, validation loss: 0.003514\n",
      "iteration 2328, train loss: 0.004759, validation loss: 0.003507\n",
      "iteration 2329, train loss: 0.004687, validation loss: 0.003278\n",
      "iteration 2330, train loss: 0.00512, validation loss: 0.003147\n",
      "iteration 2331, train loss: 0.005267, validation loss: 0.003331\n",
      "iteration 2332, train loss: 0.00463, validation loss: 0.003818\n",
      "iteration 2333, train loss: 0.005294, validation loss: 0.003582\n",
      "iteration 2334, train loss: 0.004984, validation loss: 0.002963\n",
      "iteration 2335, train loss: 0.004257, validation loss: 0.003815\n",
      "iteration 2336, train loss: 0.005123, validation loss: 0.003995\n",
      "iteration 2337, train loss: 0.005044, validation loss: 0.003138\n",
      "iteration 2338, train loss: 0.004615, validation loss: 0.003282\n",
      "iteration 2339, train loss: 0.004493, validation loss: 0.004004\n",
      "iteration 2340, train loss: 0.006077, validation loss: 0.003423\n",
      "iteration 2341, train loss: 0.004938, validation loss: 0.003046\n",
      "iteration 2342, train loss: 0.004433, validation loss: 0.003807\n",
      "iteration 2343, train loss: 0.005114, validation loss: 0.003521\n",
      "iteration 2344, train loss: 0.004903, validation loss: 0.002983\n",
      "iteration 2345, train loss: 0.004653, validation loss: 0.003447\n",
      "iteration 2346, train loss: 0.004649, validation loss: 0.00405\n",
      "iteration 2347, train loss: 0.005414, validation loss: 0.003349\n",
      "iteration 2348, train loss: 0.004926, validation loss: 0.003133\n",
      "iteration 2349, train loss: 0.004745, validation loss: 0.003336\n",
      "iteration 2350, train loss: 0.004979, validation loss: 0.003323\n",
      "iteration 2351, train loss: 0.005318, validation loss: 0.002993\n",
      "iteration 2352, train loss: 0.004445, validation loss: 0.003528\n",
      "iteration 2353, train loss: 0.005262, validation loss: 0.003457\n",
      "iteration 2354, train loss: 0.005391, validation loss: 0.003384\n",
      "iteration 2355, train loss: 0.004872, validation loss: 0.003356\n",
      "iteration 2356, train loss: 0.004688, validation loss: 0.003267\n",
      "iteration 2357, train loss: 0.005104, validation loss: 0.003123\n",
      "iteration 2358, train loss: 0.005022, validation loss: 0.003066\n",
      "iteration 2359, train loss: 0.00466, validation loss: 0.003279\n",
      "iteration 2360, train loss: 0.004602, validation loss: 0.003372\n",
      "iteration 2361, train loss: 0.005154, validation loss: 0.003049\n",
      "iteration 2362, train loss: 0.004615, validation loss: 0.003076\n",
      "iteration 2363, train loss: 0.004897, validation loss: 0.003098\n",
      "iteration 2364, train loss: 0.004626, validation loss: 0.002966\n",
      "iteration 2365, train loss: 0.004336, validation loss: \u001b[92m0.002906\u001b[0m\n",
      "iteration 2366, train loss: 0.004719, validation loss: 0.002933\n",
      "iteration 2367, train loss: 0.004581, validation loss: 0.00299\n",
      "iteration 2368, train loss: 0.004599, validation loss: 0.003323\n",
      "iteration 2369, train loss: 0.004313, validation loss: 0.00364\n",
      "iteration 2370, train loss: 0.004473, validation loss: 0.003202\n",
      "iteration 2371, train loss: 0.004656, validation loss: 0.003085\n",
      "iteration 2372, train loss: 0.004468, validation loss: 0.003138\n",
      "iteration 2373, train loss: 0.00476, validation loss: 0.00325\n",
      "iteration 2374, train loss: 0.005149, validation loss: 0.00317\n",
      "iteration 2375, train loss: 0.00453, validation loss: 0.00349\n",
      "iteration 2376, train loss: 0.004718, validation loss: 0.003379\n",
      "iteration 2377, train loss: 0.004332, validation loss: 0.003191\n",
      "iteration 2378, train loss: 0.004499, validation loss: 0.002956\n",
      "iteration 2379, train loss: 0.004528, validation loss: 0.002999\n",
      "iteration 2380, train loss: 0.004636, validation loss: 0.003003\n",
      "iteration 2381, train loss: 0.00457, validation loss: 0.00305\n",
      "iteration 2382, train loss: 0.004604, validation loss: 0.003101\n",
      "iteration 2383, train loss: 0.005102, validation loss: 0.003432\n",
      "iteration 2384, train loss: 0.004474, validation loss: 0.003257\n",
      "iteration 2385, train loss: 0.004968, validation loss: 0.002982\n",
      "iteration 2386, train loss: 0.004831, validation loss: 0.003432\n",
      "iteration 2387, train loss: 0.004671, validation loss: 0.003228\n",
      "iteration 2388, train loss: 0.004573, validation loss: 0.003048\n",
      "iteration 2389, train loss: 0.004491, validation loss: 0.003126\n",
      "iteration 2390, train loss: 0.004448, validation loss: 0.002928\n",
      "iteration 2391, train loss: 0.00451, validation loss: 0.003355\n",
      "iteration 2392, train loss: 0.004653, validation loss: 0.003302\n",
      "iteration 2393, train loss: 0.004723, validation loss: 0.002969\n",
      "iteration 2394, train loss: 0.004811, validation loss: 0.003449\n",
      "iteration 2395, train loss: 0.004816, validation loss: 0.00339\n",
      "iteration 2396, train loss: 0.00434, validation loss: 0.003045\n",
      "iteration 2397, train loss: 0.004481, validation loss: 0.003349\n",
      "iteration 2398, train loss: 0.004652, validation loss: 0.003486\n",
      "iteration 2399, train loss: 0.004974, validation loss: 0.002958\n",
      "iteration 2400, train loss: 0.004326, validation loss: 0.003061\n",
      "iteration 2401, train loss: 0.004531, validation loss: 0.003387\n",
      "iteration 2402, train loss: 0.004778, validation loss: 0.003153\n",
      "iteration 2403, train loss: 0.005058, validation loss: 0.003454\n",
      "iteration 2404, train loss: 0.004577, validation loss: 0.003379\n",
      "iteration 2405, train loss: 0.005217, validation loss: 0.003033\n",
      "iteration 2406, train loss: 0.004812, validation loss: 0.00302\n",
      "iteration 2407, train loss: 0.004577, validation loss: 0.003056\n",
      "iteration 2408, train loss: 0.004401, validation loss: 0.00297\n",
      "iteration 2409, train loss: 0.004618, validation loss: 0.003116\n",
      "iteration 2410, train loss: 0.004597, validation loss: 0.003224\n",
      "iteration 2411, train loss: \u001b[92m0.004205\u001b[0m, validation loss: 0.003476\n",
      "iteration 2412, train loss: 0.00452, validation loss: 0.003378\n",
      "iteration 2413, train loss: 0.004769, validation loss: 0.003306\n",
      "iteration 2414, train loss: 0.004319, validation loss: 0.003149\n",
      "iteration 2415, train loss: 0.005103, validation loss: 0.002962\n",
      "iteration 2416, train loss: 0.004307, validation loss: 0.003421\n",
      "iteration 2417, train loss: 0.005114, validation loss: 0.003489\n",
      "iteration 2418, train loss: 0.004359, validation loss: 0.00354\n",
      "iteration 2419, train loss: 0.004527, validation loss: 0.003478\n",
      "iteration 2420, train loss: 0.004502, validation loss: 0.003886\n",
      "iteration 2421, train loss: 0.005172, validation loss: 0.003735\n",
      "iteration 2422, train loss: 0.005093, validation loss: 0.003336\n",
      "iteration 2423, train loss: 0.004655, validation loss: 0.003345\n",
      "iteration 2424, train loss: 0.005219, validation loss: 0.003574\n",
      "iteration 2425, train loss: 0.005132, validation loss: 0.003335\n",
      "iteration 2426, train loss: 0.004308, validation loss: 0.003042\n",
      "iteration 2427, train loss: 0.004849, validation loss: 0.002938\n",
      "iteration 2428, train loss: 0.004744, validation loss: 0.00307\n",
      "iteration 2429, train loss: 0.004445, validation loss: 0.003146\n",
      "iteration 2430, train loss: 0.004514, validation loss: 0.003406\n",
      "iteration 2431, train loss: 0.004324, validation loss: 0.003582\n",
      "iteration 2432, train loss: 0.004744, validation loss: 0.003018\n",
      "iteration 2433, train loss: \u001b[92m0.004199\u001b[0m, validation loss: \u001b[92m0.002854\u001b[0m\n",
      "iteration 2434, train loss: \u001b[92m0.004055\u001b[0m, validation loss: 0.003009\n",
      "iteration 2435, train loss: 0.004597, validation loss: \u001b[92m0.002851\u001b[0m\n",
      "iteration 2436, train loss: 0.00407, validation loss: 0.002981\n",
      "iteration 2437, train loss: 0.004778, validation loss: 0.003332\n",
      "iteration 2438, train loss: 0.00473, validation loss: 0.002958\n",
      "iteration 2439, train loss: 0.004416, validation loss: 0.00302\n",
      "iteration 2440, train loss: 0.004441, validation loss: 0.003347\n",
      "iteration 2441, train loss: 0.004864, validation loss: 0.002963\n",
      "iteration 2442, train loss: 0.004323, validation loss: 0.002852\n",
      "iteration 2443, train loss: 0.004446, validation loss: 0.003046\n",
      "iteration 2444, train loss: 0.004654, validation loss: 0.002901\n",
      "iteration 2445, train loss: 0.00426, validation loss: 0.003108\n",
      "iteration 2446, train loss: 0.004314, validation loss: 0.003108\n",
      "iteration 2447, train loss: 0.004277, validation loss: 0.002953\n",
      "iteration 2448, train loss: 0.004175, validation loss: \u001b[92m0.002828\u001b[0m\n",
      "iteration 2449, train loss: 0.004259, validation loss: 0.002926\n",
      "iteration 2450, train loss: 0.004345, validation loss: 0.002857\n",
      "iteration 2451, train loss: 0.004381, validation loss: 0.003069\n",
      "iteration 2452, train loss: 0.004621, validation loss: 0.003423\n",
      "iteration 2453, train loss: 0.004764, validation loss: 0.003082\n",
      "iteration 2454, train loss: 0.004746, validation loss: 0.002955\n",
      "iteration 2455, train loss: 0.004609, validation loss: 0.003019\n",
      "iteration 2456, train loss: 0.004437, validation loss: 0.002921\n",
      "iteration 2457, train loss: 0.004323, validation loss: 0.002972\n",
      "iteration 2458, train loss: 0.004563, validation loss: 0.002973\n",
      "iteration 2459, train loss: 0.00426, validation loss: 0.003575\n",
      "iteration 2460, train loss: 0.004435, validation loss: 0.003373\n",
      "iteration 2461, train loss: 0.004666, validation loss: 0.002911\n",
      "iteration 2462, train loss: 0.004585, validation loss: 0.002974\n",
      "iteration 2463, train loss: 0.004281, validation loss: 0.003026\n",
      "iteration 2464, train loss: 0.004753, validation loss: 0.003106\n",
      "iteration 2465, train loss: 0.004449, validation loss: 0.003271\n",
      "iteration 2466, train loss: 0.005115, validation loss: 0.002897\n",
      "iteration 2467, train loss: 0.00424, validation loss: 0.002882\n",
      "iteration 2468, train loss: 0.004512, validation loss: 0.00301\n",
      "iteration 2469, train loss: 0.004279, validation loss: 0.003411\n",
      "iteration 2470, train loss: 0.004791, validation loss: 0.003181\n",
      "iteration 2471, train loss: 0.004962, validation loss: 0.00323\n",
      "iteration 2472, train loss: 0.004401, validation loss: 0.003666\n",
      "iteration 2473, train loss: 0.005063, validation loss: 0.002891\n",
      "iteration 2474, train loss: 0.004334, validation loss: 0.003383\n",
      "iteration 2475, train loss: 0.00433, validation loss: 0.004051\n",
      "iteration 2476, train loss: 0.004962, validation loss: 0.003608\n",
      "iteration 2477, train loss: 0.004675, validation loss: 0.002941\n",
      "iteration 2478, train loss: 0.004288, validation loss: 0.003228\n",
      "iteration 2479, train loss: 0.004307, validation loss: 0.003196\n",
      "iteration 2480, train loss: 0.004613, validation loss: 0.002942\n",
      "iteration 2481, train loss: 0.004635, validation loss: 0.003282\n",
      "iteration 2482, train loss: 0.004662, validation loss: 0.003278\n",
      "iteration 2483, train loss: 0.004623, validation loss: 0.003067\n",
      "iteration 2484, train loss: 0.004706, validation loss: 0.002905\n",
      "iteration 2485, train loss: 0.0043, validation loss: 0.003217\n",
      "iteration 2486, train loss: 0.004478, validation loss: 0.00303\n",
      "iteration 2487, train loss: 0.004259, validation loss: 0.002846\n",
      "iteration 2488, train loss: 0.004264, validation loss: 0.003187\n",
      "iteration 2489, train loss: 0.004793, validation loss: 0.00293\n",
      "iteration 2490, train loss: 0.004079, validation loss: 0.002872\n",
      "iteration 2491, train loss: 0.004328, validation loss: 0.002918\n",
      "iteration 2492, train loss: 0.00462, validation loss: \u001b[92m0.002823\u001b[0m\n",
      "iteration 2493, train loss: 0.004446, validation loss: \u001b[92m0.002801\u001b[0m\n",
      "iteration 2494, train loss: 0.004236, validation loss: 0.002929\n",
      "iteration 2495, train loss: 0.004445, validation loss: 0.002815\n",
      "iteration 2496, train loss: \u001b[92m0.003849\u001b[0m, validation loss: 0.002988\n",
      "iteration 2497, train loss: 0.004435, validation loss: 0.00326\n",
      "iteration 2498, train loss: 0.004887, validation loss: 0.002897\n",
      "iteration 2499, train loss: 0.004236, validation loss: 0.003281\n",
      "iteration 2500, train loss: 0.004523, validation loss: 0.00314\n",
      "iteration 2501, train loss: 0.004241, validation loss: 0.002848\n",
      "iteration 2502, train loss: 0.004037, validation loss: 0.003326\n",
      "iteration 2503, train loss: 0.004716, validation loss: 0.003238\n",
      "iteration 2504, train loss: 0.004343, validation loss: 0.002972\n",
      "iteration 2505, train loss: 0.004164, validation loss: 0.003044\n",
      "iteration 2506, train loss: 0.004389, validation loss: 0.003054\n",
      "iteration 2507, train loss: 0.004481, validation loss: 0.002958\n",
      "iteration 2508, train loss: 0.004592, validation loss: \u001b[92m0.002777\u001b[0m\n",
      "iteration 2509, train loss: 0.004325, validation loss: 0.002851\n",
      "iteration 2510, train loss: 0.004502, validation loss: 0.003043\n",
      "iteration 2511, train loss: 0.004337, validation loss: 0.003131\n",
      "iteration 2512, train loss: 0.004239, validation loss: 0.003275\n",
      "iteration 2513, train loss: 0.004761, validation loss: 0.00331\n",
      "iteration 2514, train loss: 0.004762, validation loss: 0.003148\n",
      "iteration 2515, train loss: 0.004431, validation loss: 0.002824\n",
      "iteration 2516, train loss: 0.004276, validation loss: 0.003152\n",
      "iteration 2517, train loss: 0.004779, validation loss: 0.003156\n",
      "iteration 2518, train loss: 0.004765, validation loss: 0.003083\n",
      "iteration 2519, train loss: 0.004819, validation loss: 0.003239\n",
      "iteration 2520, train loss: 0.004432, validation loss: 0.003603\n",
      "iteration 2521, train loss: 0.004905, validation loss: 0.003286\n",
      "iteration 2522, train loss: 0.004955, validation loss: 0.003231\n",
      "iteration 2523, train loss: 0.004632, validation loss: 0.003417\n",
      "iteration 2524, train loss: 0.004396, validation loss: 0.003463\n",
      "iteration 2525, train loss: 0.004908, validation loss: 0.003019\n",
      "iteration 2526, train loss: 0.004366, validation loss: 0.002982\n",
      "iteration 2527, train loss: 0.004448, validation loss: 0.003457\n",
      "iteration 2528, train loss: 0.004652, validation loss: 0.003587\n",
      "iteration 2529, train loss: 0.004686, validation loss: 0.003229\n",
      "iteration 2530, train loss: 0.004276, validation loss: 0.002903\n",
      "iteration 2531, train loss: 0.004862, validation loss: 0.003048\n",
      "iteration 2532, train loss: 0.003992, validation loss: 0.003198\n",
      "iteration 2533, train loss: 0.005082, validation loss: 0.003151\n",
      "iteration 2534, train loss: 0.004161, validation loss: 0.003424\n",
      "iteration 2535, train loss: 0.004922, validation loss: 0.003283\n",
      "iteration 2536, train loss: 0.004845, validation loss: 0.003368\n",
      "iteration 2537, train loss: 0.004782, validation loss: 0.003141\n",
      "iteration 2538, train loss: 0.004219, validation loss: 0.003383\n",
      "iteration 2539, train loss: 0.00449, validation loss: 0.00312\n",
      "iteration 2540, train loss: 0.005122, validation loss: 0.003091\n",
      "iteration 2541, train loss: 0.004525, validation loss: 0.003181\n",
      "iteration 2542, train loss: 0.004353, validation loss: 0.003044\n",
      "iteration 2543, train loss: 0.004279, validation loss: 0.002959\n",
      "iteration 2544, train loss: 0.003884, validation loss: 0.002976\n",
      "iteration 2545, train loss: 0.004597, validation loss: 0.003011\n",
      "iteration 2546, train loss: 0.004791, validation loss: 0.002865\n",
      "iteration 2547, train loss: 0.004389, validation loss: 0.003113\n",
      "iteration 2548, train loss: 0.004332, validation loss: 0.003322\n",
      "iteration 2549, train loss: 0.004475, validation loss: 0.002952\n",
      "iteration 2550, train loss: 0.004148, validation loss: 0.003462\n",
      "iteration 2551, train loss: 0.004818, validation loss: 0.003631\n",
      "iteration 2552, train loss: 0.005078, validation loss: 0.002874\n",
      "iteration 2553, train loss: 0.004639, validation loss: 0.003176\n",
      "iteration 2554, train loss: 0.004291, validation loss: 0.003779\n",
      "iteration 2555, train loss: 0.004549, validation loss: 0.003419\n",
      "iteration 2556, train loss: 0.004496, validation loss: 0.003045\n",
      "iteration 2557, train loss: 0.00419, validation loss: 0.003988\n",
      "iteration 2558, train loss: 0.005114, validation loss: 0.004015\n",
      "iteration 2559, train loss: 0.004705, validation loss: 0.003097\n",
      "iteration 2560, train loss: 0.00476, validation loss: 0.003136\n",
      "iteration 2561, train loss: 0.004529, validation loss: 0.003786\n",
      "iteration 2562, train loss: 0.005273, validation loss: 0.002921\n",
      "iteration 2563, train loss: 0.004416, validation loss: 0.003255\n",
      "iteration 2564, train loss: 0.00467, validation loss: 0.00375\n",
      "iteration 2565, train loss: 0.005762, validation loss: 0.00327\n",
      "iteration 2566, train loss: 0.005369, validation loss: 0.003018\n",
      "iteration 2567, train loss: 0.00478, validation loss: 0.003933\n",
      "iteration 2568, train loss: 0.00518, validation loss: 0.003229\n",
      "iteration 2569, train loss: 0.004321, validation loss: 0.002947\n",
      "iteration 2570, train loss: 0.004071, validation loss: 0.003442\n",
      "iteration 2571, train loss: 0.004813, validation loss: 0.003174\n",
      "iteration 2572, train loss: 0.004868, validation loss: \u001b[92m0.002732\u001b[0m\n",
      "iteration 2573, train loss: 0.00436, validation loss: 0.003097\n",
      "iteration 2574, train loss: 0.004303, validation loss: 0.003178\n",
      "iteration 2575, train loss: 0.004453, validation loss: 0.002972\n",
      "iteration 2576, train loss: 0.004913, validation loss: 0.002914\n",
      "iteration 2577, train loss: 0.00468, validation loss: 0.003199\n",
      "iteration 2578, train loss: 0.004716, validation loss: 0.003044\n",
      "iteration 2579, train loss: 0.004483, validation loss: 0.003048\n",
      "iteration 2580, train loss: 0.004364, validation loss: 0.00314\n",
      "iteration 2581, train loss: 0.004411, validation loss: 0.00292\n",
      "iteration 2582, train loss: 0.004001, validation loss: 0.002856\n",
      "iteration 2583, train loss: 0.004232, validation loss: 0.003067\n",
      "iteration 2584, train loss: 0.004296, validation loss: 0.003131\n",
      "iteration 2585, train loss: 0.004788, validation loss: 0.002979\n",
      "iteration 2586, train loss: 0.004773, validation loss: 0.003085\n",
      "iteration 2587, train loss: 0.004323, validation loss: 0.003302\n",
      "iteration 2588, train loss: 0.004568, validation loss: 0.002838\n",
      "iteration 2589, train loss: 0.004682, validation loss: 0.003101\n",
      "iteration 2590, train loss: 0.004674, validation loss: 0.003213\n",
      "iteration 2591, train loss: 0.004304, validation loss: 0.002853\n",
      "iteration 2592, train loss: 0.004355, validation loss: 0.003077\n",
      "iteration 2593, train loss: 0.004323, validation loss: 0.003237\n",
      "iteration 2594, train loss: 0.004611, validation loss: 0.002799\n",
      "iteration 2595, train loss: 0.004157, validation loss: 0.003237\n",
      "iteration 2596, train loss: 0.004481, validation loss: 0.003193\n",
      "iteration 2597, train loss: 0.004345, validation loss: 0.002848\n",
      "iteration 2598, train loss: 0.004163, validation loss: 0.002943\n",
      "iteration 2599, train loss: 0.004349, validation loss: 0.003175\n",
      "iteration 2600, train loss: 0.004736, validation loss: 0.003415\n",
      "iteration 2601, train loss: 0.00454, validation loss: 0.003646\n",
      "iteration 2602, train loss: 0.004764, validation loss: 0.003142\n",
      "iteration 2603, train loss: 0.004656, validation loss: 0.003074\n",
      "iteration 2604, train loss: 0.004686, validation loss: 0.003925\n",
      "iteration 2605, train loss: 0.00525, validation loss: 0.003364\n",
      "iteration 2606, train loss: 0.005151, validation loss: 0.003273\n",
      "iteration 2607, train loss: 0.004591, validation loss: 0.003915\n",
      "iteration 2608, train loss: 0.005172, validation loss: 0.003863\n",
      "iteration 2609, train loss: 0.004834, validation loss: 0.003066\n",
      "iteration 2610, train loss: 0.004117, validation loss: 0.002816\n",
      "iteration 2611, train loss: 0.004245, validation loss: 0.003395\n",
      "iteration 2612, train loss: 0.004415, validation loss: 0.003678\n",
      "iteration 2613, train loss: 0.004492, validation loss: 0.003164\n",
      "iteration 2614, train loss: 0.004309, validation loss: 0.002748\n",
      "iteration 2615, train loss: 0.004035, validation loss: 0.003265\n",
      "iteration 2616, train loss: 0.004469, validation loss: 0.003577\n",
      "iteration 2617, train loss: 0.005327, validation loss: 0.003094\n",
      "iteration 2618, train loss: 0.004261, validation loss: 0.002748\n",
      "iteration 2619, train loss: 0.004057, validation loss: 0.003303\n",
      "iteration 2620, train loss: 0.004475, validation loss: 0.003381\n",
      "iteration 2621, train loss: 0.004497, validation loss: 0.003092\n",
      "iteration 2622, train loss: 0.004621, validation loss: 0.002813\n",
      "iteration 2623, train loss: 0.003991, validation loss: 0.003076\n",
      "iteration 2624, train loss: 0.004315, validation loss: 0.003118\n",
      "iteration 2625, train loss: 0.004158, validation loss: 0.002984\n",
      "iteration 2626, train loss: 0.004325, validation loss: 0.002752\n",
      "iteration 2627, train loss: 0.00412, validation loss: 0.00283\n",
      "iteration 2628, train loss: 0.004303, validation loss: 0.003048\n",
      "iteration 2629, train loss: 0.004387, validation loss: 0.003085\n",
      "iteration 2630, train loss: 0.004193, validation loss: 0.002859\n",
      "iteration 2631, train loss: 0.004072, validation loss: 0.00316\n",
      "iteration 2632, train loss: 0.003952, validation loss: 0.003381\n",
      "iteration 2633, train loss: 0.004569, validation loss: 0.003387\n",
      "iteration 2634, train loss: 0.005209, validation loss: 0.003015\n",
      "iteration 2635, train loss: 0.004335, validation loss: 0.002802\n",
      "iteration 2636, train loss: 0.003978, validation loss: 0.003015\n",
      "iteration 2637, train loss: 0.004372, validation loss: 0.003016\n",
      "iteration 2638, train loss: 0.004367, validation loss: 0.002876\n",
      "iteration 2639, train loss: 0.004417, validation loss: 0.002955\n",
      "iteration 2640, train loss: 0.004089, validation loss: 0.003209\n",
      "iteration 2641, train loss: 0.004473, validation loss: 0.003167\n",
      "iteration 2642, train loss: 0.004597, validation loss: 0.00297\n",
      "iteration 2643, train loss: 0.004089, validation loss: 0.003083\n",
      "iteration 2644, train loss: 0.004425, validation loss: 0.003166\n",
      "iteration 2645, train loss: 0.00445, validation loss: 0.002777\n",
      "iteration 2646, train loss: 0.004256, validation loss: 0.003247\n",
      "iteration 2647, train loss: 0.004689, validation loss: 0.003558\n",
      "iteration 2648, train loss: 0.00489, validation loss: 0.002842\n",
      "iteration 2649, train loss: 0.004401, validation loss: 0.00312\n",
      "iteration 2650, train loss: 0.004434, validation loss: 0.003328\n",
      "iteration 2651, train loss: 0.004642, validation loss: 0.003155\n",
      "iteration 2652, train loss: 0.005092, validation loss: 0.003079\n",
      "iteration 2653, train loss: 0.004584, validation loss: 0.003679\n",
      "iteration 2654, train loss: 0.005541, validation loss: 0.002749\n",
      "iteration 2655, train loss: 0.004123, validation loss: 0.003213\n",
      "iteration 2656, train loss: 0.00481, validation loss: 0.00313\n",
      "iteration 2657, train loss: 0.004837, validation loss: 0.002753\n",
      "iteration 2658, train loss: 0.004108, validation loss: 0.003109\n",
      "iteration 2659, train loss: 0.004326, validation loss: 0.003096\n",
      "iteration 2660, train loss: 0.004333, validation loss: 0.002798\n",
      "iteration 2661, train loss: 0.004119, validation loss: 0.002802\n",
      "iteration 2662, train loss: 0.004171, validation loss: 0.002877\n",
      "iteration 2663, train loss: 0.004387, validation loss: 0.002808\n",
      "iteration 2664, train loss: 0.003957, validation loss: 0.002847\n",
      "iteration 2665, train loss: 0.00409, validation loss: 0.003355\n",
      "iteration 2666, train loss: 0.004479, validation loss: 0.003208\n",
      "iteration 2667, train loss: 0.004166, validation loss: 0.002847\n",
      "iteration 2668, train loss: \u001b[92m0.003812\u001b[0m, validation loss: 0.00299\n",
      "iteration 2669, train loss: 0.004864, validation loss: 0.002776\n",
      "iteration 2670, train loss: 0.00442, validation loss: 0.003155\n",
      "iteration 2671, train loss: 0.004362, validation loss: 0.003125\n",
      "iteration 2672, train loss: 0.004829, validation loss: 0.002988\n",
      "iteration 2673, train loss: 0.004156, validation loss: 0.003576\n",
      "iteration 2674, train loss: 0.004869, validation loss: 0.002936\n",
      "iteration 2675, train loss: 0.003989, validation loss: 0.00283\n",
      "iteration 2676, train loss: 0.004621, validation loss: 0.003072\n",
      "iteration 2677, train loss: 0.00504, validation loss: 0.002745\n",
      "iteration 2678, train loss: 0.004544, validation loss: 0.003266\n",
      "iteration 2679, train loss: 0.004087, validation loss: 0.004289\n",
      "iteration 2680, train loss: 0.005595, validation loss: 0.003029\n",
      "iteration 2681, train loss: 0.004411, validation loss: 0.003626\n",
      "iteration 2682, train loss: 0.005198, validation loss: 0.004193\n",
      "iteration 2683, train loss: 0.005457, validation loss: 0.003211\n",
      "iteration 2684, train loss: 0.004699, validation loss: 0.003094\n",
      "iteration 2685, train loss: 0.004058, validation loss: 0.004449\n",
      "iteration 2686, train loss: 0.005078, validation loss: 0.004055\n",
      "iteration 2687, train loss: 0.005234, validation loss: 0.003564\n",
      "iteration 2688, train loss: 0.004996, validation loss: 0.003698\n",
      "iteration 2689, train loss: 0.004722, validation loss: 0.003781\n",
      "iteration 2690, train loss: 0.005194, validation loss: 0.004035\n",
      "iteration 2691, train loss: 0.005319, validation loss: 0.004109\n",
      "iteration 2692, train loss: 0.006062, validation loss: 0.003455\n",
      "iteration 2693, train loss: 0.004844, validation loss: 0.003905\n",
      "iteration 2694, train loss: 0.005444, validation loss: 0.004093\n",
      "iteration 2695, train loss: 0.004586, validation loss: 0.003969\n",
      "iteration 2696, train loss: 0.005417, validation loss: 0.003328\n",
      "iteration 2697, train loss: 0.005346, validation loss: 0.003091\n",
      "iteration 2698, train loss: 0.004584, validation loss: 0.00389\n",
      "iteration 2699, train loss: 0.005881, validation loss: 0.004034\n",
      "iteration 2700, train loss: 0.006471, validation loss: 0.002843\n",
      "iteration 2701, train loss: 0.004432, validation loss: 0.00403\n",
      "iteration 2702, train loss: 0.00541, validation loss: 0.004554\n",
      "iteration 2703, train loss: 0.006184, validation loss: 0.003694\n",
      "iteration 2704, train loss: 0.00466, validation loss: 0.003004\n",
      "iteration 2705, train loss: 0.004129, validation loss: 0.003425\n",
      "iteration 2706, train loss: 0.005074, validation loss: 0.003306\n",
      "iteration 2707, train loss: 0.004999, validation loss: 0.003031\n",
      "iteration 2708, train loss: 0.004844, validation loss: 0.002815\n",
      "iteration 2709, train loss: 0.004371, validation loss: 0.002975\n",
      "iteration 2710, train loss: 0.004572, validation loss: 0.00342\n",
      "iteration 2711, train loss: 0.00504, validation loss: 0.003282\n",
      "iteration 2712, train loss: 0.004408, validation loss: 0.002774\n",
      "iteration 2713, train loss: 0.004128, validation loss: 0.002866\n",
      "iteration 2714, train loss: \u001b[92m0.003791\u001b[0m, validation loss: 0.003224\n",
      "iteration 2715, train loss: 0.005036, validation loss: 0.002945\n",
      "iteration 2716, train loss: 0.004386, validation loss: 0.002776\n",
      "iteration 2717, train loss: 0.004209, validation loss: 0.002849\n",
      "iteration 2718, train loss: 0.004182, validation loss: 0.003136\n",
      "iteration 2719, train loss: 0.004706, validation loss: 0.002846\n",
      "iteration 2720, train loss: 0.004194, validation loss: 0.002809\n",
      "iteration 2721, train loss: 0.004377, validation loss: 0.002974\n",
      "iteration 2722, train loss: 0.004488, validation loss: 0.003065\n",
      "iteration 2723, train loss: 0.004547, validation loss: 0.00294\n",
      "iteration 2724, train loss: 0.004888, validation loss: 0.002848\n",
      "iteration 2725, train loss: 0.004013, validation loss: 0.003403\n",
      "iteration 2726, train loss: 0.00452, validation loss: 0.003359\n",
      "iteration 2727, train loss: 0.004621, validation loss: 0.002774\n",
      "iteration 2728, train loss: 0.004264, validation loss: 0.002987\n",
      "iteration 2729, train loss: 0.004386, validation loss: 0.003542\n",
      "iteration 2730, train loss: 0.004885, validation loss: 0.002933\n",
      "iteration 2731, train loss: 0.00458, validation loss: 0.003005\n",
      "iteration 2732, train loss: 0.004315, validation loss: 0.003588\n",
      "iteration 2733, train loss: 0.004731, validation loss: 0.003676\n",
      "iteration 2734, train loss: 0.004303, validation loss: 0.00343\n",
      "iteration 2735, train loss: 0.004571, validation loss: 0.003007\n",
      "iteration 2736, train loss: 0.004586, validation loss: 0.002859\n",
      "iteration 2737, train loss: 0.004257, validation loss: 0.002899\n",
      "iteration 2738, train loss: 0.004172, validation loss: 0.002796\n",
      "iteration 2739, train loss: 0.004156, validation loss: 0.002902\n",
      "iteration 2740, train loss: 0.003931, validation loss: 0.003391\n",
      "iteration 2741, train loss: \u001b[92m0.003762\u001b[0m, validation loss: 0.003675\n",
      "iteration 2742, train loss: 0.004355, validation loss: 0.003207\n",
      "iteration 2743, train loss: 0.004679, validation loss: 0.002836\n",
      "iteration 2744, train loss: 0.004027, validation loss: 0.003192\n",
      "iteration 2745, train loss: 0.004699, validation loss: 0.00305\n",
      "iteration 2746, train loss: 0.004559, validation loss: \u001b[92m0.002682\u001b[0m\n",
      "iteration 2747, train loss: 0.003835, validation loss: 0.00292\n",
      "iteration 2748, train loss: 0.004465, validation loss: 0.002971\n",
      "iteration 2749, train loss: 0.004162, validation loss: 0.002838\n",
      "iteration 2750, train loss: 0.004167, validation loss: 0.002721\n",
      "iteration 2751, train loss: 0.004351, validation loss: 0.002901\n",
      "iteration 2752, train loss: 0.004485, validation loss: 0.003021\n",
      "iteration 2753, train loss: 0.004258, validation loss: 0.003366\n",
      "iteration 2754, train loss: 0.004244, validation loss: 0.003469\n",
      "iteration 2755, train loss: 0.004722, validation loss: 0.002838\n",
      "iteration 2756, train loss: 0.004078, validation loss: 0.003067\n",
      "iteration 2757, train loss: 0.004514, validation loss: 0.003484\n",
      "iteration 2758, train loss: 0.00473, validation loss: 0.002995\n",
      "iteration 2759, train loss: 0.004646, validation loss: 0.002867\n",
      "iteration 2760, train loss: 0.004279, validation loss: 0.003104\n",
      "iteration 2761, train loss: 0.003978, validation loss: 0.003152\n",
      "iteration 2762, train loss: 0.004201, validation loss: 0.00304\n",
      "iteration 2763, train loss: 0.004937, validation loss: 0.00285\n",
      "iteration 2764, train loss: 0.004478, validation loss: 0.002796\n",
      "iteration 2765, train loss: 0.003979, validation loss: 0.002861\n",
      "iteration 2766, train loss: 0.004457, validation loss: 0.003107\n",
      "iteration 2767, train loss: 0.004513, validation loss: 0.003395\n",
      "iteration 2768, train loss: 0.004754, validation loss: 0.003411\n",
      "iteration 2769, train loss: 0.004181, validation loss: 0.003548\n",
      "iteration 2770, train loss: 0.004914, validation loss: 0.003049\n",
      "iteration 2771, train loss: 0.004232, validation loss: 0.002736\n",
      "iteration 2772, train loss: 0.003987, validation loss: 0.00285\n",
      "iteration 2773, train loss: 0.003839, validation loss: 0.003126\n",
      "iteration 2774, train loss: 0.004378, validation loss: 0.002768\n",
      "iteration 2775, train loss: 0.004325, validation loss: 0.002892\n",
      "iteration 2776, train loss: 0.004276, validation loss: 0.003309\n",
      "iteration 2777, train loss: 0.004416, validation loss: 0.003254\n",
      "iteration 2778, train loss: 0.004339, validation loss: 0.003405\n",
      "iteration 2779, train loss: 0.004565, validation loss: 0.003704\n",
      "iteration 2780, train loss: 0.004926, validation loss: 0.00312\n",
      "iteration 2781, train loss: 0.004381, validation loss: 0.002993\n",
      "iteration 2782, train loss: 0.004456, validation loss: 0.003236\n",
      "iteration 2783, train loss: 0.004428, validation loss: 0.003035\n",
      "iteration 2784, train loss: 0.004254, validation loss: 0.002954\n",
      "iteration 2785, train loss: 0.004099, validation loss: 0.003353\n",
      "iteration 2786, train loss: 0.004305, validation loss: 0.003156\n",
      "iteration 2787, train loss: 0.004181, validation loss: 0.002888\n",
      "iteration 2788, train loss: 0.004332, validation loss: 0.003299\n",
      "iteration 2789, train loss: 0.004674, validation loss: 0.003444\n",
      "iteration 2790, train loss: 0.004316, validation loss: 0.003041\n",
      "iteration 2791, train loss: 0.004233, validation loss: 0.002975\n",
      "iteration 2792, train loss: 0.00435, validation loss: 0.00338\n",
      "iteration 2793, train loss: 0.004454, validation loss: 0.003068\n",
      "iteration 2794, train loss: 0.004124, validation loss: \u001b[92m0.002639\u001b[0m\n",
      "iteration 2795, train loss: 0.003854, validation loss: 0.002841\n",
      "iteration 2796, train loss: 0.004627, validation loss: 0.002883\n",
      "iteration 2797, train loss: 0.00422, validation loss: 0.002744\n",
      "iteration 2798, train loss: 0.003772, validation loss: 0.00289\n",
      "iteration 2799, train loss: 0.004205, validation loss: 0.002767\n",
      "iteration 2800, train loss: 0.003987, validation loss: \u001b[92m0.002623\u001b[0m\n",
      "iteration 2801, train loss: 0.003794, validation loss: 0.002725\n",
      "iteration 2802, train loss: 0.004193, validation loss: 0.00268\n",
      "iteration 2803, train loss: 0.004067, validation loss: 0.002628\n",
      "iteration 2804, train loss: \u001b[92m0.003744\u001b[0m, validation loss: 0.002876\n",
      "iteration 2805, train loss: 0.004193, validation loss: 0.002828\n",
      "iteration 2806, train loss: 0.004266, validation loss: 0.002662\n",
      "iteration 2807, train loss: 0.003868, validation loss: 0.002706\n",
      "iteration 2808, train loss: 0.003891, validation loss: 0.002659\n",
      "iteration 2809, train loss: 0.003767, validation loss: 0.002675\n",
      "iteration 2810, train loss: 0.004054, validation loss: 0.002718\n",
      "iteration 2811, train loss: 0.004196, validation loss: 0.002695\n",
      "iteration 2812, train loss: 0.003902, validation loss: 0.002735\n",
      "iteration 2813, train loss: 0.003997, validation loss: 0.002923\n",
      "iteration 2814, train loss: 0.00398, validation loss: 0.00291\n",
      "iteration 2815, train loss: 0.004187, validation loss: 0.00271\n",
      "iteration 2816, train loss: 0.003876, validation loss: 0.00266\n",
      "iteration 2817, train loss: 0.004052, validation loss: 0.002661\n",
      "iteration 2818, train loss: 0.004206, validation loss: 0.002647\n",
      "iteration 2819, train loss: 0.003875, validation loss: 0.002732\n",
      "iteration 2820, train loss: 0.00392, validation loss: 0.002722\n",
      "iteration 2821, train loss: 0.003804, validation loss: 0.002717\n",
      "iteration 2822, train loss: 0.003923, validation loss: 0.003096\n",
      "iteration 2823, train loss: 0.00433, validation loss: 0.00282\n",
      "iteration 2824, train loss: 0.004133, validation loss: 0.002739\n",
      "iteration 2825, train loss: 0.003874, validation loss: 0.002956\n",
      "iteration 2826, train loss: 0.004592, validation loss: 0.00277\n",
      "iteration 2827, train loss: 0.003943, validation loss: 0.00289\n",
      "iteration 2828, train loss: 0.003849, validation loss: 0.002861\n",
      "iteration 2829, train loss: 0.003877, validation loss: 0.002716\n",
      "iteration 2830, train loss: 0.004124, validation loss: 0.002773\n",
      "iteration 2831, train loss: 0.003859, validation loss: 0.00305\n",
      "iteration 2832, train loss: 0.004331, validation loss: 0.002822\n",
      "iteration 2833, train loss: 0.004116, validation loss: 0.00273\n",
      "iteration 2834, train loss: 0.004315, validation loss: 0.003113\n",
      "iteration 2835, train loss: 0.004461, validation loss: 0.002935\n",
      "iteration 2836, train loss: 0.004198, validation loss: 0.002791\n",
      "iteration 2837, train loss: 0.00403, validation loss: 0.002741\n",
      "iteration 2838, train loss: 0.004144, validation loss: 0.002687\n",
      "iteration 2839, train loss: 0.004245, validation loss: 0.002833\n",
      "iteration 2840, train loss: 0.004192, validation loss: 0.002814\n",
      "iteration 2841, train loss: 0.004135, validation loss: 0.002637\n",
      "iteration 2842, train loss: 0.004112, validation loss: 0.002874\n",
      "iteration 2843, train loss: 0.004154, validation loss: 0.003183\n",
      "iteration 2844, train loss: 0.004836, validation loss: 0.002923\n",
      "iteration 2845, train loss: 0.003908, validation loss: 0.002647\n",
      "iteration 2846, train loss: 0.004056, validation loss: 0.002716\n",
      "iteration 2847, train loss: 0.004284, validation loss: 0.002765\n",
      "iteration 2848, train loss: 0.004233, validation loss: 0.002712\n",
      "iteration 2849, train loss: 0.003921, validation loss: 0.00276\n",
      "iteration 2850, train loss: 0.003965, validation loss: 0.002766\n",
      "iteration 2851, train loss: 0.004019, validation loss: 0.002634\n",
      "iteration 2852, train loss: \u001b[92m0.003646\u001b[0m, validation loss: 0.002873\n",
      "iteration 2853, train loss: 0.004417, validation loss: 0.002938\n",
      "iteration 2854, train loss: 0.003819, validation loss: 0.002946\n",
      "iteration 2855, train loss: 0.004189, validation loss: 0.002851\n",
      "iteration 2856, train loss: 0.004288, validation loss: 0.002628\n",
      "iteration 2857, train loss: 0.003782, validation loss: 0.002683\n",
      "iteration 2858, train loss: 0.004416, validation loss: 0.002829\n",
      "iteration 2859, train loss: 0.003924, validation loss: 0.002857\n",
      "iteration 2860, train loss: 0.00401, validation loss: 0.002931\n",
      "iteration 2861, train loss: 0.004019, validation loss: 0.003056\n",
      "iteration 2862, train loss: 0.004365, validation loss: 0.002743\n",
      "iteration 2863, train loss: \u001b[92m0.003637\u001b[0m, validation loss: 0.002911\n",
      "iteration 2864, train loss: 0.004157, validation loss: 0.002705\n",
      "iteration 2865, train loss: 0.004023, validation loss: 0.002739\n",
      "iteration 2866, train loss: 0.003778, validation loss: 0.003637\n",
      "iteration 2867, train loss: 0.004619, validation loss: 0.003329\n",
      "iteration 2868, train loss: 0.004331, validation loss: \u001b[92m0.002613\u001b[0m\n",
      "iteration 2869, train loss: \u001b[92m0.00361\u001b[0m, validation loss: 0.002916\n",
      "iteration 2870, train loss: 0.004184, validation loss: 0.00304\n",
      "iteration 2871, train loss: 0.004057, validation loss: 0.002686\n",
      "iteration 2872, train loss: 0.003901, validation loss: 0.002767\n",
      "iteration 2873, train loss: 0.003921, validation loss: 0.002899\n",
      "iteration 2874, train loss: 0.004383, validation loss: 0.002725\n",
      "iteration 2875, train loss: 0.004238, validation loss: 0.002696\n",
      "iteration 2876, train loss: 0.004208, validation loss: 0.002728\n",
      "iteration 2877, train loss: 0.003987, validation loss: 0.0027\n",
      "iteration 2878, train loss: 0.00391, validation loss: 0.002659\n",
      "iteration 2879, train loss: 0.003732, validation loss: 0.002723\n",
      "iteration 2880, train loss: 0.003936, validation loss: 0.002738\n",
      "iteration 2881, train loss: 0.00376, validation loss: 0.002917\n",
      "iteration 2882, train loss: 0.003757, validation loss: 0.002928\n",
      "iteration 2883, train loss: 0.003857, validation loss: 0.002727\n",
      "iteration 2884, train loss: 0.003938, validation loss: \u001b[92m0.002575\u001b[0m\n",
      "iteration 2885, train loss: 0.003644, validation loss: 0.002691\n",
      "iteration 2886, train loss: 0.004037, validation loss: 0.002786\n",
      "iteration 2887, train loss: 0.004355, validation loss: 0.002676\n",
      "iteration 2888, train loss: 0.003682, validation loss: 0.002602\n",
      "iteration 2889, train loss: 0.004013, validation loss: 0.002882\n",
      "iteration 2890, train loss: 0.004348, validation loss: 0.003115\n",
      "iteration 2891, train loss: 0.005047, validation loss: 0.003002\n",
      "iteration 2892, train loss: 0.004634, validation loss: 0.002927\n",
      "iteration 2893, train loss: 0.003744, validation loss: 0.00369\n",
      "iteration 2894, train loss: 0.00512, validation loss: 0.003548\n",
      "iteration 2895, train loss: 0.004827, validation loss: 0.002672\n",
      "iteration 2896, train loss: 0.003908, validation loss: 0.003014\n",
      "iteration 2897, train loss: 0.004189, validation loss: 0.003142\n",
      "iteration 2898, train loss: 0.00438, validation loss: 0.002689\n",
      "iteration 2899, train loss: 0.003967, validation loss: 0.003419\n",
      "iteration 2900, train loss: 0.004351, validation loss: 0.003609\n",
      "iteration 2901, train loss: 0.004475, validation loss: 0.00313\n",
      "iteration 2902, train loss: 0.004183, validation loss: 0.003273\n",
      "iteration 2903, train loss: 0.0052, validation loss: 0.003398\n",
      "iteration 2904, train loss: 0.004138, validation loss: 0.003378\n",
      "iteration 2905, train loss: 0.00486, validation loss: 0.002727\n",
      "iteration 2906, train loss: 0.003993, validation loss: 0.003086\n",
      "iteration 2907, train loss: 0.004089, validation loss: 0.0037\n",
      "iteration 2908, train loss: 0.004581, validation loss: 0.003608\n",
      "iteration 2909, train loss: 0.005141, validation loss: 0.002882\n",
      "iteration 2910, train loss: 0.004056, validation loss: 0.002904\n",
      "iteration 2911, train loss: 0.00471, validation loss: 0.003086\n",
      "iteration 2912, train loss: 0.00427, validation loss: 0.002838\n",
      "iteration 2913, train loss: 0.003899, validation loss: 0.002642\n",
      "iteration 2914, train loss: 0.00398, validation loss: 0.002785\n",
      "iteration 2915, train loss: 0.0039, validation loss: 0.002968\n",
      "iteration 2916, train loss: 0.003928, validation loss: 0.002944\n",
      "iteration 2917, train loss: 0.00422, validation loss: 0.002631\n",
      "iteration 2918, train loss: 0.003911, validation loss: 0.002632\n",
      "iteration 2919, train loss: 0.003923, validation loss: 0.002792\n",
      "iteration 2920, train loss: 0.003764, validation loss: 0.002814\n",
      "iteration 2921, train loss: 0.004363, validation loss: 0.002649\n",
      "iteration 2922, train loss: 0.003794, validation loss: 0.003056\n",
      "iteration 2923, train loss: 0.004228, validation loss: 0.003185\n",
      "iteration 2924, train loss: 0.004824, validation loss: 0.002921\n",
      "iteration 2925, train loss: 0.00458, validation loss: 0.002652\n",
      "iteration 2926, train loss: 0.004053, validation loss: 0.002837\n",
      "iteration 2927, train loss: 0.004941, validation loss: 0.002588\n",
      "iteration 2928, train loss: 0.004147, validation loss: 0.002814\n",
      "iteration 2929, train loss: 0.004472, validation loss: 0.003085\n",
      "iteration 2930, train loss: 0.004492, validation loss: 0.002975\n",
      "iteration 2931, train loss: 0.00438, validation loss: 0.002696\n",
      "iteration 2932, train loss: 0.00389, validation loss: 0.002725\n",
      "iteration 2933, train loss: 0.004079, validation loss: 0.003088\n",
      "iteration 2934, train loss: 0.004222, validation loss: 0.003259\n",
      "iteration 2935, train loss: 0.004224, validation loss: 0.002976\n",
      "iteration 2936, train loss: 0.004295, validation loss: 0.00283\n",
      "iteration 2937, train loss: 0.004358, validation loss: 0.003236\n",
      "iteration 2938, train loss: 0.004149, validation loss: 0.003364\n",
      "iteration 2939, train loss: 0.004114, validation loss: 0.002918\n",
      "iteration 2940, train loss: 0.004217, validation loss: 0.002721\n",
      "iteration 2941, train loss: 0.004311, validation loss: 0.002951\n",
      "iteration 2942, train loss: 0.004483, validation loss: 0.002854\n",
      "iteration 2943, train loss: 0.004419, validation loss: 0.002841\n",
      "iteration 2944, train loss: 0.003681, validation loss: 0.003541\n",
      "iteration 2945, train loss: 0.004918, validation loss: 0.003025\n",
      "iteration 2946, train loss: 0.004079, validation loss: 0.002896\n",
      "iteration 2947, train loss: 0.003903, validation loss: 0.002944\n",
      "iteration 2948, train loss: 0.004459, validation loss: 0.002718\n",
      "iteration 2949, train loss: 0.004138, validation loss: 0.002837\n",
      "iteration 2950, train loss: 0.003952, validation loss: 0.003079\n",
      "iteration 2951, train loss: 0.004217, validation loss: 0.002841\n",
      "iteration 2952, train loss: 0.004076, validation loss: 0.002617\n",
      "iteration 2953, train loss: 0.003736, validation loss: 0.002732\n",
      "iteration 2954, train loss: 0.00451, validation loss: 0.002831\n",
      "iteration 2955, train loss: 0.004139, validation loss: 0.002957\n",
      "iteration 2956, train loss: 0.0041, validation loss: 0.002726\n",
      "iteration 2957, train loss: 0.004325, validation loss: 0.002831\n",
      "iteration 2958, train loss: 0.004535, validation loss: 0.003479\n",
      "iteration 2959, train loss: 0.004484, validation loss: 0.003153\n",
      "iteration 2960, train loss: 0.004224, validation loss: 0.002782\n",
      "iteration 2961, train loss: 0.004144, validation loss: 0.00286\n",
      "iteration 2962, train loss: 0.004473, validation loss: 0.002912\n",
      "iteration 2963, train loss: 0.004365, validation loss: 0.002709\n",
      "iteration 2964, train loss: 0.004095, validation loss: 0.002795\n",
      "iteration 2965, train loss: 0.004054, validation loss: 0.002876\n",
      "iteration 2966, train loss: 0.004658, validation loss: 0.003188\n",
      "iteration 2967, train loss: 0.004044, validation loss: 0.003698\n",
      "iteration 2968, train loss: 0.004352, validation loss: 0.00335\n",
      "iteration 2969, train loss: 0.004331, validation loss: 0.00281\n",
      "iteration 2970, train loss: 0.004054, validation loss: 0.002727\n",
      "iteration 2971, train loss: 0.004131, validation loss: 0.003404\n",
      "iteration 2972, train loss: 0.004753, validation loss: 0.003626\n",
      "iteration 2973, train loss: 0.005356, validation loss: 0.002918\n",
      "iteration 2974, train loss: 0.004001, validation loss: 0.003135\n",
      "iteration 2975, train loss: 0.004102, validation loss: 0.003735\n",
      "iteration 2976, train loss: 0.004302, validation loss: 0.003685\n",
      "iteration 2977, train loss: 0.00435, validation loss: 0.002972\n",
      "iteration 2978, train loss: 0.004558, validation loss: 0.002623\n",
      "iteration 2979, train loss: 0.003629, validation loss: 0.003069\n",
      "iteration 2980, train loss: 0.003978, validation loss: 0.003296\n",
      "iteration 2981, train loss: 0.004748, validation loss: 0.00269\n",
      "iteration 2982, train loss: 0.003881, validation loss: 0.002652\n",
      "iteration 2983, train loss: 0.004024, validation loss: 0.00297\n",
      "iteration 2984, train loss: 0.004116, validation loss: 0.003013\n",
      "iteration 2985, train loss: 0.004113, validation loss: 0.002676\n",
      "iteration 2986, train loss: 0.004014, validation loss: 0.002847\n",
      "iteration 2987, train loss: 0.004278, validation loss: 0.003061\n",
      "iteration 2988, train loss: 0.004669, validation loss: 0.002958\n",
      "iteration 2989, train loss: 0.004414, validation loss: \u001b[92m0.002552\u001b[0m\n",
      "iteration 2990, train loss: 0.003675, validation loss: 0.002815\n",
      "iteration 2991, train loss: 0.004306, validation loss: 0.002995\n",
      "iteration 2992, train loss: 0.004104, validation loss: 0.002826\n",
      "iteration 2993, train loss: 0.004394, validation loss: 0.002762\n",
      "iteration 2994, train loss: 0.004001, validation loss: 0.003065\n",
      "iteration 2995, train loss: 0.0044, validation loss: 0.003043\n",
      "iteration 2996, train loss: 0.004537, validation loss: 0.002638\n",
      "iteration 2997, train loss: 0.00415, validation loss: 0.002685\n",
      "iteration 2998, train loss: 0.003807, validation loss: 0.00325\n",
      "iteration 2999, train loss: 0.004178, validation loss: 0.003128\n",
      "iteration 3000, train loss: 0.003753, validation loss: 0.002753\n",
      "iteration 3001, train loss: 0.004176, validation loss: 0.002604\n",
      "iteration 3002, train loss: 0.003941, validation loss: 0.002896\n",
      "iteration 3003, train loss: 0.004401, validation loss: 0.00274\n",
      "iteration 3004, train loss: 0.004105, validation loss: 0.00274\n",
      "iteration 3005, train loss: 0.003871, validation loss: 0.003254\n",
      "iteration 3006, train loss: 0.004801, validation loss: 0.002952\n",
      "iteration 3007, train loss: 0.004433, validation loss: 0.002971\n",
      "iteration 3008, train loss: 0.003935, validation loss: 0.003271\n",
      "iteration 3009, train loss: 0.004542, validation loss: 0.002807\n",
      "iteration 3010, train loss: 0.004384, validation loss: 0.00282\n",
      "iteration 3011, train loss: 0.003889, validation loss: 0.003489\n",
      "iteration 3012, train loss: 0.004828, validation loss: 0.002733\n",
      "iteration 3013, train loss: 0.004137, validation loss: 0.00326\n",
      "iteration 3014, train loss: 0.00468, validation loss: 0.003293\n",
      "iteration 3015, train loss: 0.004655, validation loss: 0.002825\n",
      "iteration 3016, train loss: 0.004347, validation loss: 0.003412\n",
      "iteration 3017, train loss: 0.004851, validation loss: 0.003154\n",
      "iteration 3018, train loss: 0.00466, validation loss: 0.002889\n",
      "iteration 3019, train loss: 0.004051, validation loss: 0.003132\n",
      "iteration 3020, train loss: 0.004287, validation loss: 0.003238\n",
      "iteration 3021, train loss: 0.004377, validation loss: 0.003325\n",
      "iteration 3022, train loss: 0.004337, validation loss: 0.003351\n",
      "iteration 3023, train loss: 0.004101, validation loss: 0.002928\n",
      "iteration 3024, train loss: 0.004197, validation loss: 0.002895\n",
      "iteration 3025, train loss: 0.004497, validation loss: 0.002901\n",
      "iteration 3026, train loss: 0.004466, validation loss: 0.002775\n",
      "iteration 3027, train loss: 0.004094, validation loss: 0.003081\n",
      "iteration 3028, train loss: 0.004711, validation loss: 0.003326\n",
      "iteration 3029, train loss: 0.004544, validation loss: 0.003197\n",
      "iteration 3030, train loss: 0.004381, validation loss: 0.002696\n",
      "iteration 3031, train loss: 0.003782, validation loss: 0.002713\n",
      "iteration 3032, train loss: 0.003794, validation loss: 0.00307\n",
      "iteration 3033, train loss: 0.00454, validation loss: 0.002656\n",
      "iteration 3034, train loss: 0.003946, validation loss: 0.002702\n",
      "iteration 3035, train loss: 0.003631, validation loss: 0.003174\n",
      "iteration 3036, train loss: 0.004319, validation loss: 0.003236\n",
      "iteration 3037, train loss: 0.004592, validation loss: 0.002875\n",
      "iteration 3038, train loss: 0.003761, validation loss: 0.003025\n",
      "iteration 3039, train loss: 0.004063, validation loss: 0.002764\n",
      "iteration 3040, train loss: 0.004166, validation loss: 0.002557\n",
      "iteration 3041, train loss: 0.004494, validation loss: 0.002848\n",
      "iteration 3042, train loss: 0.003785, validation loss: 0.002895\n",
      "iteration 3043, train loss: 0.004007, validation loss: 0.002631\n",
      "iteration 3044, train loss: 0.003954, validation loss: 0.002772\n",
      "iteration 3045, train loss: 0.004035, validation loss: 0.002879\n",
      "iteration 3046, train loss: 0.004116, validation loss: 0.002559\n",
      "iteration 3047, train loss: 0.003824, validation loss: 0.002622\n",
      "iteration 3048, train loss: 0.003946, validation loss: 0.002825\n",
      "iteration 3049, train loss: 0.004274, validation loss: 0.002602\n",
      "iteration 3050, train loss: 0.003972, validation loss: 0.002668\n",
      "iteration 3051, train loss: 0.004137, validation loss: 0.002888\n",
      "iteration 3052, train loss: 0.003838, validation loss: 0.002732\n",
      "iteration 3053, train loss: 0.003857, validation loss: \u001b[92m0.00254\u001b[0m\n",
      "iteration 3054, train loss: 0.004022, validation loss: 0.0029\n",
      "iteration 3055, train loss: 0.004125, validation loss: 0.002938\n",
      "iteration 3056, train loss: 0.004503, validation loss: 0.002607\n",
      "iteration 3057, train loss: 0.004123, validation loss: 0.00283\n",
      "iteration 3058, train loss: 0.003805, validation loss: 0.003068\n",
      "iteration 3059, train loss: 0.004425, validation loss: 0.002656\n",
      "iteration 3060, train loss: 0.003719, validation loss: 0.002759\n",
      "iteration 3061, train loss: 0.003844, validation loss: 0.002993\n",
      "iteration 3062, train loss: 0.003948, validation loss: 0.002802\n",
      "iteration 3063, train loss: 0.003985, validation loss: 0.002703\n",
      "iteration 3064, train loss: 0.003728, validation loss: 0.003138\n",
      "iteration 3065, train loss: 0.004192, validation loss: 0.002915\n",
      "iteration 3066, train loss: 0.004067, validation loss: 0.002805\n",
      "iteration 3067, train loss: 0.004054, validation loss: 0.002812\n",
      "iteration 3068, train loss: 0.0039, validation loss: 0.002557\n",
      "iteration 3069, train loss: 0.003856, validation loss: 0.00275\n",
      "iteration 3070, train loss: 0.004004, validation loss: 0.002978\n",
      "iteration 3071, train loss: 0.003993, validation loss: 0.003061\n",
      "iteration 3072, train loss: 0.003827, validation loss: 0.002862\n",
      "iteration 3073, train loss: 0.003813, validation loss: 0.002918\n",
      "iteration 3074, train loss: 0.004049, validation loss: 0.002702\n",
      "iteration 3075, train loss: 0.003754, validation loss: 0.002621\n",
      "iteration 3076, train loss: 0.004157, validation loss: 0.002666\n",
      "iteration 3077, train loss: 0.004097, validation loss: 0.002609\n",
      "iteration 3078, train loss: 0.00401, validation loss: 0.002789\n",
      "iteration 3079, train loss: 0.003909, validation loss: 0.002953\n",
      "iteration 3080, train loss: 0.004023, validation loss: 0.002619\n",
      "iteration 3081, train loss: 0.003904, validation loss: 0.002553\n",
      "iteration 3082, train loss: 0.004375, validation loss: 0.002576\n",
      "iteration 3083, train loss: 0.004145, validation loss: 0.002948\n",
      "iteration 3084, train loss: 0.004611, validation loss: 0.002988\n",
      "iteration 3085, train loss: 0.003981, validation loss: 0.003031\n",
      "iteration 3086, train loss: 0.004451, validation loss: 0.00282\n",
      "iteration 3087, train loss: 0.004201, validation loss: 0.002653\n",
      "iteration 3088, train loss: 0.00408, validation loss: 0.00276\n",
      "iteration 3089, train loss: 0.003791, validation loss: 0.002952\n",
      "iteration 3090, train loss: 0.004205, validation loss: 0.002801\n",
      "iteration 3091, train loss: 0.004264, validation loss: 0.002623\n",
      "iteration 3092, train loss: 0.004006, validation loss: 0.002816\n",
      "iteration 3093, train loss: 0.003993, validation loss: 0.00287\n",
      "iteration 3094, train loss: 0.004119, validation loss: 0.002613\n",
      "iteration 3095, train loss: 0.004103, validation loss: 0.002636\n",
      "iteration 3096, train loss: 0.004034, validation loss: 0.002669\n",
      "iteration 3097, train loss: 0.003704, validation loss: 0.002809\n",
      "iteration 3098, train loss: 0.00393, validation loss: 0.002858\n",
      "iteration 3099, train loss: 0.003712, validation loss: 0.00265\n",
      "iteration 3100, train loss: \u001b[92m0.003502\u001b[0m, validation loss: 0.002641\n",
      "iteration 3101, train loss: 0.003971, validation loss: 0.002604\n",
      "iteration 3102, train loss: 0.003841, validation loss: 0.002704\n",
      "iteration 3103, train loss: 0.003867, validation loss: 0.002695\n",
      "iteration 3104, train loss: 0.00381, validation loss: 0.002566\n",
      "iteration 3105, train loss: 0.003675, validation loss: 0.002623\n",
      "iteration 3106, train loss: 0.003928, validation loss: 0.002627\n",
      "iteration 3107, train loss: 0.003806, validation loss: \u001b[92m0.002537\u001b[0m\n",
      "iteration 3108, train loss: 0.003991, validation loss: 0.002705\n",
      "iteration 3109, train loss: 0.003866, validation loss: 0.002541\n",
      "iteration 3110, train loss: 0.00385, validation loss: 0.002632\n",
      "iteration 3111, train loss: 0.003817, validation loss: 0.002885\n",
      "iteration 3112, train loss: 0.003979, validation loss: 0.002901\n",
      "iteration 3113, train loss: 0.003967, validation loss: \u001b[92m0.002508\u001b[0m\n",
      "iteration 3114, train loss: 0.003797, validation loss: 0.002618\n",
      "iteration 3115, train loss: 0.003841, validation loss: 0.002772\n",
      "iteration 3116, train loss: 0.004175, validation loss: 0.002547\n",
      "iteration 3117, train loss: 0.004057, validation loss: 0.002712\n",
      "iteration 3118, train loss: 0.003633, validation loss: 0.002847\n",
      "iteration 3119, train loss: 0.003997, validation loss: 0.002663\n",
      "iteration 3120, train loss: 0.003536, validation loss: 0.002599\n",
      "iteration 3121, train loss: 0.004072, validation loss: 0.002541\n",
      "iteration 3122, train loss: 0.003616, validation loss: 0.002582\n",
      "iteration 3123, train loss: 0.003675, validation loss: 0.002591\n",
      "iteration 3124, train loss: 0.003673, validation loss: \u001b[92m0.002506\u001b[0m\n",
      "iteration 3125, train loss: 0.00377, validation loss: 0.002685\n",
      "iteration 3126, train loss: 0.003931, validation loss: 0.002648\n",
      "iteration 3127, train loss: 0.004174, validation loss: 0.00258\n",
      "iteration 3128, train loss: 0.003961, validation loss: 0.002746\n",
      "iteration 3129, train loss: 0.004123, validation loss: 0.002645\n",
      "iteration 3130, train loss: 0.003648, validation loss: 0.002613\n",
      "iteration 3131, train loss: 0.003915, validation loss: 0.002931\n",
      "iteration 3132, train loss: 0.003924, validation loss: 0.002739\n",
      "iteration 3133, train loss: 0.004025, validation loss: 0.00279\n",
      "iteration 3134, train loss: 0.004043, validation loss: 0.002813\n",
      "iteration 3135, train loss: 0.004275, validation loss: 0.00267\n",
      "iteration 3136, train loss: 0.003838, validation loss: 0.002672\n",
      "iteration 3137, train loss: 0.003722, validation loss: 0.002821\n",
      "iteration 3138, train loss: 0.003928, validation loss: 0.00284\n",
      "iteration 3139, train loss: 0.004578, validation loss: 0.002613\n",
      "iteration 3140, train loss: 0.003979, validation loss: 0.003028\n",
      "iteration 3141, train loss: 0.0043, validation loss: 0.002874\n",
      "iteration 3142, train loss: 0.004164, validation loss: \u001b[92m0.002503\u001b[0m\n",
      "iteration 3143, train loss: 0.003848, validation loss: 0.002789\n",
      "iteration 3144, train loss: 0.004015, validation loss: 0.00258\n",
      "iteration 3145, train loss: 0.003587, validation loss: 0.002561\n",
      "iteration 3146, train loss: 0.003678, validation loss: 0.002674\n",
      "iteration 3147, train loss: 0.004177, validation loss: 0.002535\n",
      "iteration 3148, train loss: 0.003902, validation loss: 0.00262\n",
      "iteration 3149, train loss: 0.003855, validation loss: 0.00264\n",
      "iteration 3150, train loss: 0.00382, validation loss: 0.002662\n",
      "iteration 3151, train loss: 0.003746, validation loss: 0.002647\n",
      "iteration 3152, train loss: 0.004261, validation loss: 0.00261\n",
      "iteration 3153, train loss: 0.003832, validation loss: 0.002961\n",
      "iteration 3154, train loss: 0.004403, validation loss: 0.002907\n",
      "iteration 3155, train loss: 0.004771, validation loss: 0.002929\n",
      "iteration 3156, train loss: 0.003713, validation loss: 0.003754\n",
      "iteration 3157, train loss: 0.004679, validation loss: 0.003382\n",
      "iteration 3158, train loss: 0.005084, validation loss: 0.002586\n",
      "iteration 3159, train loss: 0.003788, validation loss: 0.002981\n",
      "iteration 3160, train loss: 0.00449, validation loss: 0.002972\n",
      "iteration 3161, train loss: 0.004461, validation loss: 0.002733\n",
      "iteration 3162, train loss: 0.004219, validation loss: 0.003019\n",
      "iteration 3163, train loss: 0.003944, validation loss: 0.002965\n",
      "iteration 3164, train loss: 0.004648, validation loss: \u001b[92m0.002483\u001b[0m\n",
      "iteration 3165, train loss: 0.00361, validation loss: 0.002852\n",
      "iteration 3166, train loss: 0.00416, validation loss: 0.003056\n",
      "iteration 3167, train loss: 0.00399, validation loss: 0.002501\n",
      "iteration 3168, train loss: 0.003832, validation loss: 0.00285\n",
      "iteration 3169, train loss: 0.004189, validation loss: 0.003269\n",
      "iteration 3170, train loss: 0.004297, validation loss: 0.00306\n",
      "iteration 3171, train loss: 0.004112, validation loss: 0.002995\n",
      "iteration 3172, train loss: 0.004291, validation loss: 0.002749\n",
      "iteration 3173, train loss: 0.003933, validation loss: 0.002585\n",
      "iteration 3174, train loss: 0.00392, validation loss: 0.002595\n",
      "iteration 3175, train loss: 0.003726, validation loss: 0.002728\n",
      "iteration 3176, train loss: 0.003943, validation loss: 0.002601\n",
      "iteration 3177, train loss: 0.003734, validation loss: 0.002648\n",
      "iteration 3178, train loss: 0.004122, validation loss: 0.002597\n",
      "iteration 3179, train loss: 0.003726, validation loss: 0.002583\n",
      "iteration 3180, train loss: 0.003838, validation loss: 0.002633\n",
      "iteration 3181, train loss: 0.004201, validation loss: 0.002702\n",
      "iteration 3182, train loss: \u001b[92m0.003423\u001b[0m, validation loss: 0.003104\n",
      "iteration 3183, train loss: 0.004339, validation loss: 0.002779\n",
      "iteration 3184, train loss: 0.003888, validation loss: 0.002928\n",
      "iteration 3185, train loss: 0.003979, validation loss: 0.003401\n",
      "iteration 3186, train loss: 0.00418, validation loss: 0.003213\n",
      "iteration 3187, train loss: 0.004069, validation loss: 0.002841\n",
      "iteration 3188, train loss: 0.003786, validation loss: 0.003113\n",
      "iteration 3189, train loss: 0.003896, validation loss: 0.00312\n",
      "iteration 3190, train loss: 0.004449, validation loss: 0.002904\n",
      "iteration 3191, train loss: 0.004086, validation loss: 0.002669\n",
      "iteration 3192, train loss: 0.003779, validation loss: 0.002592\n",
      "iteration 3193, train loss: 0.003554, validation loss: 0.002684\n",
      "iteration 3194, train loss: 0.004095, validation loss: 0.002567\n",
      "iteration 3195, train loss: 0.00381, validation loss: 0.002676\n",
      "iteration 3196, train loss: 0.003832, validation loss: 0.002796\n",
      "iteration 3197, train loss: 0.004028, validation loss: 0.002647\n",
      "iteration 3198, train loss: 0.003581, validation loss: 0.002537\n",
      "iteration 3199, train loss: 0.003555, validation loss: 0.002538\n",
      "iteration 3200, train loss: 0.003655, validation loss: 0.002594\n",
      "iteration 3201, train loss: 0.004067, validation loss: 0.002689\n",
      "iteration 3202, train loss: 0.004276, validation loss: 0.002688\n",
      "iteration 3203, train loss: 0.003825, validation loss: 0.002657\n",
      "iteration 3204, train loss: 0.004077, validation loss: 0.0025\n",
      "iteration 3205, train loss: 0.003498, validation loss: 0.002908\n",
      "iteration 3206, train loss: 0.003873, validation loss: 0.002828\n",
      "iteration 3207, train loss: 0.003627, validation loss: 0.002571\n",
      "iteration 3208, train loss: 0.003719, validation loss: 0.00283\n",
      "iteration 3209, train loss: 0.004151, validation loss: 0.002567\n",
      "iteration 3210, train loss: 0.003755, validation loss: 0.002563\n",
      "iteration 3211, train loss: 0.003705, validation loss: 0.002757\n",
      "iteration 3212, train loss: 0.004029, validation loss: 0.002666\n",
      "iteration 3213, train loss: 0.003823, validation loss: 0.002624\n",
      "iteration 3214, train loss: 0.003882, validation loss: 0.002963\n",
      "iteration 3215, train loss: 0.004562, validation loss: \u001b[92m0.002437\u001b[0m\n",
      "iteration 3216, train loss: 0.003725, validation loss: 0.002636\n",
      "iteration 3217, train loss: 0.004148, validation loss: 0.002719\n",
      "iteration 3218, train loss: 0.004056, validation loss: 0.002662\n",
      "iteration 3219, train loss: 0.003979, validation loss: 0.002552\n",
      "iteration 3220, train loss: 0.003765, validation loss: 0.002639\n",
      "iteration 3221, train loss: 0.003818, validation loss: 0.002539\n",
      "iteration 3222, train loss: 0.003924, validation loss: 0.002541\n",
      "iteration 3223, train loss: 0.003864, validation loss: 0.002526\n",
      "iteration 3224, train loss: 0.003862, validation loss: 0.002709\n",
      "iteration 3225, train loss: 0.004339, validation loss: 0.00258\n",
      "iteration 3226, train loss: 0.003798, validation loss: 0.002606\n",
      "iteration 3227, train loss: 0.004112, validation loss: 0.002707\n",
      "iteration 3228, train loss: 0.003763, validation loss: 0.002474\n",
      "iteration 3229, train loss: 0.00367, validation loss: 0.002546\n",
      "iteration 3230, train loss: 0.004109, validation loss: 0.002587\n",
      "iteration 3231, train loss: 0.004206, validation loss: 0.002503\n",
      "iteration 3232, train loss: 0.003608, validation loss: 0.002866\n",
      "iteration 3233, train loss: 0.00394, validation loss: 0.002694\n",
      "iteration 3234, train loss: 0.003893, validation loss: 0.002551\n",
      "iteration 3235, train loss: 0.003706, validation loss: 0.002995\n",
      "iteration 3236, train loss: 0.004386, validation loss: 0.002822\n",
      "iteration 3237, train loss: 0.004293, validation loss: 0.002529\n",
      "iteration 3238, train loss: 0.003571, validation loss: 0.002686\n",
      "iteration 3239, train loss: 0.004098, validation loss: 0.002819\n",
      "iteration 3240, train loss: 0.003859, validation loss: 0.002854\n",
      "iteration 3241, train loss: 0.003702, validation loss: 0.002832\n",
      "iteration 3242, train loss: 0.00397, validation loss: 0.002835\n",
      "iteration 3243, train loss: 0.004151, validation loss: 0.002573\n",
      "iteration 3244, train loss: 0.004076, validation loss: 0.002699\n",
      "iteration 3245, train loss: 0.004031, validation loss: 0.002763\n",
      "iteration 3246, train loss: 0.004331, validation loss: 0.002832\n",
      "iteration 3247, train loss: 0.004261, validation loss: 0.003041\n",
      "iteration 3248, train loss: 0.004498, validation loss: 0.002536\n",
      "iteration 3249, train loss: 0.003476, validation loss: 0.002917\n",
      "iteration 3250, train loss: 0.00442, validation loss: 0.003204\n",
      "iteration 3251, train loss: 0.00475, validation loss: 0.002746\n",
      "iteration 3252, train loss: 0.004311, validation loss: 0.003073\n",
      "iteration 3253, train loss: 0.004132, validation loss: 0.003628\n",
      "iteration 3254, train loss: 0.004762, validation loss: 0.003036\n",
      "iteration 3255, train loss: 0.004285, validation loss: 0.00247\n",
      "iteration 3256, train loss: 0.003629, validation loss: 0.002954\n",
      "iteration 3257, train loss: 0.004091, validation loss: 0.002974\n",
      "iteration 3258, train loss: 0.003897, validation loss: 0.002501\n",
      "iteration 3259, train loss: 0.003691, validation loss: 0.00258\n",
      "iteration 3260, train loss: 0.003598, validation loss: 0.002808\n",
      "iteration 3261, train loss: 0.004169, validation loss: 0.002525\n",
      "iteration 3262, train loss: 0.00376, validation loss: 0.002668\n",
      "iteration 3263, train loss: 0.00384, validation loss: 0.002888\n",
      "iteration 3264, train loss: 0.003781, validation loss: 0.002691\n",
      "iteration 3265, train loss: 0.003787, validation loss: 0.002801\n",
      "iteration 3266, train loss: 0.004181, validation loss: 0.002874\n",
      "iteration 3267, train loss: 0.00388, validation loss: 0.002644\n",
      "iteration 3268, train loss: 0.003546, validation loss: 0.002815\n",
      "iteration 3269, train loss: 0.003962, validation loss: 0.002826\n",
      "iteration 3270, train loss: 0.003959, validation loss: 0.002655\n",
      "iteration 3271, train loss: 0.0041, validation loss: 0.002745\n",
      "iteration 3272, train loss: 0.003713, validation loss: 0.003014\n",
      "iteration 3273, train loss: 0.003909, validation loss: 0.002622\n",
      "iteration 3274, train loss: 0.003707, validation loss: 0.002704\n",
      "iteration 3275, train loss: 0.003999, validation loss: 0.002818\n",
      "iteration 3276, train loss: 0.003887, validation loss: 0.002732\n",
      "iteration 3277, train loss: 0.003887, validation loss: 0.002752\n",
      "iteration 3278, train loss: 0.003588, validation loss: 0.00259\n",
      "iteration 3279, train loss: 0.003963, validation loss: 0.002597\n",
      "iteration 3280, train loss: 0.003713, validation loss: 0.002727\n",
      "iteration 3281, train loss: 0.003772, validation loss: 0.002531\n",
      "iteration 3282, train loss: \u001b[92m0.003418\u001b[0m, validation loss: 0.002632\n",
      "iteration 3283, train loss: 0.004028, validation loss: 0.002856\n",
      "iteration 3284, train loss: 0.003896, validation loss: 0.002583\n",
      "iteration 3285, train loss: 0.003782, validation loss: 0.002633\n",
      "iteration 3286, train loss: 0.003783, validation loss: 0.002987\n",
      "iteration 3287, train loss: 0.00445, validation loss: 0.002623\n",
      "iteration 3288, train loss: 0.003584, validation loss: 0.0025\n",
      "iteration 3289, train loss: 0.003695, validation loss: 0.002634\n",
      "iteration 3290, train loss: 0.003723, validation loss: 0.002616\n",
      "iteration 3291, train loss: 0.003752, validation loss: 0.002537\n",
      "iteration 3292, train loss: 0.003919, validation loss: 0.002642\n",
      "iteration 3293, train loss: 0.003689, validation loss: 0.00329\n",
      "iteration 3294, train loss: 0.004221, validation loss: 0.002984\n",
      "iteration 3295, train loss: 0.004116, validation loss: 0.002548\n",
      "iteration 3296, train loss: 0.003552, validation loss: 0.002789\n",
      "iteration 3297, train loss: 0.003839, validation loss: 0.002782\n",
      "iteration 3298, train loss: 0.00391, validation loss: 0.002607\n",
      "iteration 3299, train loss: 0.003891, validation loss: 0.002746\n",
      "iteration 3300, train loss: 0.004194, validation loss: 0.002575\n",
      "iteration 3301, train loss: 0.003496, validation loss: 0.002905\n",
      "iteration 3302, train loss: 0.003845, validation loss: 0.00298\n",
      "iteration 3303, train loss: 0.004233, validation loss: 0.002713\n",
      "iteration 3304, train loss: 0.00395, validation loss: 0.002846\n",
      "iteration 3305, train loss: 0.00415, validation loss: 0.002888\n",
      "iteration 3306, train loss: 0.004149, validation loss: 0.002692\n",
      "iteration 3307, train loss: 0.003755, validation loss: 0.002662\n",
      "iteration 3308, train loss: 0.004206, validation loss: 0.003082\n",
      "iteration 3309, train loss: 0.004045, validation loss: 0.003002\n",
      "iteration 3310, train loss: 0.004044, validation loss: \u001b[92m0.002416\u001b[0m\n",
      "iteration 3311, train loss: 0.003944, validation loss: 0.002683\n",
      "iteration 3312, train loss: 0.003804, validation loss: 0.002991\n",
      "iteration 3313, train loss: 0.004526, validation loss: 0.002543\n",
      "iteration 3314, train loss: 0.003757, validation loss: 0.002604\n",
      "iteration 3315, train loss: 0.003521, validation loss: 0.002901\n",
      "iteration 3316, train loss: 0.004269, validation loss: 0.002554\n",
      "iteration 3317, train loss: 0.00385, validation loss: 0.002632\n",
      "iteration 3318, train loss: 0.003623, validation loss: 0.002645\n",
      "iteration 3319, train loss: 0.003802, validation loss: \u001b[92m0.0024\u001b[0m\n",
      "iteration 3320, train loss: 0.00404, validation loss: 0.002566\n",
      "iteration 3321, train loss: 0.003741, validation loss: 0.002608\n",
      "iteration 3322, train loss: 0.004349, validation loss: 0.002489\n",
      "iteration 3323, train loss: 0.003738, validation loss: 0.002854\n",
      "iteration 3324, train loss: 0.004302, validation loss: 0.002695\n",
      "iteration 3325, train loss: 0.003972, validation loss: 0.002858\n",
      "iteration 3326, train loss: 0.004228, validation loss: 0.002768\n",
      "iteration 3327, train loss: 0.004351, validation loss: 0.002632\n",
      "iteration 3328, train loss: 0.003552, validation loss: 0.002735\n",
      "iteration 3329, train loss: 0.00413, validation loss: 0.002688\n",
      "iteration 3330, train loss: 0.003716, validation loss: 0.002662\n",
      "iteration 3331, train loss: 0.003791, validation loss: 0.002771\n",
      "iteration 3332, train loss: 0.004407, validation loss: 0.002624\n",
      "iteration 3333, train loss: 0.003605, validation loss: 0.003164\n",
      "iteration 3334, train loss: 0.004221, validation loss: 0.002899\n",
      "iteration 3335, train loss: 0.004222, validation loss: 0.002574\n",
      "iteration 3336, train loss: 0.003688, validation loss: 0.002977\n",
      "iteration 3337, train loss: 0.004364, validation loss: 0.002641\n",
      "iteration 3338, train loss: 0.003689, validation loss: 0.002493\n",
      "iteration 3339, train loss: 0.003685, validation loss: 0.002702\n",
      "iteration 3340, train loss: 0.004107, validation loss: 0.002608\n",
      "iteration 3341, train loss: 0.003887, validation loss: 0.002449\n",
      "iteration 3342, train loss: 0.00386, validation loss: 0.002467\n",
      "iteration 3343, train loss: 0.003994, validation loss: 0.00251\n",
      "iteration 3344, train loss: 0.003948, validation loss: 0.002455\n",
      "iteration 3345, train loss: \u001b[92m0.003356\u001b[0m, validation loss: 0.002596\n",
      "iteration 3346, train loss: 0.003746, validation loss: 0.002493\n",
      "iteration 3347, train loss: 0.003691, validation loss: 0.002401\n",
      "iteration 3348, train loss: 0.003897, validation loss: 0.002473\n",
      "iteration 3349, train loss: 0.003749, validation loss: 0.002755\n",
      "iteration 3350, train loss: 0.003831, validation loss: 0.00263\n",
      "iteration 3351, train loss: 0.003773, validation loss: 0.002489\n",
      "iteration 3352, train loss: 0.003574, validation loss: 0.002588\n",
      "iteration 3353, train loss: 0.003857, validation loss: 0.002597\n",
      "iteration 3354, train loss: 0.004266, validation loss: 0.002728\n",
      "iteration 3355, train loss: 0.003621, validation loss: 0.002841\n",
      "iteration 3356, train loss: 0.003742, validation loss: 0.002522\n",
      "iteration 3357, train loss: 0.003635, validation loss: 0.002503\n",
      "iteration 3358, train loss: 0.003789, validation loss: 0.002725\n",
      "iteration 3359, train loss: 0.003837, validation loss: 0.00274\n",
      "iteration 3360, train loss: 0.003577, validation loss: 0.002647\n",
      "iteration 3361, train loss: 0.004002, validation loss: 0.002479\n",
      "iteration 3362, train loss: 0.003801, validation loss: 0.002799\n",
      "iteration 3363, train loss: 0.00449, validation loss: 0.002759\n",
      "iteration 3364, train loss: 0.004267, validation loss: 0.002705\n",
      "iteration 3365, train loss: 0.003813, validation loss: 0.002811\n",
      "iteration 3366, train loss: 0.003728, validation loss: 0.0034\n",
      "iteration 3367, train loss: 0.004297, validation loss: 0.002808\n",
      "iteration 3368, train loss: 0.004032, validation loss: 0.002488\n",
      "iteration 3369, train loss: 0.004166, validation loss: 0.00287\n",
      "iteration 3370, train loss: 0.004231, validation loss: 0.002725\n",
      "iteration 3371, train loss: 0.004111, validation loss: 0.002766\n",
      "iteration 3372, train loss: 0.003614, validation loss: 0.002861\n",
      "iteration 3373, train loss: 0.00402, validation loss: 0.002524\n",
      "iteration 3374, train loss: 0.004271, validation loss: 0.002612\n",
      "iteration 3375, train loss: 0.004102, validation loss: 0.00283\n",
      "iteration 3376, train loss: 0.003985, validation loss: 0.002512\n",
      "iteration 3377, train loss: 0.003583, validation loss: 0.002453\n",
      "iteration 3378, train loss: 0.003565, validation loss: 0.002815\n",
      "iteration 3379, train loss: 0.00364, validation loss: 0.002767\n",
      "iteration 3380, train loss: 0.003588, validation loss: 0.002801\n",
      "iteration 3381, train loss: 0.003858, validation loss: 0.002681\n",
      "iteration 3382, train loss: 0.004044, validation loss: 0.00253\n",
      "iteration 3383, train loss: 0.003522, validation loss: 0.003017\n",
      "iteration 3384, train loss: 0.004253, validation loss: 0.002624\n",
      "iteration 3385, train loss: 0.003759, validation loss: 0.002706\n",
      "iteration 3386, train loss: 0.003832, validation loss: 0.003134\n",
      "iteration 3387, train loss: 0.004238, validation loss: 0.002769\n",
      "iteration 3388, train loss: 0.003691, validation loss: 0.002559\n",
      "iteration 3389, train loss: 0.003727, validation loss: 0.002822\n",
      "iteration 3390, train loss: 0.00417, validation loss: 0.00281\n",
      "iteration 3391, train loss: 0.003925, validation loss: \u001b[92m0.002391\u001b[0m\n",
      "iteration 3392, train loss: 0.004078, validation loss: 0.002892\n",
      "iteration 3393, train loss: 0.003947, validation loss: 0.003159\n",
      "iteration 3394, train loss: 0.004549, validation loss: 0.00268\n",
      "iteration 3395, train loss: 0.003929, validation loss: 0.002882\n",
      "iteration 3396, train loss: 0.00402, validation loss: 0.002783\n",
      "iteration 3397, train loss: 0.003803, validation loss: 0.00267\n",
      "iteration 3398, train loss: 0.004002, validation loss: 0.002686\n",
      "iteration 3399, train loss: 0.003782, validation loss: 0.00265\n",
      "iteration 3400, train loss: 0.003789, validation loss: 0.00256\n",
      "iteration 3401, train loss: 0.003754, validation loss: 0.002574\n",
      "iteration 3402, train loss: 0.003383, validation loss: 0.002713\n",
      "iteration 3403, train loss: 0.004057, validation loss: 0.002399\n",
      "iteration 3404, train loss: 0.003956, validation loss: 0.002476\n",
      "iteration 3405, train loss: 0.003889, validation loss: 0.002427\n",
      "iteration 3406, train loss: 0.003512, validation loss: 0.002546\n",
      "iteration 3407, train loss: 0.003769, validation loss: 0.002627\n",
      "iteration 3408, train loss: 0.003781, validation loss: \u001b[92m0.002385\u001b[0m\n",
      "iteration 3409, train loss: 0.003773, validation loss: 0.002531\n",
      "iteration 3410, train loss: 0.003704, validation loss: 0.002506\n",
      "iteration 3411, train loss: 0.003994, validation loss: 0.002406\n",
      "iteration 3412, train loss: \u001b[92m0.003348\u001b[0m, validation loss: 0.00258\n",
      "iteration 3413, train loss: 0.003947, validation loss: 0.002463\n",
      "iteration 3414, train loss: 0.003741, validation loss: 0.00242\n",
      "iteration 3415, train loss: 0.003556, validation loss: 0.002469\n",
      "iteration 3416, train loss: 0.003549, validation loss: 0.002398\n",
      "iteration 3417, train loss: 0.003606, validation loss: 0.002448\n",
      "iteration 3418, train loss: 0.003559, validation loss: 0.002393\n",
      "iteration 3419, train loss: 0.003655, validation loss: 0.002521\n",
      "iteration 3420, train loss: 0.003687, validation loss: 0.002525\n",
      "iteration 3421, train loss: 0.003796, validation loss: 0.002438\n",
      "iteration 3422, train loss: 0.003395, validation loss: 0.002447\n",
      "iteration 3423, train loss: 0.003447, validation loss: 0.002551\n",
      "iteration 3424, train loss: 0.003533, validation loss: 0.002666\n",
      "iteration 3425, train loss: 0.004144, validation loss: 0.00251\n",
      "iteration 3426, train loss: 0.003575, validation loss: 0.002525\n",
      "iteration 3427, train loss: 0.003793, validation loss: 0.002855\n",
      "iteration 3428, train loss: 0.003728, validation loss: 0.002764\n",
      "iteration 3429, train loss: 0.003828, validation loss: 0.002497\n",
      "iteration 3430, train loss: 0.003916, validation loss: 0.002726\n",
      "iteration 3431, train loss: 0.003923, validation loss: 0.002572\n",
      "iteration 3432, train loss: 0.00428, validation loss: 0.002402\n",
      "iteration 3433, train loss: 0.004082, validation loss: 0.002848\n",
      "iteration 3434, train loss: 0.00387, validation loss: 0.002839\n",
      "iteration 3435, train loss: 0.003819, validation loss: 0.002538\n",
      "iteration 3436, train loss: 0.003664, validation loss: 0.002746\n",
      "iteration 3437, train loss: 0.003763, validation loss: 0.002543\n",
      "iteration 3438, train loss: 0.004009, validation loss: 0.002471\n",
      "iteration 3439, train loss: 0.00353, validation loss: 0.003105\n",
      "iteration 3440, train loss: 0.003973, validation loss: 0.003141\n",
      "iteration 3441, train loss: 0.004534, validation loss: 0.003122\n",
      "iteration 3442, train loss: 0.003954, validation loss: 0.003293\n",
      "iteration 3443, train loss: 0.00414, validation loss: 0.002856\n",
      "iteration 3444, train loss: 0.004343, validation loss: 0.002899\n",
      "iteration 3445, train loss: 0.004117, validation loss: 0.002989\n",
      "iteration 3446, train loss: 0.004095, validation loss: 0.002781\n",
      "iteration 3447, train loss: 0.004006, validation loss: 0.002988\n",
      "iteration 3448, train loss: 0.003941, validation loss: 0.003193\n",
      "iteration 3449, train loss: 0.004676, validation loss: 0.002757\n",
      "iteration 3450, train loss: 0.004303, validation loss: 0.002504\n",
      "iteration 3451, train loss: \u001b[92m0.003315\u001b[0m, validation loss: 0.002679\n",
      "iteration 3452, train loss: 0.004141, validation loss: 0.002966\n",
      "iteration 3453, train loss: 0.004017, validation loss: 0.002611\n",
      "iteration 3454, train loss: 0.003741, validation loss: 0.002756\n",
      "iteration 3455, train loss: 0.00368, validation loss: 0.003171\n",
      "iteration 3456, train loss: 0.004524, validation loss: 0.002977\n",
      "iteration 3457, train loss: 0.003962, validation loss: 0.002524\n",
      "iteration 3458, train loss: 0.00368, validation loss: 0.003278\n",
      "iteration 3459, train loss: 0.00419, validation loss: 0.003128\n",
      "iteration 3460, train loss: 0.004175, validation loss: \u001b[92m0.002348\u001b[0m\n",
      "iteration 3461, train loss: 0.003554, validation loss: 0.003017\n",
      "iteration 3462, train loss: 0.004384, validation loss: 0.003099\n",
      "iteration 3463, train loss: 0.004361, validation loss: 0.002582\n",
      "iteration 3464, train loss: 0.00384, validation loss: 0.002879\n",
      "iteration 3465, train loss: 0.004349, validation loss: 0.002708\n",
      "iteration 3466, train loss: 0.003592, validation loss: 0.002969\n",
      "iteration 3467, train loss: 0.004484, validation loss: 0.002741\n",
      "iteration 3468, train loss: 0.003554, validation loss: 0.002657\n",
      "iteration 3469, train loss: 0.003601, validation loss: 0.002497\n",
      "iteration 3470, train loss: 0.00376, validation loss: 0.002779\n",
      "iteration 3471, train loss: 0.004104, validation loss: 0.003033\n",
      "iteration 3472, train loss: 0.004262, validation loss: 0.002568\n",
      "iteration 3473, train loss: 0.003655, validation loss: 0.002399\n",
      "iteration 3474, train loss: 0.004, validation loss: 0.002726\n",
      "iteration 3475, train loss: 0.003446, validation loss: 0.002739\n",
      "iteration 3476, train loss: 0.003949, validation loss: \u001b[92m0.002347\u001b[0m\n",
      "iteration 3477, train loss: 0.003446, validation loss: 0.002466\n",
      "iteration 3478, train loss: 0.003661, validation loss: 0.00255\n",
      "iteration 3479, train loss: 0.00356, validation loss: 0.002428\n",
      "iteration 3480, train loss: 0.003353, validation loss: 0.002375\n",
      "iteration 3481, train loss: 0.003594, validation loss: 0.002522\n",
      "iteration 3482, train loss: 0.003795, validation loss: 0.002648\n",
      "iteration 3483, train loss: 0.003661, validation loss: 0.002533\n",
      "iteration 3484, train loss: 0.003541, validation loss: 0.002369\n",
      "iteration 3485, train loss: \u001b[92m0.003207\u001b[0m, validation loss: 0.002418\n",
      "iteration 3486, train loss: 0.0037, validation loss: 0.002434\n",
      "iteration 3487, train loss: 0.003757, validation loss: 0.002393\n",
      "iteration 3488, train loss: 0.003311, validation loss: 0.002437\n",
      "iteration 3489, train loss: 0.003424, validation loss: 0.002581\n",
      "iteration 3490, train loss: 0.004009, validation loss: 0.002463\n",
      "iteration 3491, train loss: 0.003747, validation loss: 0.002391\n",
      "iteration 3492, train loss: 0.003592, validation loss: 0.002456\n",
      "iteration 3493, train loss: 0.003716, validation loss: 0.002488\n",
      "iteration 3494, train loss: 0.003625, validation loss: 0.002523\n",
      "iteration 3495, train loss: 0.003941, validation loss: 0.002348\n",
      "iteration 3496, train loss: 0.003499, validation loss: 0.002525\n",
      "iteration 3497, train loss: 0.003803, validation loss: 0.002654\n",
      "iteration 3498, train loss: 0.004088, validation loss: 0.0025\n",
      "iteration 3499, train loss: 0.003708, validation loss: 0.00245\n",
      "iteration 3500, train loss: 0.003457, validation loss: 0.002553\n",
      "iteration 3501, train loss: 0.003886, validation loss: 0.002385\n",
      "iteration 3502, train loss: 0.003812, validation loss: 0.002517\n",
      "iteration 3503, train loss: 0.003822, validation loss: 0.002477\n",
      "iteration 3504, train loss: 0.003571, validation loss: 0.002526\n",
      "iteration 3505, train loss: 0.003624, validation loss: 0.002526\n",
      "iteration 3506, train loss: 0.004012, validation loss: 0.002387\n",
      "iteration 3507, train loss: \u001b[92m0.003182\u001b[0m, validation loss: 0.002435\n",
      "iteration 3508, train loss: 0.003258, validation loss: 0.002462\n",
      "iteration 3509, train loss: 0.003781, validation loss: 0.002476\n",
      "iteration 3510, train loss: 0.003795, validation loss: 0.002378\n",
      "iteration 3511, train loss: 0.003329, validation loss: 0.002447\n",
      "iteration 3512, train loss: 0.003636, validation loss: 0.002502\n",
      "iteration 3513, train loss: 0.003568, validation loss: 0.002504\n",
      "iteration 3514, train loss: 0.003515, validation loss: 0.002495\n",
      "iteration 3515, train loss: 0.003698, validation loss: 0.002384\n",
      "iteration 3516, train loss: 0.003671, validation loss: 0.002652\n",
      "iteration 3517, train loss: 0.003708, validation loss: 0.00251\n",
      "iteration 3518, train loss: 0.003781, validation loss: 0.002399\n",
      "iteration 3519, train loss: 0.003588, validation loss: 0.002741\n",
      "iteration 3520, train loss: 0.003822, validation loss: 0.002816\n",
      "iteration 3521, train loss: 0.003764, validation loss: 0.00264\n",
      "iteration 3522, train loss: 0.003639, validation loss: 0.002604\n",
      "iteration 3523, train loss: 0.003382, validation loss: 0.002995\n",
      "iteration 3524, train loss: 0.0042, validation loss: 0.002679\n",
      "iteration 3525, train loss: 0.003646, validation loss: 0.00246\n",
      "iteration 3526, train loss: 0.003641, validation loss: 0.002778\n",
      "iteration 3527, train loss: 0.004034, validation loss: 0.002632\n",
      "iteration 3528, train loss: 0.003471, validation loss: 0.002395\n",
      "iteration 3529, train loss: 0.003821, validation loss: 0.002572\n",
      "iteration 3530, train loss: 0.003675, validation loss: 0.00285\n",
      "iteration 3531, train loss: 0.003933, validation loss: 0.002469\n",
      "iteration 3532, train loss: 0.003549, validation loss: 0.002671\n",
      "iteration 3533, train loss: 0.003943, validation loss: 0.003127\n",
      "iteration 3534, train loss: 0.004377, validation loss: 0.002826\n",
      "iteration 3535, train loss: 0.003831, validation loss: 0.002979\n",
      "iteration 3536, train loss: 0.003831, validation loss: 0.00294\n",
      "iteration 3537, train loss: 0.004035, validation loss: 0.002553\n",
      "iteration 3538, train loss: 0.0044, validation loss: 0.002754\n",
      "iteration 3539, train loss: 0.003794, validation loss: 0.003182\n",
      "iteration 3540, train loss: 0.003977, validation loss: 0.002724\n",
      "iteration 3541, train loss: 0.003936, validation loss: 0.00276\n",
      "iteration 3542, train loss: 0.003929, validation loss: 0.002812\n",
      "iteration 3543, train loss: 0.004132, validation loss: 0.002555\n",
      "iteration 3544, train loss: 0.003938, validation loss: 0.002795\n",
      "iteration 3545, train loss: 0.004255, validation loss: 0.002613\n",
      "iteration 3546, train loss: 0.003727, validation loss: 0.002411\n",
      "iteration 3547, train loss: 0.003733, validation loss: 0.002464\n",
      "iteration 3548, train loss: 0.003983, validation loss: 0.00238\n",
      "iteration 3549, train loss: 0.003825, validation loss: 0.002409\n",
      "iteration 3550, train loss: 0.003621, validation loss: 0.002417\n",
      "iteration 3551, train loss: 0.003807, validation loss: 0.002424\n",
      "iteration 3552, train loss: 0.003448, validation loss: 0.002518\n",
      "iteration 3553, train loss: 0.003662, validation loss: 0.002418\n",
      "iteration 3554, train loss: 0.003467, validation loss: 0.002419\n",
      "iteration 3555, train loss: 0.003191, validation loss: 0.002572\n",
      "iteration 3556, train loss: 0.004023, validation loss: \u001b[92m0.002336\u001b[0m\n",
      "iteration 3557, train loss: 0.003343, validation loss: \u001b[92m0.00233\u001b[0m\n",
      "iteration 3558, train loss: 0.003475, validation loss: 0.002582\n",
      "iteration 3559, train loss: 0.003732, validation loss: 0.002502\n",
      "iteration 3560, train loss: 0.003762, validation loss: 0.002462\n",
      "iteration 3561, train loss: 0.003444, validation loss: 0.002829\n",
      "iteration 3562, train loss: 0.003922, validation loss: 0.002858\n",
      "iteration 3563, train loss: 0.003564, validation loss: 0.002518\n",
      "iteration 3564, train loss: 0.003915, validation loss: \u001b[92m0.002313\u001b[0m\n",
      "iteration 3565, train loss: 0.003969, validation loss: 0.002535\n",
      "iteration 3566, train loss: 0.003882, validation loss: 0.002415\n",
      "iteration 3567, train loss: 0.003682, validation loss: 0.002322\n",
      "iteration 3568, train loss: 0.003518, validation loss: 0.002787\n",
      "iteration 3569, train loss: 0.004134, validation loss: 0.002615\n",
      "iteration 3570, train loss: 0.003478, validation loss: 0.002626\n",
      "iteration 3571, train loss: 0.004009, validation loss: 0.002693\n",
      "iteration 3572, train loss: 0.004225, validation loss: 0.002382\n",
      "iteration 3573, train loss: 0.003599, validation loss: 0.002576\n",
      "iteration 3574, train loss: 0.00407, validation loss: 0.002591\n",
      "iteration 3575, train loss: 0.004184, validation loss: 0.002386\n",
      "iteration 3576, train loss: 0.003553, validation loss: 0.002794\n",
      "iteration 3577, train loss: 0.003981, validation loss: 0.002554\n",
      "iteration 3578, train loss: 0.003735, validation loss: \u001b[92m0.002305\u001b[0m\n",
      "iteration 3579, train loss: 0.003573, validation loss: 0.002556\n",
      "iteration 3580, train loss: 0.00385, validation loss: 0.00253\n",
      "iteration 3581, train loss: 0.003591, validation loss: 0.00233\n",
      "iteration 3582, train loss: 0.003411, validation loss: 0.002541\n",
      "iteration 3583, train loss: 0.003459, validation loss: 0.002714\n",
      "iteration 3584, train loss: 0.003953, validation loss: 0.0025\n",
      "iteration 3585, train loss: 0.003711, validation loss: 0.002499\n",
      "iteration 3586, train loss: 0.003806, validation loss: 0.002636\n",
      "iteration 3587, train loss: 0.003907, validation loss: 0.002384\n",
      "iteration 3588, train loss: 0.003533, validation loss: 0.002604\n",
      "iteration 3589, train loss: 0.00353, validation loss: 0.002704\n",
      "iteration 3590, train loss: 0.00399, validation loss: 0.002322\n",
      "iteration 3591, train loss: 0.003342, validation loss: 0.002482\n",
      "iteration 3592, train loss: 0.003721, validation loss: 0.002518\n",
      "iteration 3593, train loss: 0.003627, validation loss: 0.002474\n",
      "iteration 3594, train loss: 0.003338, validation loss: 0.002883\n",
      "iteration 3595, train loss: 0.004173, validation loss: 0.002461\n",
      "iteration 3596, train loss: 0.003424, validation loss: 0.002342\n",
      "iteration 3597, train loss: \u001b[92m0.003163\u001b[0m, validation loss: 0.002664\n",
      "iteration 3598, train loss: 0.003682, validation loss: 0.002396\n",
      "iteration 3599, train loss: 0.003355, validation loss: 0.00243\n",
      "iteration 3600, train loss: 0.003593, validation loss: 0.002867\n",
      "iteration 3601, train loss: 0.004169, validation loss: 0.002509\n",
      "iteration 3602, train loss: 0.003736, validation loss: 0.002817\n",
      "iteration 3603, train loss: 0.003947, validation loss: 0.002737\n",
      "iteration 3604, train loss: 0.003975, validation loss: 0.002506\n",
      "iteration 3605, train loss: 0.003849, validation loss: 0.003194\n",
      "iteration 3606, train loss: 0.004529, validation loss: 0.00273\n",
      "iteration 3607, train loss: 0.003684, validation loss: 0.002748\n",
      "iteration 3608, train loss: 0.003793, validation loss: 0.002449\n",
      "iteration 3609, train loss: 0.003622, validation loss: 0.002399\n",
      "iteration 3610, train loss: 0.003438, validation loss: 0.002771\n",
      "iteration 3611, train loss: 0.003781, validation loss: 0.002493\n",
      "iteration 3612, train loss: 0.00363, validation loss: 0.002435\n",
      "iteration 3613, train loss: 0.003739, validation loss: 0.00257\n",
      "iteration 3614, train loss: 0.003748, validation loss: 0.00269\n",
      "iteration 3615, train loss: 0.004272, validation loss: 0.002489\n",
      "iteration 3616, train loss: 0.003564, validation loss: 0.002313\n",
      "iteration 3617, train loss: 0.003402, validation loss: 0.002743\n",
      "iteration 3618, train loss: 0.003985, validation loss: 0.003095\n",
      "iteration 3619, train loss: 0.004106, validation loss: 0.002726\n",
      "iteration 3620, train loss: 0.003441, validation loss: 0.002743\n",
      "iteration 3621, train loss: 0.003853, validation loss: 0.00277\n",
      "iteration 3622, train loss: 0.004129, validation loss: 0.002822\n",
      "iteration 3623, train loss: 0.003787, validation loss: 0.002391\n",
      "iteration 3624, train loss: 0.003436, validation loss: 0.002421\n",
      "iteration 3625, train loss: 0.003733, validation loss: 0.002991\n",
      "iteration 3626, train loss: 0.003624, validation loss: 0.003044\n",
      "iteration 3627, train loss: 0.004247, validation loss: 0.002443\n",
      "iteration 3628, train loss: 0.003967, validation loss: 0.002502\n",
      "iteration 3629, train loss: 0.003779, validation loss: 0.003121\n",
      "iteration 3630, train loss: 0.003971, validation loss: 0.002748\n",
      "iteration 3631, train loss: 0.003768, validation loss: 0.002323\n",
      "iteration 3632, train loss: 0.003442, validation loss: 0.002631\n",
      "iteration 3633, train loss: 0.003755, validation loss: 0.003411\n",
      "iteration 3634, train loss: 0.004514, validation loss: 0.003007\n",
      "iteration 3635, train loss: 0.003731, validation loss: 0.002554\n",
      "iteration 3636, train loss: 0.00395, validation loss: 0.002633\n",
      "iteration 3637, train loss: 0.00373, validation loss: 0.002926\n",
      "iteration 3638, train loss: 0.004294, validation loss: 0.002529\n",
      "iteration 3639, train loss: 0.003548, validation loss: 0.002477\n",
      "iteration 3640, train loss: 0.003525, validation loss: 0.003117\n",
      "iteration 3641, train loss: 0.004379, validation loss: 0.002966\n",
      "iteration 3642, train loss: 0.00389, validation loss: 0.00239\n",
      "iteration 3643, train loss: 0.003467, validation loss: 0.002532\n",
      "iteration 3644, train loss: 0.003676, validation loss: 0.002534\n",
      "iteration 3645, train loss: 0.003451, validation loss: 0.002666\n",
      "iteration 3646, train loss: 0.003722, validation loss: 0.002572\n",
      "iteration 3647, train loss: 0.003657, validation loss: 0.00233\n",
      "iteration 3648, train loss: 0.003372, validation loss: 0.002468\n",
      "iteration 3649, train loss: 0.003913, validation loss: 0.002812\n",
      "iteration 3650, train loss: 0.003675, validation loss: 0.002513\n",
      "iteration 3651, train loss: 0.003688, validation loss: 0.002437\n",
      "iteration 3652, train loss: 0.003892, validation loss: 0.002672\n",
      "iteration 3653, train loss: 0.003651, validation loss: 0.002717\n",
      "iteration 3654, train loss: 0.003529, validation loss: 0.002423\n",
      "iteration 3655, train loss: 0.003766, validation loss: \u001b[92m0.002301\u001b[0m\n",
      "iteration 3656, train loss: 0.003543, validation loss: 0.00273\n",
      "iteration 3657, train loss: 0.004037, validation loss: 0.002692\n",
      "iteration 3658, train loss: 0.003564, validation loss: 0.002608\n",
      "iteration 3659, train loss: 0.003419, validation loss: 0.00242\n",
      "iteration 3660, train loss: 0.00361, validation loss: 0.002799\n",
      "iteration 3661, train loss: 0.004035, validation loss: 0.003133\n",
      "iteration 3662, train loss: 0.004611, validation loss: 0.002703\n",
      "iteration 3663, train loss: 0.003641, validation loss: 0.002443\n",
      "iteration 3664, train loss: 0.003493, validation loss: 0.003132\n",
      "iteration 3665, train loss: 0.004089, validation loss: 0.003022\n",
      "iteration 3666, train loss: 0.004054, validation loss: 0.002538\n",
      "iteration 3667, train loss: 0.00358, validation loss: 0.002637\n",
      "iteration 3668, train loss: 0.003868, validation loss: 0.002546\n",
      "iteration 3669, train loss: 0.003944, validation loss: 0.002395\n",
      "iteration 3670, train loss: 0.003494, validation loss: 0.002523\n",
      "iteration 3671, train loss: 0.003623, validation loss: 0.002513\n",
      "iteration 3672, train loss: 0.003285, validation loss: 0.002557\n",
      "iteration 3673, train loss: 0.003716, validation loss: 0.002458\n",
      "iteration 3674, train loss: 0.003698, validation loss: 0.002308\n",
      "iteration 3675, train loss: 0.003275, validation loss: 0.002331\n",
      "iteration 3676, train loss: 0.003752, validation loss: 0.002332\n",
      "iteration 3677, train loss: 0.003279, validation loss: 0.002506\n",
      "iteration 3678, train loss: 0.003742, validation loss: 0.002731\n",
      "iteration 3679, train loss: 0.003805, validation loss: 0.002443\n",
      "iteration 3680, train loss: 0.003352, validation loss: 0.002438\n",
      "iteration 3681, train loss: 0.003651, validation loss: 0.002837\n",
      "iteration 3682, train loss: 0.003694, validation loss: 0.002682\n",
      "iteration 3683, train loss: 0.003779, validation loss: \u001b[92m0.002293\u001b[0m\n",
      "iteration 3684, train loss: 0.003259, validation loss: 0.002398\n",
      "iteration 3685, train loss: 0.003733, validation loss: 0.002369\n",
      "iteration 3686, train loss: 0.003794, validation loss: 0.002399\n",
      "iteration 3687, train loss: 0.003652, validation loss: 0.002833\n",
      "iteration 3688, train loss: 0.003782, validation loss: 0.002676\n",
      "iteration 3689, train loss: 0.004044, validation loss: \u001b[92m0.002265\u001b[0m\n",
      "iteration 3690, train loss: 0.003318, validation loss: 0.002607\n",
      "iteration 3691, train loss: 0.003854, validation loss: 0.002656\n",
      "iteration 3692, train loss: 0.003839, validation loss: 0.00256\n",
      "iteration 3693, train loss: 0.00375, validation loss: 0.002636\n",
      "iteration 3694, train loss: 0.003778, validation loss: 0.002816\n",
      "iteration 3695, train loss: 0.003348, validation loss: 0.002773\n",
      "iteration 3696, train loss: 0.003545, validation loss: 0.002399\n",
      "iteration 3697, train loss: 0.003555, validation loss: 0.002446\n",
      "iteration 3698, train loss: 0.003768, validation loss: 0.002643\n",
      "iteration 3699, train loss: 0.00382, validation loss: 0.002339\n",
      "iteration 3700, train loss: 0.00402, validation loss: 0.002503\n",
      "iteration 3701, train loss: 0.003575, validation loss: 0.003351\n",
      "iteration 3702, train loss: 0.004184, validation loss: 0.003231\n",
      "iteration 3703, train loss: 0.003736, validation loss: 0.002668\n",
      "iteration 3704, train loss: 0.003363, validation loss: 0.002302\n",
      "iteration 3705, train loss: 0.003508, validation loss: 0.002433\n",
      "iteration 3706, train loss: 0.003508, validation loss: 0.002466\n",
      "iteration 3707, train loss: 0.003508, validation loss: 0.002471\n",
      "iteration 3708, train loss: 0.00395, validation loss: 0.002496\n",
      "iteration 3709, train loss: 0.004117, validation loss: 0.002536\n",
      "iteration 3710, train loss: 0.00345, validation loss: 0.003306\n",
      "iteration 3711, train loss: 0.004395, validation loss: 0.002791\n",
      "iteration 3712, train loss: 0.003524, validation loss: 0.002556\n",
      "iteration 3713, train loss: 0.003335, validation loss: 0.002701\n",
      "iteration 3714, train loss: 0.004141, validation loss: 0.002737\n",
      "iteration 3715, train loss: 0.003753, validation loss: 0.002672\n",
      "iteration 3716, train loss: 0.004051, validation loss: 0.002616\n",
      "iteration 3717, train loss: 0.003778, validation loss: 0.002991\n",
      "iteration 3718, train loss: 0.003862, validation loss: 0.003313\n",
      "iteration 3719, train loss: 0.004155, validation loss: 0.00284\n",
      "iteration 3720, train loss: 0.004129, validation loss: 0.002744\n",
      "iteration 3721, train loss: 0.003603, validation loss: 0.002819\n",
      "iteration 3722, train loss: 0.00384, validation loss: 0.002599\n",
      "iteration 3723, train loss: 0.003608, validation loss: 0.002441\n",
      "iteration 3724, train loss: 0.003459, validation loss: 0.002535\n",
      "iteration 3725, train loss: 0.003779, validation loss: 0.002613\n",
      "iteration 3726, train loss: 0.00352, validation loss: 0.002491\n",
      "iteration 3727, train loss: 0.004186, validation loss: 0.002273\n",
      "iteration 3728, train loss: 0.003475, validation loss: 0.002556\n",
      "iteration 3729, train loss: 0.00389, validation loss: 0.002418\n",
      "iteration 3730, train loss: 0.00387, validation loss: 0.00229\n",
      "iteration 3731, train loss: 0.003196, validation loss: 0.002374\n",
      "iteration 3732, train loss: 0.003489, validation loss: 0.00248\n",
      "iteration 3733, train loss: 0.003882, validation loss: 0.002368\n",
      "iteration 3734, train loss: 0.003532, validation loss: 0.002315\n",
      "iteration 3735, train loss: 0.003273, validation loss: 0.002422\n",
      "iteration 3736, train loss: 0.00346, validation loss: 0.002408\n",
      "iteration 3737, train loss: 0.003517, validation loss: 0.002361\n",
      "iteration 3738, train loss: 0.003961, validation loss: 0.002416\n",
      "iteration 3739, train loss: 0.003275, validation loss: 0.002562\n",
      "iteration 3740, train loss: 0.003608, validation loss: 0.002409\n",
      "iteration 3741, train loss: 0.003211, validation loss: 0.002345\n",
      "iteration 3742, train loss: 0.00343, validation loss: 0.002418\n",
      "iteration 3743, train loss: 0.003361, validation loss: 0.002566\n",
      "iteration 3744, train loss: 0.003695, validation loss: 0.00248\n",
      "iteration 3745, train loss: 0.004038, validation loss: 0.002347\n",
      "iteration 3746, train loss: 0.003515, validation loss: 0.002722\n",
      "iteration 3747, train loss: 0.003856, validation loss: 0.002999\n",
      "iteration 3748, train loss: 0.003726, validation loss: 0.002692\n",
      "iteration 3749, train loss: 0.004351, validation loss: 0.002503\n",
      "iteration 3750, train loss: 0.003854, validation loss: 0.003359\n",
      "iteration 3751, train loss: 0.004916, validation loss: 0.002996\n",
      "iteration 3752, train loss: 0.003842, validation loss: 0.002353\n",
      "iteration 3753, train loss: 0.003386, validation loss: 0.003215\n",
      "iteration 3754, train loss: 0.003907, validation loss: 0.003164\n",
      "iteration 3755, train loss: 0.003647, validation loss: 0.002826\n",
      "iteration 3756, train loss: 0.004032, validation loss: 0.002662\n",
      "iteration 3757, train loss: 0.003705, validation loss: 0.002517\n",
      "iteration 3758, train loss: 0.003589, validation loss: 0.002491\n",
      "iteration 3759, train loss: 0.003624, validation loss: 0.002716\n",
      "iteration 3760, train loss: 0.003728, validation loss: 0.002377\n",
      "iteration 3761, train loss: 0.003854, validation loss: 0.002544\n",
      "iteration 3762, train loss: 0.003524, validation loss: 0.002811\n",
      "iteration 3763, train loss: 0.003962, validation loss: 0.002353\n",
      "iteration 3764, train loss: 0.003691, validation loss: 0.002354\n",
      "iteration 3765, train loss: 0.003466, validation loss: 0.0028\n",
      "iteration 3766, train loss: 0.003812, validation loss: 0.002292\n",
      "iteration 3767, train loss: 0.003437, validation loss: 0.002277\n",
      "iteration 3768, train loss: 0.003593, validation loss: 0.002561\n",
      "iteration 3769, train loss: 0.003657, validation loss: 0.002367\n",
      "iteration 3770, train loss: 0.00391, validation loss: 0.002407\n",
      "iteration 3771, train loss: 0.003411, validation loss: 0.002851\n",
      "iteration 3772, train loss: 0.00382, validation loss: 0.002765\n",
      "iteration 3773, train loss: 0.003586, validation loss: 0.002411\n",
      "iteration 3774, train loss: 0.003545, validation loss: 0.002695\n",
      "iteration 3775, train loss: 0.003832, validation loss: 0.002633\n",
      "iteration 3776, train loss: 0.003459, validation loss: \u001b[92m0.002233\u001b[0m\n",
      "iteration 3777, train loss: 0.003393, validation loss: 0.002466\n",
      "iteration 3778, train loss: 0.003319, validation loss: 0.002742\n",
      "iteration 3779, train loss: 0.003948, validation loss: 0.002304\n",
      "iteration 3780, train loss: 0.003173, validation loss: 0.002377\n",
      "iteration 3781, train loss: 0.003436, validation loss: 0.002466\n",
      "iteration 3782, train loss: 0.003651, validation loss: 0.002389\n",
      "iteration 3783, train loss: 0.003502, validation loss: 0.0023\n",
      "iteration 3784, train loss: 0.00398, validation loss: 0.002564\n",
      "iteration 3785, train loss: 0.003731, validation loss: 0.002339\n",
      "iteration 3786, train loss: 0.00354, validation loss: 0.002573\n",
      "iteration 3787, train loss: 0.004064, validation loss: 0.002645\n",
      "iteration 3788, train loss: 0.003785, validation loss: 0.002356\n",
      "iteration 3789, train loss: 0.00332, validation loss: 0.002488\n",
      "iteration 3790, train loss: 0.003411, validation loss: 0.002769\n",
      "iteration 3791, train loss: 0.004158, validation loss: 0.002405\n",
      "iteration 3792, train loss: 0.003311, validation loss: 0.002677\n",
      "iteration 3793, train loss: 0.003756, validation loss: 0.002564\n",
      "iteration 3794, train loss: 0.003729, validation loss: 0.002273\n",
      "iteration 3795, train loss: 0.003398, validation loss: 0.002387\n",
      "iteration 3796, train loss: 0.003208, validation loss: 0.002662\n",
      "iteration 3797, train loss: 0.003526, validation loss: 0.002423\n",
      "iteration 3798, train loss: 0.003499, validation loss: 0.00243\n",
      "iteration 3799, train loss: 0.00379, validation loss: 0.002499\n",
      "iteration 3800, train loss: 0.003607, validation loss: 0.002481\n",
      "iteration 3801, train loss: 0.003682, validation loss: 0.002534\n",
      "iteration 3802, train loss: 0.003551, validation loss: 0.002355\n",
      "iteration 3803, train loss: 0.003377, validation loss: 0.002364\n",
      "iteration 3804, train loss: 0.003669, validation loss: 0.002454\n",
      "iteration 3805, train loss: 0.003568, validation loss: 0.002362\n",
      "iteration 3806, train loss: 0.003606, validation loss: 0.00237\n",
      "iteration 3807, train loss: 0.003687, validation loss: 0.002493\n",
      "iteration 3808, train loss: 0.003318, validation loss: 0.002604\n",
      "iteration 3809, train loss: 0.00372, validation loss: 0.002607\n",
      "iteration 3810, train loss: 0.003347, validation loss: 0.002581\n",
      "iteration 3811, train loss: 0.00374, validation loss: 0.002391\n",
      "iteration 3812, train loss: 0.003598, validation loss: 0.002649\n",
      "iteration 3813, train loss: 0.003743, validation loss: 0.002509\n",
      "iteration 3814, train loss: 0.003309, validation loss: 0.00232\n",
      "iteration 3815, train loss: 0.003451, validation loss: 0.00249\n",
      "iteration 3816, train loss: 0.0033, validation loss: 0.002537\n",
      "iteration 3817, train loss: 0.00359, validation loss: 0.00246\n",
      "iteration 3818, train loss: 0.003273, validation loss: 0.00246\n",
      "iteration 3819, train loss: 0.003619, validation loss: 0.00231\n",
      "iteration 3820, train loss: 0.003333, validation loss: 0.002388\n",
      "iteration 3821, train loss: 0.003441, validation loss: 0.002395\n",
      "iteration 3822, train loss: 0.003344, validation loss: 0.002306\n",
      "iteration 3823, train loss: 0.003278, validation loss: 0.002343\n",
      "iteration 3824, train loss: \u001b[92m0.003104\u001b[0m, validation loss: 0.002423\n",
      "iteration 3825, train loss: 0.003393, validation loss: 0.002401\n",
      "iteration 3826, train loss: 0.003435, validation loss: 0.002357\n",
      "iteration 3827, train loss: 0.003506, validation loss: 0.002333\n",
      "iteration 3828, train loss: 0.00356, validation loss: 0.002326\n",
      "iteration 3829, train loss: 0.003374, validation loss: 0.002309\n",
      "iteration 3830, train loss: 0.003358, validation loss: 0.002424\n",
      "iteration 3831, train loss: 0.003461, validation loss: 0.002784\n",
      "iteration 3832, train loss: 0.003646, validation loss: 0.002599\n",
      "iteration 3833, train loss: 0.003609, validation loss: 0.00247\n",
      "iteration 3834, train loss: 0.003552, validation loss: 0.002717\n",
      "iteration 3835, train loss: 0.003793, validation loss: 0.002533\n",
      "iteration 3836, train loss: 0.003656, validation loss: 0.002468\n",
      "iteration 3837, train loss: 0.003349, validation loss: 0.002993\n",
      "iteration 3838, train loss: 0.003623, validation loss: 0.002938\n",
      "iteration 3839, train loss: 0.00413, validation loss: 0.002497\n",
      "iteration 3840, train loss: 0.00356, validation loss: 0.002578\n",
      "iteration 3841, train loss: 0.003503, validation loss: 0.002758\n",
      "iteration 3842, train loss: 0.004147, validation loss: 0.002281\n",
      "iteration 3843, train loss: 0.003268, validation loss: 0.002528\n",
      "iteration 3844, train loss: 0.003925, validation loss: 0.002505\n",
      "iteration 3845, train loss: 0.003588, validation loss: 0.002562\n",
      "iteration 3846, train loss: 0.003879, validation loss: 0.002565\n",
      "iteration 3847, train loss: 0.00358, validation loss: 0.002456\n",
      "iteration 3848, train loss: 0.003572, validation loss: 0.002432\n",
      "iteration 3849, train loss: 0.003567, validation loss: 0.003067\n",
      "iteration 3850, train loss: 0.004443, validation loss: 0.002734\n",
      "iteration 3851, train loss: 0.004149, validation loss: 0.002869\n",
      "iteration 3852, train loss: 0.003961, validation loss: 0.003381\n",
      "iteration 3853, train loss: 0.00512, validation loss: 0.002563\n",
      "iteration 3854, train loss: 0.003823, validation loss: 0.002977\n",
      "iteration 3855, train loss: 0.004384, validation loss: 0.003106\n",
      "iteration 3856, train loss: 0.004386, validation loss: 0.002455\n",
      "iteration 3857, train loss: 0.003488, validation loss: 0.00261\n",
      "iteration 3858, train loss: 0.003949, validation loss: 0.002757\n",
      "iteration 3859, train loss: 0.003794, validation loss: 0.002629\n",
      "iteration 3860, train loss: 0.00362, validation loss: 0.002577\n",
      "iteration 3861, train loss: 0.003839, validation loss: 0.002598\n",
      "iteration 3862, train loss: 0.003539, validation loss: 0.00269\n",
      "iteration 3863, train loss: 0.003898, validation loss: 0.002363\n",
      "iteration 3864, train loss: 0.003457, validation loss: 0.002502\n",
      "iteration 3865, train loss: 0.003511, validation loss: 0.003028\n",
      "iteration 3866, train loss: 0.003944, validation loss: 0.002968\n",
      "iteration 3867, train loss: 0.003862, validation loss: 0.002682\n",
      "iteration 3868, train loss: 0.003574, validation loss: 0.002337\n",
      "iteration 3869, train loss: 0.003282, validation loss: 0.002241\n",
      "iteration 3870, train loss: 0.003498, validation loss: 0.002314\n",
      "iteration 3871, train loss: 0.003317, validation loss: 0.002274\n",
      "iteration 3872, train loss: 0.003474, validation loss: 0.002251\n",
      "iteration 3873, train loss: 0.003798, validation loss: 0.002263\n",
      "iteration 3874, train loss: 0.003698, validation loss: \u001b[92m0.002214\u001b[0m\n",
      "iteration 3875, train loss: 0.003486, validation loss: 0.002273\n",
      "iteration 3876, train loss: 0.003727, validation loss: 0.002244\n",
      "iteration 3877, train loss: 0.003187, validation loss: 0.002371\n",
      "iteration 3878, train loss: 0.003215, validation loss: 0.002403\n",
      "iteration 3879, train loss: 0.0035, validation loss: 0.002275\n",
      "iteration 3880, train loss: 0.003246, validation loss: 0.002395\n",
      "iteration 3881, train loss: 0.003536, validation loss: 0.002553\n",
      "iteration 3882, train loss: 0.003469, validation loss: 0.002404\n",
      "iteration 3883, train loss: 0.003593, validation loss: 0.002517\n",
      "iteration 3884, train loss: 0.004287, validation loss: 0.002767\n",
      "iteration 3885, train loss: 0.004032, validation loss: 0.002412\n",
      "iteration 3886, train loss: 0.00355, validation loss: 0.00236\n",
      "iteration 3887, train loss: 0.003666, validation loss: 0.002982\n",
      "iteration 3888, train loss: 0.003649, validation loss: 0.002667\n",
      "iteration 3889, train loss: 0.00373, validation loss: 0.002526\n",
      "iteration 3890, train loss: 0.004045, validation loss: 0.002514\n",
      "iteration 3891, train loss: 0.003902, validation loss: 0.002361\n",
      "iteration 3892, train loss: 0.003118, validation loss: 0.002382\n",
      "iteration 3893, train loss: 0.003389, validation loss: 0.002391\n",
      "iteration 3894, train loss: 0.003342, validation loss: 0.00236\n",
      "iteration 3895, train loss: 0.003295, validation loss: 0.002548\n",
      "iteration 3896, train loss: 0.004096, validation loss: 0.002329\n",
      "iteration 3897, train loss: 0.003567, validation loss: 0.002397\n",
      "iteration 3898, train loss: 0.003487, validation loss: 0.002363\n",
      "iteration 3899, train loss: 0.003234, validation loss: 0.002396\n",
      "iteration 3900, train loss: 0.003365, validation loss: 0.002426\n",
      "iteration 3901, train loss: 0.0035, validation loss: 0.002446\n",
      "iteration 3902, train loss: 0.00367, validation loss: 0.002511\n",
      "iteration 3903, train loss: 0.003204, validation loss: 0.002848\n",
      "iteration 3904, train loss: 0.0041, validation loss: 0.002834\n",
      "iteration 3905, train loss: 0.003394, validation loss: 0.003171\n",
      "iteration 3906, train loss: 0.004048, validation loss: 0.003065\n",
      "iteration 3907, train loss: 0.003848, validation loss: 0.002309\n",
      "iteration 3908, train loss: 0.00347, validation loss: 0.002483\n",
      "iteration 3909, train loss: 0.003454, validation loss: 0.002962\n",
      "iteration 3910, train loss: 0.004137, validation loss: 0.002464\n",
      "iteration 3911, train loss: 0.003581, validation loss: 0.002329\n",
      "iteration 3912, train loss: 0.003447, validation loss: 0.002763\n",
      "iteration 3913, train loss: 0.003836, validation loss: 0.002583\n",
      "iteration 3914, train loss: 0.003747, validation loss: 0.002218\n",
      "iteration 3915, train loss: \u001b[92m0.003089\u001b[0m, validation loss: 0.002491\n",
      "iteration 3916, train loss: 0.003574, validation loss: 0.002706\n",
      "iteration 3917, train loss: 0.003816, validation loss: 0.00266\n",
      "iteration 3918, train loss: 0.004342, validation loss: 0.002466\n",
      "iteration 3919, train loss: 0.003772, validation loss: 0.002584\n",
      "iteration 3920, train loss: 0.003677, validation loss: 0.002645\n",
      "iteration 3921, train loss: 0.003368, validation loss: 0.002553\n",
      "iteration 3922, train loss: 0.004146, validation loss: 0.002261\n",
      "iteration 3923, train loss: 0.003254, validation loss: 0.002421\n",
      "iteration 3924, train loss: 0.003609, validation loss: 0.002338\n",
      "iteration 3925, train loss: 0.003365, validation loss: 0.002243\n",
      "iteration 3926, train loss: 0.003825, validation loss: 0.002358\n",
      "iteration 3927, train loss: 0.003691, validation loss: 0.002465\n",
      "iteration 3928, train loss: 0.003668, validation loss: 0.002461\n",
      "iteration 3929, train loss: 0.003626, validation loss: 0.002402\n",
      "iteration 3930, train loss: 0.003367, validation loss: \u001b[92m0.002178\u001b[0m\n",
      "iteration 3931, train loss: 0.003224, validation loss: 0.002305\n",
      "iteration 3932, train loss: 0.003224, validation loss: 0.002571\n",
      "iteration 3933, train loss: 0.003978, validation loss: 0.002241\n",
      "iteration 3934, train loss: 0.003567, validation loss: 0.002455\n",
      "iteration 3935, train loss: 0.003779, validation loss: 0.0028\n",
      "iteration 3936, train loss: 0.004035, validation loss: 0.002557\n",
      "iteration 3937, train loss: 0.003939, validation loss: 0.002207\n",
      "iteration 3938, train loss: 0.003409, validation loss: 0.002454\n",
      "iteration 3939, train loss: 0.003804, validation loss: 0.00289\n",
      "iteration 3940, train loss: 0.00429, validation loss: 0.002589\n",
      "iteration 3941, train loss: 0.00338, validation loss: 0.002414\n",
      "iteration 3942, train loss: 0.003283, validation loss: 0.002368\n",
      "iteration 3943, train loss: 0.003503, validation loss: 0.002717\n",
      "iteration 3944, train loss: 0.003912, validation loss: 0.002654\n",
      "iteration 3945, train loss: 0.003698, validation loss: 0.002222\n",
      "iteration 3946, train loss: 0.003399, validation loss: 0.002437\n",
      "iteration 3947, train loss: 0.00344, validation loss: 0.002623\n",
      "iteration 3948, train loss: 0.003553, validation loss: 0.002551\n",
      "iteration 3949, train loss: 0.00392, validation loss: 0.002306\n",
      "iteration 3950, train loss: 0.003423, validation loss: 0.002355\n",
      "iteration 3951, train loss: 0.003342, validation loss: 0.002672\n",
      "iteration 3952, train loss: 0.00341, validation loss: 0.002764\n",
      "iteration 3953, train loss: 0.003553, validation loss: 0.002508\n",
      "iteration 3954, train loss: 0.003207, validation loss: 0.002337\n",
      "iteration 3955, train loss: 0.003664, validation loss: 0.002331\n",
      "iteration 3956, train loss: 0.003336, validation loss: 0.002503\n",
      "iteration 3957, train loss: 0.003656, validation loss: 0.002304\n",
      "iteration 3958, train loss: 0.003492, validation loss: 0.002404\n",
      "iteration 3959, train loss: 0.003932, validation loss: 0.002945\n",
      "iteration 3960, train loss: 0.004108, validation loss: 0.002606\n",
      "iteration 3961, train loss: 0.003844, validation loss: 0.002332\n",
      "iteration 3962, train loss: 0.003355, validation loss: 0.002664\n",
      "iteration 3963, train loss: 0.003931, validation loss: 0.002685\n",
      "iteration 3964, train loss: 0.004362, validation loss: 0.002331\n",
      "iteration 3965, train loss: 0.003606, validation loss: 0.002547\n",
      "iteration 3966, train loss: 0.003482, validation loss: 0.00288\n",
      "iteration 3967, train loss: 0.003959, validation loss: 0.002443\n",
      "iteration 3968, train loss: 0.003376, validation loss: 0.0022\n",
      "iteration 3969, train loss: 0.003346, validation loss: 0.002444\n",
      "iteration 3970, train loss: 0.00343, validation loss: 0.002331\n",
      "iteration 3971, train loss: 0.003536, validation loss: 0.00228\n",
      "iteration 3972, train loss: 0.003342, validation loss: 0.002417\n",
      "iteration 3973, train loss: 0.003442, validation loss: 0.002544\n",
      "iteration 3974, train loss: 0.003527, validation loss: 0.002285\n",
      "iteration 3975, train loss: 0.003622, validation loss: 0.002214\n",
      "iteration 3976, train loss: 0.003235, validation loss: 0.002728\n",
      "iteration 3977, train loss: 0.003713, validation loss: 0.002687\n",
      "iteration 3978, train loss: 0.004133, validation loss: 0.002271\n",
      "iteration 3979, train loss: 0.003469, validation loss: 0.002589\n",
      "iteration 3980, train loss: 0.00356, validation loss: 0.002968\n",
      "iteration 3981, train loss: 0.003709, validation loss: 0.002758\n",
      "iteration 3982, train loss: 0.00396, validation loss: 0.002306\n",
      "iteration 3983, train loss: 0.003469, validation loss: 0.002385\n",
      "iteration 3984, train loss: 0.00334, validation loss: 0.002648\n",
      "iteration 3985, train loss: 0.004157, validation loss: 0.002382\n",
      "iteration 3986, train loss: 0.003495, validation loss: 0.002197\n",
      "iteration 3987, train loss: 0.003427, validation loss: 0.002539\n",
      "iteration 3988, train loss: 0.003438, validation loss: 0.002776\n",
      "iteration 3989, train loss: 0.004145, validation loss: 0.002447\n",
      "iteration 3990, train loss: 0.003535, validation loss: 0.002279\n",
      "iteration 3991, train loss: 0.003233, validation loss: 0.002847\n",
      "iteration 3992, train loss: 0.00395, validation loss: 0.00292\n",
      "iteration 3993, train loss: 0.003852, validation loss: 0.002623\n",
      "iteration 3994, train loss: 0.003774, validation loss: 0.002329\n",
      "iteration 3995, train loss: 0.003506, validation loss: 0.002877\n",
      "iteration 3996, train loss: 0.003754, validation loss: 0.003203\n",
      "iteration 3997, train loss: 0.004409, validation loss: 0.002574\n",
      "iteration 3998, train loss: 0.003742, validation loss: 0.002558\n",
      "iteration 3999, train loss: 0.003778, validation loss: 0.002904\n",
      "iteration 4000, train loss: 0.003818, validation loss: 0.002468\n",
      "iteration 4001, train loss: 0.003493, validation loss: 0.002747\n",
      "iteration 4002, train loss: 0.003806, validation loss: 0.002798\n",
      "iteration 4003, train loss: 0.004034, validation loss: 0.002375\n",
      "iteration 4004, train loss: 0.003407, validation loss: 0.003237\n",
      "iteration 4005, train loss: 0.004386, validation loss: 0.00337\n",
      "iteration 4006, train loss: 0.004074, validation loss: 0.00242\n",
      "iteration 4007, train loss: 0.003426, validation loss: 0.002723\n",
      "iteration 4008, train loss: 0.003984, validation loss: 0.002487\n",
      "iteration 4009, train loss: 0.003383, validation loss: 0.002681\n",
      "iteration 4010, train loss: 0.003806, validation loss: 0.002814\n",
      "iteration 4011, train loss: 0.003782, validation loss: 0.002385\n",
      "iteration 4012, train loss: 0.003572, validation loss: 0.002371\n",
      "iteration 4013, train loss: 0.003396, validation loss: 0.002674\n",
      "iteration 4014, train loss: 0.003513, validation loss: 0.002523\n",
      "iteration 4015, train loss: 0.004112, validation loss: 0.002352\n",
      "iteration 4016, train loss: 0.003629, validation loss: 0.002469\n",
      "iteration 4017, train loss: 0.003389, validation loss: 0.002553\n",
      "iteration 4018, train loss: 0.003642, validation loss: 0.002437\n",
      "iteration 4019, train loss: 0.003608, validation loss: 0.002302\n",
      "iteration 4020, train loss: 0.003633, validation loss: 0.00248\n",
      "iteration 4021, train loss: 0.003637, validation loss: 0.002791\n",
      "iteration 4022, train loss: 0.003741, validation loss: 0.002588\n",
      "iteration 4023, train loss: 0.003875, validation loss: 0.002435\n",
      "iteration 4024, train loss: 0.003262, validation loss: 0.00259\n",
      "iteration 4025, train loss: 0.003713, validation loss: 0.002464\n",
      "iteration 4026, train loss: 0.003434, validation loss: 0.002314\n",
      "iteration 4027, train loss: 0.003503, validation loss: 0.002452\n",
      "iteration 4028, train loss: 0.003637, validation loss: 0.002823\n",
      "iteration 4029, train loss: 0.003742, validation loss: 0.002793\n",
      "iteration 4030, train loss: 0.003844, validation loss: 0.002432\n",
      "iteration 4031, train loss: 0.00326, validation loss: 0.002497\n",
      "iteration 4032, train loss: 0.003934, validation loss: 0.00236\n",
      "iteration 4033, train loss: 0.003657, validation loss: 0.002328\n",
      "iteration 4034, train loss: 0.003282, validation loss: 0.002659\n",
      "iteration 4035, train loss: 0.00382, validation loss: 0.002484\n",
      "iteration 4036, train loss: 0.003738, validation loss: \u001b[92m0.002168\u001b[0m\n",
      "iteration 4037, train loss: 0.003302, validation loss: 0.002271\n",
      "iteration 4038, train loss: 0.003242, validation loss: 0.002266\n",
      "iteration 4039, train loss: 0.003385, validation loss: 0.002231\n",
      "iteration 4040, train loss: 0.003358, validation loss: 0.002343\n",
      "iteration 4041, train loss: 0.003557, validation loss: 0.002241\n",
      "iteration 4042, train loss: 0.003236, validation loss: 0.00233\n",
      "iteration 4043, train loss: 0.003349, validation loss: 0.00264\n",
      "iteration 4044, train loss: 0.003782, validation loss: 0.002308\n",
      "iteration 4045, train loss: 0.003765, validation loss: 0.002494\n",
      "iteration 4046, train loss: 0.003834, validation loss: 0.002503\n",
      "iteration 4047, train loss: 0.003435, validation loss: 0.002356\n",
      "iteration 4048, train loss: 0.003409, validation loss: 0.002276\n",
      "iteration 4049, train loss: 0.003093, validation loss: 0.002321\n",
      "iteration 4050, train loss: 0.003357, validation loss: 0.002245\n",
      "iteration 4051, train loss: 0.003503, validation loss: 0.002201\n",
      "iteration 4052, train loss: 0.003321, validation loss: 0.002479\n",
      "iteration 4053, train loss: 0.003567, validation loss: 0.00258\n",
      "iteration 4054, train loss: 0.003653, validation loss: 0.002373\n",
      "iteration 4055, train loss: 0.00326, validation loss: 0.002274\n",
      "iteration 4056, train loss: 0.003367, validation loss: 0.00231\n",
      "iteration 4057, train loss: 0.003256, validation loss: 0.002399\n",
      "iteration 4058, train loss: 0.003423, validation loss: 0.002384\n",
      "iteration 4059, train loss: 0.003438, validation loss: 0.002403\n",
      "iteration 4060, train loss: 0.003513, validation loss: 0.002435\n",
      "iteration 4061, train loss: 0.003353, validation loss: 0.002567\n",
      "iteration 4062, train loss: 0.00335, validation loss: 0.002353\n",
      "iteration 4063, train loss: 0.003639, validation loss: 0.002326\n",
      "iteration 4064, train loss: 0.003433, validation loss: 0.002599\n",
      "iteration 4065, train loss: 0.003677, validation loss: 0.00238\n",
      "iteration 4066, train loss: 0.003297, validation loss: 0.002338\n",
      "iteration 4067, train loss: 0.003347, validation loss: 0.002531\n",
      "iteration 4068, train loss: 0.003889, validation loss: 0.00245\n",
      "iteration 4069, train loss: 0.00366, validation loss: 0.002795\n",
      "iteration 4070, train loss: 0.003837, validation loss: 0.002692\n",
      "iteration 4071, train loss: 0.003728, validation loss: 0.002314\n",
      "iteration 4072, train loss: 0.00312, validation loss: 0.002447\n",
      "iteration 4073, train loss: 0.003734, validation loss: 0.002486\n",
      "iteration 4074, train loss: 0.003576, validation loss: \u001b[92m0.002152\u001b[0m\n",
      "iteration 4075, train loss: 0.003119, validation loss: 0.00236\n",
      "iteration 4076, train loss: 0.00354, validation loss: 0.002556\n",
      "iteration 4077, train loss: 0.003443, validation loss: 0.002433\n",
      "iteration 4078, train loss: 0.00367, validation loss: 0.002314\n",
      "iteration 4079, train loss: 0.003525, validation loss: 0.002341\n",
      "iteration 4080, train loss: 0.003578, validation loss: 0.002599\n",
      "iteration 4081, train loss: 0.004011, validation loss: 0.002551\n",
      "iteration 4082, train loss: 0.004363, validation loss: 0.002322\n",
      "iteration 4083, train loss: 0.003485, validation loss: 0.002724\n",
      "iteration 4084, train loss: 0.003979, validation loss: 0.002871\n",
      "iteration 4085, train loss: 0.003908, validation loss: 0.002352\n",
      "iteration 4086, train loss: 0.003604, validation loss: 0.002268\n",
      "iteration 4087, train loss: 0.003292, validation loss: 0.00277\n",
      "iteration 4088, train loss: 0.003827, validation loss: 0.002329\n",
      "iteration 4089, train loss: 0.003665, validation loss: 0.002277\n",
      "iteration 4090, train loss: 0.003165, validation loss: 0.0029\n",
      "iteration 4091, train loss: 0.003993, validation loss: 0.002869\n",
      "iteration 4092, train loss: 0.003578, validation loss: 0.0024\n",
      "iteration 4093, train loss: 0.003525, validation loss: 0.002658\n",
      "iteration 4094, train loss: 0.003495, validation loss: 0.002898\n",
      "iteration 4095, train loss: 0.003723, validation loss: 0.002241\n",
      "iteration 4096, train loss: \u001b[92m0.003083\u001b[0m, validation loss: 0.002312\n",
      "iteration 4097, train loss: 0.003272, validation loss: 0.002503\n",
      "iteration 4098, train loss: 0.003297, validation loss: 0.002426\n",
      "iteration 4099, train loss: 0.00362, validation loss: 0.002169\n",
      "iteration 4100, train loss: 0.003374, validation loss: 0.002291\n",
      "iteration 4101, train loss: 0.003233, validation loss: 0.002422\n",
      "iteration 4102, train loss: 0.00401, validation loss: 0.002205\n",
      "iteration 4103, train loss: 0.003202, validation loss: 0.002292\n",
      "iteration 4104, train loss: 0.003462, validation loss: 0.00239\n",
      "iteration 4105, train loss: 0.003632, validation loss: 0.002336\n",
      "iteration 4106, train loss: 0.003346, validation loss: 0.002405\n",
      "iteration 4107, train loss: 0.003382, validation loss: 0.002331\n",
      "iteration 4108, train loss: 0.003552, validation loss: 0.00224\n",
      "iteration 4109, train loss: 0.003259, validation loss: 0.002357\n",
      "iteration 4110, train loss: 0.003498, validation loss: 0.002496\n",
      "iteration 4111, train loss: 0.003895, validation loss: 0.002389\n",
      "iteration 4112, train loss: 0.003226, validation loss: 0.002288\n",
      "iteration 4113, train loss: 0.003347, validation loss: 0.002419\n",
      "iteration 4114, train loss: 0.003827, validation loss: 0.002361\n",
      "iteration 4115, train loss: 0.003218, validation loss: 0.002286\n",
      "iteration 4116, train loss: 0.003159, validation loss: 0.002351\n",
      "iteration 4117, train loss: 0.00311, validation loss: 0.002319\n",
      "iteration 4118, train loss: 0.00348, validation loss: 0.002497\n",
      "iteration 4119, train loss: 0.00343, validation loss: 0.002571\n",
      "iteration 4120, train loss: 0.003716, validation loss: 0.002194\n",
      "iteration 4121, train loss: 0.003142, validation loss: 0.002418\n",
      "iteration 4122, train loss: 0.00348, validation loss: 0.002512\n",
      "iteration 4123, train loss: 0.003565, validation loss: 0.002263\n",
      "iteration 4124, train loss: 0.00332, validation loss: 0.002463\n",
      "iteration 4125, train loss: 0.003488, validation loss: 0.002357\n",
      "iteration 4126, train loss: 0.003531, validation loss: 0.002236\n",
      "iteration 4127, train loss: 0.003291, validation loss: 0.002389\n",
      "iteration 4128, train loss: 0.003548, validation loss: 0.002532\n",
      "iteration 4129, train loss: 0.003647, validation loss: 0.002272\n",
      "iteration 4130, train loss: 0.003285, validation loss: 0.002317\n",
      "iteration 4131, train loss: 0.003623, validation loss: 0.002208\n",
      "iteration 4132, train loss: 0.003357, validation loss: 0.002201\n",
      "iteration 4133, train loss: 0.003419, validation loss: 0.0024\n",
      "iteration 4134, train loss: 0.003434, validation loss: 0.002238\n",
      "iteration 4135, train loss: \u001b[92m0.003061\u001b[0m, validation loss: 0.002225\n",
      "iteration 4136, train loss: 0.003298, validation loss: 0.002322\n",
      "iteration 4137, train loss: 0.003284, validation loss: \u001b[92m0.002146\u001b[0m\n",
      "iteration 4138, train loss: 0.003498, validation loss: 0.002449\n",
      "iteration 4139, train loss: 0.004001, validation loss: 0.002556\n",
      "iteration 4140, train loss: 0.003621, validation loss: 0.002469\n",
      "iteration 4141, train loss: 0.003911, validation loss: 0.002209\n",
      "iteration 4142, train loss: 0.003326, validation loss: 0.002274\n",
      "iteration 4143, train loss: 0.003558, validation loss: 0.00223\n",
      "iteration 4144, train loss: 0.003571, validation loss: 0.002477\n",
      "iteration 4145, train loss: 0.003676, validation loss: 0.002559\n",
      "iteration 4146, train loss: 0.00384, validation loss: 0.00226\n",
      "iteration 4147, train loss: 0.003393, validation loss: 0.002554\n",
      "iteration 4148, train loss: 0.004328, validation loss: 0.002351\n",
      "iteration 4149, train loss: 0.003781, validation loss: 0.002409\n",
      "iteration 4150, train loss: 0.003471, validation loss: 0.002691\n",
      "iteration 4151, train loss: 0.004095, validation loss: 0.002238\n",
      "iteration 4152, train loss: \u001b[92m0.002903\u001b[0m, validation loss: 0.002469\n",
      "iteration 4153, train loss: 0.00347, validation loss: 0.002617\n",
      "iteration 4154, train loss: 0.003633, validation loss: 0.002398\n",
      "iteration 4155, train loss: 0.003193, validation loss: 0.002452\n",
      "iteration 4156, train loss: 0.004232, validation loss: 0.002613\n",
      "iteration 4157, train loss: 0.003588, validation loss: 0.002732\n",
      "iteration 4158, train loss: 0.004159, validation loss: 0.002388\n",
      "iteration 4159, train loss: 0.003326, validation loss: 0.002487\n",
      "iteration 4160, train loss: 0.003691, validation loss: 0.002693\n",
      "iteration 4161, train loss: 0.003999, validation loss: 0.002513\n",
      "iteration 4162, train loss: 0.003407, validation loss: 0.00243\n",
      "iteration 4163, train loss: 0.003653, validation loss: 0.002437\n",
      "iteration 4164, train loss: 0.003572, validation loss: 0.00258\n",
      "iteration 4165, train loss: 0.003485, validation loss: 0.002632\n",
      "iteration 4166, train loss: 0.003866, validation loss: 0.002377\n",
      "iteration 4167, train loss: 0.003643, validation loss: 0.002811\n",
      "iteration 4168, train loss: 0.003542, validation loss: 0.002644\n",
      "iteration 4169, train loss: 0.003689, validation loss: 0.002363\n",
      "iteration 4170, train loss: 0.003266, validation loss: 0.002678\n",
      "iteration 4171, train loss: 0.003665, validation loss: 0.002326\n",
      "iteration 4172, train loss: 0.003463, validation loss: 0.002282\n",
      "iteration 4173, train loss: 0.003208, validation loss: 0.002557\n",
      "iteration 4174, train loss: 0.003546, validation loss: 0.002375\n",
      "iteration 4175, train loss: 0.003449, validation loss: 0.002293\n",
      "iteration 4176, train loss: 0.003317, validation loss: 0.002425\n",
      "iteration 4177, train loss: 0.003366, validation loss: 0.002275\n",
      "iteration 4178, train loss: 0.003573, validation loss: 0.002157\n",
      "iteration 4179, train loss: 0.003423, validation loss: 0.002377\n",
      "iteration 4180, train loss: 0.003474, validation loss: 0.002412\n",
      "iteration 4181, train loss: 0.003528, validation loss: 0.002248\n",
      "iteration 4182, train loss: 0.003295, validation loss: \u001b[92m0.002139\u001b[0m\n",
      "iteration 4183, train loss: 0.003203, validation loss: 0.002196\n",
      "iteration 4184, train loss: 0.00342, validation loss: 0.002163\n",
      "iteration 4185, train loss: 0.003304, validation loss: \u001b[92m0.002122\u001b[0m\n",
      "iteration 4186, train loss: 0.00303, validation loss: 0.002249\n",
      "iteration 4187, train loss: 0.003359, validation loss: 0.002348\n",
      "iteration 4188, train loss: 0.003276, validation loss: 0.002266\n",
      "iteration 4189, train loss: 0.003554, validation loss: \u001b[92m0.002107\u001b[0m\n",
      "iteration 4190, train loss: 0.003236, validation loss: 0.002199\n",
      "iteration 4191, train loss: 0.003418, validation loss: 0.002187\n",
      "iteration 4192, train loss: 0.00328, validation loss: 0.00211\n",
      "iteration 4193, train loss: 0.003136, validation loss: 0.002328\n",
      "iteration 4194, train loss: 0.003462, validation loss: 0.002325\n",
      "iteration 4195, train loss: 0.003241, validation loss: 0.002121\n",
      "iteration 4196, train loss: 0.00345, validation loss: 0.00225\n",
      "iteration 4197, train loss: 0.003484, validation loss: 0.002386\n",
      "iteration 4198, train loss: 0.00349, validation loss: 0.002262\n",
      "iteration 4199, train loss: 0.003294, validation loss: 0.002292\n",
      "iteration 4200, train loss: 0.003256, validation loss: 0.002292\n",
      "iteration 4201, train loss: 0.003449, validation loss: 0.002249\n",
      "iteration 4202, train loss: 0.003413, validation loss: 0.00236\n",
      "iteration 4203, train loss: 0.003627, validation loss: 0.002275\n",
      "iteration 4204, train loss: 0.003261, validation loss: 0.002427\n",
      "iteration 4205, train loss: 0.003213, validation loss: 0.002485\n",
      "iteration 4206, train loss: 0.003491, validation loss: 0.002471\n",
      "iteration 4207, train loss: 0.003338, validation loss: 0.00253\n",
      "iteration 4208, train loss: 0.003556, validation loss: 0.00243\n",
      "iteration 4209, train loss: 0.003435, validation loss: 0.002346\n",
      "iteration 4210, train loss: 0.003447, validation loss: 0.002635\n",
      "iteration 4211, train loss: 0.003329, validation loss: 0.002915\n",
      "iteration 4212, train loss: 0.00394, validation loss: 0.002434\n",
      "iteration 4213, train loss: 0.003669, validation loss: 0.002435\n",
      "iteration 4214, train loss: 0.003651, validation loss: 0.002583\n",
      "iteration 4215, train loss: 0.003541, validation loss: 0.002528\n",
      "iteration 4216, train loss: 0.003126, validation loss: 0.002241\n",
      "iteration 4217, train loss: 0.003235, validation loss: 0.002276\n",
      "iteration 4218, train loss: 0.003412, validation loss: 0.002272\n",
      "iteration 4219, train loss: 0.003402, validation loss: 0.0023\n",
      "iteration 4220, train loss: 0.003255, validation loss: 0.002318\n",
      "iteration 4221, train loss: 0.003205, validation loss: 0.002267\n",
      "iteration 4222, train loss: 0.003477, validation loss: 0.002314\n",
      "iteration 4223, train loss: 0.003537, validation loss: 0.00215\n",
      "iteration 4224, train loss: 0.003338, validation loss: 0.002161\n",
      "iteration 4225, train loss: 0.003279, validation loss: 0.002221\n",
      "iteration 4226, train loss: 0.003504, validation loss: 0.002196\n",
      "iteration 4227, train loss: 0.00313, validation loss: 0.002404\n",
      "iteration 4228, train loss: 0.003448, validation loss: 0.002396\n",
      "iteration 4229, train loss: 0.003655, validation loss: \u001b[92m0.002106\u001b[0m\n",
      "iteration 4230, train loss: 0.003319, validation loss: 0.002277\n",
      "iteration 4231, train loss: 0.003305, validation loss: 0.002334\n",
      "iteration 4232, train loss: 0.003761, validation loss: 0.002263\n",
      "iteration 4233, train loss: 0.003306, validation loss: 0.002512\n",
      "iteration 4234, train loss: 0.003771, validation loss: 0.002359\n",
      "iteration 4235, train loss: 0.003427, validation loss: 0.002423\n",
      "iteration 4236, train loss: 0.003376, validation loss: 0.002393\n",
      "iteration 4237, train loss: 0.003448, validation loss: 0.002257\n",
      "iteration 4238, train loss: 0.003462, validation loss: 0.00238\n",
      "iteration 4239, train loss: 0.003358, validation loss: 0.002609\n",
      "iteration 4240, train loss: 0.003349, validation loss: 0.002527\n",
      "iteration 4241, train loss: 0.003755, validation loss: 0.002338\n",
      "iteration 4242, train loss: 0.003253, validation loss: 0.002295\n",
      "iteration 4243, train loss: 0.003568, validation loss: 0.002292\n",
      "iteration 4244, train loss: 0.003169, validation loss: 0.002169\n",
      "iteration 4245, train loss: 0.003364, validation loss: 0.002195\n",
      "iteration 4246, train loss: 0.003298, validation loss: 0.002274\n",
      "iteration 4247, train loss: 0.00329, validation loss: 0.002239\n",
      "iteration 4248, train loss: 0.003474, validation loss: 0.002142\n",
      "iteration 4249, train loss: 0.003185, validation loss: 0.002186\n",
      "iteration 4250, train loss: 0.003165, validation loss: 0.002189\n",
      "iteration 4251, train loss: 0.00317, validation loss: 0.002139\n",
      "iteration 4252, train loss: 0.003232, validation loss: 0.002165\n",
      "iteration 4253, train loss: 0.003041, validation loss: 0.002364\n",
      "iteration 4254, train loss: 0.003517, validation loss: 0.002241\n",
      "iteration 4255, train loss: 0.003246, validation loss: 0.002265\n",
      "iteration 4256, train loss: 0.003109, validation loss: 0.002813\n",
      "iteration 4257, train loss: 0.003382, validation loss: 0.002597\n",
      "iteration 4258, train loss: 0.004048, validation loss: 0.002402\n",
      "iteration 4259, train loss: 0.003515, validation loss: 0.002693\n",
      "iteration 4260, train loss: 0.003997, validation loss: 0.002427\n",
      "iteration 4261, train loss: 0.003509, validation loss: 0.002334\n",
      "iteration 4262, train loss: 0.003228, validation loss: 0.002259\n",
      "iteration 4263, train loss: 0.003141, validation loss: 0.002473\n",
      "iteration 4264, train loss: 0.003245, validation loss: 0.002638\n",
      "iteration 4265, train loss: 0.003765, validation loss: 0.002173\n",
      "iteration 4266, train loss: 0.003398, validation loss: 0.00237\n",
      "iteration 4267, train loss: 0.003624, validation loss: 0.002423\n",
      "iteration 4268, train loss: 0.003755, validation loss: 0.002405\n",
      "iteration 4269, train loss: 0.003533, validation loss: 0.002534\n",
      "iteration 4270, train loss: 0.003742, validation loss: 0.002522\n",
      "iteration 4271, train loss: 0.00377, validation loss: 0.002541\n",
      "iteration 4272, train loss: 0.003621, validation loss: 0.002277\n",
      "iteration 4273, train loss: 0.003266, validation loss: 0.00246\n",
      "iteration 4274, train loss: 0.003422, validation loss: 0.002897\n",
      "iteration 4275, train loss: 0.003962, validation loss: 0.002522\n",
      "iteration 4276, train loss: 0.003414, validation loss: 0.002326\n",
      "iteration 4277, train loss: 0.003392, validation loss: 0.002801\n",
      "iteration 4278, train loss: 0.003952, validation loss: 0.002614\n",
      "iteration 4279, train loss: 0.003504, validation loss: 0.002264\n",
      "iteration 4280, train loss: \u001b[92m0.002842\u001b[0m, validation loss: 0.00238\n",
      "iteration 4281, train loss: 0.003691, validation loss: 0.002277\n",
      "iteration 4282, train loss: 0.003575, validation loss: 0.00234\n",
      "iteration 4283, train loss: 0.003286, validation loss: 0.002523\n",
      "iteration 4284, train loss: 0.003313, validation loss: 0.002673\n",
      "iteration 4285, train loss: 0.003682, validation loss: 0.002539\n",
      "iteration 4286, train loss: 0.003679, validation loss: 0.00216\n",
      "iteration 4287, train loss: 0.003202, validation loss: 0.002341\n",
      "iteration 4288, train loss: 0.003497, validation loss: 0.002566\n",
      "iteration 4289, train loss: 0.003562, validation loss: 0.002445\n",
      "iteration 4290, train loss: 0.003521, validation loss: 0.00234\n",
      "iteration 4291, train loss: 0.003373, validation loss: 0.002522\n",
      "iteration 4292, train loss: 0.003466, validation loss: 0.002546\n",
      "iteration 4293, train loss: 0.003601, validation loss: 0.002384\n",
      "iteration 4294, train loss: 0.003298, validation loss: 0.002484\n",
      "iteration 4295, train loss: 0.003157, validation loss: 0.002279\n",
      "iteration 4296, train loss: 0.003235, validation loss: 0.002378\n",
      "iteration 4297, train loss: 0.003321, validation loss: 0.002539\n",
      "iteration 4298, train loss: 0.003569, validation loss: 0.002313\n",
      "iteration 4299, train loss: 0.003945, validation loss: 0.002403\n",
      "iteration 4300, train loss: 0.003065, validation loss: 0.002858\n",
      "iteration 4301, train loss: 0.004141, validation loss: 0.00228\n",
      "iteration 4302, train loss: 0.003525, validation loss: 0.002264\n",
      "iteration 4303, train loss: 0.003191, validation loss: 0.002622\n",
      "iteration 4304, train loss: 0.003475, validation loss: 0.002543\n",
      "iteration 4305, train loss: 0.003481, validation loss: \u001b[92m0.002079\u001b[0m\n",
      "iteration 4306, train loss: 0.003083, validation loss: 0.002462\n",
      "iteration 4307, train loss: 0.003675, validation loss: 0.002522\n",
      "iteration 4308, train loss: 0.003898, validation loss: 0.002124\n",
      "iteration 4309, train loss: 0.003117, validation loss: 0.002165\n",
      "iteration 4310, train loss: 0.003348, validation loss: 0.002216\n",
      "iteration 4311, train loss: 0.003191, validation loss: 0.002173\n",
      "iteration 4312, train loss: 0.003421, validation loss: 0.002107\n",
      "iteration 4313, train loss: 0.003364, validation loss: 0.002264\n",
      "iteration 4314, train loss: 0.003174, validation loss: 0.002318\n",
      "iteration 4315, train loss: 0.003151, validation loss: 0.002395\n",
      "iteration 4316, train loss: 0.003515, validation loss: 0.002097\n",
      "iteration 4317, train loss: 0.003215, validation loss: 0.002191\n",
      "iteration 4318, train loss: 0.003206, validation loss: 0.002172\n",
      "iteration 4319, train loss: 0.00326, validation loss: 0.0022\n",
      "iteration 4320, train loss: 0.003288, validation loss: 0.002244\n",
      "iteration 4321, train loss: 0.003083, validation loss: 0.002166\n",
      "iteration 4322, train loss: 0.003445, validation loss: 0.002104\n",
      "iteration 4323, train loss: 0.00307, validation loss: 0.002286\n",
      "iteration 4324, train loss: 0.003254, validation loss: 0.002547\n",
      "iteration 4325, train loss: 0.003483, validation loss: 0.002292\n",
      "iteration 4326, train loss: 0.003258, validation loss: 0.002159\n",
      "iteration 4327, train loss: 0.00316, validation loss: 0.002306\n",
      "iteration 4328, train loss: 0.003487, validation loss: 0.002201\n",
      "iteration 4329, train loss: 0.00306, validation loss: 0.002378\n",
      "iteration 4330, train loss: 0.003785, validation loss: 0.002272\n",
      "iteration 4331, train loss: 0.003519, validation loss: 0.002139\n",
      "iteration 4332, train loss: 0.003272, validation loss: 0.002253\n",
      "iteration 4333, train loss: 0.003345, validation loss: 0.002355\n",
      "iteration 4334, train loss: 0.003147, validation loss: 0.002502\n",
      "iteration 4335, train loss: 0.003985, validation loss: 0.002106\n",
      "iteration 4336, train loss: 0.003214, validation loss: 0.002313\n",
      "iteration 4337, train loss: 0.003347, validation loss: 0.002438\n",
      "iteration 4338, train loss: 0.003796, validation loss: 0.002125\n",
      "iteration 4339, train loss: 0.0033, validation loss: 0.002363\n",
      "iteration 4340, train loss: 0.003802, validation loss: 0.002367\n",
      "iteration 4341, train loss: 0.003612, validation loss: 0.002216\n",
      "iteration 4342, train loss: 0.003105, validation loss: 0.002357\n",
      "iteration 4343, train loss: 0.003759, validation loss: 0.002108\n",
      "iteration 4344, train loss: 0.003379, validation loss: 0.00228\n",
      "iteration 4345, train loss: 0.003202, validation loss: 0.002433\n",
      "iteration 4346, train loss: 0.003261, validation loss: 0.002212\n",
      "iteration 4347, train loss: 0.003244, validation loss: 0.002127\n",
      "iteration 4348, train loss: 0.003085, validation loss: 0.002223\n",
      "iteration 4349, train loss: 0.002932, validation loss: 0.002253\n",
      "iteration 4350, train loss: 0.003377, validation loss: 0.002223\n",
      "iteration 4351, train loss: 0.003264, validation loss: 0.002226\n",
      "iteration 4352, train loss: 0.002895, validation loss: 0.002189\n",
      "iteration 4353, train loss: 0.003079, validation loss: 0.002122\n",
      "iteration 4354, train loss: 0.002967, validation loss: 0.002243\n",
      "iteration 4355, train loss: 0.003259, validation loss: 0.002368\n",
      "iteration 4356, train loss: 0.003609, validation loss: 0.002113\n",
      "iteration 4357, train loss: 0.003393, validation loss: 0.002215\n",
      "iteration 4358, train loss: 0.003104, validation loss: 0.002398\n",
      "iteration 4359, train loss: 0.003719, validation loss: 0.002168\n",
      "iteration 4360, train loss: 0.003403, validation loss: 0.002188\n",
      "iteration 4361, train loss: 0.003414, validation loss: 0.002444\n",
      "iteration 4362, train loss: 0.003246, validation loss: 0.002445\n",
      "iteration 4363, train loss: 0.003482, validation loss: 0.002081\n",
      "iteration 4364, train loss: 0.003282, validation loss: 0.002712\n",
      "iteration 4365, train loss: 0.003713, validation loss: 0.002861\n",
      "iteration 4366, train loss: 0.003613, validation loss: 0.002235\n",
      "iteration 4367, train loss: 0.00343, validation loss: 0.002237\n",
      "iteration 4368, train loss: 0.003294, validation loss: 0.002524\n",
      "iteration 4369, train loss: 0.003667, validation loss: 0.00231\n",
      "iteration 4370, train loss: 0.003224, validation loss: 0.002243\n",
      "iteration 4371, train loss: 0.003551, validation loss: 0.002341\n",
      "iteration 4372, train loss: 0.003425, validation loss: 0.002368\n",
      "iteration 4373, train loss: 0.003744, validation loss: 0.00234\n",
      "iteration 4374, train loss: 0.003137, validation loss: 0.00228\n",
      "iteration 4375, train loss: 0.003277, validation loss: 0.002197\n",
      "iteration 4376, train loss: 0.003238, validation loss: 0.002349\n",
      "iteration 4377, train loss: 0.003516, validation loss: 0.002325\n",
      "iteration 4378, train loss: 0.00331, validation loss: 0.002221\n",
      "iteration 4379, train loss: 0.003338, validation loss: 0.002143\n",
      "iteration 4380, train loss: 0.003067, validation loss: 0.00231\n",
      "iteration 4381, train loss: 0.003373, validation loss: 0.002421\n",
      "iteration 4382, train loss: 0.003244, validation loss: 0.002201\n",
      "iteration 4383, train loss: 0.003504, validation loss: 0.002162\n",
      "iteration 4384, train loss: 0.003103, validation loss: 0.002322\n",
      "iteration 4385, train loss: 0.003155, validation loss: 0.00241\n",
      "iteration 4386, train loss: 0.003591, validation loss: 0.002178\n",
      "iteration 4387, train loss: 0.003018, validation loss: 0.002134\n",
      "iteration 4388, train loss: 0.003124, validation loss: 0.002244\n",
      "iteration 4389, train loss: 0.00311, validation loss: 0.002353\n",
      "iteration 4390, train loss: 0.003247, validation loss: 0.002181\n",
      "iteration 4391, train loss: 0.003364, validation loss: 0.002194\n",
      "iteration 4392, train loss: 0.00328, validation loss: 0.002407\n",
      "iteration 4393, train loss: 0.003527, validation loss: 0.002386\n",
      "iteration 4394, train loss: 0.00339, validation loss: 0.002184\n",
      "iteration 4395, train loss: 0.003103, validation loss: 0.002471\n",
      "iteration 4396, train loss: 0.003803, validation loss: 0.002395\n",
      "iteration 4397, train loss: 0.003391, validation loss: 0.002157\n",
      "iteration 4398, train loss: 0.002971, validation loss: 0.002129\n",
      "iteration 4399, train loss: 0.003048, validation loss: 0.002136\n",
      "iteration 4400, train loss: 0.003137, validation loss: 0.002233\n",
      "iteration 4401, train loss: 0.003202, validation loss: 0.002105\n",
      "iteration 4402, train loss: 0.003016, validation loss: 0.00221\n",
      "iteration 4403, train loss: 0.003126, validation loss: 0.00228\n",
      "iteration 4404, train loss: 0.003588, validation loss: 0.002355\n",
      "iteration 4405, train loss: 0.003266, validation loss: 0.002334\n",
      "iteration 4406, train loss: 0.003502, validation loss: 0.00219\n",
      "iteration 4407, train loss: 0.003173, validation loss: 0.002332\n",
      "iteration 4408, train loss: 0.003331, validation loss: 0.002191\n",
      "iteration 4409, train loss: 0.003352, validation loss: 0.002241\n",
      "iteration 4410, train loss: 0.003116, validation loss: 0.00247\n",
      "iteration 4411, train loss: 0.003242, validation loss: 0.002539\n",
      "iteration 4412, train loss: 0.003606, validation loss: 0.002153\n",
      "iteration 4413, train loss: 0.003375, validation loss: 0.002286\n",
      "iteration 4414, train loss: 0.003154, validation loss: 0.002895\n",
      "iteration 4415, train loss: 0.004208, validation loss: 0.002213\n",
      "iteration 4416, train loss: 0.003307, validation loss: 0.002574\n",
      "iteration 4417, train loss: 0.003843, validation loss: 0.002613\n",
      "iteration 4418, train loss: 0.004128, validation loss: 0.002318\n",
      "iteration 4419, train loss: 0.003209, validation loss: 0.002455\n",
      "iteration 4420, train loss: 0.003473, validation loss: 0.0025\n",
      "iteration 4421, train loss: 0.003676, validation loss: 0.002668\n",
      "iteration 4422, train loss: 0.003285, validation loss: 0.002762\n",
      "iteration 4423, train loss: 0.003753, validation loss: 0.002321\n",
      "iteration 4424, train loss: 0.00314, validation loss: 0.002334\n",
      "iteration 4425, train loss: 0.003316, validation loss: 0.002447\n",
      "iteration 4426, train loss: 0.00368, validation loss: \u001b[92m0.002047\u001b[0m\n",
      "iteration 4427, train loss: 0.003207, validation loss: 0.002284\n",
      "iteration 4428, train loss: 0.003438, validation loss: 0.002209\n",
      "iteration 4429, train loss: 0.003521, validation loss: 0.002157\n",
      "iteration 4430, train loss: 0.003217, validation loss: 0.002453\n",
      "iteration 4431, train loss: 0.003726, validation loss: 0.002273\n",
      "iteration 4432, train loss: 0.003552, validation loss: 0.002152\n",
      "iteration 4433, train loss: 0.003273, validation loss: 0.002273\n",
      "iteration 4434, train loss: 0.003138, validation loss: 0.002292\n",
      "iteration 4435, train loss: 0.00381, validation loss: 0.002191\n",
      "iteration 4436, train loss: 0.003504, validation loss: 0.002155\n",
      "iteration 4437, train loss: 0.003378, validation loss: 0.002094\n",
      "iteration 4438, train loss: 0.003079, validation loss: 0.002448\n",
      "iteration 4439, train loss: 0.003302, validation loss: 0.002315\n",
      "iteration 4440, train loss: 0.003436, validation loss: 0.002074\n",
      "iteration 4441, train loss: 0.003085, validation loss: 0.002247\n",
      "iteration 4442, train loss: 0.003218, validation loss: 0.002348\n",
      "iteration 4443, train loss: 0.00318, validation loss: 0.002104\n",
      "iteration 4444, train loss: 0.003198, validation loss: 0.002137\n",
      "iteration 4445, train loss: 0.003212, validation loss: 0.002213\n",
      "iteration 4446, train loss: 0.003273, validation loss: 0.00213\n",
      "iteration 4447, train loss: 0.003034, validation loss: 0.002092\n",
      "iteration 4448, train loss: 0.003266, validation loss: 0.002132\n",
      "iteration 4449, train loss: 0.003094, validation loss: 0.002174\n",
      "iteration 4450, train loss: 0.002994, validation loss: 0.002145\n",
      "iteration 4451, train loss: 0.003032, validation loss: 0.002065\n",
      "iteration 4452, train loss: 0.003144, validation loss: 0.002085\n",
      "iteration 4453, train loss: 0.003004, validation loss: 0.002175\n",
      "iteration 4454, train loss: 0.003121, validation loss: 0.002089\n",
      "iteration 4455, train loss: 0.003056, validation loss: 0.002175\n",
      "iteration 4456, train loss: 0.003217, validation loss: 0.002266\n",
      "iteration 4457, train loss: 0.003047, validation loss: 0.002222\n",
      "iteration 4458, train loss: 0.003312, validation loss: 0.002317\n",
      "iteration 4459, train loss: 0.003281, validation loss: 0.00209\n",
      "iteration 4460, train loss: 0.003111, validation loss: \u001b[92m0.002046\u001b[0m\n",
      "iteration 4461, train loss: 0.00337, validation loss: 0.002091\n",
      "iteration 4462, train loss: 0.003056, validation loss: 0.002231\n",
      "iteration 4463, train loss: 0.003573, validation loss: 0.002315\n",
      "iteration 4464, train loss: 0.003425, validation loss: 0.002258\n",
      "iteration 4465, train loss: 0.003111, validation loss: 0.002117\n",
      "iteration 4466, train loss: 0.003101, validation loss: 0.002094\n",
      "iteration 4467, train loss: 0.003277, validation loss: 0.002203\n",
      "iteration 4468, train loss: 0.003247, validation loss: 0.002292\n",
      "iteration 4469, train loss: 0.003385, validation loss: 0.002105\n",
      "iteration 4470, train loss: 0.003458, validation loss: 0.002313\n",
      "iteration 4471, train loss: 0.003195, validation loss: 0.002591\n",
      "iteration 4472, train loss: 0.003402, validation loss: 0.002262\n",
      "iteration 4473, train loss: 0.003347, validation loss: 0.002214\n",
      "iteration 4474, train loss: 0.002928, validation loss: 0.002436\n",
      "iteration 4475, train loss: 0.003182, validation loss: 0.002377\n",
      "iteration 4476, train loss: 0.003073, validation loss: 0.00238\n",
      "iteration 4477, train loss: 0.003469, validation loss: 0.002204\n",
      "iteration 4478, train loss: 0.003044, validation loss: 0.002198\n",
      "iteration 4479, train loss: 0.003898, validation loss: 0.002509\n",
      "iteration 4480, train loss: 0.003498, validation loss: 0.002431\n",
      "iteration 4481, train loss: 0.003564, validation loss: 0.002225\n",
      "iteration 4482, train loss: 0.003009, validation loss: 0.002449\n",
      "iteration 4483, train loss: 0.003599, validation loss: 0.002368\n",
      "iteration 4484, train loss: 0.003542, validation loss: 0.002293\n",
      "iteration 4485, train loss: 0.003395, validation loss: 0.002387\n",
      "iteration 4486, train loss: 0.003162, validation loss: 0.002569\n",
      "iteration 4487, train loss: 0.004374, validation loss: 0.002808\n",
      "iteration 4488, train loss: 0.003826, validation loss: 0.002818\n",
      "iteration 4489, train loss: 0.00395, validation loss: 0.002687\n",
      "iteration 4490, train loss: 0.004157, validation loss: 0.002702\n",
      "iteration 4491, train loss: 0.00353, validation loss: 0.002633\n",
      "iteration 4492, train loss: 0.003748, validation loss: 0.002614\n",
      "iteration 4493, train loss: 0.003605, validation loss: 0.002722\n",
      "iteration 4494, train loss: 0.003809, validation loss: 0.002345\n",
      "iteration 4495, train loss: 0.003176, validation loss: 0.002749\n",
      "iteration 4496, train loss: 0.003876, validation loss: 0.002646\n",
      "iteration 4497, train loss: 0.003686, validation loss: 0.002341\n",
      "iteration 4498, train loss: 0.003334, validation loss: 0.002684\n",
      "iteration 4499, train loss: 0.00344, validation loss: 0.002452\n",
      "iteration 4500, train loss: 0.003345, validation loss: 0.002155\n",
      "iteration 4501, train loss: 0.002994, validation loss: 0.002471\n",
      "iteration 4502, train loss: 0.003091, validation loss: 0.002505\n",
      "iteration 4503, train loss: 0.003445, validation loss: 0.002099\n",
      "iteration 4504, train loss: 0.003362, validation loss: 0.002197\n",
      "iteration 4505, train loss: 0.003102, validation loss: 0.002675\n",
      "iteration 4506, train loss: 0.003673, validation loss: 0.002526\n",
      "iteration 4507, train loss: 0.003458, validation loss: 0.002161\n",
      "iteration 4508, train loss: 0.003047, validation loss: 0.002496\n",
      "iteration 4509, train loss: 0.003526, validation loss: 0.002823\n",
      "iteration 4510, train loss: 0.003449, validation loss: 0.002793\n",
      "iteration 4511, train loss: 0.003828, validation loss: 0.002217\n",
      "iteration 4512, train loss: 0.003199, validation loss: 0.002193\n",
      "iteration 4513, train loss: 0.003332, validation loss: 0.002612\n",
      "iteration 4514, train loss: 0.003506, validation loss: 0.002594\n",
      "iteration 4515, train loss: 0.003906, validation loss: 0.002195\n",
      "iteration 4516, train loss: 0.003506, validation loss: 0.002716\n",
      "iteration 4517, train loss: 0.003529, validation loss: 0.002843\n",
      "iteration 4518, train loss: 0.00384, validation loss: 0.002366\n",
      "iteration 4519, train loss: 0.003249, validation loss: 0.002328\n",
      "iteration 4520, train loss: 0.00351, validation loss: 0.002461\n",
      "iteration 4521, train loss: 0.003833, validation loss: 0.002499\n",
      "iteration 4522, train loss: 0.003455, validation loss: 0.002386\n",
      "iteration 4523, train loss: 0.003762, validation loss: 0.002196\n",
      "iteration 4524, train loss: 0.003224, validation loss: 0.002947\n",
      "iteration 4525, train loss: 0.003992, validation loss: 0.003182\n",
      "iteration 4526, train loss: 0.004805, validation loss: 0.002394\n",
      "iteration 4527, train loss: 0.003318, validation loss: 0.002592\n",
      "iteration 4528, train loss: 0.003743, validation loss: 0.002751\n",
      "iteration 4529, train loss: 0.003298, validation loss: 0.002688\n",
      "iteration 4530, train loss: 0.003801, validation loss: 0.00253\n",
      "iteration 4531, train loss: 0.003619, validation loss: 0.002523\n",
      "iteration 4532, train loss: 0.00385, validation loss: 0.002247\n",
      "iteration 4533, train loss: 0.003153, validation loss: 0.002307\n",
      "iteration 4534, train loss: 0.003375, validation loss: 0.002465\n",
      "iteration 4535, train loss: 0.003493, validation loss: 0.002403\n",
      "iteration 4536, train loss: 0.003245, validation loss: 0.0025\n",
      "iteration 4537, train loss: 0.003262, validation loss: 0.002404\n",
      "iteration 4538, train loss: 0.003303, validation loss: 0.002285\n",
      "iteration 4539, train loss: 0.003578, validation loss: 0.002129\n",
      "iteration 4540, train loss: 0.003133, validation loss: 0.002517\n",
      "iteration 4541, train loss: 0.00365, validation loss: 0.002537\n",
      "iteration 4542, train loss: 0.004322, validation loss: 0.00207\n",
      "iteration 4543, train loss: 0.003221, validation loss: 0.002452\n",
      "iteration 4544, train loss: 0.003472, validation loss: 0.002744\n",
      "iteration 4545, train loss: 0.004004, validation loss: 0.002435\n",
      "iteration 4546, train loss: 0.003091, validation loss: 0.002348\n",
      "iteration 4547, train loss: 0.003243, validation loss: 0.002215\n",
      "iteration 4548, train loss: 0.003421, validation loss: 0.002568\n",
      "iteration 4549, train loss: 0.003433, validation loss: 0.002547\n",
      "iteration 4550, train loss: 0.003638, validation loss: 0.002117\n",
      "iteration 4551, train loss: 0.003294, validation loss: 0.002314\n",
      "iteration 4552, train loss: 0.003305, validation loss: 0.002559\n",
      "iteration 4553, train loss: 0.003608, validation loss: 0.002292\n",
      "iteration 4554, train loss: 0.003341, validation loss: 0.0021\n",
      "iteration 4555, train loss: 0.003305, validation loss: 0.002069\n",
      "iteration 4556, train loss: 0.003127, validation loss: 0.002125\n",
      "iteration 4557, train loss: 0.003038, validation loss: 0.002177\n",
      "iteration 4558, train loss: 0.003416, validation loss: 0.002227\n",
      "iteration 4559, train loss: 0.003432, validation loss: 0.002175\n",
      "iteration 4560, train loss: 0.003228, validation loss: 0.002137\n",
      "iteration 4561, train loss: 0.003286, validation loss: 0.002212\n",
      "iteration 4562, train loss: 0.002939, validation loss: 0.002316\n",
      "iteration 4563, train loss: 0.003435, validation loss: 0.002049\n",
      "iteration 4564, train loss: 0.003316, validation loss: 0.00215\n",
      "iteration 4565, train loss: 0.003216, validation loss: 0.002283\n",
      "iteration 4566, train loss: 0.003314, validation loss: 0.002194\n",
      "iteration 4567, train loss: 0.003246, validation loss: 0.002201\n",
      "iteration 4568, train loss: 0.003078, validation loss: 0.002192\n",
      "iteration 4569, train loss: 0.003328, validation loss: 0.002089\n",
      "iteration 4570, train loss: 0.003088, validation loss: 0.002151\n",
      "iteration 4571, train loss: 0.003307, validation loss: 0.002178\n",
      "iteration 4572, train loss: 0.002984, validation loss: 0.002326\n",
      "iteration 4573, train loss: 0.00341, validation loss: 0.00231\n",
      "iteration 4574, train loss: 0.003654, validation loss: 0.002196\n",
      "iteration 4575, train loss: 0.00338, validation loss: 0.002132\n",
      "iteration 4576, train loss: 0.003129, validation loss: 0.002322\n",
      "iteration 4577, train loss: 0.00341, validation loss: 0.002272\n",
      "iteration 4578, train loss: 0.003182, validation loss: 0.002121\n",
      "iteration 4579, train loss: 0.003342, validation loss: 0.002244\n",
      "iteration 4580, train loss: 0.003084, validation loss: 0.002191\n",
      "iteration 4581, train loss: 0.003377, validation loss: 0.002116\n",
      "iteration 4582, train loss: 0.00296, validation loss: 0.002082\n",
      "iteration 4583, train loss: 0.003345, validation loss: 0.002221\n",
      "iteration 4584, train loss: 0.003247, validation loss: 0.002246\n",
      "iteration 4585, train loss: 0.003291, validation loss: 0.002084\n",
      "iteration 4586, train loss: 0.003134, validation loss: 0.002434\n",
      "iteration 4587, train loss: 0.003529, validation loss: 0.002454\n",
      "iteration 4588, train loss: 0.003591, validation loss: 0.00215\n",
      "iteration 4589, train loss: 0.003006, validation loss: 0.002257\n",
      "iteration 4590, train loss: 0.003615, validation loss: 0.002273\n",
      "iteration 4591, train loss: 0.003475, validation loss: 0.002295\n",
      "iteration 4592, train loss: 0.002965, validation loss: 0.002246\n",
      "iteration 4593, train loss: 0.003346, validation loss: 0.002152\n",
      "iteration 4594, train loss: 0.003191, validation loss: 0.002465\n",
      "iteration 4595, train loss: 0.003564, validation loss: 0.002346\n",
      "iteration 4596, train loss: 0.003567, validation loss: 0.002257\n",
      "iteration 4597, train loss: 0.003161, validation loss: 0.002679\n",
      "iteration 4598, train loss: 0.003703, validation loss: 0.002777\n",
      "iteration 4599, train loss: 0.003558, validation loss: 0.002398\n",
      "iteration 4600, train loss: 0.003391, validation loss: 0.002233\n",
      "iteration 4601, train loss: 0.003511, validation loss: 0.002287\n",
      "iteration 4602, train loss: 0.003533, validation loss: 0.002369\n",
      "iteration 4603, train loss: 0.003461, validation loss: 0.002527\n",
      "iteration 4604, train loss: 0.003465, validation loss: 0.00254\n",
      "iteration 4605, train loss: 0.003501, validation loss: 0.002306\n",
      "iteration 4606, train loss: 0.003322, validation loss: 0.002251\n",
      "iteration 4607, train loss: 0.003334, validation loss: 0.00265\n",
      "iteration 4608, train loss: 0.003383, validation loss: 0.002285\n",
      "iteration 4609, train loss: 0.003135, validation loss: 0.002185\n",
      "iteration 4610, train loss: 0.003204, validation loss: 0.00243\n",
      "iteration 4611, train loss: 0.003312, validation loss: 0.002522\n",
      "iteration 4612, train loss: 0.003605, validation loss: 0.002153\n",
      "iteration 4613, train loss: 0.003081, validation loss: 0.002047\n",
      "iteration 4614, train loss: 0.002983, validation loss: 0.002233\n",
      "iteration 4615, train loss: 0.003134, validation loss: 0.002082\n",
      "iteration 4616, train loss: 0.003095, validation loss: 0.002434\n",
      "iteration 4617, train loss: 0.003605, validation loss: 0.002251\n",
      "iteration 4618, train loss: 0.003433, validation loss: 0.002113\n",
      "iteration 4619, train loss: \u001b[92m0.002786\u001b[0m, validation loss: 0.002386\n",
      "iteration 4620, train loss: 0.003327, validation loss: 0.002379\n",
      "iteration 4621, train loss: 0.003591, validation loss: 0.002103\n",
      "iteration 4622, train loss: 0.003237, validation loss: 0.002119\n",
      "iteration 4623, train loss: 0.003117, validation loss: 0.002361\n",
      "iteration 4624, train loss: 0.003215, validation loss: 0.002319\n",
      "iteration 4625, train loss: 0.003589, validation loss: 0.002114\n",
      "iteration 4626, train loss: 0.003058, validation loss: 0.002167\n",
      "iteration 4627, train loss: 0.003452, validation loss: 0.00228\n",
      "iteration 4628, train loss: 0.003314, validation loss: 0.002104\n",
      "iteration 4629, train loss: 0.003221, validation loss: 0.002259\n",
      "iteration 4630, train loss: 0.003838, validation loss: 0.002157\n",
      "iteration 4631, train loss: 0.003058, validation loss: 0.002219\n",
      "iteration 4632, train loss: 0.002953, validation loss: 0.002512\n",
      "iteration 4633, train loss: 0.003837, validation loss: \u001b[92m0.002022\u001b[0m\n",
      "iteration 4634, train loss: 0.0033, validation loss: 0.002088\n",
      "iteration 4635, train loss: 0.003092, validation loss: 0.002238\n",
      "iteration 4636, train loss: 0.002951, validation loss: 0.002204\n",
      "iteration 4637, train loss: 0.00353, validation loss: \u001b[92m0.002021\u001b[0m\n",
      "iteration 4638, train loss: 0.003114, validation loss: 0.002212\n",
      "iteration 4639, train loss: 0.003222, validation loss: 0.002128\n",
      "iteration 4640, train loss: 0.003137, validation loss: 0.002055\n",
      "iteration 4641, train loss: 0.003595, validation loss: 0.002159\n",
      "iteration 4642, train loss: 0.003326, validation loss: 0.002053\n",
      "iteration 4643, train loss: 0.002955, validation loss: 0.002121\n",
      "iteration 4644, train loss: 0.002874, validation loss: 0.00223\n",
      "iteration 4645, train loss: 0.003294, validation loss: 0.002224\n",
      "iteration 4646, train loss: 0.003317, validation loss: 0.002196\n",
      "iteration 4647, train loss: 0.003078, validation loss: 0.002235\n",
      "iteration 4648, train loss: 0.003183, validation loss: 0.002133\n",
      "iteration 4649, train loss: 0.003161, validation loss: 0.002101\n",
      "iteration 4650, train loss: 0.003041, validation loss: 0.002283\n",
      "iteration 4651, train loss: 0.003055, validation loss: 0.002375\n",
      "iteration 4652, train loss: 0.003288, validation loss: 0.002305\n",
      "iteration 4653, train loss: 0.003274, validation loss: 0.002374\n",
      "iteration 4654, train loss: 0.003668, validation loss: 0.002103\n",
      "iteration 4655, train loss: 0.003126, validation loss: 0.002224\n",
      "iteration 4656, train loss: 0.003232, validation loss: 0.002149\n",
      "iteration 4657, train loss: 0.003146, validation loss: 0.002063\n",
      "iteration 4658, train loss: 0.003259, validation loss: \u001b[92m0.002018\u001b[0m\n",
      "iteration 4659, train loss: 0.00329, validation loss: 0.002061\n",
      "iteration 4660, train loss: 0.003115, validation loss: 0.002054\n",
      "iteration 4661, train loss: 0.003092, validation loss: \u001b[92m0.002015\u001b[0m\n",
      "iteration 4662, train loss: 0.003318, validation loss: 0.002112\n",
      "iteration 4663, train loss: 0.003177, validation loss: 0.002148\n",
      "iteration 4664, train loss: 0.003388, validation loss: 0.002085\n",
      "iteration 4665, train loss: 0.002917, validation loss: 0.002127\n",
      "iteration 4666, train loss: 0.003312, validation loss: 0.002257\n",
      "iteration 4667, train loss: 0.003023, validation loss: 0.002199\n",
      "iteration 4668, train loss: 0.003156, validation loss: 0.002018\n",
      "iteration 4669, train loss: 0.003207, validation loss: 0.002135\n",
      "iteration 4670, train loss: 0.003057, validation loss: 0.002252\n",
      "iteration 4671, train loss: 0.003199, validation loss: 0.002108\n",
      "iteration 4672, train loss: 0.003108, validation loss: 0.002168\n",
      "iteration 4673, train loss: 0.003041, validation loss: 0.002201\n",
      "iteration 4674, train loss: 0.003265, validation loss: 0.002118\n",
      "iteration 4675, train loss: 0.002923, validation loss: 0.00219\n",
      "iteration 4676, train loss: 0.00316, validation loss: 0.002137\n",
      "iteration 4677, train loss: 0.003186, validation loss: 0.002035\n",
      "iteration 4678, train loss: 0.002979, validation loss: 0.002051\n",
      "iteration 4679, train loss: 0.003309, validation loss: 0.002193\n",
      "iteration 4680, train loss: 0.003163, validation loss: 0.002144\n",
      "iteration 4681, train loss: 0.003157, validation loss: 0.002061\n",
      "iteration 4682, train loss: 0.00295, validation loss: 0.002421\n",
      "iteration 4683, train loss: 0.003132, validation loss: 0.002511\n",
      "iteration 4684, train loss: 0.003408, validation loss: 0.002101\n",
      "iteration 4685, train loss: 0.002854, validation loss: 0.002214\n",
      "iteration 4686, train loss: 0.003173, validation loss: 0.002457\n",
      "iteration 4687, train loss: 0.003937, validation loss: 0.002177\n",
      "iteration 4688, train loss: 0.003368, validation loss: 0.002347\n",
      "iteration 4689, train loss: 0.003395, validation loss: 0.003163\n",
      "iteration 4690, train loss: 0.00448, validation loss: 0.002699\n",
      "iteration 4691, train loss: 0.003883, validation loss: 0.002549\n",
      "iteration 4692, train loss: 0.003541, validation loss: 0.002628\n",
      "iteration 4693, train loss: 0.00357, validation loss: 0.002471\n",
      "iteration 4694, train loss: 0.003524, validation loss: 0.002703\n",
      "iteration 4695, train loss: 0.003584, validation loss: 0.002135\n",
      "iteration 4696, train loss: 0.002913, validation loss: 0.002512\n",
      "iteration 4697, train loss: 0.003583, validation loss: 0.002734\n",
      "iteration 4698, train loss: 0.00317, validation loss: 0.002482\n",
      "iteration 4699, train loss: 0.003237, validation loss: 0.002305\n",
      "iteration 4700, train loss: 0.00324, validation loss: 0.002217\n",
      "iteration 4701, train loss: 0.003404, validation loss: 0.002718\n",
      "iteration 4702, train loss: 0.004006, validation loss: 0.002652\n",
      "iteration 4703, train loss: 0.00356, validation loss: 0.002096\n",
      "iteration 4704, train loss: 0.003272, validation loss: 0.002208\n",
      "iteration 4705, train loss: 0.003438, validation loss: 0.002327\n",
      "iteration 4706, train loss: 0.003499, validation loss: 0.002054\n",
      "iteration 4707, train loss: 0.003031, validation loss: 0.002037\n",
      "iteration 4708, train loss: 0.003036, validation loss: 0.002126\n",
      "iteration 4709, train loss: 0.003613, validation loss: 0.002103\n",
      "iteration 4710, train loss: 0.002936, validation loss: 0.00217\n",
      "iteration 4711, train loss: 0.003131, validation loss: 0.002124\n",
      "iteration 4712, train loss: 0.003164, validation loss: 0.002239\n",
      "iteration 4713, train loss: 0.003166, validation loss: 0.00224\n",
      "iteration 4714, train loss: 0.003523, validation loss: 0.002123\n",
      "iteration 4715, train loss: 0.003334, validation loss: 0.002294\n",
      "iteration 4716, train loss: 0.003271, validation loss: 0.002116\n",
      "iteration 4717, train loss: 0.003007, validation loss: 0.002128\n",
      "iteration 4718, train loss: 0.003305, validation loss: 0.00241\n",
      "iteration 4719, train loss: 0.003337, validation loss: 0.002114\n",
      "iteration 4720, train loss: 0.002964, validation loss: 0.002163\n",
      "iteration 4721, train loss: 0.003339, validation loss: 0.002177\n",
      "iteration 4722, train loss: 0.003322, validation loss: 0.002284\n",
      "iteration 4723, train loss: 0.004007, validation loss: 0.002055\n",
      "iteration 4724, train loss: 0.003341, validation loss: 0.00223\n",
      "iteration 4725, train loss: 0.003523, validation loss: 0.0022\n",
      "iteration 4726, train loss: 0.002962, validation loss: 0.002102\n",
      "iteration 4727, train loss: 0.003235, validation loss: 0.00212\n",
      "iteration 4728, train loss: 0.003008, validation loss: 0.002157\n",
      "iteration 4729, train loss: 0.003142, validation loss: 0.002045\n",
      "iteration 4730, train loss: 0.003153, validation loss: 0.002068\n",
      "iteration 4731, train loss: 0.003164, validation loss: 0.002171\n",
      "iteration 4732, train loss: 0.003404, validation loss: 0.00212\n",
      "iteration 4733, train loss: 0.003016, validation loss: 0.002318\n",
      "iteration 4734, train loss: 0.002924, validation loss: 0.002419\n",
      "iteration 4735, train loss: 0.003267, validation loss: 0.002255\n",
      "iteration 4736, train loss: 0.003637, validation loss: 0.002161\n",
      "iteration 4737, train loss: 0.003243, validation loss: 0.002231\n",
      "iteration 4738, train loss: 0.003139, validation loss: 0.002336\n",
      "iteration 4739, train loss: 0.003072, validation loss: 0.002264\n",
      "iteration 4740, train loss: 0.003476, validation loss: 0.002061\n",
      "iteration 4741, train loss: 0.002977, validation loss: 0.002333\n",
      "iteration 4742, train loss: 0.003421, validation loss: 0.002384\n",
      "iteration 4743, train loss: 0.003241, validation loss: 0.002125\n",
      "iteration 4744, train loss: 0.003255, validation loss: 0.002131\n",
      "iteration 4745, train loss: 0.003134, validation loss: 0.002128\n",
      "iteration 4746, train loss: 0.003053, validation loss: 0.002312\n",
      "iteration 4747, train loss: 0.003066, validation loss: 0.002496\n",
      "iteration 4748, train loss: 0.003598, validation loss: 0.002106\n",
      "iteration 4749, train loss: 0.002874, validation loss: 0.002043\n",
      "iteration 4750, train loss: 0.002951, validation loss: 0.002246\n",
      "iteration 4751, train loss: 0.003389, validation loss: 0.002219\n",
      "iteration 4752, train loss: 0.003458, validation loss: 0.002506\n",
      "iteration 4753, train loss: 0.003217, validation loss: 0.002524\n",
      "iteration 4754, train loss: 0.003374, validation loss: 0.002243\n",
      "iteration 4755, train loss: 0.003624, validation loss: 0.002196\n",
      "iteration 4756, train loss: 0.003178, validation loss: 0.002201\n",
      "iteration 4757, train loss: 0.003597, validation loss: 0.002248\n",
      "iteration 4758, train loss: 0.003638, validation loss: 0.002073\n",
      "iteration 4759, train loss: 0.003176, validation loss: 0.002267\n",
      "iteration 4760, train loss: 0.003156, validation loss: 0.002375\n",
      "iteration 4761, train loss: 0.003239, validation loss: 0.00215\n",
      "iteration 4762, train loss: 0.003453, validation loss: 0.00207\n",
      "iteration 4763, train loss: 0.003155, validation loss: 0.002477\n",
      "iteration 4764, train loss: 0.003384, validation loss: 0.002213\n",
      "iteration 4765, train loss: 0.003111, validation loss: 0.002136\n",
      "iteration 4766, train loss: 0.003518, validation loss: 0.002205\n",
      "iteration 4767, train loss: 0.003134, validation loss: 0.002088\n",
      "iteration 4768, train loss: 0.003137, validation loss: 0.002044\n",
      "iteration 4769, train loss: 0.003064, validation loss: 0.002212\n",
      "iteration 4770, train loss: 0.003091, validation loss: 0.002109\n",
      "iteration 4771, train loss: 0.002987, validation loss: 0.002038\n",
      "iteration 4772, train loss: 0.002854, validation loss: 0.002068\n",
      "iteration 4773, train loss: 0.003128, validation loss: 0.00205\n",
      "iteration 4774, train loss: 0.003154, validation loss: \u001b[92m0.002009\u001b[0m\n",
      "iteration 4775, train loss: 0.003151, validation loss: 0.002023\n",
      "iteration 4776, train loss: 0.003123, validation loss: 0.002145\n",
      "iteration 4777, train loss: 0.003086, validation loss: 0.002114\n",
      "iteration 4778, train loss: 0.003187, validation loss: 0.002071\n",
      "iteration 4779, train loss: 0.00315, validation loss: 0.002031\n",
      "iteration 4780, train loss: 0.003171, validation loss: 0.002242\n",
      "iteration 4781, train loss: 0.003598, validation loss: 0.002395\n",
      "iteration 4782, train loss: 0.003311, validation loss: 0.002141\n",
      "iteration 4783, train loss: 0.003088, validation loss: 0.002095\n",
      "iteration 4784, train loss: 0.00299, validation loss: 0.00215\n",
      "iteration 4785, train loss: 0.003336, validation loss: 0.002491\n",
      "iteration 4786, train loss: 0.003161, validation loss: 0.002264\n",
      "iteration 4787, train loss: 0.003234, validation loss: 0.002058\n",
      "iteration 4788, train loss: 0.003188, validation loss: 0.002304\n",
      "iteration 4789, train loss: 0.003303, validation loss: 0.002104\n",
      "iteration 4790, train loss: 0.003269, validation loss: \u001b[92m0.002007\u001b[0m\n",
      "iteration 4791, train loss: 0.002787, validation loss: 0.002189\n",
      "iteration 4792, train loss: 0.003396, validation loss: 0.002062\n",
      "iteration 4793, train loss: 0.00293, validation loss: 0.00206\n",
      "iteration 4794, train loss: 0.003043, validation loss: 0.00207\n",
      "iteration 4795, train loss: 0.002997, validation loss: 0.002196\n",
      "iteration 4796, train loss: 0.003472, validation loss: \u001b[92m0.001986\u001b[0m\n",
      "iteration 4797, train loss: 0.003137, validation loss: 0.002108\n",
      "iteration 4798, train loss: 0.003278, validation loss: 0.002394\n",
      "iteration 4799, train loss: 0.003095, validation loss: 0.00248\n",
      "iteration 4800, train loss: 0.003374, validation loss: 0.002225\n",
      "iteration 4801, train loss: 0.002974, validation loss: 0.002248\n",
      "iteration 4802, train loss: 0.003402, validation loss: 0.002251\n",
      "iteration 4803, train loss: 0.003196, validation loss: 0.002135\n",
      "iteration 4804, train loss: 0.003254, validation loss: 0.002087\n",
      "iteration 4805, train loss: 0.003271, validation loss: 0.002231\n",
      "iteration 4806, train loss: 0.002953, validation loss: 0.00222\n",
      "iteration 4807, train loss: 0.003331, validation loss: 0.002212\n",
      "iteration 4808, train loss: 0.003277, validation loss: 0.002328\n",
      "iteration 4809, train loss: 0.00338, validation loss: 0.002034\n",
      "iteration 4810, train loss: 0.002972, validation loss: 0.002293\n",
      "iteration 4811, train loss: 0.002945, validation loss: 0.002437\n",
      "iteration 4812, train loss: 0.003317, validation loss: 0.002033\n",
      "iteration 4813, train loss: \u001b[92m0.00271\u001b[0m, validation loss: 0.002085\n",
      "iteration 4814, train loss: 0.003186, validation loss: 0.002227\n",
      "iteration 4815, train loss: 0.003334, validation loss: 0.002057\n",
      "iteration 4816, train loss: 0.003054, validation loss: 0.002293\n",
      "iteration 4817, train loss: 0.003039, validation loss: 0.002205\n",
      "iteration 4818, train loss: 0.002979, validation loss: 0.002049\n",
      "iteration 4819, train loss: 0.002993, validation loss: 0.002096\n",
      "iteration 4820, train loss: 0.003173, validation loss: 0.002081\n",
      "iteration 4821, train loss: 0.002876, validation loss: 0.002188\n",
      "iteration 4822, train loss: 0.003127, validation loss: 0.002184\n",
      "iteration 4823, train loss: 0.003423, validation loss: 0.002065\n",
      "iteration 4824, train loss: 0.003203, validation loss: 0.002242\n",
      "iteration 4825, train loss: 0.002926, validation loss: 0.002473\n",
      "iteration 4826, train loss: 0.003403, validation loss: 0.002289\n",
      "iteration 4827, train loss: 0.003068, validation loss: 0.002025\n",
      "iteration 4828, train loss: 0.002984, validation loss: 0.002128\n",
      "iteration 4829, train loss: 0.003247, validation loss: 0.002166\n",
      "iteration 4830, train loss: 0.003011, validation loss: 0.002069\n",
      "iteration 4831, train loss: 0.003537, validation loss: 0.002005\n",
      "iteration 4832, train loss: 0.002979, validation loss: 0.002243\n",
      "iteration 4833, train loss: 0.00294, validation loss: 0.002206\n",
      "iteration 4834, train loss: 0.00315, validation loss: 0.002073\n",
      "iteration 4835, train loss: 0.002945, validation loss: 0.002155\n",
      "iteration 4836, train loss: 0.002969, validation loss: 0.002268\n",
      "iteration 4837, train loss: 0.003199, validation loss: 0.002018\n",
      "iteration 4838, train loss: 0.003092, validation loss: 0.002074\n",
      "iteration 4839, train loss: 0.00288, validation loss: 0.00222\n",
      "iteration 4840, train loss: 0.003188, validation loss: 0.00206\n",
      "iteration 4841, train loss: 0.00312, validation loss: 0.002004\n",
      "iteration 4842, train loss: 0.00302, validation loss: 0.002237\n",
      "iteration 4843, train loss: 0.003198, validation loss: 0.002068\n",
      "iteration 4844, train loss: 0.00295, validation loss: \u001b[92m0.001957\u001b[0m\n",
      "iteration 4845, train loss: 0.003217, validation loss: 0.002199\n",
      "iteration 4846, train loss: 0.003204, validation loss: 0.002018\n",
      "iteration 4847, train loss: 0.003272, validation loss: 0.002076\n",
      "iteration 4848, train loss: 0.003274, validation loss: 0.002079\n",
      "iteration 4849, train loss: 0.002931, validation loss: 0.001994\n",
      "iteration 4850, train loss: 0.003314, validation loss: 0.002162\n",
      "iteration 4851, train loss: 0.003123, validation loss: 0.00201\n",
      "iteration 4852, train loss: 0.003197, validation loss: 0.002191\n",
      "iteration 4853, train loss: 0.003306, validation loss: 0.002312\n",
      "iteration 4854, train loss: 0.003314, validation loss: 0.002232\n",
      "iteration 4855, train loss: 0.003174, validation loss: 0.001982\n",
      "iteration 4856, train loss: 0.002866, validation loss: 0.001997\n",
      "iteration 4857, train loss: 0.003054, validation loss: 0.002151\n",
      "iteration 4858, train loss: 0.003116, validation loss: 0.002197\n",
      "iteration 4859, train loss: 0.003406, validation loss: 0.00206\n",
      "iteration 4860, train loss: 0.002999, validation loss: 0.002137\n",
      "iteration 4861, train loss: 0.00283, validation loss: 0.002261\n",
      "iteration 4862, train loss: 0.002924, validation loss: 0.002106\n",
      "iteration 4863, train loss: 0.002833, validation loss: 0.002114\n",
      "iteration 4864, train loss: 0.002951, validation loss: 0.002325\n",
      "iteration 4865, train loss: 0.00333, validation loss: 0.002093\n",
      "iteration 4866, train loss: 0.003409, validation loss: 0.002358\n",
      "iteration 4867, train loss: 0.003205, validation loss: 0.002845\n",
      "iteration 4868, train loss: 0.003713, validation loss: 0.002283\n",
      "iteration 4869, train loss: 0.003183, validation loss: 0.002029\n",
      "iteration 4870, train loss: 0.002892, validation loss: 0.002404\n",
      "iteration 4871, train loss: 0.003757, validation loss: 0.002384\n",
      "iteration 4872, train loss: 0.003673, validation loss: 0.001982\n",
      "iteration 4873, train loss: 0.003016, validation loss: 0.00228\n",
      "iteration 4874, train loss: 0.00344, validation loss: 0.002316\n",
      "iteration 4875, train loss: 0.003379, validation loss: 0.002175\n",
      "iteration 4876, train loss: 0.003057, validation loss: 0.002364\n",
      "iteration 4877, train loss: 0.003525, validation loss: 0.002349\n",
      "iteration 4878, train loss: 0.003511, validation loss: 0.002272\n",
      "iteration 4879, train loss: 0.003165, validation loss: 0.002208\n",
      "iteration 4880, train loss: 0.002988, validation loss: 0.00203\n",
      "iteration 4881, train loss: 0.002965, validation loss: 0.002177\n",
      "iteration 4882, train loss: 0.00292, validation loss: 0.002338\n",
      "iteration 4883, train loss: 0.003229, validation loss: 0.002179\n",
      "iteration 4884, train loss: 0.003396, validation loss: 0.001995\n",
      "iteration 4885, train loss: 0.003305, validation loss: 0.002137\n",
      "iteration 4886, train loss: 0.003037, validation loss: 0.002042\n",
      "iteration 4887, train loss: 0.002994, validation loss: 0.002004\n",
      "iteration 4888, train loss: 0.002795, validation loss: 0.002031\n",
      "iteration 4889, train loss: 0.003159, validation loss: 0.00213\n",
      "iteration 4890, train loss: 0.003154, validation loss: 0.002124\n",
      "iteration 4891, train loss: 0.003355, validation loss: 0.002008\n",
      "iteration 4892, train loss: 0.002974, validation loss: 0.002377\n",
      "iteration 4893, train loss: 0.003289, validation loss: 0.002462\n",
      "iteration 4894, train loss: 0.003227, validation loss: 0.002021\n",
      "iteration 4895, train loss: 0.00317, validation loss: 0.002073\n",
      "iteration 4896, train loss: 0.002868, validation loss: 0.002437\n",
      "iteration 4897, train loss: 0.003137, validation loss: 0.002266\n",
      "iteration 4898, train loss: 0.003242, validation loss: 0.002102\n",
      "iteration 4899, train loss: 0.003234, validation loss: 0.00223\n",
      "iteration 4900, train loss: 0.003262, validation loss: 0.002341\n",
      "iteration 4901, train loss: 0.003228, validation loss: 0.002108\n",
      "iteration 4902, train loss: 0.003091, validation loss: 0.002106\n",
      "iteration 4903, train loss: 0.002994, validation loss: 0.002308\n",
      "iteration 4904, train loss: 0.003314, validation loss: 0.002376\n",
      "iteration 4905, train loss: 0.00334, validation loss: 0.0022\n",
      "iteration 4906, train loss: 0.003446, validation loss: 0.002098\n",
      "iteration 4907, train loss: 0.003323, validation loss: 0.002059\n",
      "iteration 4908, train loss: 0.003143, validation loss: 0.002034\n",
      "iteration 4909, train loss: 0.00302, validation loss: 0.002262\n",
      "iteration 4910, train loss: 0.003052, validation loss: 0.002321\n",
      "iteration 4911, train loss: 0.003494, validation loss: 0.002284\n",
      "iteration 4912, train loss: 0.003121, validation loss: 0.002428\n",
      "iteration 4913, train loss: 0.003221, validation loss: 0.002452\n",
      "iteration 4914, train loss: 0.003596, validation loss: 0.002343\n",
      "iteration 4915, train loss: 0.003057, validation loss: 0.002173\n",
      "iteration 4916, train loss: 0.003244, validation loss: 0.002018\n",
      "iteration 4917, train loss: 0.002951, validation loss: 0.002134\n",
      "iteration 4918, train loss: 0.003166, validation loss: 0.002341\n",
      "iteration 4919, train loss: 0.003568, validation loss: 0.002016\n",
      "iteration 4920, train loss: \u001b[92m0.002676\u001b[0m, validation loss: 0.002032\n",
      "iteration 4921, train loss: 0.003021, validation loss: 0.002129\n",
      "iteration 4922, train loss: 0.003127, validation loss: 0.002029\n",
      "iteration 4923, train loss: 0.002811, validation loss: \u001b[92m0.001954\u001b[0m\n",
      "iteration 4924, train loss: \u001b[92m0.002636\u001b[0m, validation loss: 0.002128\n",
      "iteration 4925, train loss: 0.003014, validation loss: 0.002161\n",
      "iteration 4926, train loss: 0.002831, validation loss: 0.00221\n",
      "iteration 4927, train loss: 0.003191, validation loss: 0.001955\n",
      "iteration 4928, train loss: 0.003105, validation loss: 0.002088\n",
      "iteration 4929, train loss: 0.003011, validation loss: 0.002097\n",
      "iteration 4930, train loss: 0.00294, validation loss: 0.001999\n",
      "iteration 4931, train loss: 0.003001, validation loss: 0.002086\n",
      "iteration 4932, train loss: 0.003136, validation loss: 0.002114\n",
      "iteration 4933, train loss: 0.003313, validation loss: 0.002553\n",
      "iteration 4934, train loss: 0.003408, validation loss: 0.002186\n",
      "iteration 4935, train loss: 0.003229, validation loss: 0.002051\n",
      "iteration 4936, train loss: 0.003332, validation loss: 0.002154\n",
      "iteration 4937, train loss: 0.003163, validation loss: 0.00198\n",
      "iteration 4938, train loss: 0.003079, validation loss: 0.001999\n",
      "iteration 4939, train loss: 0.00299, validation loss: 0.002081\n",
      "iteration 4940, train loss: 0.0028, validation loss: 0.002156\n",
      "iteration 4941, train loss: 0.002962, validation loss: 0.002043\n",
      "iteration 4942, train loss: 0.002962, validation loss: 0.00201\n",
      "iteration 4943, train loss: 0.002965, validation loss: 0.002142\n",
      "iteration 4944, train loss: 0.003042, validation loss: 0.002099\n",
      "iteration 4945, train loss: 0.003148, validation loss: 0.001971\n",
      "iteration 4946, train loss: 0.002948, validation loss: 0.002171\n",
      "iteration 4947, train loss: 0.003098, validation loss: 0.002483\n",
      "iteration 4948, train loss: 0.003337, validation loss: 0.002158\n",
      "iteration 4949, train loss: 0.002898, validation loss: 0.002018\n",
      "iteration 4950, train loss: 0.003366, validation loss: 0.002266\n",
      "iteration 4951, train loss: 0.00367, validation loss: 0.002157\n",
      "iteration 4952, train loss: 0.003266, validation loss: 0.00196\n",
      "iteration 4953, train loss: 0.002952, validation loss: 0.002026\n",
      "iteration 4954, train loss: 0.00283, validation loss: 0.002329\n",
      "iteration 4955, train loss: 0.003655, validation loss: 0.002085\n",
      "iteration 4956, train loss: 0.003431, validation loss: 0.002519\n",
      "iteration 4957, train loss: 0.003379, validation loss: 0.002512\n",
      "iteration 4958, train loss: 0.003565, validation loss: 0.002028\n",
      "iteration 4959, train loss: 0.003058, validation loss: 0.002042\n",
      "iteration 4960, train loss: 0.003031, validation loss: 0.002066\n",
      "iteration 4961, train loss: 0.002952, validation loss: 0.002138\n",
      "iteration 4962, train loss: 0.003175, validation loss: 0.002056\n",
      "iteration 4963, train loss: 0.002968, validation loss: 0.002038\n",
      "iteration 4964, train loss: 0.003038, validation loss: 0.002116\n",
      "iteration 4965, train loss: 0.003413, validation loss: 0.002004\n",
      "iteration 4966, train loss: 0.003283, validation loss: 0.002232\n",
      "iteration 4967, train loss: 0.003362, validation loss: 0.002562\n",
      "iteration 4968, train loss: 0.003568, validation loss: 0.002304\n",
      "iteration 4969, train loss: 0.0036, validation loss: 0.002184\n",
      "iteration 4970, train loss: 0.003257, validation loss: 0.002718\n",
      "iteration 4971, train loss: 0.003776, validation loss: 0.0023\n",
      "iteration 4972, train loss: 0.003755, validation loss: 0.002455\n",
      "iteration 4973, train loss: 0.003364, validation loss: 0.002959\n",
      "iteration 4974, train loss: 0.004009, validation loss: 0.002617\n",
      "iteration 4975, train loss: 0.003905, validation loss: 0.002305\n",
      "iteration 4976, train loss: 0.00382, validation loss: 0.002663\n",
      "iteration 4977, train loss: 0.003612, validation loss: 0.002695\n",
      "iteration 4978, train loss: 0.00375, validation loss: 0.002222\n",
      "iteration 4979, train loss: 0.002976, validation loss: 0.002186\n",
      "iteration 4980, train loss: 0.003075, validation loss: 0.002188\n",
      "iteration 4981, train loss: 0.003381, validation loss: 0.002061\n",
      "iteration 4982, train loss: 0.003351, validation loss: 0.002183\n",
      "iteration 4983, train loss: 0.003146, validation loss: 0.002336\n",
      "iteration 4984, train loss: 0.003251, validation loss: 0.002034\n",
      "iteration 4985, train loss: 0.002985, validation loss: 0.001969\n",
      "iteration 4986, train loss: 0.003026, validation loss: 0.002213\n",
      "iteration 4987, train loss: 0.00349, validation loss: 0.002066\n",
      "iteration 4988, train loss: 0.003014, validation loss: 0.002137\n",
      "iteration 4989, train loss: 0.003056, validation loss: 0.002485\n",
      "iteration 4990, train loss: 0.003685, validation loss: 0.002497\n",
      "iteration 4991, train loss: 0.003494, validation loss: 0.002394\n",
      "iteration 4992, train loss: 0.003803, validation loss: 0.00237\n",
      "iteration 4993, train loss: 0.003227, validation loss: 0.002326\n",
      "iteration 4994, train loss: 0.003182, validation loss: 0.002224\n",
      "iteration 4995, train loss: 0.003226, validation loss: 0.002371\n",
      "iteration 4996, train loss: 0.003452, validation loss: 0.00209\n",
      "iteration 4997, train loss: 0.003118, validation loss: 0.002343\n",
      "iteration 4998, train loss: 0.003397, validation loss: 0.002344\n",
      "iteration 4999, train loss: 0.003234, validation loss: 0.002089\n",
      "iteration 5000, train loss: 0.002908, validation loss: 0.002253\n",
      "iteration 5001, train loss: 0.003332, validation loss: 0.002008\n",
      "iteration 5002, train loss: 0.003018, validation loss: 0.002199\n",
      "iteration 5003, train loss: 0.003233, validation loss: 0.002036\n",
      "iteration 5004, train loss: 0.003182, validation loss: 0.001958\n",
      "iteration 5005, train loss: 0.002991, validation loss: 0.001999\n",
      "iteration 5006, train loss: 0.003079, validation loss: 0.001968\n",
      "iteration 5007, train loss: 0.002963, validation loss: 0.002023\n",
      "iteration 5008, train loss: 0.003074, validation loss: \u001b[92m0.00194\u001b[0m\n",
      "iteration 5009, train loss: 0.002964, validation loss: 0.002155\n",
      "iteration 5010, train loss: 0.003127, validation loss: 0.002231\n",
      "iteration 5011, train loss: 0.003093, validation loss: 0.002204\n",
      "iteration 5012, train loss: 0.003012, validation loss: 0.002112\n",
      "iteration 5013, train loss: 0.003038, validation loss: 0.002063\n",
      "iteration 5014, train loss: 0.002806, validation loss: 0.001952\n",
      "iteration 5015, train loss: 0.002926, validation loss: 0.002003\n",
      "iteration 5016, train loss: 0.003292, validation loss: 0.002074\n",
      "iteration 5017, train loss: 0.00337, validation loss: 0.002263\n",
      "iteration 5018, train loss: 0.003443, validation loss: 0.002201\n",
      "iteration 5019, train loss: 0.002998, validation loss: 0.002177\n",
      "iteration 5020, train loss: 0.003175, validation loss: 0.002085\n",
      "iteration 5021, train loss: 0.003105, validation loss: 0.002203\n",
      "iteration 5022, train loss: 0.00336, validation loss: 0.002447\n",
      "iteration 5023, train loss: 0.00352, validation loss: 0.002105\n",
      "iteration 5024, train loss: 0.003224, validation loss: 0.002265\n",
      "iteration 5025, train loss: 0.0036, validation loss: 0.002208\n",
      "iteration 5026, train loss: 0.003224, validation loss: 0.001993\n",
      "iteration 5027, train loss: 0.003257, validation loss: 0.002312\n",
      "iteration 5028, train loss: 0.003258, validation loss: 0.002422\n",
      "iteration 5029, train loss: 0.003251, validation loss: 0.002157\n",
      "iteration 5030, train loss: 0.002919, validation loss: 0.002206\n",
      "iteration 5031, train loss: 0.003111, validation loss: 0.002314\n",
      "iteration 5032, train loss: 0.003685, validation loss: 0.002373\n",
      "iteration 5033, train loss: 0.003153, validation loss: 0.002412\n",
      "iteration 5034, train loss: 0.003166, validation loss: 0.002267\n",
      "iteration 5035, train loss: 0.003258, validation loss: 0.00244\n",
      "iteration 5036, train loss: 0.003877, validation loss: 0.002355\n",
      "iteration 5037, train loss: 0.003059, validation loss: 0.002474\n",
      "iteration 5038, train loss: 0.003302, validation loss: 0.002216\n",
      "iteration 5039, train loss: 0.003021, validation loss: 0.002223\n",
      "iteration 5040, train loss: 0.003054, validation loss: 0.002415\n",
      "iteration 5041, train loss: 0.003198, validation loss: 0.002362\n",
      "iteration 5042, train loss: 0.003763, validation loss: 0.002339\n",
      "iteration 5043, train loss: 0.003539, validation loss: 0.002174\n",
      "iteration 5044, train loss: 0.002981, validation loss: 0.002445\n",
      "iteration 5045, train loss: 0.003623, validation loss: 0.002178\n",
      "iteration 5046, train loss: 0.003563, validation loss: 0.002103\n",
      "iteration 5047, train loss: 0.003162, validation loss: 0.00268\n",
      "iteration 5048, train loss: 0.003688, validation loss: 0.002305\n",
      "iteration 5049, train loss: 0.003179, validation loss: 0.002396\n",
      "iteration 5050, train loss: 0.003428, validation loss: 0.002902\n",
      "iteration 5051, train loss: 0.004023, validation loss: 0.00265\n",
      "iteration 5052, train loss: 0.003643, validation loss: 0.002127\n",
      "iteration 5053, train loss: 0.003227, validation loss: 0.002284\n",
      "iteration 5054, train loss: 0.003298, validation loss: 0.002442\n",
      "iteration 5055, train loss: 0.00318, validation loss: 0.002165\n",
      "iteration 5056, train loss: 0.002997, validation loss: 0.00214\n",
      "iteration 5057, train loss: 0.003706, validation loss: 0.002377\n",
      "iteration 5058, train loss: 0.003152, validation loss: 0.002748\n",
      "iteration 5059, train loss: 0.003612, validation loss: 0.002331\n",
      "iteration 5060, train loss: 0.003461, validation loss: 0.002086\n",
      "iteration 5061, train loss: 0.00303, validation loss: 0.002848\n",
      "iteration 5062, train loss: 0.00362, validation loss: 0.002913\n",
      "iteration 5063, train loss: 0.003953, validation loss: 0.002208\n",
      "iteration 5064, train loss: 0.003081, validation loss: 0.002509\n",
      "iteration 5065, train loss: 0.003448, validation loss: 0.00289\n",
      "iteration 5066, train loss: 0.003911, validation loss: 0.002789\n",
      "iteration 5067, train loss: 0.003812, validation loss: 0.002721\n",
      "iteration 5068, train loss: 0.003466, validation loss: 0.002173\n",
      "iteration 5069, train loss: 0.003029, validation loss: 0.002274\n",
      "iteration 5070, train loss: 0.00354, validation loss: 0.002518\n",
      "iteration 5071, train loss: 0.00341, validation loss: 0.001954\n",
      "iteration 5072, train loss: 0.00289, validation loss: 0.002315\n",
      "iteration 5073, train loss: 0.003415, validation loss: 0.0025\n",
      "iteration 5074, train loss: 0.003526, validation loss: 0.002175\n",
      "iteration 5075, train loss: 0.003118, validation loss: 0.002129\n",
      "iteration 5076, train loss: 0.003082, validation loss: 0.002071\n",
      "iteration 5077, train loss: 0.003297, validation loss: 0.002183\n",
      "iteration 5078, train loss: 0.003264, validation loss: 0.002499\n",
      "iteration 5079, train loss: 0.003476, validation loss: 0.002207\n",
      "iteration 5080, train loss: 0.003685, validation loss: 0.002094\n",
      "iteration 5081, train loss: 0.003154, validation loss: 0.002573\n",
      "iteration 5082, train loss: 0.003477, validation loss: 0.00206\n",
      "iteration 5083, train loss: 0.003107, validation loss: 0.002355\n",
      "iteration 5084, train loss: 0.003185, validation loss: 0.002933\n",
      "iteration 5085, train loss: 0.004036, validation loss: 0.002471\n",
      "iteration 5086, train loss: 0.003625, validation loss: 0.002025\n",
      "iteration 5087, train loss: 0.002969, validation loss: 0.00305\n",
      "iteration 5088, train loss: 0.004621, validation loss: 0.002219\n",
      "iteration 5089, train loss: 0.003338, validation loss: 0.002215\n",
      "iteration 5090, train loss: 0.003362, validation loss: 0.002643\n",
      "iteration 5091, train loss: 0.004009, validation loss: 0.002499\n",
      "iteration 5092, train loss: 0.003741, validation loss: 0.002404\n",
      "iteration 5093, train loss: 0.003231, validation loss: 0.002439\n",
      "iteration 5094, train loss: 0.003554, validation loss: 0.002137\n",
      "iteration 5095, train loss: 0.003297, validation loss: 0.002195\n",
      "iteration 5096, train loss: 0.003024, validation loss: 0.002466\n",
      "iteration 5097, train loss: 0.003941, validation loss: 0.002005\n",
      "iteration 5098, train loss: 0.002924, validation loss: 0.002009\n",
      "iteration 5099, train loss: 0.003141, validation loss: 0.002499\n",
      "iteration 5100, train loss: 0.003913, validation loss: 0.002225\n",
      "iteration 5101, train loss: 0.003264, validation loss: 0.002152\n",
      "iteration 5102, train loss: 0.002952, validation loss: 0.002438\n",
      "iteration 5103, train loss: 0.003434, validation loss: 0.002253\n",
      "iteration 5104, train loss: 0.003651, validation loss: 0.002149\n",
      "iteration 5105, train loss: 0.002984, validation loss: 0.002585\n",
      "iteration 5106, train loss: 0.003594, validation loss: 0.002298\n",
      "iteration 5107, train loss: 0.003671, validation loss: 0.002056\n",
      "iteration 5108, train loss: 0.002915, validation loss: 0.002422\n",
      "iteration 5109, train loss: 0.003534, validation loss: 0.00227\n",
      "iteration 5110, train loss: 0.003281, validation loss: 0.002022\n",
      "iteration 5111, train loss: 0.002768, validation loss: 0.002584\n",
      "iteration 5112, train loss: 0.003861, validation loss: 0.002382\n",
      "iteration 5113, train loss: 0.003814, validation loss: \u001b[92m0.001921\u001b[0m\n",
      "iteration 5114, train loss: 0.003, validation loss: 0.002059\n",
      "iteration 5115, train loss: 0.002776, validation loss: 0.002256\n",
      "iteration 5116, train loss: 0.003739, validation loss: 0.001953\n",
      "iteration 5117, train loss: 0.002982, validation loss: 0.002099\n",
      "iteration 5118, train loss: 0.003219, validation loss: 0.002107\n",
      "iteration 5119, train loss: 0.003286, validation loss: \u001b[92m0.001886\u001b[0m\n",
      "iteration 5120, train loss: 0.002824, validation loss: 0.001995\n",
      "iteration 5121, train loss: 0.0029, validation loss: 0.002071\n",
      "iteration 5122, train loss: 0.002947, validation loss: 0.001982\n",
      "iteration 5123, train loss: 0.003147, validation loss: 0.001989\n",
      "iteration 5124, train loss: 0.003167, validation loss: 0.002041\n",
      "iteration 5125, train loss: 0.002867, validation loss: 0.002065\n",
      "iteration 5126, train loss: 0.003029, validation loss: 0.002197\n",
      "iteration 5127, train loss: 0.003281, validation loss: 0.002202\n",
      "iteration 5128, train loss: 0.002995, validation loss: 0.002176\n",
      "iteration 5129, train loss: 0.003077, validation loss: 0.001986\n",
      "iteration 5130, train loss: 0.002994, validation loss: 0.001963\n",
      "iteration 5131, train loss: 0.002992, validation loss: 0.002122\n",
      "iteration 5132, train loss: 0.002895, validation loss: 0.001991\n",
      "iteration 5133, train loss: 0.003102, validation loss: 0.001956\n",
      "iteration 5134, train loss: 0.003012, validation loss: 0.002039\n",
      "iteration 5135, train loss: 0.002928, validation loss: 0.001916\n",
      "iteration 5136, train loss: 0.002894, validation loss: 0.001996\n",
      "iteration 5137, train loss: 0.003374, validation loss: 0.002079\n",
      "iteration 5138, train loss: 0.003345, validation loss: 0.002211\n",
      "iteration 5139, train loss: 0.003189, validation loss: 0.002337\n",
      "iteration 5140, train loss: 0.003324, validation loss: 0.002111\n",
      "iteration 5141, train loss: 0.003028, validation loss: 0.002126\n",
      "iteration 5142, train loss: 0.002901, validation loss: 0.002284\n",
      "iteration 5143, train loss: 0.00322, validation loss: 0.001995\n",
      "iteration 5144, train loss: 0.003415, validation loss: 0.002094\n",
      "iteration 5145, train loss: 0.003416, validation loss: 0.002468\n",
      "iteration 5146, train loss: 0.003484, validation loss: 0.002434\n",
      "iteration 5147, train loss: 0.003411, validation loss: 0.002257\n",
      "iteration 5148, train loss: 0.003081, validation loss: 0.002421\n",
      "iteration 5149, train loss: 0.003009, validation loss: 0.002158\n",
      "iteration 5150, train loss: 0.002867, validation loss: 0.002246\n",
      "iteration 5151, train loss: 0.003194, validation loss: 0.002354\n",
      "iteration 5152, train loss: 0.003178, validation loss: 0.002114\n",
      "iteration 5153, train loss: 0.00303, validation loss: 0.00218\n",
      "iteration 5154, train loss: 0.003212, validation loss: 0.002416\n",
      "iteration 5155, train loss: 0.003153, validation loss: 0.002428\n",
      "iteration 5156, train loss: 0.003342, validation loss: 0.002183\n",
      "iteration 5157, train loss: 0.00322, validation loss: 0.002341\n",
      "iteration 5158, train loss: 0.003308, validation loss: 0.002154\n",
      "iteration 5159, train loss: 0.003338, validation loss: 0.001965\n",
      "iteration 5160, train loss: 0.003148, validation loss: 0.002267\n",
      "iteration 5161, train loss: 0.003031, validation loss: 0.002002\n",
      "iteration 5162, train loss: 0.003021, validation loss: 0.002064\n",
      "iteration 5163, train loss: 0.003119, validation loss: 0.002155\n",
      "iteration 5164, train loss: 0.003415, validation loss: 0.001962\n",
      "iteration 5165, train loss: 0.00307, validation loss: 0.001948\n",
      "iteration 5166, train loss: 0.002961, validation loss: 0.002099\n",
      "iteration 5167, train loss: 0.003028, validation loss: 0.002202\n",
      "iteration 5168, train loss: 0.002976, validation loss: 0.002255\n",
      "iteration 5169, train loss: 0.003129, validation loss: 0.002193\n",
      "iteration 5170, train loss: 0.003042, validation loss: 0.00215\n",
      "iteration 5171, train loss: 0.003072, validation loss: 0.002064\n",
      "iteration 5172, train loss: 0.003132, validation loss: 0.001986\n",
      "iteration 5173, train loss: 0.003389, validation loss: 0.001976\n",
      "iteration 5174, train loss: 0.002866, validation loss: 0.002144\n",
      "iteration 5175, train loss: 0.002934, validation loss: 0.002363\n",
      "iteration 5176, train loss: 0.00314, validation loss: 0.002282\n",
      "iteration 5177, train loss: 0.003216, validation loss: 0.001923\n",
      "iteration 5178, train loss: 0.003185, validation loss: 0.002167\n",
      "iteration 5179, train loss: 0.003479, validation loss: 0.002205\n",
      "iteration 5180, train loss: 0.003259, validation loss: 0.002127\n",
      "iteration 5181, train loss: 0.003313, validation loss: 0.001983\n",
      "iteration 5182, train loss: 0.003081, validation loss: 0.002078\n",
      "iteration 5183, train loss: 0.002664, validation loss: 0.002284\n",
      "iteration 5184, train loss: 0.003354, validation loss: 0.002031\n",
      "iteration 5185, train loss: 0.003254, validation loss: 0.00194\n",
      "iteration 5186, train loss: 0.003082, validation loss: 0.002099\n",
      "iteration 5187, train loss: 0.002878, validation loss: 0.0021\n",
      "iteration 5188, train loss: 0.002984, validation loss: 0.001957\n",
      "iteration 5189, train loss: 0.002819, validation loss: 0.002137\n",
      "iteration 5190, train loss: 0.003138, validation loss: 0.00223\n",
      "iteration 5191, train loss: 0.003107, validation loss: 0.002007\n",
      "iteration 5192, train loss: 0.003299, validation loss: 0.001897\n",
      "iteration 5193, train loss: 0.002974, validation loss: 0.001952\n",
      "iteration 5194, train loss: 0.002764, validation loss: 0.00197\n",
      "iteration 5195, train loss: 0.002835, validation loss: 0.001933\n",
      "iteration 5196, train loss: 0.002796, validation loss: 0.00195\n",
      "iteration 5197, train loss: 0.00289, validation loss: 0.001981\n",
      "iteration 5198, train loss: 0.003152, validation loss: 0.001911\n",
      "iteration 5199, train loss: 0.003007, validation loss: 0.00193\n",
      "iteration 5200, train loss: 0.003118, validation loss: 0.002021\n",
      "iteration 5201, train loss: 0.003108, validation loss: 0.001923\n",
      "iteration 5202, train loss: 0.002724, validation loss: 0.001971\n",
      "iteration 5203, train loss: 0.002935, validation loss: 0.002119\n",
      "iteration 5204, train loss: 0.003225, validation loss: 0.0021\n",
      "iteration 5205, train loss: 0.003048, validation loss: 0.002017\n",
      "iteration 5206, train loss: 0.002786, validation loss: 0.002074\n",
      "iteration 5207, train loss: 0.003102, validation loss: 0.001959\n",
      "iteration 5208, train loss: 0.003031, validation loss: 0.00199\n",
      "iteration 5209, train loss: 0.003309, validation loss: 0.002128\n",
      "iteration 5210, train loss: 0.002768, validation loss: 0.002122\n",
      "iteration 5211, train loss: 0.003176, validation loss: 0.002028\n",
      "iteration 5212, train loss: 0.003197, validation loss: 0.002107\n",
      "iteration 5213, train loss: 0.003121, validation loss: 0.001996\n",
      "iteration 5214, train loss: 0.002935, validation loss: 0.002044\n",
      "iteration 5215, train loss: 0.003404, validation loss: 0.002063\n",
      "iteration 5216, train loss: 0.003094, validation loss: 0.002323\n",
      "iteration 5217, train loss: 0.003183, validation loss: 0.002085\n",
      "iteration 5218, train loss: 0.003006, validation loss: 0.001946\n",
      "iteration 5219, train loss: 0.002947, validation loss: 0.002181\n",
      "iteration 5220, train loss: 0.003123, validation loss: 0.002106\n",
      "iteration 5221, train loss: 0.003267, validation loss: 0.00212\n",
      "iteration 5222, train loss: 0.002998, validation loss: 0.002207\n",
      "iteration 5223, train loss: 0.003117, validation loss: 0.002288\n",
      "iteration 5224, train loss: 0.00323, validation loss: 0.002131\n",
      "iteration 5225, train loss: 0.003102, validation loss: 0.002082\n",
      "iteration 5226, train loss: 0.003217, validation loss: 0.002146\n",
      "iteration 5227, train loss: 0.002931, validation loss: 0.002196\n",
      "iteration 5228, train loss: 0.003459, validation loss: 0.002004\n",
      "iteration 5229, train loss: 0.002888, validation loss: 0.002317\n",
      "iteration 5230, train loss: 0.003498, validation loss: 0.002152\n",
      "iteration 5231, train loss: 0.003241, validation loss: 0.002204\n",
      "iteration 5232, train loss: 0.003547, validation loss: 0.002305\n",
      "iteration 5233, train loss: 0.003694, validation loss: 0.002097\n",
      "iteration 5234, train loss: 0.003165, validation loss: 0.002338\n",
      "iteration 5235, train loss: 0.00293, validation loss: 0.002186\n",
      "iteration 5236, train loss: 0.003075, validation loss: 0.001929\n",
      "iteration 5237, train loss: 0.003037, validation loss: 0.002042\n",
      "iteration 5238, train loss: 0.002669, validation loss: 0.002136\n",
      "iteration 5239, train loss: 0.002679, validation loss: 0.002202\n",
      "iteration 5240, train loss: 0.003131, validation loss: 0.002024\n",
      "iteration 5241, train loss: 0.002908, validation loss: 0.00206\n",
      "iteration 5242, train loss: 0.003024, validation loss: 0.002146\n",
      "iteration 5243, train loss: 0.003267, validation loss: 0.002216\n",
      "iteration 5244, train loss: 0.003428, validation loss: 0.001936\n",
      "iteration 5245, train loss: 0.002721, validation loss: 0.002184\n",
      "iteration 5246, train loss: 0.002907, validation loss: 0.002446\n",
      "iteration 5247, train loss: 0.003465, validation loss: 0.002221\n",
      "iteration 5248, train loss: 0.003204, validation loss: 0.002113\n",
      "iteration 5249, train loss: 0.003065, validation loss: 0.002483\n",
      "iteration 5250, train loss: 0.003186, validation loss: 0.00263\n",
      "iteration 5251, train loss: 0.003892, validation loss: 0.002087\n",
      "iteration 5252, train loss: 0.003123, validation loss: 0.002069\n",
      "iteration 5253, train loss: 0.002783, validation loss: 0.002827\n",
      "iteration 5254, train loss: 0.003594, validation loss: 0.002842\n",
      "iteration 5255, train loss: 0.003543, validation loss: 0.002236\n",
      "iteration 5256, train loss: 0.003591, validation loss: 0.002046\n",
      "iteration 5257, train loss: 0.00306, validation loss: 0.002622\n",
      "iteration 5258, train loss: 0.003761, validation loss: 0.002591\n",
      "iteration 5259, train loss: 0.003414, validation loss: 0.002001\n",
      "iteration 5260, train loss: 0.00321, validation loss: 0.002045\n",
      "iteration 5261, train loss: 0.002909, validation loss: 0.002497\n",
      "iteration 5262, train loss: 0.00336, validation loss: 0.002332\n",
      "iteration 5263, train loss: 0.003276, validation loss: 0.001949\n",
      "iteration 5264, train loss: 0.002915, validation loss: 0.002229\n",
      "iteration 5265, train loss: 0.003007, validation loss: 0.002436\n",
      "iteration 5266, train loss: 0.003339, validation loss: 0.002268\n",
      "iteration 5267, train loss: 0.002969, validation loss: 0.00203\n",
      "iteration 5268, train loss: 0.002718, validation loss: 0.001978\n",
      "iteration 5269, train loss: 0.002951, validation loss: 0.002174\n",
      "iteration 5270, train loss: 0.003023, validation loss: 0.00212\n",
      "iteration 5271, train loss: 0.003072, validation loss: 0.002143\n",
      "iteration 5272, train loss: 0.003212, validation loss: 0.002253\n",
      "iteration 5273, train loss: 0.003166, validation loss: 0.002488\n",
      "iteration 5274, train loss: 0.003465, validation loss: 0.002407\n",
      "iteration 5275, train loss: 0.003484, validation loss: 0.002006\n",
      "iteration 5276, train loss: 0.003132, validation loss: 0.002097\n",
      "iteration 5277, train loss: 0.002828, validation loss: 0.002447\n",
      "iteration 5278, train loss: 0.003175, validation loss: 0.002268\n",
      "iteration 5279, train loss: 0.003393, validation loss: 0.001891\n",
      "iteration 5280, train loss: 0.003026, validation loss: 0.002094\n",
      "iteration 5281, train loss: 0.002922, validation loss: 0.002383\n",
      "iteration 5282, train loss: 0.003208, validation loss: 0.002124\n",
      "iteration 5283, train loss: 0.003052, validation loss: 0.002044\n",
      "iteration 5284, train loss: 0.003327, validation loss: 0.002071\n",
      "iteration 5285, train loss: 0.0031, validation loss: 0.002036\n",
      "iteration 5286, train loss: 0.00296, validation loss: 0.002091\n",
      "iteration 5287, train loss: 0.003514, validation loss: 0.001888\n",
      "iteration 5288, train loss: 0.002912, validation loss: 0.002144\n",
      "iteration 5289, train loss: 0.003068, validation loss: 0.002128\n",
      "iteration 5290, train loss: 0.003577, validation loss: 0.001899\n",
      "iteration 5291, train loss: 0.002708, validation loss: 0.002042\n",
      "iteration 5292, train loss: 0.002794, validation loss: 0.002223\n",
      "iteration 5293, train loss: 0.003293, validation loss: 0.002013\n",
      "iteration 5294, train loss: 0.002913, validation loss: 0.002022\n",
      "iteration 5295, train loss: 0.003005, validation loss: 0.002071\n",
      "iteration 5296, train loss: 0.003302, validation loss: 0.002038\n",
      "iteration 5297, train loss: 0.003368, validation loss: 0.002099\n",
      "iteration 5298, train loss: 0.003007, validation loss: 0.001943\n",
      "iteration 5299, train loss: 0.003202, validation loss: 0.00201\n",
      "iteration 5300, train loss: 0.00285, validation loss: 0.002322\n",
      "iteration 5301, train loss: 0.002742, validation loss: 0.002372\n",
      "iteration 5302, train loss: 0.003721, validation loss: 0.001897\n",
      "iteration 5303, train loss: 0.002913, validation loss: 0.002033\n",
      "iteration 5304, train loss: 0.002897, validation loss: 0.002323\n",
      "iteration 5305, train loss: 0.003221, validation loss: 0.001898\n",
      "iteration 5306, train loss: 0.002993, validation loss: 0.002229\n",
      "iteration 5307, train loss: 0.003209, validation loss: 0.002385\n",
      "iteration 5308, train loss: 0.003312, validation loss: 0.001936\n",
      "iteration 5309, train loss: 0.002737, validation loss: 0.001972\n",
      "iteration 5310, train loss: 0.003368, validation loss: 0.001935\n",
      "iteration 5311, train loss: 0.003149, validation loss: 0.001923\n",
      "iteration 5312, train loss: 0.002817, validation loss: 0.002181\n",
      "iteration 5313, train loss: 0.002862, validation loss: 0.002101\n",
      "iteration 5314, train loss: 0.002889, validation loss: \u001b[92m0.001867\u001b[0m\n",
      "iteration 5315, train loss: 0.002831, validation loss: 0.002209\n",
      "iteration 5316, train loss: 0.003057, validation loss: 0.002258\n",
      "iteration 5317, train loss: 0.002957, validation loss: 0.002005\n",
      "iteration 5318, train loss: 0.002908, validation loss: 0.00202\n",
      "iteration 5319, train loss: 0.003148, validation loss: 0.002214\n",
      "iteration 5320, train loss: 0.003148, validation loss: 0.002209\n",
      "iteration 5321, train loss: 0.003417, validation loss: 0.002124\n",
      "iteration 5322, train loss: 0.003059, validation loss: 0.002219\n",
      "iteration 5323, train loss: 0.003363, validation loss: 0.002076\n",
      "iteration 5324, train loss: 0.003031, validation loss: 0.002067\n",
      "iteration 5325, train loss: 0.002838, validation loss: 0.002043\n",
      "iteration 5326, train loss: 0.002777, validation loss: 0.001994\n",
      "iteration 5327, train loss: 0.002767, validation loss: 0.001953\n",
      "iteration 5328, train loss: 0.00291, validation loss: 0.001913\n",
      "iteration 5329, train loss: 0.003047, validation loss: 0.001932\n",
      "iteration 5330, train loss: 0.002942, validation loss: 0.001996\n",
      "iteration 5331, train loss: 0.002831, validation loss: 0.001985\n",
      "iteration 5332, train loss: \u001b[92m0.002564\u001b[0m, validation loss: 0.002146\n",
      "iteration 5333, train loss: 0.003043, validation loss: 0.002068\n",
      "iteration 5334, train loss: 0.002909, validation loss: 0.001955\n",
      "iteration 5335, train loss: 0.002842, validation loss: 0.00196\n",
      "iteration 5336, train loss: 0.002896, validation loss: 0.001976\n",
      "iteration 5337, train loss: 0.002918, validation loss: 0.002041\n",
      "iteration 5338, train loss: 0.00307, validation loss: 0.001927\n",
      "iteration 5339, train loss: 0.002918, validation loss: 0.001945\n",
      "iteration 5340, train loss: 0.002792, validation loss: 0.002072\n",
      "iteration 5341, train loss: 0.003064, validation loss: 0.001996\n",
      "iteration 5342, train loss: 0.002992, validation loss: 0.001989\n",
      "iteration 5343, train loss: 0.00278, validation loss: 0.002215\n",
      "iteration 5344, train loss: 0.00301, validation loss: 0.002033\n",
      "iteration 5345, train loss: 0.002727, validation loss: \u001b[92m0.001844\u001b[0m\n",
      "iteration 5346, train loss: 0.00297, validation loss: 0.002111\n",
      "iteration 5347, train loss: 0.003109, validation loss: 0.002082\n",
      "iteration 5348, train loss: 0.003026, validation loss: 0.002163\n",
      "iteration 5349, train loss: 0.003011, validation loss: 0.002335\n",
      "iteration 5350, train loss: 0.003227, validation loss: 0.00213\n",
      "iteration 5351, train loss: 0.002849, validation loss: 0.001956\n",
      "iteration 5352, train loss: 0.003171, validation loss: 0.002121\n",
      "iteration 5353, train loss: 0.002945, validation loss: 0.00222\n",
      "iteration 5354, train loss: 0.00347, validation loss: 0.001976\n",
      "iteration 5355, train loss: 0.002755, validation loss: 0.00199\n",
      "iteration 5356, train loss: 0.002879, validation loss: 0.002153\n",
      "iteration 5357, train loss: 0.002955, validation loss: 0.002011\n",
      "iteration 5358, train loss: 0.002884, validation loss: 0.001907\n",
      "iteration 5359, train loss: 0.00314, validation loss: 0.001983\n",
      "iteration 5360, train loss: 0.002972, validation loss: 0.002061\n",
      "iteration 5361, train loss: 0.003072, validation loss: 0.001993\n",
      "iteration 5362, train loss: 0.002958, validation loss: 0.001894\n",
      "iteration 5363, train loss: 0.002992, validation loss: 0.002048\n",
      "iteration 5364, train loss: 0.003118, validation loss: 0.002282\n",
      "iteration 5365, train loss: 0.003396, validation loss: 0.002006\n",
      "iteration 5366, train loss: 0.003175, validation loss: \u001b[92m0.001838\u001b[0m\n",
      "iteration 5367, train loss: 0.002891, validation loss: 0.002154\n",
      "iteration 5368, train loss: 0.002915, validation loss: 0.002567\n",
      "iteration 5369, train loss: 0.003286, validation loss: 0.0024\n",
      "iteration 5370, train loss: 0.003375, validation loss: 0.001937\n",
      "iteration 5371, train loss: 0.003032, validation loss: 0.00213\n",
      "iteration 5372, train loss: 0.003001, validation loss: 0.002362\n",
      "iteration 5373, train loss: 0.003562, validation loss: 0.002185\n",
      "iteration 5374, train loss: 0.003524, validation loss: 0.001855\n",
      "iteration 5375, train loss: 0.002817, validation loss: 0.002085\n",
      "iteration 5376, train loss: 0.002919, validation loss: 0.00229\n",
      "iteration 5377, train loss: 0.003647, validation loss: 0.002151\n",
      "iteration 5378, train loss: 0.003317, validation loss: 0.002088\n",
      "iteration 5379, train loss: 0.003197, validation loss: 0.002194\n",
      "iteration 5380, train loss: 0.003052, validation loss: 0.002279\n",
      "iteration 5381, train loss: 0.003059, validation loss: 0.002039\n",
      "iteration 5382, train loss: 0.002902, validation loss: 0.001969\n",
      "iteration 5383, train loss: 0.002864, validation loss: 0.002171\n",
      "iteration 5384, train loss: 0.00321, validation loss: 0.002035\n",
      "iteration 5385, train loss: 0.003046, validation loss: 0.001921\n",
      "iteration 5386, train loss: 0.002917, validation loss: 0.001886\n",
      "iteration 5387, train loss: 0.002814, validation loss: 0.001946\n",
      "iteration 5388, train loss: 0.002939, validation loss: 0.001936\n",
      "iteration 5389, train loss: 0.003258, validation loss: 0.001902\n",
      "iteration 5390, train loss: 0.00287, validation loss: 0.001984\n",
      "iteration 5391, train loss: 0.003199, validation loss: 0.001913\n",
      "iteration 5392, train loss: 0.002951, validation loss: 0.001906\n",
      "iteration 5393, train loss: 0.003062, validation loss: 0.002042\n",
      "iteration 5394, train loss: 0.003094, validation loss: 0.002064\n",
      "iteration 5395, train loss: 0.002967, validation loss: 0.002147\n",
      "iteration 5396, train loss: 0.003078, validation loss: 0.002289\n",
      "iteration 5397, train loss: 0.003061, validation loss: 0.002162\n",
      "iteration 5398, train loss: 0.003531, validation loss: 0.002008\n",
      "iteration 5399, train loss: 0.002976, validation loss: 0.002427\n",
      "iteration 5400, train loss: 0.003021, validation loss: 0.002578\n",
      "iteration 5401, train loss: 0.003354, validation loss: 0.002251\n",
      "iteration 5402, train loss: 0.003198, validation loss: 0.002587\n",
      "iteration 5403, train loss: 0.003411, validation loss: 0.002839\n",
      "iteration 5404, train loss: 0.004514, validation loss: 0.002465\n",
      "iteration 5405, train loss: 0.003665, validation loss: 0.002275\n",
      "iteration 5406, train loss: 0.003448, validation loss: 0.002181\n",
      "iteration 5407, train loss: 0.002897, validation loss: 0.002704\n",
      "iteration 5408, train loss: 0.004033, validation loss: 0.002484\n",
      "iteration 5409, train loss: 0.003357, validation loss: 0.00223\n",
      "iteration 5410, train loss: 0.003075, validation loss: 0.002263\n",
      "iteration 5411, train loss: 0.003156, validation loss: 0.002359\n",
      "iteration 5412, train loss: 0.003289, validation loss: 0.002068\n",
      "iteration 5413, train loss: 0.003051, validation loss: 0.001916\n",
      "iteration 5414, train loss: 0.002955, validation loss: 0.002138\n",
      "iteration 5415, train loss: 0.003253, validation loss: 0.002062\n",
      "iteration 5416, train loss: 0.003116, validation loss: 0.001894\n",
      "iteration 5417, train loss: 0.002847, validation loss: 0.002041\n",
      "iteration 5418, train loss: 0.002809, validation loss: 0.002401\n",
      "iteration 5419, train loss: 0.003388, validation loss: 0.002126\n",
      "iteration 5420, train loss: 0.003062, validation loss: 0.001971\n",
      "iteration 5421, train loss: 0.002811, validation loss: 0.002272\n",
      "iteration 5422, train loss: 0.003537, validation loss: 0.002155\n",
      "iteration 5423, train loss: 0.002773, validation loss: 0.002019\n",
      "iteration 5424, train loss: 0.002854, validation loss: 0.002129\n",
      "iteration 5425, train loss: 0.002956, validation loss: 0.002233\n",
      "iteration 5426, train loss: 0.00313, validation loss: 0.002008\n",
      "iteration 5427, train loss: 0.002976, validation loss: 0.001907\n",
      "iteration 5428, train loss: 0.003009, validation loss: 0.002006\n",
      "iteration 5429, train loss: 0.002776, validation loss: 0.002004\n",
      "iteration 5430, train loss: 0.00294, validation loss: 0.002038\n",
      "iteration 5431, train loss: 0.002876, validation loss: 0.001935\n",
      "iteration 5432, train loss: 0.002785, validation loss: 0.001902\n",
      "iteration 5433, train loss: 0.00285, validation loss: 0.002093\n",
      "iteration 5434, train loss: 0.003201, validation loss: 0.002084\n",
      "iteration 5435, train loss: 0.003195, validation loss: 0.001983\n",
      "iteration 5436, train loss: 0.003048, validation loss: 0.002212\n",
      "iteration 5437, train loss: 0.003364, validation loss: 0.002034\n",
      "iteration 5438, train loss: 0.002992, validation loss: 0.001865\n",
      "iteration 5439, train loss: 0.002888, validation loss: 0.001906\n",
      "iteration 5440, train loss: 0.002967, validation loss: 0.001888\n",
      "iteration 5441, train loss: 0.002769, validation loss: 0.001929\n",
      "iteration 5442, train loss: 0.003095, validation loss: 0.001851\n",
      "iteration 5443, train loss: 0.003136, validation loss: \u001b[92m0.001834\u001b[0m\n",
      "iteration 5444, train loss: 0.002809, validation loss: 0.001859\n",
      "iteration 5445, train loss: 0.002668, validation loss: 0.00201\n",
      "iteration 5446, train loss: 0.00313, validation loss: 0.002021\n",
      "iteration 5447, train loss: 0.003156, validation loss: 0.001986\n",
      "iteration 5448, train loss: 0.00288, validation loss: 0.002072\n",
      "iteration 5449, train loss: 0.002979, validation loss: 0.001983\n",
      "iteration 5450, train loss: 0.002787, validation loss: 0.001952\n",
      "iteration 5451, train loss: 0.003207, validation loss: 0.001929\n",
      "iteration 5452, train loss: 0.00329, validation loss: 0.002171\n",
      "iteration 5453, train loss: 0.003089, validation loss: 0.002379\n",
      "iteration 5454, train loss: 0.003176, validation loss: 0.00218\n",
      "iteration 5455, train loss: 0.002803, validation loss: 0.001879\n",
      "iteration 5456, train loss: 0.002793, validation loss: 0.002053\n",
      "iteration 5457, train loss: 0.003039, validation loss: 0.002295\n",
      "iteration 5458, train loss: 0.00303, validation loss: 0.002077\n",
      "iteration 5459, train loss: 0.003223, validation loss: 0.001968\n",
      "iteration 5460, train loss: 0.002769, validation loss: 0.002299\n",
      "iteration 5461, train loss: 0.003335, validation loss: 0.002428\n",
      "iteration 5462, train loss: 0.003176, validation loss: 0.001979\n",
      "iteration 5463, train loss: 0.003052, validation loss: 0.001885\n",
      "iteration 5464, train loss: 0.0028, validation loss: 0.002397\n",
      "iteration 5465, train loss: 0.003081, validation loss: 0.002369\n",
      "iteration 5466, train loss: 0.003261, validation loss: 0.001891\n",
      "iteration 5467, train loss: 0.002636, validation loss: 0.002293\n",
      "iteration 5468, train loss: 0.003214, validation loss: 0.002232\n",
      "iteration 5469, train loss: 0.003588, validation loss: 0.002042\n",
      "iteration 5470, train loss: 0.002978, validation loss: 0.002066\n",
      "iteration 5471, train loss: 0.003128, validation loss: 0.001917\n",
      "iteration 5472, train loss: 0.003182, validation loss: 0.001914\n",
      "iteration 5473, train loss: 0.003022, validation loss: 0.002291\n",
      "iteration 5474, train loss: 0.003471, validation loss: 0.001915\n",
      "iteration 5475, train loss: 0.002758, validation loss: 0.002116\n",
      "iteration 5476, train loss: 0.003099, validation loss: 0.002052\n",
      "iteration 5477, train loss: 0.003188, validation loss: 0.001843\n",
      "iteration 5478, train loss: 0.003096, validation loss: 0.001946\n",
      "iteration 5479, train loss: 0.002731, validation loss: 0.002121\n",
      "iteration 5480, train loss: 0.003462, validation loss: 0.002009\n",
      "iteration 5481, train loss: 0.002863, validation loss: 0.001879\n",
      "iteration 5482, train loss: 0.002886, validation loss: 0.002033\n",
      "iteration 5483, train loss: 0.002895, validation loss: 0.002459\n",
      "iteration 5484, train loss: 0.004032, validation loss: 0.001989\n",
      "iteration 5485, train loss: 0.00312, validation loss: 0.002052\n",
      "iteration 5486, train loss: 0.003011, validation loss: 0.002464\n",
      "iteration 5487, train loss: 0.003572, validation loss: 0.002175\n",
      "iteration 5488, train loss: 0.003494, validation loss: 0.001932\n",
      "iteration 5489, train loss: 0.002919, validation loss: 0.002194\n",
      "iteration 5490, train loss: 0.002898, validation loss: 0.002333\n",
      "iteration 5491, train loss: 0.003041, validation loss: 0.002068\n",
      "iteration 5492, train loss: 0.003184, validation loss: 0.002225\n",
      "iteration 5493, train loss: 0.002896, validation loss: 0.002534\n",
      "iteration 5494, train loss: 0.004092, validation loss: 0.001992\n",
      "iteration 5495, train loss: 0.003062, validation loss: 0.002412\n",
      "iteration 5496, train loss: 0.00368, validation loss: 0.002428\n",
      "iteration 5497, train loss: 0.002829, validation loss: 0.002204\n",
      "iteration 5498, train loss: 0.003211, validation loss: 0.002169\n",
      "iteration 5499, train loss: 0.003246, validation loss: 0.002462\n",
      "iteration 5500, train loss: 0.003779, validation loss: 0.002012\n",
      "iteration 5501, train loss: 0.003015, validation loss: 0.002007\n",
      "iteration 5502, train loss: 0.003076, validation loss: 0.002241\n",
      "iteration 5503, train loss: 0.003152, validation loss: 0.00195\n",
      "iteration 5504, train loss: 0.002993, validation loss: 0.001948\n",
      "iteration 5505, train loss: 0.00299, validation loss: 0.00208\n",
      "iteration 5506, train loss: 0.002829, validation loss: 0.001968\n",
      "iteration 5507, train loss: 0.003085, validation loss: 0.001892\n",
      "iteration 5508, train loss: 0.002859, validation loss: 0.001919\n",
      "iteration 5509, train loss: 0.002917, validation loss: 0.001935\n",
      "iteration 5510, train loss: 0.002705, validation loss: 0.002025\n",
      "iteration 5511, train loss: 0.003127, validation loss: \u001b[92m0.00183\u001b[0m\n",
      "iteration 5512, train loss: 0.002856, validation loss: 0.001904\n",
      "iteration 5513, train loss: 0.002747, validation loss: 0.002057\n",
      "iteration 5514, train loss: 0.003097, validation loss: 0.001883\n",
      "iteration 5515, train loss: 0.002951, validation loss: 0.001962\n",
      "iteration 5516, train loss: 0.003079, validation loss: 0.001955\n",
      "iteration 5517, train loss: 0.002917, validation loss: 0.001851\n",
      "iteration 5518, train loss: 0.002999, validation loss: 0.002034\n",
      "iteration 5519, train loss: 0.002923, validation loss: 0.002112\n",
      "iteration 5520, train loss: 0.003404, validation loss: 0.001941\n",
      "iteration 5521, train loss: 0.003297, validation loss: 0.001958\n",
      "iteration 5522, train loss: 0.002918, validation loss: 0.001953\n",
      "iteration 5523, train loss: 0.002883, validation loss: 0.001958\n",
      "iteration 5524, train loss: 0.003219, validation loss: 0.001899\n",
      "iteration 5525, train loss: 0.002967, validation loss: 0.002002\n",
      "iteration 5526, train loss: 0.002941, validation loss: 0.001958\n",
      "iteration 5527, train loss: 0.002665, validation loss: 0.001869\n",
      "iteration 5528, train loss: 0.003017, validation loss: 0.001906\n",
      "iteration 5529, train loss: 0.002959, validation loss: 0.001898\n",
      "iteration 5530, train loss: 0.002917, validation loss: 0.002003\n",
      "iteration 5531, train loss: 0.002895, validation loss: 0.002038\n",
      "iteration 5532, train loss: 0.003117, validation loss: 0.001882\n",
      "iteration 5533, train loss: 0.002914, validation loss: 0.00213\n",
      "iteration 5534, train loss: 0.003236, validation loss: 0.00183\n",
      "iteration 5535, train loss: 0.002838, validation loss: 0.002025\n",
      "iteration 5536, train loss: 0.003089, validation loss: 0.002177\n",
      "iteration 5537, train loss: 0.003172, validation loss: 0.001938\n",
      "iteration 5538, train loss: 0.002794, validation loss: 0.002029\n",
      "iteration 5539, train loss: 0.002664, validation loss: 0.002373\n",
      "iteration 5540, train loss: 0.00358, validation loss: 0.001958\n",
      "iteration 5541, train loss: 0.002718, validation loss: 0.001992\n",
      "iteration 5542, train loss: 0.002909, validation loss: 0.001963\n",
      "iteration 5543, train loss: 0.002952, validation loss: 0.001915\n",
      "iteration 5544, train loss: 0.002828, validation loss: 0.001855\n",
      "iteration 5545, train loss: 0.002854, validation loss: 0.00185\n",
      "iteration 5546, train loss: 0.002814, validation loss: 0.001998\n",
      "iteration 5547, train loss: 0.002839, validation loss: 0.00215\n",
      "iteration 5548, train loss: 0.002849, validation loss: 0.002018\n",
      "iteration 5549, train loss: 0.002892, validation loss: 0.001912\n",
      "iteration 5550, train loss: 0.002924, validation loss: 0.002038\n",
      "iteration 5551, train loss: 0.003082, validation loss: 0.001996\n",
      "iteration 5552, train loss: 0.002682, validation loss: 0.001986\n",
      "iteration 5553, train loss: 0.002777, validation loss: 0.002037\n",
      "iteration 5554, train loss: 0.002724, validation loss: 0.002198\n",
      "iteration 5555, train loss: 0.002951, validation loss: 0.002266\n",
      "iteration 5556, train loss: 0.00343, validation loss: 0.001839\n",
      "iteration 5557, train loss: 0.002875, validation loss: 0.00194\n",
      "iteration 5558, train loss: 0.003157, validation loss: 0.001942\n",
      "iteration 5559, train loss: 0.003252, validation loss: \u001b[92m0.00182\u001b[0m\n",
      "iteration 5560, train loss: 0.00272, validation loss: 0.002215\n",
      "iteration 5561, train loss: 0.003036, validation loss: 0.002291\n",
      "iteration 5562, train loss: 0.003167, validation loss: 0.001891\n",
      "iteration 5563, train loss: \u001b[92m0.002539\u001b[0m, validation loss: 0.00192\n",
      "iteration 5564, train loss: 0.002896, validation loss: 0.001958\n",
      "iteration 5565, train loss: 0.00285, validation loss: \u001b[92m0.001817\u001b[0m\n",
      "iteration 5566, train loss: 0.002689, validation loss: 0.002145\n",
      "iteration 5567, train loss: 0.002858, validation loss: 0.002307\n",
      "iteration 5568, train loss: 0.003202, validation loss: 0.001984\n",
      "iteration 5569, train loss: 0.003005, validation loss: 0.001833\n",
      "iteration 5570, train loss: 0.003145, validation loss: 0.002066\n",
      "iteration 5571, train loss: 0.002869, validation loss: 0.002221\n",
      "iteration 5572, train loss: 0.003127, validation loss: 0.001902\n",
      "iteration 5573, train loss: 0.003323, validation loss: 0.002049\n",
      "iteration 5574, train loss: 0.002963, validation loss: 0.002556\n",
      "iteration 5575, train loss: 0.00364, validation loss: 0.002095\n",
      "iteration 5576, train loss: 0.003098, validation loss: 0.001829\n",
      "iteration 5577, train loss: 0.002801, validation loss: 0.002161\n",
      "iteration 5578, train loss: 0.003584, validation loss: 0.002066\n",
      "iteration 5579, train loss: 0.003221, validation loss: 0.001956\n",
      "iteration 5580, train loss: 0.003134, validation loss: 0.001983\n",
      "iteration 5581, train loss: 0.002936, validation loss: 0.002087\n",
      "iteration 5582, train loss: 0.002725, validation loss: 0.002145\n",
      "iteration 5583, train loss: 0.003328, validation loss: 0.001945\n",
      "iteration 5584, train loss: 0.003494, validation loss: 0.001955\n",
      "iteration 5585, train loss: 0.00304, validation loss: 0.002148\n",
      "iteration 5586, train loss: 0.00348, validation loss: 0.002226\n",
      "iteration 5587, train loss: 0.003235, validation loss: 0.002023\n",
      "iteration 5588, train loss: 0.003172, validation loss: 0.002001\n",
      "iteration 5589, train loss: \u001b[92m0.00253\u001b[0m, validation loss: 0.002244\n",
      "iteration 5590, train loss: 0.003566, validation loss: 0.001936\n",
      "iteration 5591, train loss: 0.003073, validation loss: 0.00189\n",
      "iteration 5592, train loss: 0.002685, validation loss: 0.001933\n",
      "iteration 5593, train loss: 0.003007, validation loss: 0.001931\n",
      "iteration 5594, train loss: 0.002981, validation loss: 0.001971\n",
      "iteration 5595, train loss: 0.003113, validation loss: 0.001947\n",
      "iteration 5596, train loss: 0.003089, validation loss: 0.001885\n",
      "iteration 5597, train loss: 0.002783, validation loss: 0.001849\n",
      "iteration 5598, train loss: 0.003018, validation loss: 0.001882\n",
      "iteration 5599, train loss: 0.002936, validation loss: 0.001863\n",
      "iteration 5600, train loss: 0.002873, validation loss: 0.001926\n",
      "iteration 5601, train loss: 0.003035, validation loss: 0.002166\n",
      "iteration 5602, train loss: 0.002963, validation loss: 0.002104\n",
      "iteration 5603, train loss: 0.002897, validation loss: 0.001948\n",
      "iteration 5604, train loss: 0.002833, validation loss: \u001b[92m0.001816\u001b[0m\n",
      "iteration 5605, train loss: 0.002719, validation loss: 0.001994\n",
      "iteration 5606, train loss: 0.002987, validation loss: 0.001861\n",
      "iteration 5607, train loss: 0.002851, validation loss: 0.001985\n",
      "iteration 5608, train loss: 0.002897, validation loss: 0.002061\n",
      "iteration 5609, train loss: 0.002882, validation loss: 0.001905\n",
      "iteration 5610, train loss: 0.002617, validation loss: 0.002235\n",
      "iteration 5611, train loss: 0.003277, validation loss: 0.002004\n",
      "iteration 5612, train loss: 0.003045, validation loss: 0.001925\n",
      "iteration 5613, train loss: 0.002919, validation loss: 0.001985\n",
      "iteration 5614, train loss: 0.003112, validation loss: 0.001881\n",
      "iteration 5615, train loss: 0.002746, validation loss: 0.001821\n",
      "iteration 5616, train loss: 0.002677, validation loss: 0.001944\n",
      "iteration 5617, train loss: 0.003231, validation loss: 0.001988\n",
      "iteration 5618, train loss: 0.002772, validation loss: 0.002016\n",
      "iteration 5619, train loss: 0.002952, validation loss: 0.001878\n",
      "iteration 5620, train loss: 0.002904, validation loss: 0.001912\n",
      "iteration 5621, train loss: 0.003481, validation loss: 0.001936\n",
      "iteration 5622, train loss: 0.002756, validation loss: 0.0021\n",
      "iteration 5623, train loss: 0.003026, validation loss: 0.002006\n",
      "iteration 5624, train loss: 0.003057, validation loss: 0.001921\n",
      "iteration 5625, train loss: 0.002668, validation loss: 0.002023\n",
      "iteration 5626, train loss: 0.003178, validation loss: 0.001856\n",
      "iteration 5627, train loss: 0.002661, validation loss: 0.001943\n",
      "iteration 5628, train loss: 0.003074, validation loss: 0.001912\n",
      "iteration 5629, train loss: 0.002846, validation loss: 0.001932\n",
      "iteration 5630, train loss: 0.002924, validation loss: 0.001966\n",
      "iteration 5631, train loss: 0.00304, validation loss: 0.00198\n",
      "iteration 5632, train loss: 0.003008, validation loss: 0.001845\n",
      "iteration 5633, train loss: 0.003202, validation loss: 0.001835\n",
      "iteration 5634, train loss: 0.002907, validation loss: 0.001946\n",
      "iteration 5635, train loss: 0.002799, validation loss: 0.001825\n",
      "iteration 5636, train loss: 0.00275, validation loss: 0.001938\n",
      "iteration 5637, train loss: 0.003343, validation loss: 0.001998\n",
      "iteration 5638, train loss: 0.003104, validation loss: 0.00183\n",
      "iteration 5639, train loss: 0.002553, validation loss: 0.001822\n",
      "iteration 5640, train loss: 0.003029, validation loss: 0.001826\n",
      "iteration 5641, train loss: 0.002837, validation loss: \u001b[92m0.00181\u001b[0m\n",
      "iteration 5642, train loss: 0.002795, validation loss: 0.00186\n",
      "iteration 5643, train loss: 0.002671, validation loss: 0.001927\n",
      "iteration 5644, train loss: 0.00306, validation loss: \u001b[92m0.001806\u001b[0m\n",
      "iteration 5645, train loss: 0.002773, validation loss: 0.00184\n",
      "iteration 5646, train loss: 0.00295, validation loss: 0.001912\n",
      "iteration 5647, train loss: 0.003078, validation loss: 0.001923\n",
      "iteration 5648, train loss: 0.002883, validation loss: 0.001897\n",
      "iteration 5649, train loss: 0.002798, validation loss: 0.001983\n",
      "iteration 5650, train loss: 0.003186, validation loss: 0.001911\n",
      "iteration 5651, train loss: 0.002945, validation loss: 0.00202\n",
      "iteration 5652, train loss: 0.00285, validation loss: 0.001982\n",
      "iteration 5653, train loss: 0.002621, validation loss: 0.001956\n",
      "iteration 5654, train loss: 0.002885, validation loss: 0.001928\n",
      "iteration 5655, train loss: 0.002794, validation loss: 0.002118\n",
      "iteration 5656, train loss: 0.003043, validation loss: 0.001918\n",
      "iteration 5657, train loss: 0.002683, validation loss: 0.001853\n",
      "iteration 5658, train loss: 0.002631, validation loss: 0.002108\n",
      "iteration 5659, train loss: 0.003023, validation loss: 0.001924\n",
      "iteration 5660, train loss: 0.002899, validation loss: 0.001962\n",
      "iteration 5661, train loss: 0.002702, validation loss: 0.002282\n",
      "iteration 5662, train loss: 0.003437, validation loss: 0.001879\n",
      "iteration 5663, train loss: 0.003264, validation loss: 0.002045\n",
      "iteration 5664, train loss: 0.003202, validation loss: 0.002206\n",
      "iteration 5665, train loss: 0.002882, validation loss: 0.00208\n",
      "iteration 5666, train loss: 0.003168, validation loss: 0.001894\n",
      "iteration 5667, train loss: 0.002867, validation loss: 0.002033\n",
      "iteration 5668, train loss: 0.00305, validation loss: 0.001901\n",
      "iteration 5669, train loss: 0.002632, validation loss: 0.00181\n",
      "iteration 5670, train loss: 0.002859, validation loss: 0.001952\n",
      "iteration 5671, train loss: 0.002931, validation loss: 0.002154\n",
      "iteration 5672, train loss: 0.003048, validation loss: 0.001852\n",
      "iteration 5673, train loss: 0.002826, validation loss: 0.001844\n",
      "iteration 5674, train loss: 0.002904, validation loss: 0.002223\n",
      "iteration 5675, train loss: 0.003168, validation loss: 0.002104\n",
      "iteration 5676, train loss: 0.002943, validation loss: 0.001864\n",
      "iteration 5677, train loss: 0.002564, validation loss: 0.002151\n",
      "iteration 5678, train loss: 0.003065, validation loss: 0.002122\n",
      "iteration 5679, train loss: 0.003257, validation loss: 0.001869\n",
      "iteration 5680, train loss: 0.003114, validation loss: 0.00187\n",
      "iteration 5681, train loss: 0.002858, validation loss: 0.002214\n",
      "iteration 5682, train loss: 0.003144, validation loss: 0.002072\n",
      "iteration 5683, train loss: 0.002818, validation loss: 0.001986\n",
      "iteration 5684, train loss: 0.003237, validation loss: 0.002026\n",
      "iteration 5685, train loss: 0.002909, validation loss: 0.002188\n",
      "iteration 5686, train loss: 0.003397, validation loss: 0.002004\n",
      "iteration 5687, train loss: 0.003013, validation loss: 0.001884\n",
      "iteration 5688, train loss: 0.002839, validation loss: 0.002204\n",
      "iteration 5689, train loss: 0.00277, validation loss: 0.002352\n",
      "iteration 5690, train loss: 0.003398, validation loss: 0.002143\n",
      "iteration 5691, train loss: 0.003066, validation loss: 0.001867\n",
      "iteration 5692, train loss: \u001b[92m0.002481\u001b[0m, validation loss: 0.002002\n",
      "iteration 5693, train loss: 0.003001, validation loss: 0.002262\n",
      "iteration 5694, train loss: 0.003136, validation loss: 0.002136\n",
      "iteration 5695, train loss: 0.003185, validation loss: 0.002165\n",
      "iteration 5696, train loss: 0.002907, validation loss: 0.002467\n",
      "iteration 5697, train loss: 0.003639, validation loss: 0.002336\n",
      "iteration 5698, train loss: 0.003353, validation loss: 0.00217\n",
      "iteration 5699, train loss: 0.003228, validation loss: 0.002286\n",
      "iteration 5700, train loss: 0.003079, validation loss: 0.002268\n",
      "iteration 5701, train loss: 0.003118, validation loss: 0.001922\n",
      "iteration 5702, train loss: 0.002689, validation loss: 0.002064\n",
      "iteration 5703, train loss: 0.002879, validation loss: 0.002253\n",
      "iteration 5704, train loss: 0.003129, validation loss: 0.001986\n",
      "iteration 5705, train loss: 0.002817, validation loss: 0.00193\n",
      "iteration 5706, train loss: 0.002709, validation loss: 0.002053\n",
      "iteration 5707, train loss: 0.002741, validation loss: 0.002104\n",
      "iteration 5708, train loss: 0.003199, validation loss: \u001b[92m0.001791\u001b[0m\n",
      "iteration 5709, train loss: 0.002794, validation loss: \u001b[92m0.001778\u001b[0m\n",
      "iteration 5710, train loss: 0.002895, validation loss: 0.001799\n",
      "iteration 5711, train loss: 0.003052, validation loss: 0.001796\n",
      "iteration 5712, train loss: 0.002856, validation loss: 0.001869\n",
      "iteration 5713, train loss: 0.003018, validation loss: 0.001891\n",
      "iteration 5714, train loss: 0.002936, validation loss: 0.001785\n",
      "iteration 5715, train loss: 0.002713, validation loss: 0.001999\n",
      "iteration 5716, train loss: 0.003145, validation loss: 0.002111\n",
      "iteration 5717, train loss: 0.003314, validation loss: 0.001874\n",
      "iteration 5718, train loss: 0.002799, validation loss: 0.002149\n",
      "iteration 5719, train loss: 0.003322, validation loss: 0.001942\n",
      "iteration 5720, train loss: 0.002963, validation loss: 0.001879\n",
      "iteration 5721, train loss: 0.002934, validation loss: 0.00206\n",
      "iteration 5722, train loss: 0.002935, validation loss: 0.001953\n",
      "iteration 5723, train loss: 0.003084, validation loss: 0.002059\n",
      "iteration 5724, train loss: 0.003034, validation loss: 0.002153\n",
      "iteration 5725, train loss: 0.00306, validation loss: 0.001915\n",
      "iteration 5726, train loss: 0.002964, validation loss: 0.002317\n",
      "iteration 5727, train loss: 0.00301, validation loss: 0.002558\n",
      "iteration 5728, train loss: 0.003546, validation loss: 0.002025\n",
      "iteration 5729, train loss: 0.003042, validation loss: 0.002653\n",
      "iteration 5730, train loss: 0.003704, validation loss: 0.002572\n",
      "iteration 5731, train loss: 0.003404, validation loss: 0.001978\n",
      "iteration 5732, train loss: 0.002807, validation loss: 0.002901\n",
      "iteration 5733, train loss: 0.003957, validation loss: 0.001982\n",
      "iteration 5734, train loss: 0.002937, validation loss: 0.001933\n",
      "iteration 5735, train loss: 0.0029, validation loss: 0.002256\n",
      "iteration 5736, train loss: 0.003084, validation loss: 0.002312\n",
      "iteration 5737, train loss: 0.003487, validation loss: 0.001915\n",
      "iteration 5738, train loss: 0.003075, validation loss: 0.001784\n",
      "iteration 5739, train loss: 0.002751, validation loss: 0.00208\n",
      "iteration 5740, train loss: 0.00339, validation loss: 0.002256\n",
      "iteration 5741, train loss: 0.003452, validation loss: 0.001974\n",
      "iteration 5742, train loss: 0.002985, validation loss: 0.002014\n",
      "iteration 5743, train loss: 0.002846, validation loss: 0.002333\n",
      "iteration 5744, train loss: 0.003232, validation loss: 0.001952\n",
      "iteration 5745, train loss: 0.002819, validation loss: 0.001841\n",
      "iteration 5746, train loss: 0.002829, validation loss: 0.002096\n",
      "iteration 5747, train loss: 0.00314, validation loss: 0.002029\n",
      "iteration 5748, train loss: 0.003097, validation loss: 0.001778\n",
      "iteration 5749, train loss: 0.002762, validation loss: 0.002121\n",
      "iteration 5750, train loss: 0.003038, validation loss: 0.002089\n",
      "iteration 5751, train loss: 0.002821, validation loss: 0.001843\n",
      "iteration 5752, train loss: 0.003007, validation loss: 0.002152\n",
      "iteration 5753, train loss: 0.003185, validation loss: 0.002333\n",
      "iteration 5754, train loss: 0.002891, validation loss: 0.0021\n",
      "iteration 5755, train loss: 0.00357, validation loss: 0.001925\n",
      "iteration 5756, train loss: 0.002864, validation loss: 0.002284\n",
      "iteration 5757, train loss: 0.003264, validation loss: 0.001997\n",
      "iteration 5758, train loss: 0.003263, validation loss: 0.002293\n",
      "iteration 5759, train loss: 0.002984, validation loss: 0.002613\n",
      "iteration 5760, train loss: 0.003431, validation loss: 0.002104\n",
      "iteration 5761, train loss: 0.003266, validation loss: 0.002068\n",
      "iteration 5762, train loss: 0.003125, validation loss: 0.002214\n",
      "iteration 5763, train loss: 0.003602, validation loss: 0.001834\n",
      "iteration 5764, train loss: 0.002885, validation loss: 0.002321\n",
      "iteration 5765, train loss: 0.003435, validation loss: 0.002224\n",
      "iteration 5766, train loss: 0.003467, validation loss: 0.001812\n",
      "iteration 5767, train loss: 0.002945, validation loss: 0.002356\n",
      "iteration 5768, train loss: 0.003169, validation loss: 0.002074\n",
      "iteration 5769, train loss: 0.003279, validation loss: 0.002023\n",
      "iteration 5770, train loss: 0.00292, validation loss: 0.002632\n",
      "iteration 5771, train loss: 0.003995, validation loss: 0.002009\n",
      "iteration 5772, train loss: 0.002918, validation loss: 0.001878\n",
      "iteration 5773, train loss: 0.003156, validation loss: 0.00201\n",
      "iteration 5774, train loss: 0.002955, validation loss: 0.002064\n",
      "iteration 5775, train loss: 0.003325, validation loss: 0.002043\n",
      "iteration 5776, train loss: 0.002962, validation loss: 0.002013\n",
      "iteration 5777, train loss: 0.003039, validation loss: 0.001905\n",
      "iteration 5778, train loss: 0.002836, validation loss: 0.00196\n",
      "iteration 5779, train loss: 0.003343, validation loss: 0.001803\n",
      "iteration 5780, train loss: 0.002963, validation loss: 0.001903\n",
      "iteration 5781, train loss: 0.003133, validation loss: 0.002048\n",
      "iteration 5782, train loss: 0.002792, validation loss: 0.002244\n",
      "iteration 5783, train loss: 0.003287, validation loss: 0.001992\n",
      "iteration 5784, train loss: 0.002944, validation loss: 0.001886\n",
      "iteration 5785, train loss: 0.002847, validation loss: 0.002101\n",
      "iteration 5786, train loss: 0.00328, validation loss: 0.002065\n",
      "iteration 5787, train loss: 0.003104, validation loss: 0.002036\n",
      "iteration 5788, train loss: 0.002998, validation loss: 0.002068\n",
      "iteration 5789, train loss: 0.003209, validation loss: 0.002025\n",
      "iteration 5790, train loss: 0.003207, validation loss: 0.001897\n",
      "iteration 5791, train loss: 0.003122, validation loss: 0.001796\n",
      "iteration 5792, train loss: 0.002903, validation loss: 0.001974\n",
      "iteration 5793, train loss: 0.003045, validation loss: 0.001891\n",
      "iteration 5794, train loss: 0.002597, validation loss: 0.001872\n",
      "iteration 5795, train loss: 0.002922, validation loss: 0.00201\n",
      "iteration 5796, train loss: 0.002834, validation loss: 0.001895\n",
      "iteration 5797, train loss: 0.002616, validation loss: 0.001909\n",
      "iteration 5798, train loss: 0.003104, validation loss: 0.001985\n",
      "iteration 5799, train loss: 0.002842, validation loss: 0.00184\n",
      "iteration 5800, train loss: 0.002775, validation loss: 0.001895\n",
      "iteration 5801, train loss: 0.002796, validation loss: 0.001953\n",
      "iteration 5802, train loss: 0.003183, validation loss: 0.001814\n",
      "iteration 5803, train loss: 0.002777, validation loss: 0.001853\n",
      "iteration 5804, train loss: 0.002934, validation loss: 0.001864\n",
      "iteration 5805, train loss: 0.002767, validation loss: 0.001947\n",
      "iteration 5806, train loss: 0.00279, validation loss: 0.002048\n",
      "iteration 5807, train loss: 0.00304, validation loss: 0.00211\n",
      "iteration 5808, train loss: 0.00295, validation loss: 0.001887\n",
      "iteration 5809, train loss: 0.003427, validation loss: 0.002551\n",
      "iteration 5810, train loss: 0.003331, validation loss: 0.002997\n",
      "iteration 5811, train loss: 0.003329, validation loss: 0.002584\n",
      "iteration 5812, train loss: 0.003678, validation loss: 0.00232\n",
      "iteration 5813, train loss: 0.003593, validation loss: 0.003046\n",
      "iteration 5814, train loss: 0.004239, validation loss: 0.002392\n",
      "iteration 5815, train loss: 0.003479, validation loss: 0.002889\n",
      "iteration 5816, train loss: 0.004078, validation loss: 0.002946\n",
      "iteration 5817, train loss: 0.003528, validation loss: 0.002674\n",
      "iteration 5818, train loss: 0.00348, validation loss: 0.002484\n",
      "iteration 5819, train loss: 0.003062, validation loss: 0.002762\n",
      "iteration 5820, train loss: 0.004397, validation loss: 0.002168\n",
      "iteration 5821, train loss: 0.00297, validation loss: 0.002674\n",
      "iteration 5822, train loss: 0.003535, validation loss: 0.002463\n",
      "iteration 5823, train loss: 0.003509, validation loss: 0.002051\n",
      "iteration 5824, train loss: 0.003224, validation loss: 0.002341\n",
      "iteration 5825, train loss: 0.003148, validation loss: 0.002404\n",
      "iteration 5826, train loss: 0.003355, validation loss: 0.002183\n",
      "iteration 5827, train loss: 0.003169, validation loss: 0.002377\n",
      "iteration 5828, train loss: 0.003357, validation loss: 0.002257\n",
      "iteration 5829, train loss: 0.003525, validation loss: 0.002052\n",
      "iteration 5830, train loss: 0.003313, validation loss: 0.002421\n",
      "iteration 5831, train loss: 0.003605, validation loss: 0.002229\n",
      "iteration 5832, train loss: 0.003468, validation loss: 0.001852\n",
      "iteration 5833, train loss: 0.002793, validation loss: 0.002206\n",
      "iteration 5834, train loss: 0.003401, validation loss: 0.002226\n",
      "iteration 5835, train loss: 0.003493, validation loss: 0.00198\n",
      "iteration 5836, train loss: 0.003235, validation loss: 0.002051\n",
      "iteration 5837, train loss: 0.00362, validation loss: 0.002222\n",
      "iteration 5838, train loss: 0.003184, validation loss: 0.001941\n",
      "iteration 5839, train loss: 0.002803, validation loss: 0.001956\n",
      "iteration 5840, train loss: 0.003027, validation loss: 0.002066\n",
      "iteration 5841, train loss: 0.002764, validation loss: 0.002086\n",
      "iteration 5842, train loss: 0.00325, validation loss: 0.001804\n",
      "iteration 5843, train loss: 0.002966, validation loss: 0.001901\n",
      "iteration 5844, train loss: 0.00284, validation loss: 0.001996\n",
      "iteration 5845, train loss: 0.003373, validation loss: 0.001832\n",
      "iteration 5846, train loss: 0.003078, validation loss: 0.001834\n",
      "iteration 5847, train loss: 0.002927, validation loss: \u001b[92m0.001752\u001b[0m\n",
      "iteration 5848, train loss: 0.00284, validation loss: 0.001763\n",
      "iteration 5849, train loss: 0.002775, validation loss: 0.001772\n",
      "iteration 5850, train loss: 0.003056, validation loss: 0.001776\n",
      "iteration 5851, train loss: 0.002687, validation loss: 0.001935\n",
      "iteration 5852, train loss: 0.002983, validation loss: 0.001867\n",
      "iteration 5853, train loss: 0.0031, validation loss: 0.001852\n",
      "iteration 5854, train loss: 0.00308, validation loss: 0.001875\n",
      "iteration 5855, train loss: 0.002896, validation loss: 0.001965\n",
      "iteration 5856, train loss: 0.002778, validation loss: 0.001928\n",
      "iteration 5857, train loss: 0.002827, validation loss: 0.001897\n",
      "iteration 5858, train loss: 0.002957, validation loss: 0.001957\n",
      "iteration 5859, train loss: 0.002975, validation loss: 0.001996\n",
      "iteration 5860, train loss: 0.003353, validation loss: 0.001933\n",
      "iteration 5861, train loss: 0.002775, validation loss: 0.001919\n",
      "iteration 5862, train loss: 0.002868, validation loss: 0.001921\n",
      "iteration 5863, train loss: 0.002978, validation loss: 0.0019\n",
      "iteration 5864, train loss: 0.002852, validation loss: 0.001802\n",
      "iteration 5865, train loss: 0.002816, validation loss: 0.001824\n",
      "iteration 5866, train loss: 0.002833, validation loss: 0.001854\n",
      "iteration 5867, train loss: 0.002632, validation loss: 0.002002\n",
      "iteration 5868, train loss: 0.002765, validation loss: 0.002083\n",
      "iteration 5869, train loss: 0.002984, validation loss: 0.001825\n",
      "iteration 5870, train loss: 0.002657, validation loss: 0.001964\n",
      "iteration 5871, train loss: 0.002935, validation loss: 0.002003\n",
      "iteration 5872, train loss: 0.002875, validation loss: 0.001819\n",
      "iteration 5873, train loss: 0.002951, validation loss: 0.001902\n",
      "iteration 5874, train loss: 0.002548, validation loss: 0.002208\n",
      "iteration 5875, train loss: 0.003168, validation loss: 0.001955\n",
      "iteration 5876, train loss: 0.002847, validation loss: 0.001804\n",
      "iteration 5877, train loss: 0.002832, validation loss: 0.001831\n",
      "iteration 5878, train loss: 0.002678, validation loss: 0.00199\n",
      "iteration 5879, train loss: 0.002568, validation loss: 0.001971\n",
      "iteration 5880, train loss: 0.003008, validation loss: 0.001815\n",
      "iteration 5881, train loss: 0.002916, validation loss: 0.001847\n",
      "iteration 5882, train loss: 0.003139, validation loss: 0.001803\n",
      "iteration 5883, train loss: 0.002581, validation loss: 0.001961\n",
      "iteration 5884, train loss: 0.002959, validation loss: 0.00193\n",
      "iteration 5885, train loss: 0.003227, validation loss: 0.00186\n",
      "iteration 5886, train loss: 0.00296, validation loss: 0.002016\n",
      "iteration 5887, train loss: 0.003103, validation loss: 0.001968\n",
      "iteration 5888, train loss: 0.003258, validation loss: 0.001917\n",
      "iteration 5889, train loss: 0.002748, validation loss: 0.001926\n",
      "iteration 5890, train loss: 0.002815, validation loss: 0.00183\n",
      "iteration 5891, train loss: 0.002768, validation loss: 0.001807\n",
      "iteration 5892, train loss: 0.002922, validation loss: 0.002186\n",
      "iteration 5893, train loss: 0.003192, validation loss: 0.002073\n",
      "iteration 5894, train loss: 0.003142, validation loss: 0.001804\n",
      "iteration 5895, train loss: 0.002994, validation loss: 0.001997\n",
      "iteration 5896, train loss: 0.003171, validation loss: 0.001972\n",
      "iteration 5897, train loss: 0.002963, validation loss: 0.001767\n",
      "iteration 5898, train loss: 0.002634, validation loss: 0.001959\n",
      "iteration 5899, train loss: 0.002982, validation loss: 0.001986\n",
      "iteration 5900, train loss: 0.002813, validation loss: 0.001855\n",
      "iteration 5901, train loss: 0.002929, validation loss: 0.001844\n",
      "iteration 5902, train loss: 0.00315, validation loss: 0.001849\n",
      "iteration 5903, train loss: 0.002724, validation loss: 0.001797\n",
      "iteration 5904, train loss: 0.002867, validation loss: 0.001969\n",
      "iteration 5905, train loss: 0.003172, validation loss: 0.002119\n",
      "iteration 5906, train loss: 0.003282, validation loss: 0.001804\n",
      "iteration 5907, train loss: 0.002733, validation loss: 0.001961\n",
      "iteration 5908, train loss: 0.002766, validation loss: 0.002255\n",
      "iteration 5909, train loss: 0.003283, validation loss: 0.00213\n",
      "iteration 5910, train loss: 0.003101, validation loss: 0.002136\n",
      "iteration 5911, train loss: 0.002835, validation loss: 0.002389\n",
      "iteration 5912, train loss: 0.002927, validation loss: 0.002521\n",
      "iteration 5913, train loss: 0.003363, validation loss: 0.002149\n",
      "iteration 5914, train loss: 0.003018, validation loss: 0.002\n",
      "iteration 5915, train loss: 0.002984, validation loss: 0.002244\n",
      "iteration 5916, train loss: 0.002968, validation loss: 0.002435\n",
      "iteration 5917, train loss: 0.002884, validation loss: 0.002325\n",
      "iteration 5918, train loss: 0.003373, validation loss: 0.002029\n",
      "iteration 5919, train loss: 0.003206, validation loss: 0.00183\n",
      "iteration 5920, train loss: 0.00278, validation loss: 0.002014\n",
      "iteration 5921, train loss: 0.003088, validation loss: 0.001869\n",
      "iteration 5922, train loss: 0.002902, validation loss: 0.001846\n",
      "iteration 5923, train loss: 0.002977, validation loss: 0.001997\n",
      "iteration 5924, train loss: 0.002651, validation loss: 0.002056\n",
      "iteration 5925, train loss: 0.003131, validation loss: 0.001804\n",
      "iteration 5926, train loss: 0.002708, validation loss: 0.001894\n",
      "iteration 5927, train loss: 0.002688, validation loss: 0.002103\n",
      "iteration 5928, train loss: 0.003529, validation loss: 0.001785\n",
      "iteration 5929, train loss: 0.003041, validation loss: 0.002054\n",
      "iteration 5930, train loss: 0.003102, validation loss: 0.002059\n",
      "iteration 5931, train loss: 0.002895, validation loss: 0.001834\n",
      "iteration 5932, train loss: 0.002799, validation loss: 0.002027\n",
      "iteration 5933, train loss: 0.003188, validation loss: 0.001899\n",
      "iteration 5934, train loss: 0.003039, validation loss: \u001b[92m0.001738\u001b[0m\n",
      "iteration 5935, train loss: 0.002618, validation loss: 0.001919\n",
      "iteration 5936, train loss: 0.002786, validation loss: 0.001909\n",
      "iteration 5937, train loss: 0.00298, validation loss: \u001b[92m0.001733\u001b[0m\n",
      "iteration 5938, train loss: 0.002736, validation loss: 0.001872\n",
      "iteration 5939, train loss: 0.002816, validation loss: 0.001957\n",
      "iteration 5940, train loss: 0.002863, validation loss: 0.001956\n",
      "iteration 5941, train loss: 0.002963, validation loss: 0.001859\n",
      "iteration 5942, train loss: 0.00268, validation loss: 0.001928\n",
      "iteration 5943, train loss: 0.00273, validation loss: 0.002155\n",
      "iteration 5944, train loss: 0.003012, validation loss: 0.002287\n",
      "iteration 5945, train loss: 0.003201, validation loss: 0.001944\n",
      "iteration 5946, train loss: 0.002917, validation loss: 0.001973\n",
      "iteration 5947, train loss: 0.002654, validation loss: 0.002367\n",
      "iteration 5948, train loss: 0.003221, validation loss: 0.002097\n",
      "iteration 5949, train loss: 0.003302, validation loss: 0.001827\n",
      "iteration 5950, train loss: 0.002695, validation loss: 0.002005\n",
      "iteration 5951, train loss: 0.002939, validation loss: 0.001981\n",
      "iteration 5952, train loss: 0.002935, validation loss: 0.001967\n",
      "iteration 5953, train loss: 0.003176, validation loss: 0.00197\n",
      "iteration 5954, train loss: 0.002743, validation loss: 0.001893\n",
      "iteration 5955, train loss: 0.00267, validation loss: 0.00175\n",
      "iteration 5956, train loss: 0.002662, validation loss: 0.001795\n",
      "iteration 5957, train loss: 0.002923, validation loss: 0.001742\n",
      "iteration 5958, train loss: 0.00277, validation loss: 0.001849\n",
      "iteration 5959, train loss: 0.002647, validation loss: 0.002009\n",
      "iteration 5960, train loss: 0.002695, validation loss: 0.002002\n",
      "iteration 5961, train loss: 0.002786, validation loss: 0.001851\n",
      "iteration 5962, train loss: 0.002586, validation loss: 0.001992\n",
      "iteration 5963, train loss: 0.002928, validation loss: 0.001781\n",
      "iteration 5964, train loss: 0.002904, validation loss: 0.001794\n",
      "iteration 5965, train loss: 0.002872, validation loss: 0.001996\n",
      "iteration 5966, train loss: 0.003201, validation loss: 0.001933\n",
      "iteration 5967, train loss: 0.002983, validation loss: 0.001965\n",
      "iteration 5968, train loss: 0.002643, validation loss: 0.002002\n",
      "iteration 5969, train loss: 0.00315, validation loss: 0.001878\n",
      "iteration 5970, train loss: 0.002893, validation loss: 0.001781\n",
      "iteration 5971, train loss: 0.003008, validation loss: 0.002047\n",
      "iteration 5972, train loss: 0.003057, validation loss: 0.00203\n",
      "iteration 5973, train loss: 0.002859, validation loss: 0.002075\n",
      "iteration 5974, train loss: 0.003038, validation loss: 0.001976\n",
      "iteration 5975, train loss: 0.002852, validation loss: 0.00189\n",
      "iteration 5976, train loss: 0.002984, validation loss: 0.001897\n",
      "iteration 5977, train loss: 0.002806, validation loss: 0.001953\n",
      "iteration 5978, train loss: 0.003013, validation loss: 0.002013\n",
      "iteration 5979, train loss: 0.002897, validation loss: 0.001929\n",
      "iteration 5980, train loss: 0.003015, validation loss: 0.001746\n",
      "iteration 5981, train loss: 0.002589, validation loss: 0.002044\n",
      "iteration 5982, train loss: 0.003058, validation loss: 0.002038\n",
      "iteration 5983, train loss: 0.002965, validation loss: 0.001756\n",
      "iteration 5984, train loss: 0.002517, validation loss: 0.001897\n",
      "iteration 5985, train loss: 0.00272, validation loss: 0.002001\n",
      "iteration 5986, train loss: 0.003029, validation loss: 0.001921\n",
      "iteration 5987, train loss: 0.002882, validation loss: 0.001915\n",
      "iteration 5988, train loss: 0.002795, validation loss: 0.001842\n",
      "iteration 5989, train loss: 0.003121, validation loss: 0.001852\n",
      "iteration 5990, train loss: 0.002912, validation loss: 0.001831\n",
      "iteration 5991, train loss: 0.002981, validation loss: 0.001877\n",
      "iteration 5992, train loss: 0.002851, validation loss: 0.002031\n",
      "iteration 5993, train loss: 0.002895, validation loss: 0.001928\n",
      "iteration 5994, train loss: 0.003049, validation loss: 0.00178\n",
      "iteration 5995, train loss: 0.002827, validation loss: 0.001918\n",
      "iteration 5996, train loss: 0.002847, validation loss: 0.001932\n",
      "iteration 5997, train loss: 0.003228, validation loss: 0.002037\n",
      "iteration 5998, train loss: 0.003163, validation loss: 0.002039\n",
      "iteration 5999, train loss: 0.002806, validation loss: 0.001924\n",
      "iteration 6000, train loss: 0.002968, validation loss: 0.001848\n",
      "iteration 6001, train loss: 0.003085, validation loss: 0.001865\n",
      "iteration 6002, train loss: 0.002811, validation loss: 0.001808\n",
      "iteration 6003, train loss: 0.002719, validation loss: 0.001961\n",
      "iteration 6004, train loss: 0.003141, validation loss: 0.001959\n",
      "iteration 6005, train loss: 0.00309, validation loss: 0.001836\n",
      "iteration 6006, train loss: 0.002779, validation loss: 0.001841\n",
      "iteration 6007, train loss: 0.002929, validation loss: 0.001854\n",
      "iteration 6008, train loss: 0.002998, validation loss: 0.002073\n",
      "iteration 6009, train loss: 0.002952, validation loss: 0.002324\n",
      "iteration 6010, train loss: 0.003785, validation loss: 0.001888\n",
      "iteration 6011, train loss: 0.002757, validation loss: 0.001961\n",
      "iteration 6012, train loss: 0.003124, validation loss: 0.002128\n",
      "iteration 6013, train loss: 0.003515, validation loss: 0.001774\n",
      "iteration 6014, train loss: 0.002668, validation loss: 0.002269\n",
      "iteration 6015, train loss: 0.00299, validation loss: 0.002377\n",
      "iteration 6016, train loss: 0.003237, validation loss: 0.001875\n",
      "iteration 6017, train loss: 0.002799, validation loss: 0.001903\n",
      "iteration 6018, train loss: 0.003199, validation loss: 0.002132\n",
      "iteration 6019, train loss: 0.003207, validation loss: 0.001893\n",
      "iteration 6020, train loss: 0.002821, validation loss: 0.002013\n",
      "iteration 6021, train loss: 0.003157, validation loss: 0.002144\n",
      "iteration 6022, train loss: 0.003112, validation loss: 0.002084\n",
      "iteration 6023, train loss: 0.002912, validation loss: 0.001825\n",
      "iteration 6024, train loss: 0.002893, validation loss: 0.002134\n",
      "iteration 6025, train loss: 0.00342, validation loss: 0.002044\n",
      "iteration 6026, train loss: 0.003206, validation loss: 0.002041\n",
      "iteration 6027, train loss: 0.002943, validation loss: 0.002479\n",
      "iteration 6028, train loss: 0.00362, validation loss: 0.002103\n",
      "iteration 6029, train loss: 0.002987, validation loss: 0.001961\n",
      "iteration 6030, train loss: 0.003144, validation loss: 0.00197\n",
      "iteration 6031, train loss: 0.003078, validation loss: 0.002075\n",
      "iteration 6032, train loss: 0.003434, validation loss: 0.002215\n",
      "iteration 6033, train loss: 0.003491, validation loss: 0.002139\n",
      "iteration 6034, train loss: 0.002953, validation loss: 0.002267\n",
      "iteration 6035, train loss: 0.003205, validation loss: 0.00201\n",
      "iteration 6036, train loss: 0.00302, validation loss: 0.001773\n",
      "iteration 6037, train loss: 0.002757, validation loss: 0.002078\n",
      "iteration 6038, train loss: 0.003306, validation loss: 0.002027\n",
      "iteration 6039, train loss: 0.002995, validation loss: 0.002021\n",
      "iteration 6040, train loss: 0.002873, validation loss: 0.002077\n",
      "iteration 6041, train loss: 0.002979, validation loss: 0.001904\n",
      "iteration 6042, train loss: 0.00279, validation loss: 0.001946\n",
      "iteration 6043, train loss: 0.002865, validation loss: 0.00183\n",
      "iteration 6044, train loss: 0.002662, validation loss: 0.001913\n",
      "iteration 6045, train loss: 0.00303, validation loss: 0.001858\n",
      "iteration 6046, train loss: 0.00293, validation loss: 0.00183\n",
      "iteration 6047, train loss: 0.002684, validation loss: 0.001789\n",
      "iteration 6048, train loss: 0.0027, validation loss: 0.001839\n",
      "iteration 6049, train loss: 0.003021, validation loss: 0.0018\n",
      "iteration 6050, train loss: 0.002753, validation loss: 0.001798\n",
      "iteration 6051, train loss: 0.002659, validation loss: 0.001818\n",
      "iteration 6052, train loss: 0.002914, validation loss: 0.001832\n",
      "iteration 6053, train loss: 0.002795, validation loss: 0.00205\n",
      "iteration 6054, train loss: 0.00279, validation loss: 0.001986\n",
      "iteration 6055, train loss: 0.003257, validation loss: 0.001848\n",
      "iteration 6056, train loss: 0.002921, validation loss: 0.002008\n",
      "iteration 6057, train loss: 0.003305, validation loss: 0.00231\n",
      "iteration 6058, train loss: 0.003185, validation loss: 0.002135\n",
      "iteration 6059, train loss: 0.002977, validation loss: 0.001908\n",
      "iteration 6060, train loss: 0.002875, validation loss: 0.00231\n",
      "iteration 6061, train loss: 0.002718, validation loss: 0.002531\n",
      "iteration 6062, train loss: 0.003374, validation loss: 0.002032\n",
      "iteration 6063, train loss: 0.002652, validation loss: 0.002251\n",
      "iteration 6064, train loss: 0.003019, validation loss: 0.002468\n",
      "iteration 6065, train loss: 0.003248, validation loss: 0.001977\n",
      "iteration 6066, train loss: 0.003037, validation loss: 0.001907\n",
      "iteration 6067, train loss: 0.003187, validation loss: 0.002375\n",
      "iteration 6068, train loss: 0.003246, validation loss: 0.002284\n",
      "iteration 6069, train loss: 0.003452, validation loss: 0.002094\n",
      "iteration 6070, train loss: 0.002956, validation loss: 0.002471\n",
      "iteration 6071, train loss: 0.003315, validation loss: 0.002333\n",
      "iteration 6072, train loss: 0.003682, validation loss: 0.001927\n",
      "iteration 6073, train loss: 0.002909, validation loss: 0.002282\n",
      "iteration 6074, train loss: 0.003134, validation loss: 0.002268\n",
      "iteration 6075, train loss: 0.002687, validation loss: 0.002184\n",
      "iteration 6076, train loss: 0.00311, validation loss: 0.002181\n",
      "iteration 6077, train loss: 0.002795, validation loss: 0.002073\n",
      "iteration 6078, train loss: 0.002969, validation loss: 0.001893\n",
      "iteration 6079, train loss: 0.002916, validation loss: 0.002236\n",
      "iteration 6080, train loss: 0.002932, validation loss: 0.00229\n",
      "iteration 6081, train loss: 0.003343, validation loss: 0.001826\n",
      "iteration 6082, train loss: 0.002731, validation loss: 0.002049\n",
      "iteration 6083, train loss: 0.003318, validation loss: 0.002067\n",
      "iteration 6084, train loss: 0.003159, validation loss: 0.001812\n",
      "iteration 6085, train loss: 0.002974, validation loss: 0.001901\n",
      "iteration 6086, train loss: 0.002858, validation loss: 0.001916\n",
      "iteration 6087, train loss: 0.002816, validation loss: 0.001847\n",
      "iteration 6088, train loss: 0.002868, validation loss: 0.001841\n",
      "iteration 6089, train loss: 0.002667, validation loss: 0.001824\n",
      "iteration 6090, train loss: 0.002661, validation loss: 0.00181\n",
      "iteration 6091, train loss: 0.002947, validation loss: 0.002036\n",
      "iteration 6092, train loss: 0.003384, validation loss: 0.001855\n",
      "iteration 6093, train loss: 0.002495, validation loss: 0.001742\n",
      "iteration 6094, train loss: 0.002958, validation loss: 0.001783\n",
      "iteration 6095, train loss: 0.002704, validation loss: 0.001935\n",
      "iteration 6096, train loss: 0.003434, validation loss: 0.00202\n",
      "iteration 6097, train loss: 0.002732, validation loss: 0.002031\n",
      "iteration 6098, train loss: 0.002558, validation loss: 0.001925\n",
      "iteration 6099, train loss: 0.003175, validation loss: 0.0021\n",
      "iteration 6100, train loss: 0.002975, validation loss: 0.0022\n",
      "iteration 6101, train loss: 0.003003, validation loss: 0.001964\n",
      "iteration 6102, train loss: 0.003125, validation loss: 0.002174\n",
      "iteration 6103, train loss: 0.003489, validation loss: 0.002231\n",
      "iteration 6104, train loss: 0.003115, validation loss: 0.00222\n",
      "iteration 6105, train loss: 0.003422, validation loss: 0.001865\n",
      "iteration 6106, train loss: 0.002866, validation loss: 0.002132\n",
      "iteration 6107, train loss: 0.003062, validation loss: 0.002249\n",
      "iteration 6108, train loss: 0.002895, validation loss: 0.001912\n",
      "iteration 6109, train loss: 0.002816, validation loss: 0.001851\n",
      "iteration 6110, train loss: 0.002872, validation loss: 0.00204\n",
      "iteration 6111, train loss: 0.00296, validation loss: 0.001964\n",
      "iteration 6112, train loss: 0.003062, validation loss: 0.001738\n",
      "iteration 6113, train loss: 0.002807, validation loss: 0.001813\n",
      "iteration 6114, train loss: 0.002645, validation loss: 0.001967\n",
      "iteration 6115, train loss: 0.002961, validation loss: 0.001825\n",
      "iteration 6116, train loss: 0.002865, validation loss: 0.00182\n",
      "iteration 6117, train loss: 0.002582, validation loss: 0.002145\n",
      "iteration 6118, train loss: 0.003244, validation loss: 0.002012\n",
      "iteration 6119, train loss: 0.002869, validation loss: 0.001762\n",
      "iteration 6120, train loss: 0.003289, validation loss: 0.001946\n",
      "iteration 6121, train loss: 0.003031, validation loss: 0.00198\n",
      "iteration 6122, train loss: 0.003142, validation loss: 0.001859\n",
      "iteration 6123, train loss: 0.002828, validation loss: 0.001887\n",
      "iteration 6124, train loss: 0.002848, validation loss: 0.002023\n",
      "iteration 6125, train loss: 0.002858, validation loss: 0.001962\n",
      "iteration 6126, train loss: 0.003017, validation loss: 0.002098\n",
      "iteration 6127, train loss: 0.003266, validation loss: 0.001878\n",
      "iteration 6128, train loss: 0.003113, validation loss: 0.002111\n",
      "iteration 6129, train loss: 0.002973, validation loss: 0.002339\n",
      "iteration 6130, train loss: 0.003277, validation loss: 0.001822\n",
      "iteration 6131, train loss: 0.003045, validation loss: 0.0019\n",
      "iteration 6132, train loss: 0.002891, validation loss: 0.002125\n",
      "iteration 6133, train loss: 0.003182, validation loss: 0.001814\n",
      "iteration 6134, train loss: 0.002777, validation loss: 0.002032\n",
      "iteration 6135, train loss: 0.003324, validation loss: 0.002088\n",
      "iteration 6136, train loss: 0.002784, validation loss: 0.001914\n",
      "iteration 6137, train loss: 0.002928, validation loss: 0.001962\n",
      "iteration 6138, train loss: 0.002624, validation loss: 0.001974\n",
      "iteration 6139, train loss: 0.002964, validation loss: 0.001941\n",
      "iteration 6140, train loss: 0.002881, validation loss: 0.001987\n",
      "iteration 6141, train loss: 0.002944, validation loss: 0.0019\n",
      "iteration 6142, train loss: 0.003071, validation loss: 0.001764\n",
      "iteration 6143, train loss: 0.00299, validation loss: 0.001874\n",
      "iteration 6144, train loss: 0.002757, validation loss: 0.001858\n",
      "iteration 6145, train loss: 0.003131, validation loss: 0.001733\n",
      "iteration 6146, train loss: \u001b[92m0.002469\u001b[0m, validation loss: 0.002034\n",
      "iteration 6147, train loss: 0.00294, validation loss: 0.002088\n",
      "iteration 6148, train loss: 0.002768, validation loss: 0.001869\n",
      "iteration 6149, train loss: 0.002605, validation loss: 0.001828\n",
      "iteration 6150, train loss: 0.002626, validation loss: 0.002065\n",
      "iteration 6151, train loss: 0.003025, validation loss: 0.002104\n",
      "iteration 6152, train loss: 0.002896, validation loss: 0.002121\n",
      "iteration 6153, train loss: 0.003737, validation loss: 0.002004\n",
      "iteration 6154, train loss: 0.002769, validation loss: 0.002394\n",
      "iteration 6155, train loss: 0.003586, validation loss: 0.002539\n",
      "iteration 6156, train loss: 0.003542, validation loss: 0.001935\n",
      "iteration 6157, train loss: 0.002989, validation loss: 0.001823\n",
      "iteration 6158, train loss: 0.002704, validation loss: 0.001915\n",
      "iteration 6159, train loss: 0.002825, validation loss: 0.001766\n",
      "iteration 6160, train loss: 0.002615, validation loss: 0.001775\n",
      "iteration 6161, train loss: 0.0027, validation loss: 0.002265\n",
      "iteration 6162, train loss: 0.002813, validation loss: 0.00225\n",
      "iteration 6163, train loss: 0.003311, validation loss: 0.001856\n",
      "iteration 6164, train loss: 0.002719, validation loss: 0.001986\n",
      "iteration 6165, train loss: 0.003372, validation loss: 0.001871\n",
      "iteration 6166, train loss: 0.002754, validation loss: 0.001765\n",
      "iteration 6167, train loss: 0.00262, validation loss: 0.002014\n",
      "iteration 6168, train loss: 0.003049, validation loss: 0.001905\n",
      "iteration 6169, train loss: 0.002938, validation loss: 0.001849\n",
      "iteration 6170, train loss: 0.002868, validation loss: 0.002017\n",
      "iteration 6171, train loss: 0.002926, validation loss: 0.002069\n",
      "iteration 6172, train loss: 0.003096, validation loss: 0.001807\n",
      "iteration 6173, train loss: 0.002651, validation loss: 0.001889\n",
      "iteration 6174, train loss: 0.003354, validation loss: 0.001796\n",
      "iteration 6175, train loss: 0.002798, validation loss: 0.001848\n",
      "iteration 6176, train loss: 0.002859, validation loss: 0.001986\n",
      "iteration 6177, train loss: 0.003017, validation loss: 0.001843\n",
      "iteration 6178, train loss: 0.002662, validation loss: 0.001955\n",
      "iteration 6179, train loss: 0.002955, validation loss: 0.002012\n",
      "iteration 6180, train loss: 0.002745, validation loss: 0.001792\n",
      "iteration 6181, train loss: 0.002684, validation loss: 0.001751\n",
      "iteration 6182, train loss: 0.002658, validation loss: 0.001931\n",
      "iteration 6183, train loss: 0.002702, validation loss: 0.001831\n",
      "iteration 6184, train loss: 0.002879, validation loss: 0.00182\n",
      "iteration 6185, train loss: 0.002824, validation loss: 0.001927\n",
      "iteration 6186, train loss: 0.002686, validation loss: 0.001899\n",
      "iteration 6187, train loss: 0.00272, validation loss: 0.001898\n",
      "iteration 6188, train loss: 0.002985, validation loss: 0.001817\n",
      "iteration 6189, train loss: 0.002741, validation loss: 0.001904\n",
      "iteration 6190, train loss: 0.002927, validation loss: 0.001949\n",
      "iteration 6191, train loss: 0.002977, validation loss: 0.002026\n",
      "iteration 6192, train loss: 0.003203, validation loss: 0.002028\n",
      "iteration 6193, train loss: 0.002852, validation loss: 0.00195\n",
      "iteration 6194, train loss: 0.002795, validation loss: 0.002129\n",
      "iteration 6195, train loss: 0.003214, validation loss: 0.002202\n",
      "iteration 6196, train loss: 0.003279, validation loss: 0.001854\n",
      "iteration 6197, train loss: 0.002709, validation loss: 0.001971\n",
      "iteration 6198, train loss: 0.00275, validation loss: 0.001948\n",
      "iteration 6199, train loss: 0.002904, validation loss: 0.001992\n",
      "iteration 6200, train loss: 0.002591, validation loss: 0.002165\n",
      "iteration 6201, train loss: 0.003068, validation loss: 0.001986\n",
      "iteration 6202, train loss: 0.00282, validation loss: 0.001775\n",
      "iteration 6203, train loss: 0.002546, validation loss: 0.001968\n",
      "iteration 6204, train loss: 0.002703, validation loss: 0.002322\n",
      "iteration 6205, train loss: 0.003072, validation loss: 0.00177\n",
      "iteration 6206, train loss: 0.002909, validation loss: 0.001882\n",
      "iteration 6207, train loss: 0.002926, validation loss: 0.002213\n",
      "iteration 6208, train loss: 0.003307, validation loss: 0.001841\n",
      "iteration 6209, train loss: 0.003222, validation loss: 0.001765\n",
      "iteration 6210, train loss: 0.002799, validation loss: 0.001879\n",
      "iteration 6211, train loss: 0.003092, validation loss: \u001b[92m0.001727\u001b[0m\n",
      "iteration 6212, train loss: 0.002609, validation loss: 0.001962\n",
      "iteration 6213, train loss: 0.002855, validation loss: 0.001959\n",
      "iteration 6214, train loss: 0.002585, validation loss: 0.001846\n",
      "iteration 6215, train loss: 0.002631, validation loss: 0.001862\n",
      "iteration 6216, train loss: 0.002726, validation loss: 0.001754\n",
      "iteration 6217, train loss: 0.003075, validation loss: 0.001768\n",
      "iteration 6218, train loss: 0.00299, validation loss: 0.001909\n",
      "iteration 6219, train loss: 0.002923, validation loss: 0.001993\n",
      "iteration 6220, train loss: 0.00282, validation loss: 0.002043\n",
      "iteration 6221, train loss: 0.0027, validation loss: 0.001819\n",
      "iteration 6222, train loss: 0.003049, validation loss: 0.001846\n",
      "iteration 6223, train loss: 0.002718, validation loss: 0.001961\n",
      "iteration 6224, train loss: 0.002611, validation loss: 0.001977\n",
      "iteration 6225, train loss: 0.002777, validation loss: 0.001729\n",
      "iteration 6226, train loss: 0.002768, validation loss: 0.001894\n",
      "iteration 6227, train loss: 0.002895, validation loss: 0.002018\n",
      "iteration 6228, train loss: 0.002862, validation loss: 0.001984\n",
      "iteration 6229, train loss: 0.002913, validation loss: 0.001958\n",
      "iteration 6230, train loss: 0.002788, validation loss: 0.002007\n",
      "iteration 6231, train loss: 0.002978, validation loss: 0.001864\n",
      "iteration 6232, train loss: 0.003038, validation loss: 0.001823\n",
      "iteration 6233, train loss: 0.002746, validation loss: 0.00197\n",
      "iteration 6234, train loss: 0.002712, validation loss: 0.002219\n",
      "iteration 6235, train loss: 0.002963, validation loss: 0.002165\n",
      "iteration 6236, train loss: 0.003513, validation loss: \u001b[92m0.001718\u001b[0m\n",
      "iteration 6237, train loss: 0.002861, validation loss: 0.002062\n",
      "iteration 6238, train loss: 0.003231, validation loss: 0.002098\n",
      "iteration 6239, train loss: 0.003119, validation loss: 0.001846\n",
      "iteration 6240, train loss: 0.003013, validation loss: 0.002064\n",
      "iteration 6241, train loss: 0.002719, validation loss: 0.002439\n",
      "iteration 6242, train loss: 0.003649, validation loss: 0.001859\n",
      "iteration 6243, train loss: 0.002893, validation loss: 0.001897\n",
      "iteration 6244, train loss: 0.00298, validation loss: 0.001978\n",
      "iteration 6245, train loss: 0.002986, validation loss: 0.001935\n",
      "iteration 6246, train loss: 0.002834, validation loss: 0.001884\n",
      "iteration 6247, train loss: 0.003071, validation loss: 0.001942\n",
      "iteration 6248, train loss: 0.002519, validation loss: 0.001895\n",
      "iteration 6249, train loss: 0.002924, validation loss: 0.001777\n",
      "iteration 6250, train loss: 0.00276, validation loss: 0.001949\n",
      "iteration 6251, train loss: 0.002847, validation loss: 0.001937\n",
      "iteration 6252, train loss: 0.003109, validation loss: 0.001788\n",
      "iteration 6253, train loss: 0.002737, validation loss: 0.00214\n",
      "iteration 6254, train loss: 0.003608, validation loss: 0.00178\n",
      "iteration 6255, train loss: 0.002652, validation loss: 0.002082\n",
      "iteration 6256, train loss: 0.00317, validation loss: 0.002276\n",
      "iteration 6257, train loss: 0.003242, validation loss: 0.001834\n",
      "iteration 6258, train loss: 0.002876, validation loss: 0.001935\n",
      "iteration 6259, train loss: 0.002711, validation loss: 0.002223\n",
      "iteration 6260, train loss: 0.00314, validation loss: 0.001811\n",
      "iteration 6261, train loss: 0.002616, validation loss: 0.001975\n",
      "iteration 6262, train loss: 0.002798, validation loss: 0.002179\n",
      "iteration 6263, train loss: 0.003192, validation loss: 0.002189\n",
      "iteration 6264, train loss: 0.003008, validation loss: 0.00219\n",
      "iteration 6265, train loss: 0.003004, validation loss: 0.001925\n",
      "iteration 6266, train loss: 0.002958, validation loss: 0.00187\n",
      "iteration 6267, train loss: 0.002624, validation loss: 0.001931\n",
      "iteration 6268, train loss: 0.002914, validation loss: 0.001842\n",
      "iteration 6269, train loss: 0.002561, validation loss: 0.001793\n",
      "iteration 6270, train loss: 0.0029, validation loss: 0.001915\n",
      "iteration 6271, train loss: 0.003084, validation loss: 0.001868\n",
      "iteration 6272, train loss: 0.00277, validation loss: 0.001755\n",
      "iteration 6273, train loss: 0.002812, validation loss: 0.001819\n",
      "iteration 6274, train loss: 0.002882, validation loss: 0.001843\n",
      "iteration 6275, train loss: 0.002718, validation loss: 0.001823\n",
      "iteration 6276, train loss: 0.002676, validation loss: 0.001765\n",
      "iteration 6277, train loss: 0.00275, validation loss: 0.001746\n",
      "iteration 6278, train loss: 0.002626, validation loss: 0.001764\n",
      "iteration 6279, train loss: 0.002707, validation loss: 0.001888\n",
      "iteration 6280, train loss: 0.002845, validation loss: 0.00178\n",
      "iteration 6281, train loss: 0.002584, validation loss: 0.001738\n",
      "iteration 6282, train loss: 0.002618, validation loss: 0.001907\n",
      "iteration 6283, train loss: 0.002757, validation loss: 0.00187\n",
      "iteration 6284, train loss: 0.002854, validation loss: 0.001727\n",
      "iteration 6285, train loss: 0.002794, validation loss: 0.001979\n",
      "iteration 6286, train loss: 0.003113, validation loss: 0.00184\n",
      "iteration 6287, train loss: 0.002829, validation loss: 0.001736\n",
      "iteration 6288, train loss: 0.002771, validation loss: 0.001846\n",
      "iteration 6289, train loss: 0.002662, validation loss: 0.001891\n",
      "iteration 6290, train loss: 0.002695, validation loss: 0.001811\n",
      "iteration 6291, train loss: 0.002581, validation loss: 0.002062\n",
      "iteration 6292, train loss: 0.003095, validation loss: 0.001891\n",
      "iteration 6293, train loss: 0.002963, validation loss: 0.001792\n",
      "iteration 6294, train loss: 0.002923, validation loss: 0.001882\n",
      "iteration 6295, train loss: 0.002925, validation loss: 0.001964\n",
      "iteration 6296, train loss: 0.002852, validation loss: 0.00178\n",
      "iteration 6297, train loss: 0.003099, validation loss: 0.001917\n",
      "iteration 6298, train loss: 0.002888, validation loss: 0.002283\n",
      "iteration 6299, train loss: 0.002967, validation loss: 0.002215\n",
      "iteration 6300, train loss: 0.003013, validation loss: 0.001939\n",
      "iteration 6301, train loss: 0.002685, validation loss: 0.001949\n",
      "iteration 6302, train loss: 0.003366, validation loss: \u001b[92m0.001702\u001b[0m\n",
      "iteration 6303, train loss: 0.002789, validation loss: 0.001918\n",
      "iteration 6304, train loss: 0.002829, validation loss: 0.001948\n",
      "iteration 6305, train loss: 0.002817, validation loss: 0.00178\n",
      "iteration 6306, train loss: 0.002739, validation loss: 0.001703\n",
      "iteration 6307, train loss: 0.002575, validation loss: 0.001924\n",
      "iteration 6308, train loss: 0.003067, validation loss: 0.001763\n",
      "iteration 6309, train loss: 0.002815, validation loss: 0.001729\n",
      "iteration 6310, train loss: 0.002714, validation loss: 0.00181\n",
      "iteration 6311, train loss: 0.002701, validation loss: 0.001784\n",
      "iteration 6312, train loss: 0.002659, validation loss: 0.001766\n",
      "iteration 6313, train loss: 0.002808, validation loss: 0.001767\n",
      "iteration 6314, train loss: 0.002743, validation loss: 0.00176\n",
      "iteration 6315, train loss: 0.002532, validation loss: 0.001852\n",
      "iteration 6316, train loss: 0.003145, validation loss: 0.001857\n",
      "iteration 6317, train loss: \u001b[92m0.002405\u001b[0m, validation loss: 0.001728\n",
      "iteration 6318, train loss: 0.00287, validation loss: 0.001804\n",
      "iteration 6319, train loss: 0.002525, validation loss: 0.001934\n",
      "iteration 6320, train loss: 0.003181, validation loss: 0.001725\n",
      "iteration 6321, train loss: 0.002512, validation loss: 0.001906\n",
      "iteration 6322, train loss: 0.002614, validation loss: 0.002147\n",
      "iteration 6323, train loss: 0.002917, validation loss: 0.002036\n",
      "iteration 6324, train loss: 0.002902, validation loss: 0.001836\n",
      "iteration 6325, train loss: 0.002608, validation loss: 0.002142\n",
      "iteration 6326, train loss: 0.003066, validation loss: 0.002017\n",
      "iteration 6327, train loss: 0.002819, validation loss: 0.001841\n",
      "iteration 6328, train loss: 0.002728, validation loss: 0.00194\n",
      "iteration 6329, train loss: 0.002909, validation loss: 0.001996\n",
      "iteration 6330, train loss: 0.002767, validation loss: 0.002452\n",
      "iteration 6331, train loss: 0.003229, validation loss: 0.002293\n",
      "iteration 6332, train loss: 0.003104, validation loss: 0.002027\n",
      "iteration 6333, train loss: 0.002954, validation loss: 0.002108\n",
      "iteration 6334, train loss: 0.003239, validation loss: 0.002327\n",
      "iteration 6335, train loss: 0.003568, validation loss: 0.002077\n",
      "iteration 6336, train loss: 0.003038, validation loss: 0.001951\n",
      "iteration 6337, train loss: 0.00291, validation loss: 0.002339\n",
      "iteration 6338, train loss: 0.003727, validation loss: 0.002112\n",
      "iteration 6339, train loss: 0.00307, validation loss: 0.001836\n",
      "iteration 6340, train loss: 0.002878, validation loss: 0.001928\n",
      "iteration 6341, train loss: 0.002817, validation loss: 0.001929\n",
      "iteration 6342, train loss: 0.00284, validation loss: 0.001909\n",
      "iteration 6343, train loss: 0.003295, validation loss: 0.00193\n",
      "iteration 6344, train loss: 0.002886, validation loss: 0.002174\n",
      "iteration 6345, train loss: 0.003173, validation loss: 0.002001\n",
      "iteration 6346, train loss: 0.003141, validation loss: 0.001829\n",
      "iteration 6347, train loss: 0.002845, validation loss: 0.002129\n",
      "iteration 6348, train loss: 0.002906, validation loss: 0.002073\n",
      "iteration 6349, train loss: 0.003145, validation loss: 0.001748\n",
      "iteration 6350, train loss: 0.002645, validation loss: 0.001812\n",
      "iteration 6351, train loss: 0.002686, validation loss: 0.001951\n",
      "iteration 6352, train loss: 0.002935, validation loss: 0.001854\n",
      "iteration 6353, train loss: 0.002846, validation loss: 0.001778\n",
      "iteration 6354, train loss: 0.002838, validation loss: 0.0021\n",
      "iteration 6355, train loss: 0.00284, validation loss: 0.002004\n",
      "iteration 6356, train loss: 0.002869, validation loss: 0.001806\n",
      "iteration 6357, train loss: 0.002952, validation loss: 0.001979\n",
      "iteration 6358, train loss: 0.003002, validation loss: 0.00197\n",
      "iteration 6359, train loss: 0.003079, validation loss: 0.00175\n",
      "iteration 6360, train loss: 0.002717, validation loss: 0.002089\n",
      "iteration 6361, train loss: 0.003183, validation loss: 0.00207\n",
      "iteration 6362, train loss: 0.003, validation loss: 0.001869\n",
      "iteration 6363, train loss: 0.00309, validation loss: 0.00187\n",
      "iteration 6364, train loss: 0.002786, validation loss: 0.001981\n",
      "iteration 6365, train loss: 0.003161, validation loss: 0.001717\n",
      "iteration 6366, train loss: 0.002662, validation loss: 0.00189\n",
      "iteration 6367, train loss: 0.0031, validation loss: 0.001853\n",
      "iteration 6368, train loss: 0.002879, validation loss: 0.001767\n",
      "iteration 6369, train loss: 0.002646, validation loss: 0.001879\n",
      "iteration 6370, train loss: 0.002859, validation loss: 0.002019\n",
      "iteration 6371, train loss: 0.002757, validation loss: 0.001748\n",
      "iteration 6372, train loss: 0.002648, validation loss: 0.002117\n",
      "iteration 6373, train loss: 0.002967, validation loss: 0.002136\n",
      "iteration 6374, train loss: 0.003124, validation loss: 0.001738\n",
      "iteration 6375, train loss: 0.002697, validation loss: 0.001871\n",
      "iteration 6376, train loss: 0.002711, validation loss: 0.002197\n",
      "iteration 6377, train loss: 0.003374, validation loss: 0.001966\n",
      "iteration 6378, train loss: 0.002986, validation loss: 0.001769\n",
      "iteration 6379, train loss: 0.002808, validation loss: 0.002052\n",
      "iteration 6380, train loss: 0.002839, validation loss: 0.002071\n",
      "iteration 6381, train loss: 0.003353, validation loss: 0.00181\n",
      "iteration 6382, train loss: 0.002791, validation loss: 0.00193\n",
      "iteration 6383, train loss: 0.002943, validation loss: 0.001744\n",
      "iteration 6384, train loss: 0.002888, validation loss: 0.001919\n",
      "iteration 6385, train loss: 0.002934, validation loss: 0.002187\n",
      "iteration 6386, train loss: 0.003371, validation loss: 0.00196\n",
      "iteration 6387, train loss: 0.002728, validation loss: 0.002078\n",
      "iteration 6388, train loss: 0.003208, validation loss: 0.001833\n",
      "iteration 6389, train loss: 0.002766, validation loss: 0.001921\n",
      "iteration 6390, train loss: 0.003003, validation loss: 0.00235\n",
      "iteration 6391, train loss: 0.003253, validation loss: 0.002137\n",
      "iteration 6392, train loss: 0.003334, validation loss: 0.001754\n",
      "iteration 6393, train loss: 0.002688, validation loss: 0.002362\n",
      "iteration 6394, train loss: 0.003534, validation loss: 0.001915\n",
      "iteration 6395, train loss: 0.002743, validation loss: 0.00187\n",
      "iteration 6396, train loss: 0.002778, validation loss: 0.002128\n",
      "iteration 6397, train loss: 0.002994, validation loss: 0.001963\n",
      "iteration 6398, train loss: 0.003132, validation loss: 0.001895\n",
      "iteration 6399, train loss: 0.002672, validation loss: 0.00196\n",
      "iteration 6400, train loss: 0.002476, validation loss: 0.001925\n",
      "iteration 6401, train loss: 0.003003, validation loss: 0.001804\n",
      "iteration 6402, train loss: 0.002865, validation loss: 0.00197\n",
      "iteration 6403, train loss: 0.003069, validation loss: 0.0019\n",
      "iteration 6404, train loss: 0.002746, validation loss: 0.001794\n",
      "iteration 6405, train loss: 0.00298, validation loss: 0.002166\n",
      "iteration 6406, train loss: 0.003256, validation loss: 0.001926\n",
      "iteration 6407, train loss: 0.002873, validation loss: 0.001841\n",
      "iteration 6408, train loss: 0.002861, validation loss: 0.001875\n",
      "iteration 6409, train loss: 0.002885, validation loss: 0.002061\n",
      "iteration 6410, train loss: 0.00282, validation loss: 0.002177\n",
      "iteration 6411, train loss: 0.002781, validation loss: 0.001984\n",
      "iteration 6412, train loss: 0.002831, validation loss: 0.00185\n",
      "iteration 6413, train loss: 0.002888, validation loss: 0.001923\n",
      "iteration 6414, train loss: 0.002975, validation loss: 0.001983\n",
      "iteration 6415, train loss: 0.00269, validation loss: 0.001921\n",
      "iteration 6416, train loss: 0.00311, validation loss: 0.001905\n",
      "iteration 6417, train loss: 0.00302, validation loss: 0.001725\n",
      "iteration 6418, train loss: 0.00279, validation loss: 0.00191\n",
      "iteration 6419, train loss: 0.002834, validation loss: 0.002033\n",
      "iteration 6420, train loss: 0.003326, validation loss: 0.001894\n",
      "iteration 6421, train loss: 0.002684, validation loss: 0.002113\n",
      "iteration 6422, train loss: 0.00325, validation loss: 0.002173\n",
      "iteration 6423, train loss: 0.003063, validation loss: 0.001988\n",
      "iteration 6424, train loss: 0.002864, validation loss: 0.002008\n",
      "iteration 6425, train loss: 0.003069, validation loss: 0.002171\n",
      "iteration 6426, train loss: 0.003435, validation loss: 0.001951\n",
      "iteration 6427, train loss: 0.002871, validation loss: 0.001992\n",
      "iteration 6428, train loss: 0.003276, validation loss: 0.002223\n",
      "iteration 6429, train loss: 0.003023, validation loss: 0.002172\n",
      "iteration 6430, train loss: 0.003292, validation loss: 0.001791\n",
      "iteration 6431, train loss: 0.00277, validation loss: 0.002027\n",
      "iteration 6432, train loss: 0.002825, validation loss: 0.002138\n",
      "iteration 6433, train loss: 0.00313, validation loss: 0.001993\n",
      "iteration 6434, train loss: 0.002484, validation loss: 0.002105\n",
      "iteration 6435, train loss: 0.002921, validation loss: 0.001984\n",
      "iteration 6436, train loss: 0.002995, validation loss: 0.002061\n",
      "iteration 6437, train loss: 0.003131, validation loss: 0.002197\n",
      "iteration 6438, train loss: 0.003292, validation loss: 0.001864\n",
      "iteration 6439, train loss: 0.002829, validation loss: 0.001863\n",
      "iteration 6440, train loss: 0.002777, validation loss: 0.002058\n",
      "iteration 6441, train loss: 0.002893, validation loss: 0.001846\n",
      "iteration 6442, train loss: 0.002938, validation loss: 0.001881\n",
      "iteration 6443, train loss: 0.002907, validation loss: 0.002059\n",
      "iteration 6444, train loss: 0.00318, validation loss: 0.001777\n",
      "iteration 6445, train loss: 0.002527, validation loss: 0.001988\n",
      "iteration 6446, train loss: 0.002765, validation loss: 0.00191\n",
      "iteration 6447, train loss: 0.002412, validation loss: \u001b[92m0.001691\u001b[0m\n",
      "iteration 6448, train loss: 0.002803, validation loss: 0.001808\n",
      "iteration 6449, train loss: 0.002925, validation loss: 0.00179\n",
      "iteration 6450, train loss: 0.00279, validation loss: 0.001722\n",
      "iteration 6451, train loss: 0.002468, validation loss: 0.001845\n",
      "iteration 6452, train loss: 0.002827, validation loss: 0.001837\n",
      "iteration 6453, train loss: 0.002771, validation loss: 0.001708\n",
      "iteration 6454, train loss: 0.002655, validation loss: 0.001692\n",
      "iteration 6455, train loss: 0.002895, validation loss: 0.001851\n",
      "iteration 6456, train loss: 0.002819, validation loss: 0.00187\n",
      "iteration 6457, train loss: 0.003101, validation loss: 0.001732\n",
      "iteration 6458, train loss: 0.002845, validation loss: 0.001919\n",
      "iteration 6459, train loss: 0.002582, validation loss: 0.002062\n",
      "iteration 6460, train loss: 0.003248, validation loss: 0.001833\n",
      "iteration 6461, train loss: 0.002849, validation loss: 0.001962\n",
      "iteration 6462, train loss: 0.002863, validation loss: 0.001983\n",
      "iteration 6463, train loss: 0.002902, validation loss: 0.002021\n",
      "iteration 6464, train loss: 0.002972, validation loss: 0.002046\n",
      "iteration 6465, train loss: 0.003269, validation loss: 0.001698\n",
      "iteration 6466, train loss: 0.00288, validation loss: 0.00213\n",
      "iteration 6467, train loss: 0.00321, validation loss: 0.002455\n",
      "iteration 6468, train loss: 0.003171, validation loss: 0.001978\n",
      "iteration 6469, train loss: 0.003451, validation loss: 0.00174\n",
      "iteration 6470, train loss: 0.003015, validation loss: 0.002179\n",
      "iteration 6471, train loss: 0.003047, validation loss: 0.002205\n",
      "iteration 6472, train loss: 0.002898, validation loss: 0.00186\n",
      "iteration 6473, train loss: 0.00316, validation loss: 0.001779\n",
      "iteration 6474, train loss: 0.002549, validation loss: 0.002083\n",
      "iteration 6475, train loss: 0.002905, validation loss: 0.001972\n",
      "iteration 6476, train loss: 0.002888, validation loss: 0.001721\n",
      "iteration 6477, train loss: 0.002513, validation loss: 0.001877\n",
      "iteration 6478, train loss: 0.003031, validation loss: 0.00188\n",
      "iteration 6479, train loss: 0.003294, validation loss: 0.001831\n",
      "iteration 6480, train loss: 0.002849, validation loss: 0.001935\n",
      "iteration 6481, train loss: 0.002884, validation loss: 0.002069\n",
      "iteration 6482, train loss: 0.002747, validation loss: 0.002081\n",
      "iteration 6483, train loss: 0.002801, validation loss: 0.00185\n",
      "iteration 6484, train loss: 0.002616, validation loss: 0.001726\n",
      "iteration 6485, train loss: 0.002647, validation loss: 0.001827\n",
      "iteration 6486, train loss: 0.00299, validation loss: 0.001816\n",
      "iteration 6487, train loss: 0.002867, validation loss: 0.001898\n",
      "iteration 6488, train loss: 0.002584, validation loss: 0.002012\n",
      "iteration 6489, train loss: 0.003024, validation loss: 0.001818\n",
      "iteration 6490, train loss: 0.002757, validation loss: 0.001903\n",
      "iteration 6491, train loss: 0.003018, validation loss: 0.001777\n",
      "iteration 6492, train loss: 0.00272, validation loss: 0.001848\n",
      "iteration 6493, train loss: 0.002849, validation loss: 0.001764\n",
      "iteration 6494, train loss: 0.002669, validation loss: 0.001927\n",
      "iteration 6495, train loss: 0.002833, validation loss: 0.001845\n",
      "iteration 6496, train loss: 0.002629, validation loss: 0.001761\n",
      "iteration 6497, train loss: 0.002659, validation loss: 0.002023\n",
      "iteration 6498, train loss: 0.00293, validation loss: 0.001909\n",
      "iteration 6499, train loss: 0.003, validation loss: 0.001812\n",
      "iteration 6500, train loss: 0.002802, validation loss: 0.00205\n",
      "iteration 6501, train loss: 0.003174, validation loss: 0.001714\n",
      "iteration 6502, train loss: 0.002581, validation loss: 0.001898\n",
      "iteration 6503, train loss: 0.002742, validation loss: 0.002039\n",
      "iteration 6504, train loss: 0.003066, validation loss: 0.001928\n",
      "iteration 6505, train loss: 0.003245, validation loss: 0.001839\n",
      "iteration 6506, train loss: 0.002659, validation loss: 0.001985\n",
      "iteration 6507, train loss: 0.002953, validation loss: 0.001847\n",
      "iteration 6508, train loss: 0.00255, validation loss: 0.002011\n",
      "iteration 6509, train loss: 0.003074, validation loss: 0.001852\n",
      "iteration 6510, train loss: 0.002888, validation loss: 0.001772\n",
      "iteration 6511, train loss: 0.002492, validation loss: 0.002123\n",
      "iteration 6512, train loss: 0.003103, validation loss: 0.001786\n",
      "iteration 6513, train loss: 0.002848, validation loss: 0.001749\n",
      "iteration 6514, train loss: 0.00329, validation loss: 0.002195\n",
      "iteration 6515, train loss: 0.003108, validation loss: 0.002202\n",
      "iteration 6516, train loss: 0.003251, validation loss: 0.001914\n",
      "iteration 6517, train loss: 0.002679, validation loss: 0.002359\n",
      "iteration 6518, train loss: 0.003082, validation loss: 0.00241\n",
      "iteration 6519, train loss: 0.003426, validation loss: 0.002045\n",
      "iteration 6520, train loss: 0.002854, validation loss: 0.002171\n",
      "iteration 6521, train loss: 0.003085, validation loss: 0.002108\n",
      "iteration 6522, train loss: 0.00352, validation loss: 0.001871\n",
      "iteration 6523, train loss: 0.003142, validation loss: 0.002356\n",
      "iteration 6524, train loss: 0.003197, validation loss: 0.001987\n",
      "iteration 6525, train loss: 0.002874, validation loss: 0.002068\n",
      "iteration 6526, train loss: 0.002882, validation loss: 0.002414\n",
      "iteration 6527, train loss: 0.003352, validation loss: 0.002181\n",
      "iteration 6528, train loss: 0.002818, validation loss: 0.001894\n",
      "iteration 6529, train loss: 0.002803, validation loss: 0.001877\n",
      "iteration 6530, train loss: 0.002858, validation loss: 0.001808\n",
      "iteration 6531, train loss: 0.003323, validation loss: 0.001808\n",
      "iteration 6532, train loss: 0.00259, validation loss: 0.001765\n",
      "iteration 6533, train loss: 0.002673, validation loss: 0.001709\n",
      "iteration 6534, train loss: 0.002632, validation loss: 0.002163\n",
      "iteration 6535, train loss: 0.00277, validation loss: 0.002374\n",
      "iteration 6536, train loss: 0.00333, validation loss: 0.001738\n",
      "iteration 6537, train loss: 0.002968, validation loss: 0.002157\n",
      "iteration 6538, train loss: 0.003012, validation loss: 0.002615\n",
      "iteration 6539, train loss: 0.004138, validation loss: 0.001716\n",
      "iteration 6540, train loss: 0.002832, validation loss: 0.002525\n",
      "iteration 6541, train loss: 0.003444, validation loss: 0.00299\n",
      "iteration 6542, train loss: 0.004151, validation loss: 0.00227\n",
      "iteration 6543, train loss: 0.003323, validation loss: 0.001924\n",
      "iteration 6544, train loss: 0.002689, validation loss: 0.002835\n",
      "iteration 6545, train loss: 0.003903, validation loss: 0.002318\n",
      "iteration 6546, train loss: 0.003313, validation loss: 0.001844\n",
      "iteration 6547, train loss: 0.002653, validation loss: 0.002648\n",
      "iteration 6548, train loss: 0.003691, validation loss: 0.002154\n",
      "iteration 6549, train loss: 0.003442, validation loss: 0.001721\n",
      "iteration 6550, train loss: 0.002531, validation loss: 0.002566\n",
      "iteration 6551, train loss: 0.003538, validation loss: 0.002531\n",
      "iteration 6552, train loss: 0.003445, validation loss: 0.001812\n",
      "iteration 6553, train loss: 0.002935, validation loss: 0.001932\n",
      "iteration 6554, train loss: 0.00308, validation loss: 0.001897\n",
      "iteration 6555, train loss: 0.002816, validation loss: 0.001793\n",
      "iteration 6556, train loss: 0.002597, validation loss: 0.00182\n",
      "iteration 6557, train loss: 0.003072, validation loss: 0.001798\n",
      "iteration 6558, train loss: 0.002943, validation loss: 0.001728\n",
      "iteration 6559, train loss: 0.002769, validation loss: 0.001843\n",
      "iteration 6560, train loss: 0.002888, validation loss: 0.001783\n",
      "iteration 6561, train loss: 0.002768, validation loss: 0.001873\n",
      "iteration 6562, train loss: 0.002918, validation loss: 0.002003\n",
      "iteration 6563, train loss: 0.002888, validation loss: 0.00189\n",
      "iteration 6564, train loss: 0.002798, validation loss: 0.001718\n",
      "iteration 6565, train loss: 0.002745, validation loss: 0.001865\n",
      "iteration 6566, train loss: 0.00295, validation loss: 0.001993\n",
      "iteration 6567, train loss: 0.002772, validation loss: 0.0021\n",
      "iteration 6568, train loss: 0.003093, validation loss: 0.001961\n",
      "iteration 6569, train loss: 0.002849, validation loss: 0.001746\n",
      "iteration 6570, train loss: 0.002625, validation loss: 0.001861\n",
      "iteration 6571, train loss: 0.002643, validation loss: 0.001951\n",
      "iteration 6572, train loss: 0.003053, validation loss: 0.001717\n",
      "iteration 6573, train loss: 0.002725, validation loss: 0.001933\n",
      "iteration 6574, train loss: 0.00273, validation loss: 0.002123\n",
      "iteration 6575, train loss: 0.00315, validation loss: 0.001956\n",
      "iteration 6576, train loss: 0.00271, validation loss: 0.001983\n",
      "iteration 6577, train loss: 0.002988, validation loss: 0.001953\n",
      "iteration 6578, train loss: 0.002614, validation loss: 0.002216\n",
      "iteration 6579, train loss: 0.00313, validation loss: 0.00207\n",
      "iteration 6580, train loss: 0.002992, validation loss: 0.001935\n",
      "iteration 6581, train loss: 0.003126, validation loss: 0.002096\n",
      "iteration 6582, train loss: 0.003192, validation loss: 0.00214\n",
      "iteration 6583, train loss: 0.00317, validation loss: 0.00185\n",
      "iteration 6584, train loss: 0.002583, validation loss: 0.001836\n",
      "iteration 6585, train loss: 0.002692, validation loss: 0.001924\n",
      "iteration 6586, train loss: 0.002811, validation loss: 0.001723\n",
      "iteration 6587, train loss: 0.002684, validation loss: 0.001768\n",
      "iteration 6588, train loss: 0.002575, validation loss: 0.002064\n",
      "iteration 6589, train loss: 0.003161, validation loss: 0.001906\n",
      "iteration 6590, train loss: 0.00264, validation loss: 0.001925\n",
      "iteration 6591, train loss: 0.002596, validation loss: 0.001975\n",
      "iteration 6592, train loss: 0.003053, validation loss: 0.001739\n",
      "iteration 6593, train loss: 0.002773, validation loss: 0.001975\n",
      "iteration 6594, train loss: 0.003126, validation loss: 0.001917\n",
      "iteration 6595, train loss: 0.003067, validation loss: 0.001917\n",
      "iteration 6596, train loss: 0.002658, validation loss: 0.001868\n",
      "iteration 6597, train loss: 0.002963, validation loss: 0.00173\n",
      "iteration 6598, train loss: 0.002637, validation loss: 0.001789\n",
      "iteration 6599, train loss: 0.002669, validation loss: 0.001789\n",
      "iteration 6600, train loss: 0.003144, validation loss: 0.001765\n",
      "iteration 6601, train loss: 0.002765, validation loss: 0.001694\n",
      "iteration 6602, train loss: 0.002799, validation loss: 0.001753\n",
      "iteration 6603, train loss: 0.002631, validation loss: 0.001763\n",
      "iteration 6604, train loss: 0.002833, validation loss: 0.001772\n",
      "iteration 6605, train loss: 0.002409, validation loss: 0.001823\n",
      "iteration 6606, train loss: 0.002937, validation loss: 0.001777\n",
      "iteration 6607, train loss: 0.00268, validation loss: 0.001805\n",
      "iteration 6608, train loss: 0.002652, validation loss: 0.001804\n",
      "iteration 6609, train loss: 0.002776, validation loss: 0.001782\n",
      "iteration 6610, train loss: 0.002443, validation loss: 0.001801\n",
      "iteration 6611, train loss: 0.00285, validation loss: 0.001781\n",
      "iteration 6612, train loss: 0.002917, validation loss: 0.00171\n",
      "iteration 6613, train loss: 0.002656, validation loss: 0.001998\n",
      "iteration 6614, train loss: 0.002989, validation loss: 0.002012\n",
      "iteration 6615, train loss: 0.002584, validation loss: 0.001865\n",
      "iteration 6616, train loss: 0.002856, validation loss: \u001b[92m0.001689\u001b[0m\n",
      "iteration 6617, train loss: 0.002832, validation loss: 0.00198\n",
      "iteration 6618, train loss: 0.003053, validation loss: 0.001993\n",
      "iteration 6619, train loss: 0.002931, validation loss: 0.001742\n",
      "iteration 6620, train loss: 0.002568, validation loss: 0.001852\n",
      "iteration 6621, train loss: 0.002749, validation loss: 0.001934\n",
      "iteration 6622, train loss: 0.002741, validation loss: 0.001814\n",
      "iteration 6623, train loss: 0.002541, validation loss: 0.001919\n",
      "iteration 6624, train loss: 0.002799, validation loss: 0.001907\n",
      "iteration 6625, train loss: 0.002976, validation loss: 0.001733\n",
      "iteration 6626, train loss: 0.002655, validation loss: 0.001847\n",
      "iteration 6627, train loss: 0.002905, validation loss: 0.001785\n",
      "iteration 6628, train loss: 0.002581, validation loss: 0.001859\n",
      "iteration 6629, train loss: 0.003082, validation loss: 0.001931\n",
      "iteration 6630, train loss: 0.00282, validation loss: 0.001767\n",
      "iteration 6631, train loss: 0.002638, validation loss: 0.002075\n",
      "iteration 6632, train loss: 0.002978, validation loss: 0.001918\n",
      "iteration 6633, train loss: 0.002774, validation loss: 0.001705\n",
      "iteration 6634, train loss: 0.002765, validation loss: 0.001872\n",
      "iteration 6635, train loss: 0.003293, validation loss: 0.001743\n",
      "iteration 6636, train loss: 0.002647, validation loss: 0.001788\n",
      "iteration 6637, train loss: 0.002849, validation loss: 0.002045\n",
      "iteration 6638, train loss: 0.002674, validation loss: 0.00213\n",
      "iteration 6639, train loss: 0.002941, validation loss: 0.001734\n",
      "iteration 6640, train loss: 0.002574, validation loss: 0.001973\n",
      "iteration 6641, train loss: 0.00329, validation loss: 0.00177\n",
      "iteration 6642, train loss: 0.002643, validation loss: 0.001791\n",
      "iteration 6643, train loss: 0.002776, validation loss: 0.001861\n",
      "iteration 6644, train loss: 0.002546, validation loss: 0.001797\n",
      "iteration 6645, train loss: 0.002839, validation loss: 0.00173\n",
      "iteration 6646, train loss: 0.002641, validation loss: 0.001976\n",
      "iteration 6647, train loss: 0.003061, validation loss: 0.001793\n",
      "iteration 6648, train loss: 0.002549, validation loss: 0.001839\n",
      "iteration 6649, train loss: 0.002666, validation loss: 0.001836\n",
      "iteration 6650, train loss: 0.002743, validation loss: 0.001849\n",
      "iteration 6651, train loss: 0.002625, validation loss: 0.00187\n",
      "iteration 6652, train loss: 0.002664, validation loss: 0.001803\n",
      "iteration 6653, train loss: 0.002613, validation loss: 0.001895\n",
      "iteration 6654, train loss: 0.002851, validation loss: 0.001948\n",
      "iteration 6655, train loss: 0.003116, validation loss: 0.001748\n",
      "iteration 6656, train loss: 0.00251, validation loss: 0.001804\n",
      "iteration 6657, train loss: 0.002824, validation loss: 0.002002\n",
      "iteration 6658, train loss: 0.002528, validation loss: 0.002199\n",
      "iteration 6659, train loss: 0.002869, validation loss: 0.001732\n",
      "iteration 6660, train loss: 0.002856, validation loss: 0.002004\n",
      "iteration 6661, train loss: 0.002744, validation loss: 0.002635\n",
      "iteration 6662, train loss: 0.003839, validation loss: 0.002019\n",
      "iteration 6663, train loss: 0.003367, validation loss: 0.001944\n",
      "iteration 6664, train loss: 0.002863, validation loss: 0.00261\n",
      "iteration 6665, train loss: 0.003861, validation loss: 0.002016\n",
      "iteration 6666, train loss: 0.003098, validation loss: 0.001986\n",
      "iteration 6667, train loss: 0.003309, validation loss: 0.002354\n",
      "iteration 6668, train loss: 0.003161, validation loss: 0.002138\n",
      "iteration 6669, train loss: 0.003163, validation loss: 0.00195\n",
      "iteration 6670, train loss: 0.002663, validation loss: 0.002105\n",
      "iteration 6671, train loss: 0.00259, validation loss: 0.00258\n",
      "iteration 6672, train loss: 0.002899, validation loss: 0.00243\n",
      "iteration 6673, train loss: 0.003258, validation loss: 0.001836\n",
      "iteration 6674, train loss: 0.002825, validation loss: 0.001812\n",
      "iteration 6675, train loss: 0.002788, validation loss: 0.001929\n",
      "iteration 6676, train loss: 0.002867, validation loss: 0.001763\n",
      "iteration 6677, train loss: 0.002775, validation loss: 0.001711\n",
      "iteration 6678, train loss: 0.002874, validation loss: 0.001745\n",
      "iteration 6679, train loss: 0.002659, validation loss: 0.001767\n",
      "iteration 6680, train loss: 0.002741, validation loss: \u001b[92m0.001663\u001b[0m\n",
      "iteration 6681, train loss: 0.00243, validation loss: 0.001669\n",
      "iteration 6682, train loss: 0.002406, validation loss: 0.001729\n",
      "iteration 6683, train loss: 0.002693, validation loss: 0.001816\n",
      "iteration 6684, train loss: 0.002745, validation loss: 0.00177\n",
      "iteration 6685, train loss: 0.002741, validation loss: 0.001932\n",
      "iteration 6686, train loss: 0.002803, validation loss: 0.001869\n",
      "iteration 6687, train loss: 0.002571, validation loss: 0.001764\n",
      "iteration 6688, train loss: 0.002565, validation loss: 0.001718\n",
      "iteration 6689, train loss: 0.002599, validation loss: 0.00171\n",
      "iteration 6690, train loss: 0.002584, validation loss: 0.001717\n",
      "iteration 6691, train loss: 0.00257, validation loss: 0.001843\n",
      "iteration 6692, train loss: 0.002894, validation loss: 0.001794\n",
      "iteration 6693, train loss: 0.002469, validation loss: 0.001752\n",
      "iteration 6694, train loss: 0.002669, validation loss: \u001b[92m0.001653\u001b[0m\n",
      "iteration 6695, train loss: \u001b[92m0.00236\u001b[0m, validation loss: 0.001772\n",
      "iteration 6696, train loss: 0.00267, validation loss: 0.001736\n",
      "iteration 6697, train loss: 0.002748, validation loss: 0.001768\n",
      "iteration 6698, train loss: 0.00289, validation loss: 0.001702\n",
      "iteration 6699, train loss: 0.002584, validation loss: 0.001695\n",
      "iteration 6700, train loss: 0.002953, validation loss: 0.0017\n",
      "iteration 6701, train loss: 0.002586, validation loss: 0.001723\n",
      "iteration 6702, train loss: 0.002595, validation loss: 0.001817\n",
      "iteration 6703, train loss: 0.002992, validation loss: 0.001702\n",
      "iteration 6704, train loss: 0.002767, validation loss: 0.001662\n",
      "iteration 6705, train loss: 0.002736, validation loss: 0.001749\n",
      "iteration 6706, train loss: 0.002804, validation loss: 0.001804\n",
      "iteration 6707, train loss: \u001b[92m0.002302\u001b[0m, validation loss: 0.001896\n",
      "iteration 6708, train loss: 0.003037, validation loss: 0.00182\n",
      "iteration 6709, train loss: 0.002771, validation loss: 0.001872\n",
      "iteration 6710, train loss: 0.002808, validation loss: 0.001917\n",
      "iteration 6711, train loss: 0.002767, validation loss: 0.001884\n",
      "iteration 6712, train loss: 0.002802, validation loss: 0.001738\n",
      "iteration 6713, train loss: 0.002757, validation loss: 0.001747\n",
      "iteration 6714, train loss: 0.002931, validation loss: 0.001738\n",
      "iteration 6715, train loss: 0.002727, validation loss: 0.001683\n",
      "iteration 6716, train loss: 0.002429, validation loss: 0.001921\n",
      "iteration 6717, train loss: 0.002853, validation loss: 0.001907\n",
      "iteration 6718, train loss: 0.00292, validation loss: 0.001704\n",
      "iteration 6719, train loss: 0.002679, validation loss: 0.001871\n",
      "iteration 6720, train loss: 0.002746, validation loss: 0.001835\n",
      "iteration 6721, train loss: 0.002834, validation loss: 0.001775\n",
      "iteration 6722, train loss: 0.002648, validation loss: 0.002047\n",
      "iteration 6723, train loss: 0.002742, validation loss: 0.002029\n",
      "iteration 6724, train loss: 0.00292, validation loss: 0.001971\n",
      "iteration 6725, train loss: 0.002887, validation loss: 0.001882\n",
      "iteration 6726, train loss: 0.002605, validation loss: 0.001839\n",
      "iteration 6727, train loss: 0.002973, validation loss: 0.001834\n",
      "iteration 6728, train loss: 0.002722, validation loss: 0.001939\n",
      "iteration 6729, train loss: 0.003095, validation loss: 0.00185\n",
      "iteration 6730, train loss: 0.002705, validation loss: 0.001967\n",
      "iteration 6731, train loss: 0.003181, validation loss: 0.002162\n",
      "iteration 6732, train loss: 0.002981, validation loss: 0.001853\n",
      "iteration 6733, train loss: 0.003001, validation loss: 0.001749\n",
      "iteration 6734, train loss: 0.002581, validation loss: 0.001862\n",
      "iteration 6735, train loss: 0.002943, validation loss: 0.001849\n",
      "iteration 6736, train loss: 0.002905, validation loss: 0.001674\n",
      "iteration 6737, train loss: 0.002628, validation loss: 0.001896\n",
      "iteration 6738, train loss: 0.00302, validation loss: 0.002195\n",
      "iteration 6739, train loss: 0.002937, validation loss: 0.001984\n",
      "iteration 6740, train loss: 0.002915, validation loss: 0.001823\n",
      "iteration 6741, train loss: 0.002895, validation loss: 0.002323\n",
      "iteration 6742, train loss: 0.003076, validation loss: 0.002383\n",
      "iteration 6743, train loss: 0.003235, validation loss: 0.001874\n",
      "iteration 6744, train loss: 0.002884, validation loss: 0.002304\n",
      "iteration 6745, train loss: 0.003156, validation loss: 0.00263\n",
      "iteration 6746, train loss: 0.004012, validation loss: 0.002494\n",
      "iteration 6747, train loss: 0.003334, validation loss: 0.002198\n",
      "iteration 6748, train loss: 0.00302, validation loss: 0.002028\n",
      "iteration 6749, train loss: 0.003354, validation loss: 0.002133\n",
      "iteration 6750, train loss: 0.003328, validation loss: 0.002255\n",
      "iteration 6751, train loss: 0.003235, validation loss: 0.002063\n",
      "iteration 6752, train loss: 0.003284, validation loss: 0.001899\n",
      "iteration 6753, train loss: 0.00306, validation loss: 0.002087\n",
      "iteration 6754, train loss: 0.00315, validation loss: 0.002092\n",
      "iteration 6755, train loss: 0.003093, validation loss: 0.00168\n",
      "iteration 6756, train loss: 0.002818, validation loss: 0.001956\n",
      "iteration 6757, train loss: 0.002469, validation loss: 0.002359\n",
      "iteration 6758, train loss: 0.00355, validation loss: 0.001994\n",
      "iteration 6759, train loss: 0.002997, validation loss: 0.001772\n",
      "iteration 6760, train loss: 0.002715, validation loss: 0.001913\n",
      "iteration 6761, train loss: 0.002586, validation loss: 0.001972\n",
      "iteration 6762, train loss: 0.002957, validation loss: 0.001719\n",
      "iteration 6763, train loss: 0.002712, validation loss: 0.001717\n",
      "iteration 6764, train loss: 0.002708, validation loss: 0.001861\n",
      "iteration 6765, train loss: 0.002872, validation loss: 0.001814\n",
      "iteration 6766, train loss: 0.002616, validation loss: 0.001739\n",
      "iteration 6767, train loss: 0.002658, validation loss: 0.001809\n",
      "iteration 6768, train loss: 0.002704, validation loss: 0.001847\n",
      "iteration 6769, train loss: 0.002665, validation loss: 0.001795\n",
      "iteration 6770, train loss: 0.002812, validation loss: 0.001751\n",
      "iteration 6771, train loss: 0.002579, validation loss: 0.002157\n",
      "iteration 6772, train loss: 0.00287, validation loss: 0.002072\n",
      "iteration 6773, train loss: 0.003074, validation loss: 0.001791\n",
      "iteration 6774, train loss: 0.002807, validation loss: 0.001887\n",
      "iteration 6775, train loss: 0.003107, validation loss: 0.001862\n",
      "iteration 6776, train loss: 0.002555, validation loss: 0.001863\n",
      "iteration 6777, train loss: 0.002572, validation loss: 0.00191\n",
      "iteration 6778, train loss: 0.002965, validation loss: 0.00191\n",
      "iteration 6779, train loss: 0.002941, validation loss: 0.001899\n",
      "iteration 6780, train loss: 0.002728, validation loss: 0.001794\n",
      "iteration 6781, train loss: 0.002869, validation loss: 0.00174\n",
      "iteration 6782, train loss: 0.002811, validation loss: 0.001761\n",
      "iteration 6783, train loss: 0.002905, validation loss: 0.00186\n",
      "iteration 6784, train loss: 0.002563, validation loss: 0.001994\n",
      "iteration 6785, train loss: 0.0028, validation loss: 0.001875\n",
      "iteration 6786, train loss: 0.003004, validation loss: 0.001789\n",
      "iteration 6787, train loss: 0.002979, validation loss: 0.001903\n",
      "iteration 6788, train loss: 0.002832, validation loss: 0.002034\n",
      "iteration 6789, train loss: 0.002778, validation loss: 0.001993\n",
      "iteration 6790, train loss: 0.002796, validation loss: 0.002066\n",
      "iteration 6791, train loss: 0.002994, validation loss: 0.002279\n",
      "iteration 6792, train loss: 0.003279, validation loss: 0.002026\n",
      "iteration 6793, train loss: 0.003111, validation loss: 0.001962\n",
      "iteration 6794, train loss: 0.002828, validation loss: 0.001863\n",
      "iteration 6795, train loss: 0.003014, validation loss: 0.001793\n",
      "iteration 6796, train loss: 0.0031, validation loss: 0.002173\n",
      "iteration 6797, train loss: 0.003244, validation loss: 0.002087\n",
      "iteration 6798, train loss: 0.003062, validation loss: 0.001774\n",
      "iteration 6799, train loss: 0.002455, validation loss: 0.002042\n",
      "iteration 6800, train loss: 0.00333, validation loss: 0.001857\n",
      "iteration 6801, train loss: 0.002689, validation loss: 0.001976\n",
      "iteration 6802, train loss: 0.003315, validation loss: 0.002002\n",
      "iteration 6803, train loss: 0.003015, validation loss: 0.001697\n",
      "iteration 6804, train loss: 0.002621, validation loss: 0.001879\n",
      "iteration 6805, train loss: 0.002758, validation loss: 0.002192\n",
      "iteration 6806, train loss: 0.003321, validation loss: 0.00177\n",
      "iteration 6807, train loss: 0.002765, validation loss: 0.00184\n",
      "iteration 6808, train loss: 0.003173, validation loss: 0.001904\n",
      "iteration 6809, train loss: 0.002981, validation loss: 0.001827\n",
      "iteration 6810, train loss: 0.003118, validation loss: 0.001707\n",
      "iteration 6811, train loss: 0.00274, validation loss: 0.002059\n",
      "iteration 6812, train loss: 0.00308, validation loss: 0.002015\n",
      "iteration 6813, train loss: 0.002741, validation loss: 0.001866\n",
      "iteration 6814, train loss: 0.002919, validation loss: 0.001705\n",
      "iteration 6815, train loss: 0.002455, validation loss: 0.00212\n",
      "iteration 6816, train loss: 0.003215, validation loss: 0.002144\n",
      "iteration 6817, train loss: 0.003196, validation loss: 0.001755\n",
      "iteration 6818, train loss: 0.002712, validation loss: 0.002068\n",
      "iteration 6819, train loss: 0.003143, validation loss: 0.002206\n",
      "iteration 6820, train loss: 0.002811, validation loss: 0.002143\n",
      "iteration 6821, train loss: 0.002746, validation loss: 0.002075\n",
      "iteration 6822, train loss: 0.003696, validation loss: 0.001834\n",
      "iteration 6823, train loss: 0.002832, validation loss: 0.002236\n",
      "iteration 6824, train loss: 0.003062, validation loss: 0.002364\n",
      "iteration 6825, train loss: 0.003062, validation loss: 0.002105\n",
      "iteration 6826, train loss: 0.003199, validation loss: 0.001917\n",
      "iteration 6827, train loss: 0.003181, validation loss: 0.002121\n",
      "iteration 6828, train loss: 0.003089, validation loss: 0.002026\n",
      "iteration 6829, train loss: 0.002832, validation loss: 0.001863\n",
      "iteration 6830, train loss: 0.002798, validation loss: 0.002295\n",
      "iteration 6831, train loss: 0.003121, validation loss: 0.002494\n",
      "iteration 6832, train loss: 0.003478, validation loss: 0.002036\n",
      "iteration 6833, train loss: 0.003176, validation loss: 0.002035\n",
      "iteration 6834, train loss: 0.002856, validation loss: 0.002262\n",
      "iteration 6835, train loss: 0.002872, validation loss: 0.00225\n",
      "iteration 6836, train loss: 0.003148, validation loss: 0.00216\n",
      "iteration 6837, train loss: 0.003354, validation loss: 0.002255\n",
      "iteration 6838, train loss: 0.003263, validation loss: 0.002512\n",
      "iteration 6839, train loss: 0.003311, validation loss: 0.002347\n",
      "iteration 6840, train loss: 0.003509, validation loss: 0.001757\n",
      "iteration 6841, train loss: 0.002642, validation loss: 0.002254\n",
      "iteration 6842, train loss: 0.003507, validation loss: 0.002442\n",
      "iteration 6843, train loss: 0.002955, validation loss: 0.002137\n",
      "iteration 6844, train loss: 0.003405, validation loss: 0.001781\n",
      "iteration 6845, train loss: 0.002874, validation loss: 0.001971\n",
      "iteration 6846, train loss: 0.003037, validation loss: 0.00187\n",
      "iteration 6847, train loss: 0.003197, validation loss: 0.001818\n",
      "iteration 6848, train loss: 0.002718, validation loss: 0.002102\n",
      "iteration 6849, train loss: 0.002774, validation loss: 0.002224\n",
      "iteration 6850, train loss: 0.003276, validation loss: 0.002009\n",
      "iteration 6851, train loss: 0.002913, validation loss: 0.002091\n",
      "iteration 6852, train loss: 0.00328, validation loss: 0.001955\n",
      "iteration 6853, train loss: 0.003105, validation loss: 0.001704\n",
      "iteration 6854, train loss: 0.002678, validation loss: 0.002015\n",
      "iteration 6855, train loss: 0.002996, validation loss: 0.001993\n",
      "iteration 6856, train loss: 0.003058, validation loss: 0.001758\n",
      "iteration 6857, train loss: 0.002773, validation loss: 0.001927\n",
      "iteration 6858, train loss: 0.002678, validation loss: 0.002295\n",
      "iteration 6859, train loss: 0.002943, validation loss: 0.001974\n",
      "iteration 6860, train loss: 0.003004, validation loss: 0.001673\n",
      "iteration 6861, train loss: \u001b[92m0.002259\u001b[0m, validation loss: 0.00209\n",
      "iteration 6862, train loss: 0.003114, validation loss: 0.001992\n",
      "iteration 6863, train loss: 0.003131, validation loss: 0.001727\n",
      "iteration 6864, train loss: 0.002896, validation loss: 0.002284\n",
      "iteration 6865, train loss: 0.003201, validation loss: 0.002315\n",
      "iteration 6866, train loss: 0.002715, validation loss: 0.002097\n",
      "iteration 6867, train loss: 0.00278, validation loss: 0.002005\n",
      "iteration 6868, train loss: 0.002918, validation loss: 0.002119\n",
      "iteration 6869, train loss: 0.003072, validation loss: 0.00199\n",
      "iteration 6870, train loss: 0.002893, validation loss: 0.001965\n",
      "iteration 6871, train loss: 0.002958, validation loss: 0.002057\n",
      "iteration 6872, train loss: 0.003159, validation loss: 0.001866\n",
      "iteration 6873, train loss: 0.002913, validation loss: 0.002402\n",
      "iteration 6874, train loss: 0.003107, validation loss: 0.002673\n",
      "iteration 6875, train loss: 0.003297, validation loss: 0.002107\n",
      "iteration 6876, train loss: 0.002918, validation loss: 0.001772\n",
      "iteration 6877, train loss: 0.002631, validation loss: 0.00236\n",
      "iteration 6878, train loss: 0.003456, validation loss: 0.002115\n",
      "iteration 6879, train loss: 0.002812, validation loss: 0.001683\n",
      "iteration 6880, train loss: 0.002667, validation loss: 0.001853\n",
      "iteration 6881, train loss: 0.002988, validation loss: 0.002015\n",
      "iteration 6882, train loss: 0.002949, validation loss: 0.001917\n",
      "iteration 6883, train loss: 0.0031, validation loss: \u001b[92m0.00165\u001b[0m\n",
      "iteration 6884, train loss: 0.002795, validation loss: 0.002014\n",
      "iteration 6885, train loss: 0.002695, validation loss: 0.002209\n",
      "iteration 6886, train loss: 0.003414, validation loss: 0.001971\n",
      "iteration 6887, train loss: 0.003006, validation loss: 0.00178\n",
      "iteration 6888, train loss: 0.002506, validation loss: 0.001985\n",
      "iteration 6889, train loss: 0.003175, validation loss: 0.002124\n",
      "iteration 6890, train loss: 0.00346, validation loss: 0.001756\n",
      "iteration 6891, train loss: 0.002432, validation loss: \u001b[92m0.001629\u001b[0m\n",
      "iteration 6892, train loss: 0.002567, validation loss: 0.001769\n",
      "iteration 6893, train loss: 0.002771, validation loss: 0.001914\n",
      "iteration 6894, train loss: 0.002858, validation loss: 0.001809\n",
      "iteration 6895, train loss: 0.002769, validation loss: 0.001776\n",
      "iteration 6896, train loss: 0.002865, validation loss: 0.001695\n",
      "iteration 6897, train loss: 0.002689, validation loss: 0.001703\n",
      "iteration 6898, train loss: 0.002652, validation loss: 0.001765\n",
      "iteration 6899, train loss: 0.002792, validation loss: 0.00169\n",
      "iteration 6900, train loss: 0.002934, validation loss: 0.001774\n",
      "iteration 6901, train loss: 0.002795, validation loss: 0.001869\n",
      "iteration 6902, train loss: 0.002785, validation loss: 0.00177\n",
      "iteration 6903, train loss: 0.002783, validation loss: 0.001738\n",
      "iteration 6904, train loss: 0.002662, validation loss: 0.001739\n",
      "iteration 6905, train loss: 0.002462, validation loss: 0.001729\n",
      "iteration 6906, train loss: 0.002597, validation loss: 0.001838\n",
      "iteration 6907, train loss: 0.002872, validation loss: 0.001669\n",
      "iteration 6908, train loss: 0.002823, validation loss: 0.001785\n",
      "iteration 6909, train loss: 0.002855, validation loss: 0.00187\n",
      "iteration 6910, train loss: 0.002932, validation loss: 0.001821\n",
      "iteration 6911, train loss: 0.002806, validation loss: 0.001747\n",
      "iteration 6912, train loss: 0.002495, validation loss: 0.001671\n",
      "iteration 6913, train loss: 0.002669, validation loss: 0.001796\n",
      "iteration 6914, train loss: 0.002992, validation loss: 0.001786\n",
      "iteration 6915, train loss: 0.002873, validation loss: 0.001705\n",
      "iteration 6916, train loss: 0.002522, validation loss: 0.002003\n",
      "iteration 6917, train loss: 0.002604, validation loss: 0.002167\n",
      "iteration 6918, train loss: 0.003386, validation loss: 0.001847\n",
      "iteration 6919, train loss: 0.002942, validation loss: 0.002539\n",
      "iteration 6920, train loss: 0.003315, validation loss: 0.002816\n",
      "iteration 6921, train loss: 0.003301, validation loss: 0.00235\n",
      "iteration 6922, train loss: 0.003222, validation loss: 0.001828\n",
      "iteration 6923, train loss: 0.002768, validation loss: 0.002179\n",
      "iteration 6924, train loss: 0.003038, validation loss: 0.002212\n",
      "iteration 6925, train loss: 0.002704, validation loss: 0.002155\n",
      "iteration 6926, train loss: 0.003138, validation loss: 0.001967\n",
      "iteration 6927, train loss: 0.002987, validation loss: 0.001899\n",
      "iteration 6928, train loss: 0.003082, validation loss: 0.002081\n",
      "iteration 6929, train loss: 0.003204, validation loss: 0.001957\n",
      "iteration 6930, train loss: 0.003266, validation loss: 0.00176\n",
      "iteration 6931, train loss: 0.002722, validation loss: 0.002143\n",
      "iteration 6932, train loss: 0.003047, validation loss: 0.002194\n",
      "iteration 6933, train loss: 0.003123, validation loss: 0.00199\n",
      "iteration 6934, train loss: 0.002698, validation loss: 0.001937\n",
      "iteration 6935, train loss: 0.003043, validation loss: 0.001808\n",
      "iteration 6936, train loss: 0.003073, validation loss: 0.002006\n",
      "iteration 6937, train loss: 0.003138, validation loss: 0.001996\n",
      "iteration 6938, train loss: 0.002957, validation loss: 0.002046\n",
      "iteration 6939, train loss: 0.00348, validation loss: 0.002009\n",
      "iteration 6940, train loss: 0.003071, validation loss: 0.001796\n",
      "iteration 6941, train loss: 0.002696, validation loss: 0.001862\n",
      "iteration 6942, train loss: 0.002769, validation loss: 0.002046\n",
      "iteration 6943, train loss: 0.003293, validation loss: 0.001675\n",
      "iteration 6944, train loss: 0.002419, validation loss: 0.001914\n",
      "iteration 6945, train loss: 0.003022, validation loss: 0.002165\n",
      "iteration 6946, train loss: 0.003475, validation loss: 0.001832\n",
      "iteration 6947, train loss: 0.002942, validation loss: 0.001796\n",
      "iteration 6948, train loss: 0.002973, validation loss: 0.002013\n",
      "iteration 6949, train loss: 0.002975, validation loss: 0.001779\n",
      "iteration 6950, train loss: 0.003064, validation loss: 0.001763\n",
      "iteration 6951, train loss: 0.002685, validation loss: 0.001926\n",
      "iteration 6952, train loss: 0.00268, validation loss: 0.001911\n",
      "iteration 6953, train loss: 0.003047, validation loss: 0.001665\n",
      "iteration 6954, train loss: 0.002704, validation loss: 0.001725\n",
      "iteration 6955, train loss: 0.002896, validation loss: 0.001734\n",
      "iteration 6956, train loss: 0.002636, validation loss: 0.001667\n",
      "iteration 6957, train loss: 0.002772, validation loss: 0.001717\n",
      "iteration 6958, train loss: 0.002752, validation loss: 0.001786\n",
      "iteration 6959, train loss: 0.002685, validation loss: 0.001874\n",
      "iteration 6960, train loss: 0.002545, validation loss: 0.001862\n",
      "iteration 6961, train loss: 0.002871, validation loss: 0.001767\n",
      "iteration 6962, train loss: 0.003002, validation loss: 0.001662\n",
      "iteration 6963, train loss: 0.002661, validation loss: 0.001924\n",
      "iteration 6964, train loss: 0.002957, validation loss: 0.002087\n",
      "iteration 6965, train loss: 0.003084, validation loss: 0.00177\n",
      "iteration 6966, train loss: 0.002596, validation loss: 0.001687\n",
      "iteration 6967, train loss: 0.002564, validation loss: 0.001839\n",
      "iteration 6968, train loss: 0.002893, validation loss: 0.001688\n",
      "iteration 6969, train loss: 0.00266, validation loss: 0.001694\n",
      "iteration 6970, train loss: 0.002736, validation loss: 0.001909\n",
      "iteration 6971, train loss: 0.002915, validation loss: 0.0018\n",
      "iteration 6972, train loss: 0.002584, validation loss: 0.001725\n",
      "iteration 6973, train loss: 0.002739, validation loss: 0.001763\n",
      "iteration 6974, train loss: 0.002816, validation loss: 0.00169\n",
      "iteration 6975, train loss: 0.002829, validation loss: 0.001715\n",
      "iteration 6976, train loss: 0.002589, validation loss: 0.001886\n",
      "iteration 6977, train loss: 0.002718, validation loss: 0.001884\n",
      "iteration 6978, train loss: 0.002845, validation loss: 0.001795\n",
      "iteration 6979, train loss: 0.002611, validation loss: 0.002041\n",
      "iteration 6980, train loss: 0.003069, validation loss: 0.001816\n",
      "iteration 6981, train loss: 0.002703, validation loss: 0.001886\n",
      "iteration 6982, train loss: 0.002867, validation loss: 0.002065\n",
      "iteration 6983, train loss: 0.003173, validation loss: 0.001947\n",
      "iteration 6984, train loss: 0.002805, validation loss: 0.001874\n",
      "iteration 6985, train loss: 0.002792, validation loss: 0.001735\n",
      "iteration 6986, train loss: 0.002676, validation loss: 0.00173\n",
      "iteration 6987, train loss: 0.00245, validation loss: 0.00179\n",
      "iteration 6988, train loss: 0.002705, validation loss: 0.001701\n",
      "iteration 6989, train loss: 0.002418, validation loss: 0.0017\n",
      "iteration 6990, train loss: 0.002519, validation loss: 0.00176\n",
      "iteration 6991, train loss: 0.002885, validation loss: 0.001723\n",
      "iteration 6992, train loss: 0.002765, validation loss: 0.001699\n",
      "iteration 6993, train loss: 0.002637, validation loss: 0.001746\n",
      "iteration 6994, train loss: 0.002632, validation loss: 0.001844\n",
      "iteration 6995, train loss: 0.002687, validation loss: 0.001777\n",
      "iteration 6996, train loss: 0.002631, validation loss: 0.00166\n",
      "iteration 6997, train loss: 0.002593, validation loss: 0.001987\n",
      "iteration 6998, train loss: 0.002752, validation loss: 0.002195\n",
      "iteration 6999, train loss: 0.002784, validation loss: 0.002149\n",
      "iteration 7000, train loss: 0.003067, validation loss: 0.001739\n",
      "iteration 7001, train loss: 0.002505, validation loss: 0.001981\n",
      "iteration 7002, train loss: 0.002974, validation loss: 0.002202\n",
      "iteration 7003, train loss: 0.002954, validation loss: 0.001981\n",
      "iteration 7004, train loss: 0.002936, validation loss: 0.001656\n",
      "iteration 7005, train loss: 0.002797, validation loss: 0.002182\n",
      "iteration 7006, train loss: 0.00311, validation loss: 0.002348\n",
      "iteration 7007, train loss: 0.002994, validation loss: 0.001972\n",
      "iteration 7008, train loss: 0.003226, validation loss: 0.001696\n",
      "iteration 7009, train loss: 0.002918, validation loss: 0.002205\n",
      "iteration 7010, train loss: 0.002937, validation loss: 0.002255\n",
      "iteration 7011, train loss: 0.002824, validation loss: 0.001925\n",
      "iteration 7012, train loss: 0.002956, validation loss: 0.001707\n",
      "iteration 7013, train loss: 0.002298, validation loss: 0.002187\n",
      "iteration 7014, train loss: 0.002857, validation loss: 0.002387\n",
      "iteration 7015, train loss: 0.003011, validation loss: 0.002113\n",
      "iteration 7016, train loss: 0.003282, validation loss: 0.001793\n",
      "iteration 7017, train loss: 0.002367, validation loss: 0.002113\n",
      "iteration 7018, train loss: 0.003494, validation loss: 0.001926\n",
      "iteration 7019, train loss: 0.003064, validation loss: 0.001716\n",
      "iteration 7020, train loss: 0.002903, validation loss: 0.001869\n",
      "iteration 7021, train loss: 0.00272, validation loss: 0.002315\n",
      "iteration 7022, train loss: 0.003159, validation loss: 0.002169\n",
      "iteration 7023, train loss: 0.002577, validation loss: 0.001805\n",
      "iteration 7024, train loss: 0.002718, validation loss: 0.001972\n",
      "iteration 7025, train loss: 0.002877, validation loss: 0.002302\n",
      "iteration 7026, train loss: 0.003436, validation loss: 0.002127\n",
      "iteration 7027, train loss: 0.003165, validation loss: 0.001787\n",
      "iteration 7028, train loss: 0.00249, validation loss: 0.001734\n",
      "iteration 7029, train loss: 0.002749, validation loss: 0.001891\n",
      "iteration 7030, train loss: 0.00296, validation loss: 0.002076\n",
      "iteration 7031, train loss: 0.002859, validation loss: 0.001967\n",
      "iteration 7032, train loss: 0.002722, validation loss: 0.001921\n",
      "iteration 7033, train loss: 0.003229, validation loss: 0.001674\n",
      "iteration 7034, train loss: 0.00255, validation loss: 0.001802\n",
      "iteration 7035, train loss: 0.002651, validation loss: 0.001841\n",
      "iteration 7036, train loss: 0.002798, validation loss: 0.001886\n",
      "iteration 7037, train loss: 0.002761, validation loss: 0.001939\n",
      "iteration 7038, train loss: 0.00256, validation loss: 0.001974\n",
      "iteration 7039, train loss: 0.002761, validation loss: 0.002153\n",
      "iteration 7040, train loss: 0.003318, validation loss: 0.00192\n",
      "iteration 7041, train loss: 0.002743, validation loss: 0.001792\n",
      "iteration 7042, train loss: 0.002837, validation loss: 0.002116\n",
      "iteration 7043, train loss: 0.003199, validation loss: 0.002033\n",
      "iteration 7044, train loss: 0.002648, validation loss: 0.001661\n",
      "iteration 7045, train loss: 0.002578, validation loss: 0.001873\n",
      "iteration 7046, train loss: 0.002757, validation loss: 0.001925\n",
      "iteration 7047, train loss: 0.002637, validation loss: 0.001756\n",
      "iteration 7048, train loss: 0.002653, validation loss: 0.001663\n",
      "iteration 7049, train loss: 0.002642, validation loss: 0.001681\n",
      "iteration 7050, train loss: 0.002472, validation loss: 0.001795\n",
      "iteration 7051, train loss: 0.00282, validation loss: 0.001679\n",
      "iteration 7052, train loss: 0.002972, validation loss: 0.001764\n",
      "iteration 7053, train loss: 0.002561, validation loss: 0.001953\n",
      "iteration 7054, train loss: 0.003163, validation loss: 0.001967\n",
      "iteration 7055, train loss: 0.002641, validation loss: 0.002194\n",
      "iteration 7056, train loss: 0.002834, validation loss: 0.001773\n",
      "iteration 7057, train loss: 0.00302, validation loss: 0.001819\n",
      "iteration 7058, train loss: 0.002547, validation loss: 0.002348\n",
      "iteration 7059, train loss: 0.00291, validation loss: 0.002234\n",
      "iteration 7060, train loss: 0.003084, validation loss: 0.001726\n",
      "iteration 7061, train loss: 0.002562, validation loss: 0.001893\n",
      "iteration 7062, train loss: 0.002811, validation loss: 0.002026\n",
      "iteration 7063, train loss: 0.00321, validation loss: 0.001772\n",
      "iteration 7064, train loss: 0.002579, validation loss: 0.002189\n",
      "iteration 7065, train loss: 0.003233, validation loss: 0.002033\n",
      "iteration 7066, train loss: 0.003264, validation loss: 0.001704\n",
      "iteration 7067, train loss: 0.002827, validation loss: 0.002128\n",
      "iteration 7068, train loss: 0.003013, validation loss: 0.001976\n",
      "iteration 7069, train loss: 0.003302, validation loss: 0.001762\n",
      "iteration 7070, train loss: 0.00267, validation loss: 0.002247\n",
      "iteration 7071, train loss: 0.003384, validation loss: 0.002268\n",
      "iteration 7072, train loss: 0.002879, validation loss: 0.001964\n",
      "iteration 7073, train loss: 0.003065, validation loss: 0.002105\n",
      "iteration 7074, train loss: 0.002914, validation loss: 0.002149\n",
      "iteration 7075, train loss: 0.002859, validation loss: 0.002054\n",
      "iteration 7076, train loss: 0.003247, validation loss: 0.001897\n",
      "iteration 7077, train loss: 0.002681, validation loss: 0.002263\n",
      "iteration 7078, train loss: 0.003455, validation loss: 0.002047\n",
      "iteration 7079, train loss: 0.003, validation loss: 0.001778\n",
      "iteration 7080, train loss: 0.003039, validation loss: 0.001744\n",
      "iteration 7081, train loss: 0.002876, validation loss: 0.001776\n",
      "iteration 7082, train loss: 0.002605, validation loss: 0.001733\n",
      "iteration 7083, train loss: 0.002386, validation loss: 0.00181\n",
      "iteration 7084, train loss: 0.002806, validation loss: 0.001821\n",
      "iteration 7085, train loss: 0.00266, validation loss: 0.001701\n",
      "iteration 7086, train loss: 0.002666, validation loss: 0.001683\n",
      "iteration 7087, train loss: 0.002624, validation loss: 0.00181\n",
      "iteration 7088, train loss: 0.00261, validation loss: 0.001745\n",
      "iteration 7089, train loss: 0.002719, validation loss: 0.001649\n",
      "iteration 7090, train loss: 0.002551, validation loss: 0.0018\n",
      "iteration 7091, train loss: 0.002702, validation loss: 0.001896\n",
      "iteration 7092, train loss: 0.002558, validation loss: 0.001883\n",
      "iteration 7093, train loss: 0.00242, validation loss: 0.001936\n",
      "iteration 7094, train loss: 0.002933, validation loss: 0.001781\n",
      "iteration 7095, train loss: 0.002578, validation loss: 0.001865\n",
      "iteration 7096, train loss: 0.002663, validation loss: 0.001886\n",
      "iteration 7097, train loss: 0.003004, validation loss: 0.001851\n",
      "iteration 7098, train loss: 0.00278, validation loss: 0.00185\n",
      "iteration 7099, train loss: 0.002793, validation loss: 0.00166\n",
      "iteration 7100, train loss: 0.002824, validation loss: 0.001721\n",
      "iteration 7101, train loss: 0.002858, validation loss: 0.002055\n",
      "iteration 7102, train loss: 0.002776, validation loss: 0.001912\n",
      "iteration 7103, train loss: 0.002777, validation loss: 0.00175\n",
      "iteration 7104, train loss: 0.002873, validation loss: 0.00218\n",
      "iteration 7105, train loss: 0.003266, validation loss: 0.002164\n",
      "iteration 7106, train loss: 0.003372, validation loss: 0.001766\n",
      "iteration 7107, train loss: 0.002599, validation loss: 0.002029\n",
      "iteration 7108, train loss: 0.003279, validation loss: 0.002002\n",
      "iteration 7109, train loss: 0.003188, validation loss: 0.001957\n",
      "iteration 7110, train loss: 0.003035, validation loss: 0.002237\n",
      "iteration 7111, train loss: 0.003395, validation loss: 0.002072\n",
      "iteration 7112, train loss: 0.002792, validation loss: 0.001838\n",
      "iteration 7113, train loss: 0.002754, validation loss: 0.00198\n",
      "iteration 7114, train loss: 0.003095, validation loss: 0.001856\n",
      "iteration 7115, train loss: 0.002739, validation loss: 0.002051\n",
      "iteration 7116, train loss: 0.002821, validation loss: 0.00226\n",
      "iteration 7117, train loss: 0.003019, validation loss: 0.002249\n",
      "iteration 7118, train loss: 0.003063, validation loss: 0.00195\n",
      "iteration 7119, train loss: 0.003146, validation loss: 0.001783\n",
      "iteration 7120, train loss: 0.002813, validation loss: 0.00214\n",
      "iteration 7121, train loss: 0.002886, validation loss: 0.002132\n",
      "iteration 7122, train loss: 0.003264, validation loss: 0.001675\n",
      "iteration 7123, train loss: 0.002751, validation loss: 0.001867\n",
      "iteration 7124, train loss: 0.002571, validation loss: 0.002247\n",
      "iteration 7125, train loss: 0.003313, validation loss: 0.001825\n",
      "iteration 7126, train loss: 0.002942, validation loss: 0.001983\n",
      "iteration 7127, train loss: 0.003234, validation loss: 0.001971\n",
      "iteration 7128, train loss: 0.003078, validation loss: 0.001866\n",
      "iteration 7129, train loss: 0.003104, validation loss: 0.002133\n",
      "iteration 7130, train loss: 0.003107, validation loss: 0.00192\n",
      "iteration 7131, train loss: 0.003121, validation loss: 0.001803\n",
      "iteration 7132, train loss: 0.002719, validation loss: 0.001951\n",
      "iteration 7133, train loss: 0.00297, validation loss: 0.0017\n",
      "iteration 7134, train loss: 0.002611, validation loss: 0.001731\n",
      "iteration 7135, train loss: 0.002596, validation loss: 0.001929\n",
      "iteration 7136, train loss: 0.002984, validation loss: 0.001667\n",
      "iteration 7137, train loss: 0.002552, validation loss: 0.001856\n",
      "iteration 7138, train loss: 0.002621, validation loss: 0.002116\n",
      "iteration 7139, train loss: 0.002659, validation loss: 0.001876\n",
      "iteration 7140, train loss: 0.002973, validation loss: 0.001754\n",
      "iteration 7141, train loss: 0.002788, validation loss: 0.001966\n",
      "iteration 7142, train loss: 0.002794, validation loss: 0.001806\n",
      "iteration 7143, train loss: 0.0025, validation loss: 0.001648\n",
      "iteration 7144, train loss: 0.002666, validation loss: 0.001914\n",
      "iteration 7145, train loss: 0.002848, validation loss: 0.001884\n",
      "iteration 7146, train loss: 0.002687, validation loss: 0.001739\n",
      "iteration 7147, train loss: 0.002712, validation loss: 0.001768\n",
      "iteration 7148, train loss: 0.00278, validation loss: 0.001941\n",
      "iteration 7149, train loss: 0.002709, validation loss: 0.002024\n",
      "iteration 7150, train loss: 0.003186, validation loss: 0.002059\n",
      "iteration 7151, train loss: 0.00278, validation loss: 0.002217\n",
      "iteration 7152, train loss: 0.003007, validation loss: 0.002164\n",
      "iteration 7153, train loss: 0.003153, validation loss: 0.002008\n",
      "iteration 7154, train loss: 0.003118, validation loss: 0.002081\n",
      "iteration 7155, train loss: 0.003173, validation loss: 0.001907\n",
      "iteration 7156, train loss: 0.002805, validation loss: 0.001934\n",
      "iteration 7157, train loss: 0.002738, validation loss: 0.001936\n",
      "iteration 7158, train loss: 0.002918, validation loss: 0.00196\n",
      "iteration 7159, train loss: 0.002619, validation loss: 0.002591\n",
      "iteration 7160, train loss: 0.00364, validation loss: 0.002247\n",
      "iteration 7161, train loss: 0.002966, validation loss: 0.001789\n",
      "iteration 7162, train loss: 0.00302, validation loss: 0.002441\n",
      "iteration 7163, train loss: 0.003725, validation loss: 0.002827\n",
      "iteration 7164, train loss: 0.003943, validation loss: 0.001954\n",
      "iteration 7165, train loss: 0.002915, validation loss: 0.001938\n",
      "iteration 7166, train loss: 0.002781, validation loss: 0.002692\n",
      "iteration 7167, train loss: 0.003289, validation loss: 0.002149\n",
      "iteration 7168, train loss: 0.002892, validation loss: 0.001997\n",
      "iteration 7169, train loss: 0.002769, validation loss: 0.002511\n",
      "iteration 7170, train loss: 0.003045, validation loss: 0.00237\n",
      "iteration 7171, train loss: 0.003177, validation loss: 0.001749\n",
      "iteration 7172, train loss: 0.002775, validation loss: 0.001928\n",
      "iteration 7173, train loss: 0.00281, validation loss: 0.002206\n",
      "iteration 7174, train loss: 0.003229, validation loss: 0.001839\n",
      "iteration 7175, train loss: 0.002941, validation loss: 0.001897\n",
      "iteration 7176, train loss: 0.002648, validation loss: 0.002341\n",
      "iteration 7177, train loss: 0.003453, validation loss: 0.002021\n",
      "iteration 7178, train loss: 0.002776, validation loss: 0.001711\n",
      "iteration 7179, train loss: 0.002467, validation loss: 0.001914\n",
      "iteration 7180, train loss: 0.002715, validation loss: 0.00201\n",
      "iteration 7181, train loss: 0.002905, validation loss: 0.001703\n",
      "iteration 7182, train loss: 0.002943, validation loss: 0.001763\n",
      "iteration 7183, train loss: 0.002496, validation loss: 0.00237\n",
      "iteration 7184, train loss: 0.003205, validation loss: 0.002214\n",
      "iteration 7185, train loss: 0.002802, validation loss: 0.001764\n",
      "iteration 7186, train loss: 0.002989, validation loss: 0.002239\n",
      "iteration 7187, train loss: 0.002956, validation loss: 0.00275\n",
      "iteration 7188, train loss: 0.003798, validation loss: 0.002266\n",
      "iteration 7189, train loss: 0.002962, validation loss: 0.001732\n",
      "iteration 7190, train loss: 0.002644, validation loss: 0.002058\n",
      "iteration 7191, train loss: 0.002737, validation loss: 0.002377\n",
      "iteration 7192, train loss: 0.003258, validation loss: 0.002014\n",
      "iteration 7193, train loss: 0.003258, validation loss: 0.001692\n",
      "iteration 7194, train loss: 0.002559, validation loss: 0.002383\n",
      "iteration 7195, train loss: 0.003084, validation loss: 0.002308\n",
      "iteration 7196, train loss: 0.00365, validation loss: 0.001869\n",
      "iteration 7197, train loss: 0.00293, validation loss: 0.001901\n",
      "iteration 7198, train loss: 0.002928, validation loss: 0.00204\n",
      "iteration 7199, train loss: 0.003141, validation loss: 0.001867\n",
      "iteration 7200, train loss: 0.002904, validation loss: 0.001946\n",
      "iteration 7201, train loss: 0.003041, validation loss: 0.001908\n",
      "iteration 7202, train loss: 0.00283, validation loss: 0.001851\n",
      "iteration 7203, train loss: 0.002925, validation loss: 0.001844\n",
      "iteration 7204, train loss: 0.003027, validation loss: 0.001848\n",
      "iteration 7205, train loss: 0.003113, validation loss: 0.002019\n",
      "iteration 7206, train loss: 0.002942, validation loss: 0.002038\n",
      "iteration 7207, train loss: 0.003118, validation loss: 0.001987\n",
      "iteration 7208, train loss: 0.002911, validation loss: 0.0019\n",
      "iteration 7209, train loss: 0.00284, validation loss: 0.001654\n",
      "iteration 7210, train loss: 0.002852, validation loss: 0.001706\n",
      "iteration 7211, train loss: 0.003048, validation loss: 0.001961\n",
      "iteration 7212, train loss: 0.003077, validation loss: 0.001752\n",
      "iteration 7213, train loss: 0.002939, validation loss: 0.001755\n",
      "iteration 7214, train loss: 0.00255, validation loss: 0.001894\n",
      "iteration 7215, train loss: 0.002642, validation loss: 0.001859\n",
      "iteration 7216, train loss: 0.002913, validation loss: 0.001704\n",
      "iteration 7217, train loss: 0.002536, validation loss: 0.002089\n",
      "iteration 7218, train loss: 0.003445, validation loss: 0.001839\n",
      "iteration 7219, train loss: 0.002738, validation loss: 0.001798\n",
      "iteration 7220, train loss: 0.00267, validation loss: 0.001955\n",
      "iteration 7221, train loss: 0.002656, validation loss: 0.001981\n",
      "iteration 7222, train loss: 0.00268, validation loss: 0.001908\n",
      "iteration 7223, train loss: 0.002887, validation loss: 0.001908\n",
      "iteration 7224, train loss: 0.003335, validation loss: 0.001875\n",
      "iteration 7225, train loss: 0.002596, validation loss: 0.001918\n",
      "iteration 7226, train loss: 0.003198, validation loss: 0.001943\n",
      "iteration 7227, train loss: 0.003038, validation loss: 0.001972\n",
      "iteration 7228, train loss: 0.003119, validation loss: 0.002112\n",
      "iteration 7229, train loss: 0.003019, validation loss: 0.002288\n",
      "iteration 7230, train loss: 0.003006, validation loss: 0.002262\n",
      "iteration 7231, train loss: 0.003324, validation loss: 0.001834\n",
      "iteration 7232, train loss: 0.002574, validation loss: 0.001984\n",
      "iteration 7233, train loss: 0.002634, validation loss: 0.002414\n",
      "iteration 7234, train loss: 0.003732, validation loss: 0.001947\n",
      "iteration 7235, train loss: 0.002916, validation loss: 0.002238\n",
      "iteration 7236, train loss: 0.002907, validation loss: 0.00297\n",
      "iteration 7237, train loss: 0.003993, validation loss: 0.002254\n",
      "iteration 7238, train loss: 0.003117, validation loss: 0.001973\n",
      "iteration 7239, train loss: 0.002566, validation loss: 0.002563\n",
      "iteration 7240, train loss: 0.003447, validation loss: 0.002561\n",
      "iteration 7241, train loss: 0.003667, validation loss: 0.002071\n",
      "iteration 7242, train loss: 0.003222, validation loss: 0.002609\n",
      "iteration 7243, train loss: 0.003769, validation loss: 0.002361\n",
      "iteration 7244, train loss: 0.003126, validation loss: 0.001895\n",
      "iteration 7245, train loss: 0.002494, validation loss: 0.002082\n",
      "iteration 7246, train loss: 0.002995, validation loss: 0.002065\n",
      "iteration 7247, train loss: 0.003268, validation loss: 0.001682\n",
      "iteration 7248, train loss: 0.002489, validation loss: 0.001841\n",
      "iteration 7249, train loss: 0.002929, validation loss: 0.002043\n",
      "iteration 7250, train loss: 0.003121, validation loss: 0.001812\n",
      "iteration 7251, train loss: 0.002563, validation loss: 0.001676\n",
      "iteration 7252, train loss: 0.002527, validation loss: 0.001962\n",
      "iteration 7253, train loss: 0.002409, validation loss: 0.00219\n",
      "iteration 7254, train loss: 0.003321, validation loss: 0.001739\n",
      "iteration 7255, train loss: 0.002762, validation loss: 0.001805\n",
      "iteration 7256, train loss: 0.002822, validation loss: 0.002246\n",
      "iteration 7257, train loss: 0.003137, validation loss: 0.002459\n",
      "iteration 7258, train loss: 0.003813, validation loss: 0.001908\n",
      "iteration 7259, train loss: 0.002764, validation loss: 0.001784\n",
      "iteration 7260, train loss: 0.002599, validation loss: 0.002437\n",
      "iteration 7261, train loss: 0.003667, validation loss: 0.002184\n",
      "iteration 7262, train loss: 0.003362, validation loss: 0.001751\n",
      "iteration 7263, train loss: 0.002685, validation loss: 0.001954\n",
      "iteration 7264, train loss: 0.002608, validation loss: 0.00218\n",
      "iteration 7265, train loss: 0.003248, validation loss: 0.001871\n",
      "iteration 7266, train loss: 0.002655, validation loss: 0.001646\n",
      "iteration 7267, train loss: 0.002452, validation loss: 0.001811\n",
      "iteration 7268, train loss: 0.002705, validation loss: 0.001877\n",
      "iteration 7269, train loss: 0.002749, validation loss: 0.001729\n",
      "iteration 7270, train loss: 0.002558, validation loss: 0.001709\n",
      "iteration 7271, train loss: 0.002642, validation loss: 0.001792\n",
      "iteration 7272, train loss: 0.002708, validation loss: 0.0017\n",
      "iteration 7273, train loss: 0.002809, validation loss: 0.001642\n",
      "iteration 7274, train loss: 0.002432, validation loss: 0.001795\n",
      "iteration 7275, train loss: 0.002795, validation loss: 0.001919\n",
      "iteration 7276, train loss: 0.002688, validation loss: 0.001866\n",
      "iteration 7277, train loss: 0.002858, validation loss: 0.001739\n",
      "iteration 7278, train loss: 0.002663, validation loss: 0.001746\n",
      "iteration 7279, train loss: 0.002678, validation loss: 0.001756\n",
      "iteration 7280, train loss: 0.002429, validation loss: 0.001814\n",
      "iteration 7281, train loss: 0.002722, validation loss: 0.001753\n",
      "iteration 7282, train loss: 0.003026, validation loss: 0.001883\n",
      "iteration 7283, train loss: 0.002973, validation loss: 0.002128\n",
      "iteration 7284, train loss: 0.002829, validation loss: 0.002121\n",
      "iteration 7285, train loss: 0.003069, validation loss: 0.001741\n",
      "iteration 7286, train loss: 0.003056, validation loss: 0.001778\n",
      "iteration 7287, train loss: 0.002995, validation loss: 0.002085\n",
      "iteration 7288, train loss: 0.00287, validation loss: 0.002124\n",
      "iteration 7289, train loss: 0.003211, validation loss: 0.001688\n",
      "iteration 7290, train loss: 0.002571, validation loss: 0.001902\n",
      "iteration 7291, train loss: 0.002553, validation loss: 0.002128\n",
      "iteration 7292, train loss: 0.003058, validation loss: 0.001873\n",
      "iteration 7293, train loss: 0.002755, validation loss: 0.001778\n",
      "iteration 7294, train loss: 0.002456, validation loss: 0.002184\n",
      "iteration 7295, train loss: 0.002728, validation loss: 0.002141\n",
      "iteration 7296, train loss: 0.003344, validation loss: 0.001648\n",
      "iteration 7297, train loss: 0.00266, validation loss: 0.001807\n",
      "iteration 7298, train loss: 0.003028, validation loss: 0.001912\n",
      "iteration 7299, train loss: 0.002725, validation loss: 0.001734\n",
      "iteration 7300, train loss: 0.002674, validation loss: 0.00182\n",
      "iteration 7301, train loss: 0.00297, validation loss: 0.001932\n",
      "iteration 7302, train loss: 0.003021, validation loss: 0.001859\n",
      "iteration 7303, train loss: 0.002558, validation loss: 0.001708\n",
      "iteration 7304, train loss: 0.002587, validation loss: 0.001708\n",
      "iteration 7305, train loss: 0.00268, validation loss: 0.001844\n",
      "iteration 7306, train loss: 0.002583, validation loss: 0.001864\n",
      "iteration 7307, train loss: 0.003163, validation loss: 0.001712\n",
      "iteration 7308, train loss: 0.002626, validation loss: 0.001948\n",
      "iteration 7309, train loss: 0.00304, validation loss: 0.001923\n",
      "iteration 7310, train loss: 0.002716, validation loss: 0.001907\n",
      "iteration 7311, train loss: 0.002632, validation loss: 0.001904\n",
      "iteration 7312, train loss: 0.002851, validation loss: 0.001903\n",
      "iteration 7313, train loss: 0.003207, validation loss: 0.00175\n",
      "iteration 7314, train loss: 0.002662, validation loss: 0.001805\n",
      "iteration 7315, train loss: 0.002564, validation loss: 0.001988\n",
      "iteration 7316, train loss: 0.002829, validation loss: 0.001805\n",
      "iteration 7317, train loss: 0.002938, validation loss: 0.00184\n",
      "iteration 7318, train loss: 0.003125, validation loss: 0.001833\n",
      "iteration 7319, train loss: 0.002816, validation loss: 0.001963\n",
      "iteration 7320, train loss: 0.003016, validation loss: 0.002012\n",
      "iteration 7321, train loss: 0.002835, validation loss: 0.002031\n",
      "iteration 7322, train loss: 0.00268, validation loss: 0.001842\n",
      "iteration 7323, train loss: 0.002989, validation loss: 0.001834\n",
      "iteration 7324, train loss: 0.002813, validation loss: 0.0019\n",
      "iteration 7325, train loss: 0.002607, validation loss: 0.00195\n",
      "iteration 7326, train loss: 0.002562, validation loss: 0.001839\n",
      "iteration 7327, train loss: 0.002888, validation loss: 0.001702\n",
      "iteration 7328, train loss: 0.002508, validation loss: 0.001744\n",
      "iteration 7329, train loss: 0.002871, validation loss: 0.001762\n",
      "iteration 7330, train loss: 0.002974, validation loss: \u001b[92m0.001614\u001b[0m\n",
      "iteration 7331, train loss: 0.002526, validation loss: 0.00194\n",
      "iteration 7332, train loss: 0.002697, validation loss: 0.00202\n",
      "iteration 7333, train loss: 0.002917, validation loss: 0.001782\n",
      "iteration 7334, train loss: 0.002878, validation loss: 0.0018\n",
      "iteration 7335, train loss: 0.002831, validation loss: 0.002038\n",
      "iteration 7336, train loss: 0.002886, validation loss: 0.001771\n",
      "iteration 7337, train loss: 0.002832, validation loss: 0.001695\n",
      "iteration 7338, train loss: 0.002489, validation loss: 0.001852\n",
      "iteration 7339, train loss: 0.00299, validation loss: 0.001673\n",
      "iteration 7340, train loss: 0.002561, validation loss: 0.001671\n",
      "iteration 7341, train loss: 0.002488, validation loss: 0.001897\n",
      "iteration 7342, train loss: 0.002838, validation loss: 0.001758\n",
      "iteration 7343, train loss: 0.002801, validation loss: 0.001681\n",
      "iteration 7344, train loss: 0.002712, validation loss: 0.001959\n",
      "iteration 7345, train loss: 0.00278, validation loss: 0.002017\n",
      "iteration 7346, train loss: 0.003132, validation loss: 0.001694\n",
      "iteration 7347, train loss: 0.002774, validation loss: 0.001869\n",
      "iteration 7348, train loss: 0.003102, validation loss: 0.001879\n",
      "iteration 7349, train loss: 0.002466, validation loss: 0.0018\n",
      "iteration 7350, train loss: 0.002782, validation loss: 0.001743\n",
      "iteration 7351, train loss: 0.002776, validation loss: 0.001802\n",
      "iteration 7352, train loss: 0.002615, validation loss: 0.002135\n",
      "iteration 7353, train loss: 0.002592, validation loss: 0.001995\n",
      "iteration 7354, train loss: 0.002656, validation loss: 0.001679\n",
      "iteration 7355, train loss: 0.002635, validation loss: 0.001754\n",
      "iteration 7356, train loss: 0.002727, validation loss: 0.001939\n",
      "iteration 7357, train loss: 0.002928, validation loss: 0.001739\n",
      "iteration 7358, train loss: 0.002435, validation loss: 0.001732\n",
      "iteration 7359, train loss: 0.002482, validation loss: 0.001793\n",
      "iteration 7360, train loss: 0.002562, validation loss: 0.00171\n",
      "iteration 7361, train loss: 0.002761, validation loss: 0.001752\n",
      "iteration 7362, train loss: 0.002776, validation loss: 0.001782\n",
      "iteration 7363, train loss: 0.002934, validation loss: 0.001701\n",
      "iteration 7364, train loss: 0.002834, validation loss: 0.001694\n",
      "iteration 7365, train loss: 0.002468, validation loss: 0.001898\n",
      "iteration 7366, train loss: 0.002784, validation loss: 0.001968\n",
      "iteration 7367, train loss: 0.002822, validation loss: 0.001721\n",
      "iteration 7368, train loss: 0.00252, validation loss: 0.001768\n",
      "iteration 7369, train loss: 0.002825, validation loss: 0.001877\n",
      "iteration 7370, train loss: 0.002812, validation loss: 0.001904\n",
      "iteration 7371, train loss: 0.00274, validation loss: 0.001752\n",
      "iteration 7372, train loss: 0.002493, validation loss: 0.001639\n",
      "iteration 7373, train loss: 0.002471, validation loss: 0.001842\n",
      "iteration 7374, train loss: 0.002657, validation loss: 0.002095\n",
      "iteration 7375, train loss: 0.003059, validation loss: 0.001851\n",
      "iteration 7376, train loss: 0.002791, validation loss: 0.00169\n",
      "iteration 7377, train loss: 0.002565, validation loss: 0.001686\n",
      "iteration 7378, train loss: 0.00291, validation loss: 0.001777\n",
      "iteration 7379, train loss: 0.002781, validation loss: 0.001855\n",
      "iteration 7380, train loss: 0.002834, validation loss: 0.001693\n",
      "iteration 7381, train loss: 0.002526, validation loss: 0.001679\n",
      "iteration 7382, train loss: 0.002641, validation loss: 0.001875\n",
      "iteration 7383, train loss: 0.002907, validation loss: 0.001701\n",
      "iteration 7384, train loss: 0.002805, validation loss: 0.001819\n",
      "iteration 7385, train loss: 0.002843, validation loss: 0.001901\n",
      "iteration 7386, train loss: 0.002554, validation loss: 0.001783\n",
      "iteration 7387, train loss: 0.002545, validation loss: 0.001788\n",
      "iteration 7388, train loss: 0.002782, validation loss: 0.001819\n",
      "iteration 7389, train loss: 0.002644, validation loss: 0.001723\n",
      "iteration 7390, train loss: 0.002649, validation loss: 0.001865\n",
      "iteration 7391, train loss: 0.002612, validation loss: 0.002016\n",
      "iteration 7392, train loss: 0.002777, validation loss: 0.001922\n",
      "iteration 7393, train loss: 0.002708, validation loss: 0.001818\n",
      "iteration 7394, train loss: 0.002878, validation loss: 0.001769\n",
      "iteration 7395, train loss: 0.002669, validation loss: 0.002152\n",
      "iteration 7396, train loss: 0.002801, validation loss: 0.002164\n",
      "iteration 7397, train loss: 0.002851, validation loss: 0.001807\n",
      "iteration 7398, train loss: 0.002539, validation loss: 0.001833\n",
      "iteration 7399, train loss: 0.002779, validation loss: 0.001971\n",
      "iteration 7400, train loss: 0.002908, validation loss: 0.001715\n",
      "iteration 7401, train loss: 0.002521, validation loss: 0.001691\n",
      "iteration 7402, train loss: 0.002543, validation loss: 0.001747\n",
      "iteration 7403, train loss: 0.002349, validation loss: 0.001941\n",
      "iteration 7404, train loss: 0.002874, validation loss: 0.002013\n",
      "iteration 7405, train loss: 0.002627, validation loss: 0.001911\n",
      "iteration 7406, train loss: 0.002567, validation loss: 0.001798\n",
      "iteration 7407, train loss: 0.002384, validation loss: 0.001701\n",
      "iteration 7408, train loss: 0.002675, validation loss: 0.001639\n",
      "iteration 7409, train loss: 0.002658, validation loss: 0.001621\n",
      "iteration 7410, train loss: 0.002949, validation loss: 0.001736\n",
      "iteration 7411, train loss: 0.002926, validation loss: 0.0017\n",
      "iteration 7412, train loss: 0.002669, validation loss: \u001b[92m0.001586\u001b[0m\n",
      "iteration 7413, train loss: 0.002521, validation loss: 0.00171\n",
      "iteration 7414, train loss: 0.002342, validation loss: 0.001802\n",
      "iteration 7415, train loss: 0.00301, validation loss: 0.001706\n",
      "iteration 7416, train loss: 0.002586, validation loss: 0.002236\n",
      "iteration 7417, train loss: 0.003283, validation loss: 0.001992\n",
      "iteration 7418, train loss: 0.0026, validation loss: 0.001956\n",
      "iteration 7419, train loss: 0.003444, validation loss: 0.001795\n",
      "iteration 7420, train loss: 0.002773, validation loss: 0.001717\n",
      "iteration 7421, train loss: 0.002869, validation loss: 0.001736\n",
      "iteration 7422, train loss: 0.002744, validation loss: 0.001776\n",
      "iteration 7423, train loss: 0.002449, validation loss: 0.001791\n",
      "iteration 7424, train loss: 0.002703, validation loss: 0.001677\n",
      "iteration 7425, train loss: 0.002701, validation loss: 0.001808\n",
      "iteration 7426, train loss: 0.002511, validation loss: 0.001732\n",
      "iteration 7427, train loss: 0.002685, validation loss: 0.001669\n",
      "iteration 7428, train loss: 0.002666, validation loss: 0.001835\n",
      "iteration 7429, train loss: 0.002579, validation loss: 0.002158\n",
      "iteration 7430, train loss: 0.002757, validation loss: 0.002165\n",
      "iteration 7431, train loss: 0.002957, validation loss: 0.00178\n",
      "iteration 7432, train loss: 0.002673, validation loss: 0.001779\n",
      "iteration 7433, train loss: 0.002564, validation loss: 0.00203\n",
      "iteration 7434, train loss: 0.002936, validation loss: 0.001895\n",
      "iteration 7435, train loss: 0.002758, validation loss: 0.001706\n",
      "iteration 7436, train loss: 0.002877, validation loss: 0.001879\n",
      "iteration 7437, train loss: 0.003065, validation loss: 0.001907\n",
      "iteration 7438, train loss: 0.00269, validation loss: 0.001917\n",
      "iteration 7439, train loss: 0.002958, validation loss: 0.001742\n",
      "iteration 7440, train loss: 0.002599, validation loss: 0.001734\n",
      "iteration 7441, train loss: 0.002587, validation loss: 0.001949\n",
      "iteration 7442, train loss: 0.002885, validation loss: 0.001717\n",
      "iteration 7443, train loss: 0.002708, validation loss: 0.00173\n",
      "iteration 7444, train loss: 0.002514, validation loss: 0.001822\n",
      "iteration 7445, train loss: 0.002664, validation loss: 0.001691\n",
      "iteration 7446, train loss: 0.002483, validation loss: 0.001618\n",
      "iteration 7447, train loss: 0.002654, validation loss: 0.001713\n",
      "iteration 7448, train loss: 0.002612, validation loss: 0.001744\n",
      "iteration 7449, train loss: 0.002807, validation loss: 0.001621\n",
      "iteration 7450, train loss: 0.002598, validation loss: 0.001806\n",
      "iteration 7451, train loss: 0.002633, validation loss: 0.001943\n",
      "iteration 7452, train loss: 0.003048, validation loss: 0.001696\n",
      "iteration 7453, train loss: 0.002546, validation loss: 0.001603\n",
      "iteration 7454, train loss: 0.002746, validation loss: 0.001667\n",
      "iteration 7455, train loss: 0.002752, validation loss: 0.001666\n",
      "iteration 7456, train loss: 0.002521, validation loss: 0.001694\n",
      "iteration 7457, train loss: 0.00256, validation loss: 0.001888\n",
      "iteration 7458, train loss: 0.002672, validation loss: 0.002133\n",
      "iteration 7459, train loss: 0.002922, validation loss: 0.001848\n",
      "iteration 7460, train loss: 0.002825, validation loss: 0.001731\n",
      "iteration 7461, train loss: 0.002812, validation loss: 0.001867\n",
      "iteration 7462, train loss: 0.002795, validation loss: 0.00197\n",
      "iteration 7463, train loss: 0.002945, validation loss: 0.002033\n",
      "iteration 7464, train loss: 0.002839, validation loss: 0.002256\n",
      "iteration 7465, train loss: 0.002773, validation loss: 0.002481\n",
      "iteration 7466, train loss: 0.002999, validation loss: 0.002367\n",
      "iteration 7467, train loss: 0.003002, validation loss: 0.002152\n",
      "iteration 7468, train loss: 0.003047, validation loss: 0.002014\n",
      "iteration 7469, train loss: 0.002999, validation loss: 0.0021\n",
      "iteration 7470, train loss: 0.003587, validation loss: 0.002116\n",
      "iteration 7471, train loss: 0.003048, validation loss: 0.001833\n",
      "iteration 7472, train loss: 0.002598, validation loss: 0.001645\n",
      "iteration 7473, train loss: 0.002785, validation loss: 0.001906\n",
      "iteration 7474, train loss: 0.002862, validation loss: 0.001951\n",
      "iteration 7475, train loss: 0.003225, validation loss: 0.001632\n",
      "iteration 7476, train loss: 0.0025, validation loss: 0.001793\n",
      "iteration 7477, train loss: 0.003016, validation loss: 0.001679\n",
      "iteration 7478, train loss: 0.002865, validation loss: 0.001685\n",
      "iteration 7479, train loss: 0.002968, validation loss: 0.001792\n",
      "iteration 7480, train loss: 0.003141, validation loss: 0.001614\n",
      "iteration 7481, train loss: \u001b[92m0.002238\u001b[0m, validation loss: 0.001804\n",
      "iteration 7482, train loss: 0.002554, validation loss: 0.001748\n",
      "iteration 7483, train loss: 0.002722, validation loss: 0.001617\n",
      "iteration 7484, train loss: 0.002885, validation loss: 0.001675\n",
      "iteration 7485, train loss: 0.002924, validation loss: 0.001628\n",
      "iteration 7486, train loss: 0.002643, validation loss: 0.001656\n",
      "iteration 7487, train loss: 0.002533, validation loss: 0.001735\n",
      "iteration 7488, train loss: 0.002574, validation loss: 0.001791\n",
      "iteration 7489, train loss: 0.002623, validation loss: 0.001792\n",
      "iteration 7490, train loss: 0.002567, validation loss: 0.001805\n",
      "iteration 7491, train loss: 0.002468, validation loss: 0.00171\n",
      "iteration 7492, train loss: 0.002681, validation loss: 0.001616\n",
      "iteration 7493, train loss: 0.002739, validation loss: 0.001776\n",
      "iteration 7494, train loss: 0.002625, validation loss: 0.001725\n",
      "iteration 7495, train loss: 0.002762, validation loss: 0.001607\n",
      "iteration 7496, train loss: 0.002495, validation loss: 0.001798\n",
      "iteration 7497, train loss: 0.002605, validation loss: 0.001801\n",
      "iteration 7498, train loss: 0.002833, validation loss: 0.001705\n",
      "iteration 7499, train loss: 0.00297, validation loss: 0.001685\n",
      "iteration 7500, train loss: 0.002545, validation loss: 0.001754\n",
      "iteration 7501, train loss: 0.002842, validation loss: 0.001771\n",
      "iteration 7502, train loss: 0.002717, validation loss: 0.001641\n",
      "iteration 7503, train loss: 0.002462, validation loss: 0.001651\n",
      "iteration 7504, train loss: 0.002661, validation loss: 0.001697\n",
      "iteration 7505, train loss: 0.00276, validation loss: 0.001702\n",
      "iteration 7506, train loss: 0.002591, validation loss: 0.001728\n",
      "iteration 7507, train loss: 0.002343, validation loss: 0.001706\n",
      "iteration 7508, train loss: 0.002843, validation loss: 0.001665\n",
      "iteration 7509, train loss: 0.002357, validation loss: 0.001656\n",
      "iteration 7510, train loss: 0.002449, validation loss: 0.001616\n",
      "iteration 7511, train loss: 0.002441, validation loss: 0.00168\n",
      "iteration 7512, train loss: 0.002846, validation loss: 0.001702\n",
      "iteration 7513, train loss: 0.002602, validation loss: 0.001809\n",
      "iteration 7514, train loss: 0.002858, validation loss: 0.001639\n",
      "iteration 7515, train loss: 0.00265, validation loss: 0.001653\n",
      "iteration 7516, train loss: 0.00256, validation loss: 0.001625\n",
      "iteration 7517, train loss: 0.002362, validation loss: 0.001654\n",
      "iteration 7518, train loss: 0.002483, validation loss: 0.001707\n",
      "iteration 7519, train loss: 0.002717, validation loss: 0.001777\n",
      "iteration 7520, train loss: 0.002804, validation loss: 0.001924\n",
      "iteration 7521, train loss: 0.002723, validation loss: 0.001895\n",
      "iteration 7522, train loss: 0.002903, validation loss: 0.001657\n",
      "iteration 7523, train loss: 0.002539, validation loss: 0.002006\n",
      "iteration 7524, train loss: 0.002711, validation loss: 0.002246\n",
      "iteration 7525, train loss: 0.002769, validation loss: 0.001948\n",
      "iteration 7526, train loss: 0.002651, validation loss: 0.001727\n",
      "iteration 7527, train loss: 0.00258, validation loss: 0.001931\n",
      "iteration 7528, train loss: 0.002875, validation loss: 0.001978\n",
      "iteration 7529, train loss: 0.002795, validation loss: 0.00182\n",
      "iteration 7530, train loss: 0.002845, validation loss: 0.00179\n",
      "iteration 7531, train loss: 0.002805, validation loss: 0.002259\n",
      "iteration 7532, train loss: 0.003343, validation loss: 0.002658\n",
      "iteration 7533, train loss: 0.003712, validation loss: 0.002083\n",
      "iteration 7534, train loss: 0.002602, validation loss: 0.001851\n",
      "iteration 7535, train loss: 0.002571, validation loss: 0.002197\n",
      "iteration 7536, train loss: 0.003591, validation loss: 0.002102\n",
      "iteration 7537, train loss: 0.003351, validation loss: 0.001921\n",
      "iteration 7538, train loss: 0.002844, validation loss: 0.001974\n",
      "iteration 7539, train loss: 0.003002, validation loss: 0.001837\n",
      "iteration 7540, train loss: 0.002509, validation loss: 0.001844\n",
      "iteration 7541, train loss: 0.002882, validation loss: 0.001832\n",
      "iteration 7542, train loss: 0.002871, validation loss: 0.001641\n",
      "iteration 7543, train loss: 0.002661, validation loss: 0.00188\n",
      "iteration 7544, train loss: 0.002847, validation loss: 0.001932\n",
      "iteration 7545, train loss: 0.003017, validation loss: 0.001617\n",
      "iteration 7546, train loss: 0.002535, validation loss: 0.001756\n",
      "iteration 7547, train loss: 0.002641, validation loss: 0.001856\n",
      "iteration 7548, train loss: 0.002898, validation loss: 0.001701\n",
      "iteration 7549, train loss: 0.002825, validation loss: 0.001633\n",
      "iteration 7550, train loss: 0.00261, validation loss: 0.001726\n",
      "iteration 7551, train loss: 0.002929, validation loss: 0.001843\n",
      "iteration 7552, train loss: 0.002838, validation loss: 0.00163\n",
      "iteration 7553, train loss: 0.002435, validation loss: 0.00177\n",
      "iteration 7554, train loss: 0.002531, validation loss: 0.00195\n",
      "iteration 7555, train loss: 0.002661, validation loss: 0.001883\n",
      "iteration 7556, train loss: 0.002635, validation loss: 0.001917\n",
      "iteration 7557, train loss: 0.00271, validation loss: 0.001673\n",
      "iteration 7558, train loss: 0.002423, validation loss: 0.00165\n",
      "iteration 7559, train loss: 0.002644, validation loss: 0.001688\n",
      "iteration 7560, train loss: 0.002488, validation loss: 0.00164\n",
      "iteration 7561, train loss: 0.002516, validation loss: 0.001611\n",
      "iteration 7562, train loss: 0.002419, validation loss: 0.001673\n",
      "iteration 7563, train loss: 0.002603, validation loss: 0.001689\n",
      "iteration 7564, train loss: 0.002315, validation loss: 0.001732\n",
      "iteration 7565, train loss: 0.002503, validation loss: 0.001838\n",
      "iteration 7566, train loss: 0.002661, validation loss: 0.001892\n",
      "iteration 7567, train loss: 0.002912, validation loss: 0.001709\n",
      "iteration 7568, train loss: 0.002717, validation loss: 0.001795\n",
      "iteration 7569, train loss: 0.002665, validation loss: 0.002008\n",
      "iteration 7570, train loss: 0.002386, validation loss: 0.002119\n",
      "iteration 7571, train loss: 0.003191, validation loss: 0.001878\n",
      "iteration 7572, train loss: 0.002703, validation loss: 0.001837\n",
      "iteration 7573, train loss: 0.002931, validation loss: 0.00195\n",
      "iteration 7574, train loss: 0.002839, validation loss: 0.002045\n",
      "iteration 7575, train loss: 0.002928, validation loss: 0.001898\n",
      "iteration 7576, train loss: 0.00327, validation loss: 0.001682\n",
      "iteration 7577, train loss: 0.002469, validation loss: 0.001824\n",
      "iteration 7578, train loss: 0.002905, validation loss: 0.001835\n",
      "iteration 7579, train loss: 0.002712, validation loss: 0.001678\n",
      "iteration 7580, train loss: 0.002347, validation loss: 0.001827\n",
      "iteration 7581, train loss: 0.00256, validation loss: 0.001889\n",
      "iteration 7582, train loss: 0.002796, validation loss: 0.001627\n",
      "iteration 7583, train loss: 0.002571, validation loss: 0.001694\n",
      "iteration 7584, train loss: 0.002424, validation loss: 0.001951\n",
      "iteration 7585, train loss: 0.003363, validation loss: 0.001713\n",
      "iteration 7586, train loss: 0.002758, validation loss: 0.001625\n",
      "iteration 7587, train loss: 0.002288, validation loss: 0.001951\n",
      "iteration 7588, train loss: 0.002838, validation loss: 0.001812\n",
      "iteration 7589, train loss: 0.002722, validation loss: 0.0016\n",
      "iteration 7590, train loss: 0.002957, validation loss: 0.00176\n",
      "iteration 7591, train loss: 0.00245, validation loss: 0.001782\n",
      "iteration 7592, train loss: 0.002635, validation loss: 0.001752\n",
      "iteration 7593, train loss: 0.0025, validation loss: 0.001873\n",
      "iteration 7594, train loss: 0.002762, validation loss: 0.002082\n",
      "iteration 7595, train loss: 0.002874, validation loss: 0.001986\n",
      "iteration 7596, train loss: 0.002894, validation loss: 0.001834\n",
      "iteration 7597, train loss: 0.002916, validation loss: 0.001806\n",
      "iteration 7598, train loss: 0.002906, validation loss: 0.001773\n",
      "iteration 7599, train loss: 0.002669, validation loss: 0.001885\n",
      "iteration 7600, train loss: 0.002772, validation loss: 0.001806\n",
      "iteration 7601, train loss: 0.002655, validation loss: 0.001794\n",
      "iteration 7602, train loss: 0.002849, validation loss: 0.001823\n",
      "iteration 7603, train loss: 0.002691, validation loss: 0.001735\n",
      "iteration 7604, train loss: 0.002831, validation loss: 0.001694\n",
      "iteration 7605, train loss: 0.002787, validation loss: 0.001632\n",
      "iteration 7606, train loss: 0.002586, validation loss: 0.001639\n",
      "iteration 7607, train loss: 0.002617, validation loss: 0.001758\n",
      "iteration 7608, train loss: 0.002669, validation loss: 0.001759\n",
      "iteration 7609, train loss: 0.00285, validation loss: 0.001616\n",
      "iteration 7610, train loss: 0.002319, validation loss: 0.001643\n",
      "iteration 7611, train loss: 0.002595, validation loss: 0.00168\n",
      "iteration 7612, train loss: 0.002632, validation loss: 0.001645\n",
      "iteration 7613, train loss: 0.002401, validation loss: 0.001734\n",
      "iteration 7614, train loss: 0.002703, validation loss: 0.001811\n",
      "iteration 7615, train loss: 0.002791, validation loss: 0.001805\n",
      "iteration 7616, train loss: 0.002705, validation loss: 0.001767\n",
      "iteration 7617, train loss: 0.002694, validation loss: 0.001661\n",
      "iteration 7618, train loss: 0.002517, validation loss: 0.001722\n",
      "iteration 7619, train loss: 0.002642, validation loss: 0.001713\n",
      "iteration 7620, train loss: 0.002897, validation loss: 0.001638\n",
      "iteration 7621, train loss: 0.002474, validation loss: 0.001711\n",
      "iteration 7622, train loss: 0.002525, validation loss: 0.001714\n",
      "iteration 7623, train loss: 0.002561, validation loss: 0.001615\n",
      "iteration 7624, train loss: 0.002432, validation loss: 0.001709\n",
      "iteration 7625, train loss: 0.002565, validation loss: 0.001698\n",
      "iteration 7626, train loss: 0.002668, validation loss: 0.00163\n",
      "iteration 7627, train loss: 0.002626, validation loss: 0.001691\n",
      "iteration 7628, train loss: 0.002712, validation loss: 0.001653\n",
      "iteration 7629, train loss: 0.00234, validation loss: 0.001731\n",
      "iteration 7630, train loss: 0.002619, validation loss: 0.001866\n",
      "iteration 7631, train loss: 0.002538, validation loss: 0.001729\n",
      "iteration 7632, train loss: 0.002536, validation loss: 0.001641\n",
      "iteration 7633, train loss: 0.00226, validation loss: 0.001805\n",
      "iteration 7634, train loss: 0.002686, validation loss: 0.001706\n",
      "iteration 7635, train loss: 0.002593, validation loss: 0.001691\n",
      "iteration 7636, train loss: 0.002588, validation loss: 0.001758\n",
      "iteration 7637, train loss: 0.002955, validation loss: 0.00167\n",
      "iteration 7638, train loss: 0.002485, validation loss: 0.001799\n",
      "iteration 7639, train loss: 0.002396, validation loss: 0.001854\n",
      "iteration 7640, train loss: 0.002942, validation loss: 0.00163\n",
      "iteration 7641, train loss: 0.002396, validation loss: 0.001689\n",
      "iteration 7642, train loss: 0.002344, validation loss: 0.001855\n",
      "iteration 7643, train loss: 0.002623, validation loss: 0.001844\n",
      "iteration 7644, train loss: 0.002524, validation loss: 0.001696\n",
      "iteration 7645, train loss: 0.002699, validation loss: 0.001601\n",
      "iteration 7646, train loss: 0.00241, validation loss: 0.001784\n",
      "iteration 7647, train loss: 0.00279, validation loss: 0.001907\n",
      "iteration 7648, train loss: 0.002661, validation loss: 0.001837\n",
      "iteration 7649, train loss: 0.002672, validation loss: 0.00172\n",
      "iteration 7650, train loss: 0.002409, validation loss: 0.001708\n",
      "iteration 7651, train loss: 0.002603, validation loss: 0.001811\n",
      "iteration 7652, train loss: 0.002958, validation loss: 0.001684\n",
      "iteration 7653, train loss: 0.002554, validation loss: 0.001653\n",
      "iteration 7654, train loss: 0.002749, validation loss: 0.001655\n",
      "iteration 7655, train loss: 0.002475, validation loss: 0.001621\n",
      "iteration 7656, train loss: 0.002594, validation loss: 0.001663\n",
      "iteration 7657, train loss: 0.002904, validation loss: 0.001592\n",
      "iteration 7658, train loss: 0.002441, validation loss: 0.001611\n",
      "iteration 7659, train loss: 0.00243, validation loss: 0.00163\n",
      "iteration 7660, train loss: 0.002621, validation loss: 0.001688\n",
      "iteration 7661, train loss: 0.002542, validation loss: 0.001721\n",
      "iteration 7662, train loss: 0.00277, validation loss: 0.001797\n",
      "iteration 7663, train loss: 0.002795, validation loss: 0.00175\n",
      "iteration 7664, train loss: 0.002562, validation loss: 0.001639\n",
      "iteration 7665, train loss: 0.002434, validation loss: 0.00169\n",
      "iteration 7666, train loss: 0.002752, validation loss: 0.001682\n",
      "iteration 7667, train loss: 0.002547, validation loss: 0.001627\n",
      "iteration 7668, train loss: 0.002544, validation loss: 0.001767\n",
      "iteration 7669, train loss: 0.002895, validation loss: 0.001628\n",
      "iteration 7670, train loss: 0.002426, validation loss: 0.001662\n",
      "iteration 7671, train loss: 0.002768, validation loss: 0.001682\n",
      "iteration 7672, train loss: 0.002595, validation loss: 0.001755\n",
      "iteration 7673, train loss: 0.002698, validation loss: 0.001917\n",
      "iteration 7674, train loss: 0.002736, validation loss: 0.00217\n",
      "iteration 7675, train loss: 0.003159, validation loss: 0.001784\n",
      "iteration 7676, train loss: 0.002619, validation loss: 0.001923\n",
      "iteration 7677, train loss: 0.002867, validation loss: 0.001934\n",
      "iteration 7678, train loss: 0.002744, validation loss: 0.00165\n",
      "iteration 7679, train loss: 0.002398, validation loss: 0.001751\n",
      "iteration 7680, train loss: 0.002827, validation loss: 0.001819\n",
      "iteration 7681, train loss: 0.002726, validation loss: 0.001846\n",
      "iteration 7682, train loss: 0.002596, validation loss: 0.002118\n",
      "iteration 7683, train loss: 0.002821, validation loss: 0.001925\n",
      "iteration 7684, train loss: 0.003152, validation loss: 0.001622\n",
      "iteration 7685, train loss: 0.002428, validation loss: 0.002178\n",
      "iteration 7686, train loss: 0.003256, validation loss: 0.00203\n",
      "iteration 7687, train loss: 0.002888, validation loss: 0.001652\n",
      "iteration 7688, train loss: 0.002549, validation loss: 0.00223\n",
      "iteration 7689, train loss: 0.002968, validation loss: 0.002259\n",
      "iteration 7690, train loss: 0.003387, validation loss: 0.001698\n",
      "iteration 7691, train loss: 0.002559, validation loss: 0.002039\n",
      "iteration 7692, train loss: 0.00272, validation loss: 0.001973\n",
      "iteration 7693, train loss: 0.003142, validation loss: 0.001787\n",
      "iteration 7694, train loss: 0.002703, validation loss: 0.002096\n",
      "iteration 7695, train loss: 0.002709, validation loss: 0.002065\n",
      "iteration 7696, train loss: 0.002936, validation loss: 0.001823\n",
      "iteration 7697, train loss: 0.002313, validation loss: 0.002002\n",
      "iteration 7698, train loss: 0.002826, validation loss: 0.001836\n",
      "iteration 7699, train loss: 0.002867, validation loss: 0.001907\n",
      "iteration 7700, train loss: 0.002534, validation loss: 0.002078\n",
      "iteration 7701, train loss: 0.003256, validation loss: 0.001758\n",
      "iteration 7702, train loss: 0.002598, validation loss: 0.001706\n",
      "iteration 7703, train loss: 0.002861, validation loss: 0.001984\n",
      "iteration 7704, train loss: 0.002888, validation loss: 0.001666\n",
      "iteration 7705, train loss: 0.002467, validation loss: 0.001885\n",
      "iteration 7706, train loss: 0.002965, validation loss: 0.001958\n",
      "iteration 7707, train loss: 0.002801, validation loss: 0.001628\n",
      "iteration 7708, train loss: 0.002672, validation loss: 0.00177\n",
      "iteration 7709, train loss: 0.002682, validation loss: 0.001942\n",
      "iteration 7710, train loss: 0.002981, validation loss: 0.001614\n",
      "iteration 7711, train loss: 0.002822, validation loss: 0.001748\n",
      "iteration 7712, train loss: 0.00257, validation loss: 0.002104\n",
      "iteration 7713, train loss: 0.003075, validation loss: 0.001953\n",
      "iteration 7714, train loss: 0.002649, validation loss: 0.001892\n",
      "iteration 7715, train loss: 0.002656, validation loss: 0.001824\n",
      "iteration 7716, train loss: 0.002804, validation loss: 0.0017\n",
      "iteration 7717, train loss: 0.002581, validation loss: 0.001896\n",
      "iteration 7718, train loss: 0.002953, validation loss: 0.001988\n",
      "iteration 7719, train loss: 0.003199, validation loss: 0.001637\n",
      "iteration 7720, train loss: 0.002621, validation loss: 0.001893\n",
      "iteration 7721, train loss: 0.002592, validation loss: 0.002217\n",
      "iteration 7722, train loss: 0.003266, validation loss: 0.001798\n",
      "iteration 7723, train loss: 0.002617, validation loss: 0.002002\n",
      "iteration 7724, train loss: 0.002942, validation loss: 0.002066\n",
      "iteration 7725, train loss: 0.002787, validation loss: 0.001778\n",
      "iteration 7726, train loss: 0.002997, validation loss: 0.001958\n",
      "iteration 7727, train loss: 0.002958, validation loss: 0.002188\n",
      "iteration 7728, train loss: 0.00338, validation loss: 0.00197\n",
      "iteration 7729, train loss: 0.002957, validation loss: 0.001937\n",
      "iteration 7730, train loss: 0.002909, validation loss: 0.002372\n",
      "iteration 7731, train loss: 0.003189, validation loss: 0.00224\n",
      "iteration 7732, train loss: 0.003301, validation loss: 0.002007\n",
      "iteration 7733, train loss: 0.003077, validation loss: 0.001941\n",
      "iteration 7734, train loss: 0.002789, validation loss: 0.001922\n",
      "iteration 7735, train loss: 0.00326, validation loss: 0.00205\n",
      "iteration 7736, train loss: 0.002784, validation loss: 0.002014\n",
      "iteration 7737, train loss: 0.003343, validation loss: 0.001865\n",
      "iteration 7738, train loss: 0.002506, validation loss: 0.001931\n",
      "iteration 7739, train loss: 0.002866, validation loss: 0.001901\n",
      "iteration 7740, train loss: 0.003318, validation loss: 0.001709\n",
      "iteration 7741, train loss: 0.002821, validation loss: 0.001827\n",
      "iteration 7742, train loss: 0.002946, validation loss: 0.001955\n",
      "iteration 7743, train loss: 0.002729, validation loss: 0.001898\n",
      "iteration 7744, train loss: 0.002606, validation loss: 0.001803\n",
      "iteration 7745, train loss: 0.002522, validation loss: 0.00187\n",
      "iteration 7746, train loss: 0.003234, validation loss: \u001b[92m0.001584\u001b[0m\n",
      "iteration 7747, train loss: 0.002474, validation loss: 0.001832\n",
      "iteration 7748, train loss: 0.002951, validation loss: 0.001732\n",
      "iteration 7749, train loss: 0.002759, validation loss: 0.00169\n",
      "iteration 7750, train loss: 0.002609, validation loss: 0.001904\n",
      "iteration 7751, train loss: 0.002983, validation loss: 0.001619\n",
      "iteration 7752, train loss: 0.002663, validation loss: 0.001692\n",
      "iteration 7753, train loss: 0.002561, validation loss: 0.001812\n",
      "iteration 7754, train loss: 0.002673, validation loss: 0.001678\n",
      "iteration 7755, train loss: 0.002706, validation loss: 0.001675\n",
      "iteration 7756, train loss: 0.002754, validation loss: 0.001639\n",
      "iteration 7757, train loss: 0.002576, validation loss: 0.001721\n",
      "iteration 7758, train loss: 0.00275, validation loss: 0.001693\n",
      "iteration 7759, train loss: 0.002752, validation loss: 0.00161\n",
      "iteration 7760, train loss: 0.002355, validation loss: 0.001586\n",
      "iteration 7761, train loss: 0.002397, validation loss: \u001b[92m0.001562\u001b[0m\n",
      "iteration 7762, train loss: 0.002601, validation loss: 0.001597\n",
      "iteration 7763, train loss: 0.002557, validation loss: 0.00167\n",
      "iteration 7764, train loss: 0.002405, validation loss: 0.001735\n",
      "iteration 7765, train loss: 0.002745, validation loss: 0.001719\n",
      "iteration 7766, train loss: 0.002793, validation loss: 0.001651\n",
      "iteration 7767, train loss: 0.002661, validation loss: 0.001976\n",
      "iteration 7768, train loss: 0.003318, validation loss: 0.001828\n",
      "iteration 7769, train loss: 0.002432, validation loss: 0.001743\n",
      "iteration 7770, train loss: 0.002718, validation loss: 0.001885\n",
      "iteration 7771, train loss: 0.002969, validation loss: 0.001668\n",
      "iteration 7772, train loss: 0.002385, validation loss: 0.001887\n",
      "iteration 7773, train loss: 0.002697, validation loss: 0.002025\n",
      "iteration 7774, train loss: 0.00265, validation loss: 0.001851\n",
      "iteration 7775, train loss: 0.002326, validation loss: 0.001648\n",
      "iteration 7776, train loss: 0.002643, validation loss: 0.001767\n",
      "iteration 7777, train loss: 0.002583, validation loss: 0.002182\n",
      "iteration 7778, train loss: 0.003066, validation loss: 0.001746\n",
      "iteration 7779, train loss: 0.0026, validation loss: 0.00186\n",
      "iteration 7780, train loss: 0.002784, validation loss: 0.002271\n",
      "iteration 7781, train loss: 0.003173, validation loss: 0.001921\n",
      "iteration 7782, train loss: 0.002872, validation loss: 0.001775\n",
      "iteration 7783, train loss: 0.002593, validation loss: 0.00204\n",
      "iteration 7784, train loss: 0.002716, validation loss: 0.001858\n",
      "iteration 7785, train loss: 0.002965, validation loss: 0.001733\n",
      "iteration 7786, train loss: 0.002722, validation loss: 0.001939\n",
      "iteration 7787, train loss: 0.002976, validation loss: 0.001912\n",
      "iteration 7788, train loss: 0.002729, validation loss: 0.00186\n",
      "iteration 7789, train loss: 0.002563, validation loss: 0.001976\n",
      "iteration 7790, train loss: 0.002755, validation loss: 0.002085\n",
      "iteration 7791, train loss: 0.003516, validation loss: 0.001742\n",
      "iteration 7792, train loss: 0.002746, validation loss: 0.001893\n",
      "iteration 7793, train loss: 0.002654, validation loss: 0.002021\n",
      "iteration 7794, train loss: 0.003227, validation loss: 0.00171\n",
      "iteration 7795, train loss: 0.002916, validation loss: 0.001687\n",
      "iteration 7796, train loss: 0.00256, validation loss: 0.001723\n",
      "iteration 7797, train loss: 0.002435, validation loss: 0.00177\n",
      "iteration 7798, train loss: 0.002949, validation loss: 0.001707\n",
      "iteration 7799, train loss: 0.002832, validation loss: 0.001713\n",
      "iteration 7800, train loss: 0.002751, validation loss: 0.001786\n",
      "iteration 7801, train loss: 0.002638, validation loss: 0.001725\n",
      "iteration 7802, train loss: 0.002786, validation loss: 0.001804\n",
      "iteration 7803, train loss: 0.002789, validation loss: 0.002089\n",
      "iteration 7804, train loss: 0.003269, validation loss: 0.001617\n",
      "iteration 7805, train loss: 0.002554, validation loss: 0.002005\n",
      "iteration 7806, train loss: 0.003295, validation loss: 0.002005\n",
      "iteration 7807, train loss: 0.002748, validation loss: 0.001752\n",
      "iteration 7808, train loss: 0.002882, validation loss: 0.001742\n",
      "iteration 7809, train loss: 0.002755, validation loss: 0.001767\n",
      "iteration 7810, train loss: 0.002535, validation loss: 0.001781\n",
      "iteration 7811, train loss: 0.003003, validation loss: 0.001868\n",
      "iteration 7812, train loss: 0.002768, validation loss: 0.001731\n",
      "iteration 7813, train loss: 0.002569, validation loss: 0.001775\n",
      "iteration 7814, train loss: 0.002535, validation loss: 0.002047\n",
      "iteration 7815, train loss: 0.003075, validation loss: 0.001868\n",
      "iteration 7816, train loss: 0.002502, validation loss: 0.001671\n",
      "iteration 7817, train loss: 0.002661, validation loss: 0.001955\n",
      "iteration 7818, train loss: 0.002744, validation loss: 0.002111\n",
      "iteration 7819, train loss: 0.002804, validation loss: 0.00179\n",
      "iteration 7820, train loss: 0.002977, validation loss: 0.001808\n",
      "iteration 7821, train loss: 0.002639, validation loss: 0.00233\n",
      "iteration 7822, train loss: 0.0032, validation loss: 0.001841\n",
      "iteration 7823, train loss: 0.002456, validation loss: 0.001795\n",
      "iteration 7824, train loss: 0.002442, validation loss: 0.002388\n",
      "iteration 7825, train loss: 0.003435, validation loss: 0.002124\n",
      "iteration 7826, train loss: 0.003017, validation loss: 0.001664\n",
      "iteration 7827, train loss: 0.002504, validation loss: 0.001978\n",
      "iteration 7828, train loss: 0.002684, validation loss: 0.002193\n",
      "iteration 7829, train loss: 0.003705, validation loss: 0.001814\n",
      "iteration 7830, train loss: 0.002826, validation loss: 0.002326\n",
      "iteration 7831, train loss: 0.003534, validation loss: 0.002369\n",
      "iteration 7832, train loss: 0.003408, validation loss: 0.001971\n",
      "iteration 7833, train loss: 0.002926, validation loss: 0.001956\n",
      "iteration 7834, train loss: 0.002829, validation loss: 0.002244\n",
      "iteration 7835, train loss: 0.003193, validation loss: 0.00256\n",
      "iteration 7836, train loss: 0.003098, validation loss: 0.0027\n",
      "iteration 7837, train loss: 0.003168, validation loss: 0.002188\n",
      "iteration 7838, train loss: 0.003126, validation loss: 0.001886\n",
      "iteration 7839, train loss: 0.002981, validation loss: 0.002014\n",
      "iteration 7840, train loss: 0.002875, validation loss: 0.002524\n",
      "iteration 7841, train loss: 0.003302, validation loss: 0.002362\n",
      "iteration 7842, train loss: 0.003315, validation loss: 0.001853\n",
      "iteration 7843, train loss: 0.002506, validation loss: 0.002096\n",
      "iteration 7844, train loss: 0.003168, validation loss: 0.002201\n",
      "iteration 7845, train loss: 0.003213, validation loss: 0.001884\n",
      "iteration 7846, train loss: 0.002658, validation loss: 0.00168\n",
      "iteration 7847, train loss: 0.002523, validation loss: 0.001976\n",
      "iteration 7848, train loss: 0.00281, validation loss: 0.001991\n",
      "iteration 7849, train loss: 0.003015, validation loss: 0.001655\n",
      "iteration 7850, train loss: 0.00266, validation loss: 0.001963\n",
      "iteration 7851, train loss: 0.003233, validation loss: 0.001862\n",
      "iteration 7852, train loss: 0.002917, validation loss: 0.001767\n",
      "iteration 7853, train loss: 0.002595, validation loss: 0.001912\n",
      "iteration 7854, train loss: 0.002856, validation loss: 0.001849\n",
      "iteration 7855, train loss: 0.002713, validation loss: 0.001716\n",
      "iteration 7856, train loss: 0.00257, validation loss: 0.001765\n",
      "iteration 7857, train loss: 0.003054, validation loss: 0.001593\n",
      "iteration 7858, train loss: 0.0025, validation loss: 0.00182\n",
      "iteration 7859, train loss: 0.00274, validation loss: 0.00175\n",
      "iteration 7860, train loss: 0.002653, validation loss: 0.001569\n",
      "iteration 7861, train loss: 0.002715, validation loss: 0.001672\n",
      "iteration 7862, train loss: 0.002559, validation loss: 0.001755\n",
      "iteration 7863, train loss: 0.002505, validation loss: 0.001852\n",
      "iteration 7864, train loss: 0.00317, validation loss: 0.00164\n",
      "iteration 7865, train loss: 0.002774, validation loss: 0.001634\n",
      "iteration 7866, train loss: 0.002835, validation loss: 0.001882\n",
      "iteration 7867, train loss: 0.002967, validation loss: 0.001721\n",
      "iteration 7868, train loss: 0.002535, validation loss: 0.001668\n",
      "iteration 7869, train loss: 0.002876, validation loss: 0.00168\n",
      "iteration 7870, train loss: 0.002479, validation loss: 0.001707\n",
      "iteration 7871, train loss: 0.002851, validation loss: 0.001578\n",
      "iteration 7872, train loss: 0.002358, validation loss: 0.001716\n",
      "iteration 7873, train loss: 0.002635, validation loss: 0.001681\n",
      "iteration 7874, train loss: 0.002831, validation loss: 0.00163\n",
      "iteration 7875, train loss: 0.00273, validation loss: 0.001766\n",
      "iteration 7876, train loss: 0.002972, validation loss: 0.00167\n",
      "iteration 7877, train loss: 0.002572, validation loss: 0.001705\n",
      "iteration 7878, train loss: 0.002569, validation loss: 0.001673\n",
      "iteration 7879, train loss: 0.002694, validation loss: 0.001644\n",
      "iteration 7880, train loss: 0.002337, validation loss: 0.001792\n",
      "iteration 7881, train loss: 0.00267, validation loss: 0.001892\n",
      "iteration 7882, train loss: 0.002617, validation loss: 0.001763\n",
      "iteration 7883, train loss: 0.002575, validation loss: 0.00159\n",
      "iteration 7884, train loss: 0.002571, validation loss: 0.00162\n",
      "iteration 7885, train loss: 0.002637, validation loss: 0.001757\n",
      "iteration 7886, train loss: 0.00281, validation loss: 0.001755\n",
      "iteration 7887, train loss: 0.002355, validation loss: 0.00176\n",
      "iteration 7888, train loss: 0.002381, validation loss: 0.00163\n",
      "iteration 7889, train loss: 0.00265, validation loss: 0.001722\n",
      "iteration 7890, train loss: 0.002417, validation loss: 0.001816\n",
      "iteration 7891, train loss: 0.003047, validation loss: 0.001597\n",
      "iteration 7892, train loss: 0.002574, validation loss: 0.001673\n",
      "iteration 7893, train loss: 0.002907, validation loss: 0.001845\n",
      "iteration 7894, train loss: 0.002888, validation loss: 0.001759\n",
      "iteration 7895, train loss: 0.002713, validation loss: 0.001632\n",
      "iteration 7896, train loss: 0.002572, validation loss: 0.001736\n",
      "iteration 7897, train loss: 0.002636, validation loss: 0.001641\n",
      "iteration 7898, train loss: 0.002582, validation loss: 0.001691\n",
      "iteration 7899, train loss: 0.002432, validation loss: 0.001929\n",
      "iteration 7900, train loss: 0.002719, validation loss: 0.002068\n",
      "iteration 7901, train loss: 0.00272, validation loss: 0.001764\n",
      "iteration 7902, train loss: 0.002816, validation loss: 0.001689\n",
      "iteration 7903, train loss: 0.002615, validation loss: 0.001918\n",
      "iteration 7904, train loss: 0.003068, validation loss: 0.001695\n",
      "iteration 7905, train loss: 0.002916, validation loss: 0.001863\n",
      "iteration 7906, train loss: 0.002554, validation loss: 0.00228\n",
      "iteration 7907, train loss: 0.0031, validation loss: 0.001831\n",
      "iteration 7908, train loss: 0.002536, validation loss: 0.00171\n",
      "iteration 7909, train loss: 0.002686, validation loss: 0.001837\n",
      "iteration 7910, train loss: 0.002688, validation loss: 0.001985\n",
      "iteration 7911, train loss: 0.002649, validation loss: 0.00225\n",
      "iteration 7912, train loss: 0.003495, validation loss: 0.001841\n",
      "iteration 7913, train loss: 0.002886, validation loss: 0.001855\n",
      "iteration 7914, train loss: 0.002709, validation loss: 0.002231\n",
      "iteration 7915, train loss: 0.002986, validation loss: 0.002085\n",
      "iteration 7916, train loss: 0.002948, validation loss: 0.001735\n",
      "iteration 7917, train loss: 0.002742, validation loss: 0.001919\n",
      "iteration 7918, train loss: 0.003189, validation loss: 0.001752\n",
      "iteration 7919, train loss: 0.002709, validation loss: 0.001829\n",
      "iteration 7920, train loss: 0.002677, validation loss: 0.001917\n",
      "iteration 7921, train loss: 0.002945, validation loss: 0.001812\n",
      "iteration 7922, train loss: 0.002683, validation loss: 0.002039\n",
      "iteration 7923, train loss: 0.002862, validation loss: 0.001954\n",
      "iteration 7924, train loss: 0.002958, validation loss: 0.001631\n",
      "iteration 7925, train loss: 0.002665, validation loss: 0.001901\n",
      "iteration 7926, train loss: 0.002563, validation loss: 0.001904\n",
      "iteration 7927, train loss: 0.0028, validation loss: \u001b[92m0.00156\u001b[0m\n",
      "iteration 7928, train loss: 0.002606, validation loss: 0.001872\n",
      "iteration 7929, train loss: 0.002769, validation loss: 0.002017\n",
      "iteration 7930, train loss: 0.002839, validation loss: 0.001798\n",
      "iteration 7931, train loss: 0.00265, validation loss: 0.001826\n",
      "iteration 7932, train loss: 0.002621, validation loss: 0.002183\n",
      "iteration 7933, train loss: 0.003568, validation loss: 0.001648\n",
      "iteration 7934, train loss: 0.002829, validation loss: 0.001827\n",
      "iteration 7935, train loss: 0.002736, validation loss: 0.001972\n",
      "iteration 7936, train loss: 0.003141, validation loss: 0.001968\n",
      "iteration 7937, train loss: 0.003008, validation loss: 0.002312\n",
      "iteration 7938, train loss: 0.003025, validation loss: 0.002044\n",
      "iteration 7939, train loss: 0.002822, validation loss: 0.001694\n",
      "iteration 7940, train loss: 0.00257, validation loss: 0.001707\n",
      "iteration 7941, train loss: 0.002656, validation loss: 0.001804\n",
      "iteration 7942, train loss: 0.002674, validation loss: 0.001624\n",
      "iteration 7943, train loss: 0.002506, validation loss: 0.00168\n",
      "iteration 7944, train loss: 0.002645, validation loss: 0.001809\n",
      "iteration 7945, train loss: 0.002884, validation loss: 0.001815\n",
      "iteration 7946, train loss: 0.002562, validation loss: 0.00174\n",
      "iteration 7947, train loss: 0.002786, validation loss: 0.001651\n",
      "iteration 7948, train loss: 0.002433, validation loss: 0.001743\n",
      "iteration 7949, train loss: 0.002655, validation loss: 0.001681\n",
      "iteration 7950, train loss: 0.002497, validation loss: 0.001725\n",
      "iteration 7951, train loss: 0.00276, validation loss: 0.001762\n",
      "iteration 7952, train loss: 0.002573, validation loss: 0.00207\n",
      "iteration 7953, train loss: 0.002821, validation loss: 0.001812\n",
      "iteration 7954, train loss: 0.002523, validation loss: 0.001628\n",
      "iteration 7955, train loss: 0.002433, validation loss: 0.001854\n",
      "iteration 7956, train loss: 0.002942, validation loss: 0.001908\n",
      "iteration 7957, train loss: 0.002925, validation loss: 0.001618\n",
      "iteration 7958, train loss: 0.002241, validation loss: 0.002023\n",
      "iteration 7959, train loss: 0.002678, validation loss: 0.00221\n",
      "iteration 7960, train loss: 0.002887, validation loss: 0.001724\n",
      "iteration 7961, train loss: 0.002721, validation loss: 0.001817\n",
      "iteration 7962, train loss: 0.002876, validation loss: 0.002178\n",
      "iteration 7963, train loss: 0.002986, validation loss: 0.001899\n",
      "iteration 7964, train loss: 0.002549, validation loss: 0.001767\n",
      "iteration 7965, train loss: 0.002603, validation loss: 0.002289\n",
      "iteration 7966, train loss: 0.003219, validation loss: 0.001982\n",
      "iteration 7967, train loss: 0.003051, validation loss: 0.00159\n",
      "iteration 7968, train loss: 0.00289, validation loss: 0.001813\n",
      "iteration 7969, train loss: 0.002757, validation loss: 0.001697\n",
      "iteration 7970, train loss: 0.002707, validation loss: 0.001582\n",
      "iteration 7971, train loss: 0.00271, validation loss: 0.001948\n",
      "iteration 7972, train loss: 0.003232, validation loss: 0.00175\n",
      "iteration 7973, train loss: 0.002736, validation loss: 0.001655\n",
      "iteration 7974, train loss: 0.002539, validation loss: 0.001975\n",
      "iteration 7975, train loss: 0.003047, validation loss: 0.001877\n",
      "iteration 7976, train loss: 0.002981, validation loss: 0.001614\n",
      "iteration 7977, train loss: 0.002623, validation loss: 0.001807\n",
      "iteration 7978, train loss: 0.002757, validation loss: 0.001936\n",
      "iteration 7979, train loss: 0.003126, validation loss: 0.001742\n",
      "iteration 7980, train loss: 0.002683, validation loss: 0.001812\n",
      "iteration 7981, train loss: 0.002944, validation loss: 0.001934\n",
      "iteration 7982, train loss: 0.002618, validation loss: 0.001784\n",
      "iteration 7983, train loss: 0.002853, validation loss: 0.001661\n",
      "iteration 7984, train loss: 0.002663, validation loss: 0.001789\n",
      "iteration 7985, train loss: 0.002544, validation loss: 0.001791\n",
      "iteration 7986, train loss: 0.002474, validation loss: 0.001657\n",
      "iteration 7987, train loss: 0.002506, validation loss: 0.001631\n",
      "iteration 7988, train loss: 0.002765, validation loss: 0.001761\n",
      "iteration 7989, train loss: 0.002813, validation loss: 0.001763\n",
      "iteration 7990, train loss: 0.002731, validation loss: 0.001781\n",
      "iteration 7991, train loss: 0.002518, validation loss: 0.001894\n",
      "iteration 7992, train loss: 0.002565, validation loss: 0.001799\n",
      "iteration 7993, train loss: 0.002553, validation loss: 0.001659\n",
      "iteration 7994, train loss: 0.002735, validation loss: 0.001691\n",
      "iteration 7995, train loss: 0.002619, validation loss: 0.00171\n",
      "iteration 7996, train loss: 0.002538, validation loss: 0.001849\n",
      "iteration 7997, train loss: 0.002891, validation loss: 0.001639\n",
      "iteration 7998, train loss: 0.002706, validation loss: 0.001657\n",
      "iteration 7999, train loss: 0.002607, validation loss: 0.001835\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "rnn_trained, train_losses, val_losses, net_params = trainer.run_training(train_mask=mask, same_batch=same_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16f8128b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize loss\n",
    "\n",
    "fig_trainloss = plt.figure(figsize=(10, 3))\n",
    "plt.plot(train_losses, color='r', label='train loss (log scale)')\n",
    "plt.plot(val_losses, color='b', label='valid loss (log scale)')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"# epoch\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=np.zeros(N),\n",
    "                      y_init=net_params[\"y_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_valid, target_batch_valid = task.get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_timeseries=input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "output = RNN_valid.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize RNN output\n",
    "n_r, n_l = task_params[\"n_rights\"], task_params[\"n_lefts\"]\n",
    "n_c = task_params[\"n_catches\"]\n",
    "print(n_r, n_l, n_c)\n",
    "ch_names = [\"Left\", \"Right\"]\n",
    "\n",
    "for i in range(output_size): # left, right\n",
    "    fig = plt.figure(figsize = (10, 2))\n",
    "    plt.plot(np.average(output[i,:,:n_r], axis=1), c='tab:blue', label=\"right\")\n",
    "    plt.plot(np.average(output[i,:,n_r:n_r+n_l], axis=1), c='tab:red', ls=\"-.\", label=\"left\")\n",
    "    plt.plot(np.average(output[i,:,n_r+n_l:n_c+n_r+n_l], axis=1), c='tab:purple', ls=\"--\", label=\"catch\")\n",
    "    plt.title(f\"Output Channle: {ch_names[i]}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_trained, train_losses, val_losses, net_params\n",
    "# save RNN data\n",
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_batch_valid, sigma_inp=sigma_inp, sigma_rec=sigma_rec)\n",
    "neural_traces = RNN_valid.get_history()\n",
    "traces_data = {}\n",
    "traces_data[\"inputs\"] = input_batch_valid\n",
    "traces_data[\"targets\"] = target_batch_valid\n",
    "traces_data[\"traces\"] = neural_traces\n",
    "traces_data[\"outputs\"] = RNN_valid.get_output()\n",
    "traces_data[\"net_params\"] = net_params\n",
    "\n",
    "path_to_traces = os.path.join(\"../data/trained_RNNs/ALM/\",\"test\",\"trained_RNN_lambda_orth=2_in_out_orth.pkl\")\n",
    "pickle.dump(traces_data, open(path_to_traces, \"wb+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_coach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
