{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a86c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.DataSaver import DataSaver\n",
    "from src.DynamicSystemAnalyzer import DynamicSystemAnalyzer\n",
    "from src.PerformanceAnalyzer import PerformanceAnalyzer\n",
    "from src.RNN_numpy import RNN_numpy\n",
    "from src.utils import get_project_root, numpify, orthonormalize\n",
    "from src.Trainer import Trainer\n",
    "from src.RNN_torch import RNN_torch\n",
    "from src.Task import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf901029",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = True\n",
    "activation = \"tanh\"\n",
    "taskname = \"MemoryAntiNumber\"\n",
    "train_config_file = f\"train_config_{taskname}_{activation}.json\"\n",
    "config_dict = json.load(open(os.path.join(get_project_root(), \"data\", \"configs\", train_config_file), mode=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfbe67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining RNN:\n",
    "N = config_dict[\"N\"]\n",
    "activation_name = config_dict[\"activation\"]\n",
    "if activation_name == 'relu':\n",
    "    activation = lambda x: torch.maximum(x, torch.tensor(0))\n",
    "elif activation_name == 'tanh':\n",
    "    activation = torch.tanh\n",
    "elif activation_name == 'sigmoid':\n",
    "    activation = lambda x: 1/(1 + torch.exp(-x))\n",
    "elif activation_name == 'softplus':\n",
    "    activation = lambda x: torch.log(1 + torch.exp(5 * x))\n",
    "\n",
    "dt = config_dict[\"dt\"]\n",
    "tau = config_dict[\"tau\"]\n",
    "constrained = config_dict[\"constrained\"]\n",
    "connectivity_density_rec = config_dict[\"connectivity_density_rec\"]\n",
    "spectral_rad = config_dict[\"sr\"]\n",
    "sigma_inp = config_dict[\"sigma_inp\"]\n",
    "sigma_rec = config_dict[\"sigma_rec\"]\n",
    "seed = config_dict[\"seed\"]\n",
    "rng = torch.Generator()\n",
    "if not seed is None:\n",
    "    rng.manual_seed(seed)\n",
    "input_size = config_dict[\"num_inputs\"]\n",
    "output_size = config_dict[\"num_outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2567cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "n_steps = config_dict[\"n_steps\"]\n",
    "task_params = config_dict[\"task_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a94006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer:\n",
    "lambda_orth = config_dict[\"lambda_orth\"]\n",
    "lambda_r = config_dict[\"lambda_r\"]\n",
    "mask = np.array(config_dict[\"mask\"])\n",
    "max_iter = config_dict[\"max_iter\"]\n",
    "tol = config_dict[\"tol\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "same_batch = config_dict[\"same_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a81aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General:\n",
    "tag = config_dict[\"tag\"]\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "data_folder = os.path.join(config_dict[\"data_folder\"], timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a8979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating instances:\n",
    "rnn_torch = RNN_torch(N=N, dt=dt, tau=tau, input_size=input_size, output_size=output_size,\n",
    "                      activation=activation, constrained=constrained,\n",
    "                      sigma_inp=sigma_inp, sigma_rec=sigma_rec,\n",
    "                      connectivity_density_rec=connectivity_density_rec,\n",
    "                      spectral_rad=spectral_rad,\n",
    "                      random_generator=rng)\n",
    "task = eval(\"Task\" + taskname)(n_steps=n_steps, n_inputs=input_size, n_outputs=output_size, task_params=task_params)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_torch.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "trainer = Trainer(RNN=rnn_torch, Task=task,\n",
    "                  max_iter=max_iter, tol=tol,\n",
    "                  optimizer=optimizer, criterion=criterion,\n",
    "                  lambda_orth=lambda_orth, lambda_r=lambda_r)\n",
    "datasaver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedfd750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets, conditions = task.get_batch()\n",
    "ind = 11\n",
    "\n",
    "fig = plt.figure(figsize = (10, 2))\n",
    "plt.plot(inputs[0, :, ind], color = 'red', label = 'input 1')\n",
    "plt.plot(inputs[1, :, ind], color = 'tomato', alpha = 0.5, label = 'input 2')\n",
    "plt.plot(targets[0, :, ind], color = 'blue', linestyle = '--', label = 'target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a8c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: \u001b[92m1.550812\u001b[0m, validation loss: \u001b[92m1.448594\u001b[0m\n",
      "iteration 1, train loss: \u001b[92m1.45006\u001b[0m, validation loss: \u001b[92m1.405636\u001b[0m\n",
      "iteration 2, train loss: \u001b[92m1.407104\u001b[0m, validation loss: 1.407742\n",
      "iteration 3, train loss: 1.409713, validation loss: \u001b[92m1.385058\u001b[0m\n",
      "iteration 4, train loss: \u001b[92m1.387198\u001b[0m, validation loss: \u001b[92m1.367049\u001b[0m\n",
      "iteration 5, train loss: \u001b[92m1.368605\u001b[0m, validation loss: \u001b[92m1.347924\u001b[0m\n",
      "iteration 6, train loss: \u001b[92m1.349479\u001b[0m, validation loss: \u001b[92m1.286571\u001b[0m\n",
      "iteration 7, train loss: \u001b[92m1.292582\u001b[0m, validation loss: \u001b[92m1.140357\u001b[0m\n",
      "iteration 8, train loss: \u001b[92m1.187775\u001b[0m, validation loss: \u001b[92m1.040278\u001b[0m\n",
      "iteration 9, train loss: \u001b[92m1.143733\u001b[0m, validation loss: \u001b[92m1.031551\u001b[0m\n",
      "iteration 10, train loss: \u001b[92m1.073298\u001b[0m, validation loss: 1.043596\n",
      "iteration 11, train loss: \u001b[92m1.072658\u001b[0m, validation loss: \u001b[92m0.911943\u001b[0m\n",
      "iteration 12, train loss: \u001b[92m1.03348\u001b[0m, validation loss: \u001b[92m0.888896\u001b[0m\n",
      "iteration 13, train loss: \u001b[92m0.940616\u001b[0m, validation loss: \u001b[92m0.864506\u001b[0m\n",
      "iteration 14, train loss: \u001b[92m0.913005\u001b[0m, validation loss: \u001b[92m0.797225\u001b[0m\n",
      "iteration 15, train loss: \u001b[92m0.852228\u001b[0m, validation loss: \u001b[92m0.78819\u001b[0m\n",
      "iteration 16, train loss: \u001b[92m0.833967\u001b[0m, validation loss: 0.792057\n",
      "iteration 17, train loss: \u001b[92m0.819855\u001b[0m, validation loss: \u001b[92m0.7506\u001b[0m\n",
      "iteration 18, train loss: \u001b[92m0.770011\u001b[0m, validation loss: \u001b[92m0.709585\u001b[0m\n",
      "iteration 19, train loss: \u001b[92m0.731764\u001b[0m, validation loss: \u001b[92m0.701162\u001b[0m\n",
      "iteration 20, train loss: \u001b[92m0.717832\u001b[0m, validation loss: \u001b[92m0.677782\u001b[0m\n",
      "iteration 21, train loss: \u001b[92m0.701061\u001b[0m, validation loss: \u001b[92m0.651663\u001b[0m\n",
      "iteration 22, train loss: \u001b[92m0.667506\u001b[0m, validation loss: \u001b[92m0.624749\u001b[0m\n",
      "iteration 23, train loss: \u001b[92m0.642305\u001b[0m, validation loss: \u001b[92m0.5979\u001b[0m\n",
      "iteration 24, train loss: \u001b[92m0.609747\u001b[0m, validation loss: \u001b[92m0.578788\u001b[0m\n",
      "iteration 25, train loss: \u001b[92m0.60037\u001b[0m, validation loss: \u001b[92m0.55587\u001b[0m\n",
      "iteration 26, train loss: \u001b[92m0.575843\u001b[0m, validation loss: \u001b[92m0.525127\u001b[0m\n",
      "iteration 27, train loss: \u001b[92m0.542058\u001b[0m, validation loss: \u001b[92m0.502952\u001b[0m\n",
      "iteration 28, train loss: \u001b[92m0.519238\u001b[0m, validation loss: \u001b[92m0.486877\u001b[0m\n",
      "iteration 29, train loss: \u001b[92m0.502345\u001b[0m, validation loss: \u001b[92m0.461188\u001b[0m\n",
      "iteration 30, train loss: \u001b[92m0.48046\u001b[0m, validation loss: \u001b[92m0.43365\u001b[0m\n",
      "iteration 31, train loss: \u001b[92m0.452055\u001b[0m, validation loss: \u001b[92m0.415568\u001b[0m\n",
      "iteration 32, train loss: \u001b[92m0.432463\u001b[0m, validation loss: \u001b[92m0.397556\u001b[0m\n",
      "iteration 33, train loss: \u001b[92m0.413984\u001b[0m, validation loss: \u001b[92m0.378622\u001b[0m\n",
      "iteration 34, train loss: \u001b[92m0.395256\u001b[0m, validation loss: \u001b[92m0.353676\u001b[0m\n",
      "iteration 35, train loss: \u001b[92m0.378862\u001b[0m, validation loss: 0.355239\n",
      "iteration 36, train loss: \u001b[92m0.372874\u001b[0m, validation loss: \u001b[92m0.320956\u001b[0m\n",
      "iteration 37, train loss: \u001b[92m0.341574\u001b[0m, validation loss: \u001b[92m0.293188\u001b[0m\n",
      "iteration 38, train loss: \u001b[92m0.31206\u001b[0m, validation loss: \u001b[92m0.289884\u001b[0m\n",
      "iteration 39, train loss: \u001b[92m0.306076\u001b[0m, validation loss: \u001b[92m0.265901\u001b[0m\n",
      "iteration 40, train loss: \u001b[92m0.287957\u001b[0m, validation loss: \u001b[92m0.240039\u001b[0m\n",
      "iteration 41, train loss: \u001b[92m0.256966\u001b[0m, validation loss: \u001b[92m0.229281\u001b[0m\n",
      "iteration 42, train loss: \u001b[92m0.248564\u001b[0m, validation loss: \u001b[92m0.206746\u001b[0m\n",
      "iteration 43, train loss: \u001b[92m0.223446\u001b[0m, validation loss: \u001b[92m0.188994\u001b[0m\n",
      "iteration 44, train loss: \u001b[92m0.195719\u001b[0m, validation loss: \u001b[92m0.177111\u001b[0m\n",
      "iteration 45, train loss: \u001b[92m0.188499\u001b[0m, validation loss: \u001b[92m0.159855\u001b[0m\n",
      "iteration 46, train loss: \u001b[92m0.174586\u001b[0m, validation loss: \u001b[92m0.155269\u001b[0m\n",
      "iteration 47, train loss: \u001b[92m0.17018\u001b[0m, validation loss: 0.162713\n",
      "iteration 48, train loss: 0.172699, validation loss: 0.169164\n",
      "iteration 49, train loss: 0.183429, validation loss: 0.170092\n",
      "iteration 50, train loss: 0.186539, validation loss: 0.170138\n",
      "iteration 51, train loss: 0.187468, validation loss: 0.171923\n",
      "iteration 52, train loss: 0.183457, validation loss: 0.170764\n",
      "iteration 53, train loss: 0.183686, validation loss: 0.184161\n",
      "iteration 54, train loss: 0.201928, validation loss: 0.253407\n",
      "iteration 55, train loss: 0.26577, validation loss: 0.175961\n",
      "iteration 56, train loss: 0.195835, validation loss: \u001b[92m0.136098\u001b[0m\n",
      "iteration 57, train loss: \u001b[92m0.152935\u001b[0m, validation loss: 0.174555\n",
      "iteration 58, train loss: 0.189112, validation loss: \u001b[92m0.127864\u001b[0m\n",
      "iteration 59, train loss: \u001b[92m0.145445\u001b[0m, validation loss: 0.153177\n",
      "iteration 60, train loss: 0.166202, validation loss: 0.143063\n",
      "iteration 61, train loss: 0.159014, validation loss: 0.156409\n",
      "iteration 62, train loss: 0.173282, validation loss: 0.140559\n",
      "iteration 63, train loss: 0.153564, validation loss: 0.148817\n",
      "iteration 64, train loss: 0.162557, validation loss: 0.134083\n",
      "iteration 65, train loss: 0.148984, validation loss: 0.130081\n",
      "iteration 66, train loss: \u001b[92m0.142196\u001b[0m, validation loss: \u001b[92m0.121199\u001b[0m\n",
      "iteration 67, train loss: \u001b[92m0.135199\u001b[0m, validation loss: 0.128417\n",
      "iteration 68, train loss: 0.144764, validation loss: 0.129523\n",
      "iteration 69, train loss: 0.144591, validation loss: 0.127848\n",
      "iteration 70, train loss: 0.142597, validation loss: 0.123852\n",
      "iteration 71, train loss: 0.135227, validation loss: \u001b[92m0.119199\u001b[0m\n",
      "iteration 72, train loss: 0.136256, validation loss: \u001b[92m0.117903\u001b[0m\n",
      "iteration 73, train loss: \u001b[92m0.13305\u001b[0m, validation loss: 0.12077\n",
      "iteration 74, train loss: 0.137485, validation loss: 0.120853\n",
      "iteration 75, train loss: 0.137613, validation loss: 0.11817\n",
      "iteration 76, train loss: \u001b[92m0.132606\u001b[0m, validation loss: \u001b[92m0.113528\u001b[0m\n",
      "iteration 77, train loss: \u001b[92m0.129278\u001b[0m, validation loss: \u001b[92m0.10861\u001b[0m\n",
      "iteration 78, train loss: \u001b[92m0.126147\u001b[0m, validation loss: 0.112438\n",
      "iteration 79, train loss: 0.132793, validation loss: 0.124837\n",
      "iteration 80, train loss: 0.139617, validation loss: 0.134294\n",
      "iteration 81, train loss: 0.147735, validation loss: 0.114089\n",
      "iteration 82, train loss: 0.130488, validation loss: \u001b[92m0.103994\u001b[0m\n",
      "iteration 83, train loss: \u001b[92m0.120773\u001b[0m, validation loss: 0.111566\n",
      "iteration 84, train loss: 0.124341, validation loss: 0.11501\n",
      "iteration 85, train loss: 0.129114, validation loss: 0.113941\n",
      "iteration 86, train loss: 0.129585, validation loss: 0.108874\n",
      "iteration 87, train loss: 0.125459, validation loss: 0.105241\n",
      "iteration 88, train loss: 0.121124, validation loss: 0.108485\n",
      "iteration 89, train loss: 0.121199, validation loss: 0.110505\n",
      "iteration 90, train loss: 0.124314, validation loss: 0.114566\n",
      "iteration 91, train loss: 0.127033, validation loss: 0.116639\n",
      "iteration 92, train loss: 0.130243, validation loss: 0.109626\n",
      "iteration 93, train loss: 0.123689, validation loss: 0.114204\n",
      "iteration 94, train loss: 0.124438, validation loss: 0.110043\n",
      "iteration 95, train loss: 0.122296, validation loss: 0.11217\n",
      "iteration 96, train loss: 0.125849, validation loss: 0.12728\n",
      "iteration 97, train loss: 0.142221, validation loss: 0.147755\n",
      "iteration 98, train loss: 0.16061, validation loss: 0.159197\n",
      "iteration 99, train loss: 0.170118, validation loss: 0.113746\n",
      "iteration 100, train loss: 0.125703, validation loss: 0.145531\n",
      "iteration 101, train loss: 0.158227, validation loss: 0.1425\n",
      "iteration 102, train loss: 0.155081, validation loss: 0.113787\n",
      "iteration 103, train loss: 0.129027, validation loss: 0.167001\n",
      "iteration 104, train loss: 0.177109, validation loss: 0.121466\n",
      "iteration 105, train loss: 0.136218, validation loss: 0.138678\n",
      "iteration 106, train loss: 0.149508, validation loss: 0.118016\n",
      "iteration 107, train loss: 0.128616, validation loss: 0.131457\n",
      "iteration 108, train loss: 0.14195, validation loss: 0.114082\n",
      "iteration 109, train loss: 0.124444, validation loss: 0.127738\n",
      "iteration 110, train loss: 0.137468, validation loss: 0.116304\n",
      "iteration 111, train loss: 0.129423, validation loss: 0.122327\n",
      "iteration 112, train loss: 0.136324, validation loss: 0.115348\n",
      "iteration 113, train loss: 0.12354, validation loss: 0.116923\n",
      "iteration 114, train loss: 0.128249, validation loss: 0.122371\n",
      "iteration 115, train loss: 0.133312, validation loss: 0.118592\n"
     ]
    }
   ],
   "source": [
    "rnn_trained, train_losses, val_losses, net_params = trainer.run_training(train_mask=mask, same_batch=same_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_trainloss = plt.figure(figsize=(10, 3))\n",
    "plt.plot(train_losses, color='r', label='train loss (log scale)')\n",
    "plt.plot(val_losses, color='b', label='valid loss (log scale)')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=16)\n",
    "if disp:\n",
    "    plt.show()\n",
    "if not (datasaver is None): datasaver.save_figure(fig_trainloss, \"train&valid_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_params = pickle.load(open(os.path.join(get_project_root(), \"data\", \"trained_RNNs\", \"MemoryAntiNumber\", \"20230122-220547\", \"params_MemoryAntiNumber_0.02719.pkl\"), \"rb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=net_params[\"bias_rec\"],\n",
    "                      y_init=net_params[\"y_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = PerformanceAnalyzer(RNN_valid)\n",
    "score_function = lambda x, y: np.mean((x - y) ** 2)\n",
    "input_batch_valid, target_batch_valid, conditions_valid = task.get_batch()\n",
    "score = analyzer.get_validation_score(score_function, input_batch_valid, target_batch_valid,\n",
    "                                      mask, sigma_rec=sigma_rec, sigma_inp=sigma_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE validation: {np.round(score, 5)}\")\n",
    "if not (datasaver is None): datasaver.save_data(config_dict, \"config.json\")\n",
    "if not (datasaver is None): datasaver.save_data(best_net_params, f\"params_{taskname}_{np.round(score, 5)}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b39db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Plotting random trials\")\n",
    "inds = np.random.choice(np.arange(input_batch_valid.shape[-1]), 12)\n",
    "inputs = input_batch_valid[..., inds]\n",
    "targets = target_batch_valid[..., inds]\n",
    "\n",
    "fig_trials = analyzer.plot_trials(inputs, targets, mask, sigma_rec=sigma_rec, sigma_inp=sigma_inp)\n",
    "if disp:\n",
    "    plt.show()\n",
    "if not (datasaver is None): datasaver.save_figure(fig_trials, \"random_trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62599870",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa = DynamicSystemAnalyzer(RNN_valid)\n",
    "params = {\"fun_tol\" : 0.05,\n",
    "          \"diff_cutoff\":1e-4,\n",
    "          \"sigma_init_guess\":15,\n",
    "          \"patience\":100,\n",
    "          \"stop_length\":100,\n",
    "          \"mode\":\"approx\"}\n",
    "dsa.get_fixed_points(Input=np.array([0, 0]), **params)\n",
    "dsa.get_fixed_points(Input=np.array([0, 1]), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.vstack([dsa.fp_data[str([0, 0])][type] for type in list(dsa.fp_data[str([0, 0])].keys())])\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(all_points)\n",
    "P = np.zeros((RNN_valid.N, 2))\n",
    "P[:, 0] = RNN_valid.W_out.flatten()\n",
    "P[:, 1] = pca.components_.flatten()\n",
    "# P = orthonormalize(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd47d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fp = dsa.plot_fixed_points(projection='2D', P=P)\n",
    "plt.xlabel(\"Output axis\", fontsize=16)\n",
    "plt.ylabel(\"Line\", fontsize=16)\n",
    "if disp:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba506f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
