{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d44d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from src.DataSaver import DataSaver\n",
    "from src.DynamicSystemAnalyzer import *\n",
    "from src.PerformanceAnalyzer import *\n",
    "from src.RNN_numpy import RNN_numpy\n",
    "from src.utils import get_project_root, numpify, orthonormalize\n",
    "from src.Trainer import Trainer\n",
    "from src.RNN_torch import RNN_torch\n",
    "from src.Task import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "# from src.datajoint_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d64d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = True\n",
    "activation = \"relu\"\n",
    "taskname = \"ColorDiscrimination\"\n",
    "config_dict = json.load(open(os.path.join(get_project_root(), \"data\", \"configs\",\n",
    "                                          'train_config_ColorDiscrimination_relu;N=50;lmbdr=0.5;lmbdo=0.3.json'), mode=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4679b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining RNN:\n",
    "N = config_dict[\"N\"]\n",
    "activation_name = config_dict[\"activation\"]\n",
    "if activation_name == 'relu':\n",
    "    activation = lambda x: torch.maximum(x, torch.tensor(0))\n",
    "elif activation_name == 'tanh':\n",
    "    activation = torch.tanh\n",
    "elif activation_name == 'sigmoid':\n",
    "    activation = lambda x: 1/(1 + torch.exp(-x))\n",
    "elif activation_name == 'softplus':\n",
    "    activation = lambda x: torch.log(1 + torch.exp(5 * x))\n",
    "\n",
    "dt = config_dict[\"dt\"]\n",
    "tau = config_dict[\"tau\"]\n",
    "constrained = config_dict[\"constrained\"]\n",
    "connectivity_density_rec = config_dict[\"connectivity_density_rec\"]\n",
    "spectral_rad = config_dict[\"sr\"]\n",
    "sigma_inp = config_dict[\"sigma_inp\"]\n",
    "sigma_rec = config_dict[\"sigma_rec\"]\n",
    "seed = config_dict[\"seed\"]\n",
    "rng = torch.Generator()\n",
    "if not seed is None:\n",
    "    rng.manual_seed(seed)\n",
    "input_size = config_dict[\"num_inputs\"]\n",
    "output_size = config_dict[\"num_outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65573026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "n_steps = config_dict[\"n_steps\"]\n",
    "task_params = config_dict[\"task_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abe5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer:\n",
    "lambda_orth = config_dict[\"lambda_orth\"]\n",
    "lambda_r = config_dict[\"lambda_r\"]\n",
    "mask = np.array(config_dict[\"mask\"])\n",
    "max_iter = config_dict[\"max_iter\"]\n",
    "tol = config_dict[\"tol\"]\n",
    "lr = config_dict[\"lr\"]\n",
    "weight_decay = config_dict[\"weight_decay\"]\n",
    "same_batch = config_dict[\"same_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db30bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict[\"constrained\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63bf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for RNN!\n"
     ]
    }
   ],
   "source": [
    "# creating instances:\n",
    "rnn_torch = RNN_torch(N=N, dt=dt, tau=tau, input_size=input_size, output_size=output_size,\n",
    "                      activation=activation, constrained=constrained,\n",
    "                      sigma_inp=sigma_inp, sigma_rec=sigma_rec,\n",
    "                      connectivity_density_rec=connectivity_density_rec,\n",
    "                      spectral_rad=spectral_rad,\n",
    "                      random_generator=rng)\n",
    "task = TaskColorDiscrimination(n_steps=n_steps, task_params=task_params)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_torch.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "trainer = Trainer(RNN=rnn_torch, Task=task,\n",
    "                  max_iter=max_iter, tol=tol,\n",
    "                  optimizer=optimizer, criterion=criterion,\n",
    "                  lambda_orth=lambda_orth, lambda_r=lambda_r)\n",
    "datasaver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5833ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, conditions = task.get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2237210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 120, 360)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aeaa713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate only pure colors with rgb!\n",
    "# lms to color mapping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5356a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: \u001b[92m0.99773\u001b[0m, validation loss: \u001b[92m0.802859\u001b[0m\n",
      "iteration 1, train loss: \u001b[92m0.826625\u001b[0m, validation loss: \u001b[92m0.801797\u001b[0m\n",
      "iteration 2, train loss: \u001b[92m0.809484\u001b[0m, validation loss: 0.803406\n",
      "iteration 3, train loss: \u001b[92m0.804446\u001b[0m, validation loss: 0.80337\n",
      "iteration 4, train loss: \u001b[92m0.802154\u001b[0m, validation loss: 0.8021\n",
      "iteration 5, train loss: \u001b[92m0.800585\u001b[0m, validation loss: \u001b[92m0.80004\u001b[0m\n",
      "iteration 6, train loss: \u001b[92m0.798686\u001b[0m, validation loss: \u001b[92m0.797411\u001b[0m\n",
      "iteration 7, train loss: \u001b[92m0.796346\u001b[0m, validation loss: \u001b[92m0.794249\u001b[0m\n",
      "iteration 8, train loss: \u001b[92m0.793392\u001b[0m, validation loss: \u001b[92m0.790517\u001b[0m\n",
      "iteration 9, train loss: \u001b[92m0.79014\u001b[0m, validation loss: \u001b[92m0.786145\u001b[0m\n",
      "iteration 10, train loss: \u001b[92m0.785883\u001b[0m, validation loss: \u001b[92m0.781059\u001b[0m\n",
      "iteration 11, train loss: \u001b[92m0.780941\u001b[0m, validation loss: \u001b[92m0.775176\u001b[0m\n",
      "iteration 12, train loss: \u001b[92m0.775351\u001b[0m, validation loss: \u001b[92m0.768412\u001b[0m\n",
      "iteration 13, train loss: \u001b[92m0.768506\u001b[0m, validation loss: \u001b[92m0.760681\u001b[0m\n",
      "iteration 14, train loss: \u001b[92m0.761016\u001b[0m, validation loss: \u001b[92m0.751894\u001b[0m\n",
      "iteration 15, train loss: \u001b[92m0.75212\u001b[0m, validation loss: \u001b[92m0.741964\u001b[0m\n",
      "iteration 16, train loss: \u001b[92m0.742468\u001b[0m, validation loss: \u001b[92m0.730808\u001b[0m\n",
      "iteration 17, train loss: \u001b[92m0.731197\u001b[0m, validation loss: \u001b[92m0.718347\u001b[0m\n",
      "iteration 18, train loss: \u001b[92m0.718741\u001b[0m, validation loss: \u001b[92m0.704511\u001b[0m\n",
      "iteration 19, train loss: \u001b[92m0.70493\u001b[0m, validation loss: \u001b[92m0.689242\u001b[0m\n",
      "iteration 20, train loss: \u001b[92m0.689644\u001b[0m, validation loss: \u001b[92m0.67249\u001b[0m\n",
      "iteration 21, train loss: \u001b[92m0.673027\u001b[0m, validation loss: \u001b[92m0.654224\u001b[0m\n",
      "iteration 22, train loss: \u001b[92m0.65482\u001b[0m, validation loss: \u001b[92m0.634785\u001b[0m\n",
      "iteration 23, train loss: \u001b[92m0.635241\u001b[0m, validation loss: \u001b[92m0.61411\u001b[0m\n",
      "iteration 24, train loss: \u001b[92m0.614548\u001b[0m, validation loss: \u001b[92m0.59337\u001b[0m\n",
      "iteration 25, train loss: \u001b[92m0.593786\u001b[0m, validation loss: \u001b[92m0.571274\u001b[0m\n",
      "iteration 26, train loss: \u001b[92m0.571526\u001b[0m, validation loss: \u001b[92m0.54836\u001b[0m\n",
      "iteration 27, train loss: \u001b[92m0.54871\u001b[0m, validation loss: \u001b[92m0.526799\u001b[0m\n",
      "iteration 28, train loss: \u001b[92m0.52709\u001b[0m, validation loss: \u001b[92m0.50472\u001b[0m\n",
      "iteration 29, train loss: \u001b[92m0.505017\u001b[0m, validation loss: \u001b[92m0.482148\u001b[0m\n",
      "iteration 30, train loss: \u001b[92m0.482404\u001b[0m, validation loss: \u001b[92m0.46022\u001b[0m\n",
      "iteration 31, train loss: \u001b[92m0.46049\u001b[0m, validation loss: \u001b[92m0.439291\u001b[0m\n",
      "iteration 32, train loss: \u001b[92m0.439389\u001b[0m, validation loss: \u001b[92m0.418535\u001b[0m\n",
      "iteration 33, train loss: \u001b[92m0.418797\u001b[0m, validation loss: \u001b[92m0.39834\u001b[0m\n",
      "iteration 34, train loss: \u001b[92m0.398598\u001b[0m, validation loss: \u001b[92m0.379328\u001b[0m\n",
      "iteration 35, train loss: \u001b[92m0.379526\u001b[0m, validation loss: \u001b[92m0.361337\u001b[0m\n",
      "iteration 36, train loss: \u001b[92m0.361732\u001b[0m, validation loss: \u001b[92m0.343638\u001b[0m\n",
      "iteration 37, train loss: \u001b[92m0.344032\u001b[0m, validation loss: \u001b[92m0.326932\u001b[0m\n",
      "iteration 38, train loss: \u001b[92m0.327327\u001b[0m, validation loss: \u001b[92m0.311478\u001b[0m\n",
      "iteration 39, train loss: \u001b[92m0.31177\u001b[0m, validation loss: \u001b[92m0.296506\u001b[0m\n",
      "iteration 40, train loss: \u001b[92m0.29694\u001b[0m, validation loss: \u001b[92m0.282243\u001b[0m\n",
      "iteration 41, train loss: \u001b[92m0.282705\u001b[0m, validation loss: \u001b[92m0.268651\u001b[0m\n",
      "iteration 42, train loss: \u001b[92m0.269316\u001b[0m, validation loss: \u001b[92m0.25547\u001b[0m\n",
      "iteration 43, train loss: \u001b[92m0.256191\u001b[0m, validation loss: \u001b[92m0.242746\u001b[0m\n",
      "iteration 44, train loss: \u001b[92m0.243529\u001b[0m, validation loss: \u001b[92m0.230245\u001b[0m\n",
      "iteration 45, train loss: \u001b[92m0.231138\u001b[0m, validation loss: \u001b[92m0.217882\u001b[0m\n",
      "iteration 46, train loss: \u001b[92m0.219025\u001b[0m, validation loss: \u001b[92m0.206187\u001b[0m\n",
      "iteration 47, train loss: \u001b[92m0.207449\u001b[0m, validation loss: \u001b[92m0.195042\u001b[0m\n",
      "iteration 48, train loss: \u001b[92m0.196511\u001b[0m, validation loss: \u001b[92m0.184702\u001b[0m\n",
      "iteration 49, train loss: \u001b[92m0.186348\u001b[0m, validation loss: \u001b[92m0.175238\u001b[0m\n",
      "iteration 50, train loss: \u001b[92m0.177092\u001b[0m, validation loss: \u001b[92m0.166588\u001b[0m\n",
      "iteration 51, train loss: \u001b[92m0.168608\u001b[0m, validation loss: \u001b[92m0.158788\u001b[0m\n",
      "iteration 52, train loss: \u001b[92m0.161023\u001b[0m, validation loss: \u001b[92m0.152291\u001b[0m\n",
      "iteration 53, train loss: \u001b[92m0.154682\u001b[0m, validation loss: \u001b[92m0.14609\u001b[0m\n",
      "iteration 54, train loss: \u001b[92m0.148632\u001b[0m, validation loss: \u001b[92m0.140163\u001b[0m\n",
      "iteration 55, train loss: \u001b[92m0.14281\u001b[0m, validation loss: \u001b[92m0.134508\u001b[0m\n",
      "iteration 56, train loss: \u001b[92m0.137262\u001b[0m, validation loss: \u001b[92m0.129119\u001b[0m\n",
      "iteration 57, train loss: \u001b[92m0.132081\u001b[0m, validation loss: \u001b[92m0.123995\u001b[0m\n",
      "iteration 58, train loss: \u001b[92m0.127055\u001b[0m, validation loss: \u001b[92m0.119262\u001b[0m\n",
      "iteration 59, train loss: \u001b[92m0.122359\u001b[0m, validation loss: \u001b[92m0.115073\u001b[0m\n",
      "iteration 60, train loss: \u001b[92m0.118299\u001b[0m, validation loss: \u001b[92m0.111314\u001b[0m\n",
      "iteration 61, train loss: \u001b[92m0.114609\u001b[0m, validation loss: \u001b[92m0.107918\u001b[0m\n",
      "iteration 62, train loss: \u001b[92m0.111212\u001b[0m, validation loss: \u001b[92m0.104714\u001b[0m\n",
      "iteration 63, train loss: \u001b[92m0.10801\u001b[0m, validation loss: \u001b[92m0.101698\u001b[0m\n",
      "iteration 64, train loss: \u001b[92m0.105022\u001b[0m, validation loss: \u001b[92m0.098912\u001b[0m\n",
      "iteration 65, train loss: \u001b[92m0.10232\u001b[0m, validation loss: \u001b[92m0.096396\u001b[0m\n",
      "iteration 66, train loss: \u001b[92m0.099744\u001b[0m, validation loss: \u001b[92m0.094073\u001b[0m\n",
      "iteration 67, train loss: \u001b[92m0.097429\u001b[0m, validation loss: \u001b[92m0.091976\u001b[0m\n",
      "iteration 68, train loss: \u001b[92m0.095252\u001b[0m, validation loss: \u001b[92m0.090179\u001b[0m\n",
      "iteration 69, train loss: \u001b[92m0.093422\u001b[0m, validation loss: \u001b[92m0.088553\u001b[0m\n",
      "iteration 70, train loss: \u001b[92m0.091855\u001b[0m, validation loss: \u001b[92m0.087109\u001b[0m\n",
      "iteration 71, train loss: \u001b[92m0.090318\u001b[0m, validation loss: \u001b[92m0.085788\u001b[0m\n",
      "iteration 72, train loss: \u001b[92m0.088971\u001b[0m, validation loss: \u001b[92m0.084577\u001b[0m\n",
      "iteration 73, train loss: \u001b[92m0.087629\u001b[0m, validation loss: \u001b[92m0.083469\u001b[0m\n",
      "iteration 74, train loss: \u001b[92m0.086551\u001b[0m, validation loss: \u001b[92m0.08243\u001b[0m\n",
      "iteration 75, train loss: \u001b[92m0.085415\u001b[0m, validation loss: \u001b[92m0.081459\u001b[0m\n",
      "iteration 76, train loss: \u001b[92m0.084441\u001b[0m, validation loss: \u001b[92m0.080555\u001b[0m\n",
      "iteration 77, train loss: \u001b[92m0.083528\u001b[0m, validation loss: \u001b[92m0.07971\u001b[0m\n",
      "iteration 78, train loss: \u001b[92m0.082575\u001b[0m, validation loss: \u001b[92m0.078933\u001b[0m\n",
      "iteration 79, train loss: \u001b[92m0.081762\u001b[0m, validation loss: \u001b[92m0.078242\u001b[0m\n",
      "iteration 80, train loss: \u001b[92m0.081109\u001b[0m, validation loss: \u001b[92m0.077595\u001b[0m\n",
      "iteration 81, train loss: \u001b[92m0.080356\u001b[0m, validation loss: \u001b[92m0.076992\u001b[0m\n",
      "iteration 82, train loss: \u001b[92m0.079773\u001b[0m, validation loss: \u001b[92m0.076502\u001b[0m\n",
      "iteration 83, train loss: \u001b[92m0.079255\u001b[0m, validation loss: \u001b[92m0.07608\u001b[0m\n",
      "iteration 84, train loss: \u001b[92m0.078747\u001b[0m, validation loss: \u001b[92m0.075687\u001b[0m\n",
      "iteration 85, train loss: \u001b[92m0.07837\u001b[0m, validation loss: \u001b[92m0.075327\u001b[0m\n",
      "iteration 86, train loss: \u001b[92m0.078032\u001b[0m, validation loss: \u001b[92m0.075006\u001b[0m\n",
      "iteration 87, train loss: \u001b[92m0.077621\u001b[0m, validation loss: \u001b[92m0.074724\u001b[0m\n",
      "iteration 88, train loss: \u001b[92m0.077389\u001b[0m, validation loss: \u001b[92m0.074454\u001b[0m\n",
      "iteration 89, train loss: \u001b[92m0.077052\u001b[0m, validation loss: \u001b[92m0.074195\u001b[0m\n",
      "iteration 90, train loss: \u001b[92m0.076833\u001b[0m, validation loss: \u001b[92m0.073988\u001b[0m\n",
      "iteration 91, train loss: \u001b[92m0.07655\u001b[0m, validation loss: \u001b[92m0.07381\u001b[0m\n",
      "iteration 92, train loss: \u001b[92m0.076406\u001b[0m, validation loss: \u001b[92m0.073674\u001b[0m\n",
      "iteration 93, train loss: \u001b[92m0.076249\u001b[0m, validation loss: \u001b[92m0.07356\u001b[0m\n",
      "iteration 94, train loss: \u001b[92m0.076099\u001b[0m, validation loss: \u001b[92m0.073451\u001b[0m\n",
      "iteration 95, train loss: \u001b[92m0.076046\u001b[0m, validation loss: \u001b[92m0.073346\u001b[0m\n",
      "iteration 96, train loss: \u001b[92m0.075911\u001b[0m, validation loss: \u001b[92m0.073246\u001b[0m\n",
      "iteration 97, train loss: \u001b[92m0.0758\u001b[0m, validation loss: \u001b[92m0.073163\u001b[0m\n",
      "iteration 98, train loss: \u001b[92m0.075692\u001b[0m, validation loss: \u001b[92m0.073081\u001b[0m\n",
      "iteration 99, train loss: \u001b[92m0.075689\u001b[0m, validation loss: \u001b[92m0.073002\u001b[0m\n",
      "iteration 100, train loss: \u001b[92m0.075539\u001b[0m, validation loss: \u001b[92m0.072923\u001b[0m\n",
      "iteration 101, train loss: \u001b[92m0.07549\u001b[0m, validation loss: \u001b[92m0.072846\u001b[0m\n",
      "iteration 102, train loss: \u001b[92m0.075411\u001b[0m, validation loss: \u001b[92m0.07277\u001b[0m\n",
      "iteration 103, train loss: \u001b[92m0.075331\u001b[0m, validation loss: \u001b[92m0.072696\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 104, train loss: \u001b[92m0.075269\u001b[0m, validation loss: \u001b[92m0.072625\u001b[0m\n",
      "iteration 105, train loss: \u001b[92m0.075142\u001b[0m, validation loss: \u001b[92m0.072554\u001b[0m\n",
      "iteration 106, train loss: 0.075147, validation loss: \u001b[92m0.072485\u001b[0m\n",
      "iteration 107, train loss: \u001b[92m0.075116\u001b[0m, validation loss: \u001b[92m0.072417\u001b[0m\n",
      "iteration 108, train loss: \u001b[92m0.075017\u001b[0m, validation loss: \u001b[92m0.072352\u001b[0m\n",
      "iteration 109, train loss: \u001b[92m0.07495\u001b[0m, validation loss: \u001b[92m0.072289\u001b[0m\n",
      "iteration 110, train loss: \u001b[92m0.074924\u001b[0m, validation loss: \u001b[92m0.072228\u001b[0m\n",
      "iteration 111, train loss: \u001b[92m0.074878\u001b[0m, validation loss: \u001b[92m0.072169\u001b[0m\n",
      "iteration 112, train loss: \u001b[92m0.074709\u001b[0m, validation loss: \u001b[92m0.072112\u001b[0m\n",
      "iteration 113, train loss: 0.074741, validation loss: \u001b[92m0.072057\u001b[0m\n",
      "iteration 114, train loss: \u001b[92m0.074644\u001b[0m, validation loss: \u001b[92m0.072003\u001b[0m\n",
      "iteration 115, train loss: \u001b[92m0.074578\u001b[0m, validation loss: \u001b[92m0.071952\u001b[0m\n",
      "iteration 116, train loss: \u001b[92m0.074576\u001b[0m, validation loss: \u001b[92m0.071902\u001b[0m\n",
      "iteration 117, train loss: \u001b[92m0.074505\u001b[0m, validation loss: \u001b[92m0.071854\u001b[0m\n",
      "iteration 118, train loss: \u001b[92m0.074431\u001b[0m, validation loss: \u001b[92m0.071808\u001b[0m\n",
      "iteration 119, train loss: 0.074456, validation loss: \u001b[92m0.071764\u001b[0m\n",
      "iteration 120, train loss: \u001b[92m0.0744\u001b[0m, validation loss: \u001b[92m0.071721\u001b[0m\n",
      "iteration 121, train loss: \u001b[92m0.074304\u001b[0m, validation loss: \u001b[92m0.071681\u001b[0m\n",
      "iteration 122, train loss: 0.074312, validation loss: \u001b[92m0.071642\u001b[0m\n",
      "iteration 123, train loss: \u001b[92m0.074282\u001b[0m, validation loss: \u001b[92m0.071605\u001b[0m\n",
      "iteration 124, train loss: \u001b[92m0.074245\u001b[0m, validation loss: \u001b[92m0.07157\u001b[0m\n",
      "iteration 125, train loss: 0.074257, validation loss: \u001b[92m0.071538\u001b[0m\n",
      "iteration 126, train loss: \u001b[92m0.074186\u001b[0m, validation loss: \u001b[92m0.071508\u001b[0m\n",
      "iteration 127, train loss: \u001b[92m0.074162\u001b[0m, validation loss: \u001b[92m0.071481\u001b[0m\n",
      "iteration 128, train loss: 0.074172, validation loss: \u001b[92m0.071456\u001b[0m\n",
      "iteration 129, train loss: \u001b[92m0.07409\u001b[0m, validation loss: \u001b[92m0.071432\u001b[0m\n",
      "iteration 130, train loss: \u001b[92m0.07403\u001b[0m, validation loss: \u001b[92m0.071409\u001b[0m\n",
      "iteration 131, train loss: 0.074076, validation loss: \u001b[92m0.071387\u001b[0m\n",
      "iteration 132, train loss: \u001b[92m0.074025\u001b[0m, validation loss: \u001b[92m0.071365\u001b[0m\n",
      "iteration 133, train loss: \u001b[92m0.074014\u001b[0m, validation loss: \u001b[92m0.071344\u001b[0m\n",
      "iteration 134, train loss: 0.074033, validation loss: \u001b[92m0.071323\u001b[0m\n",
      "iteration 135, train loss: \u001b[92m0.073976\u001b[0m, validation loss: \u001b[92m0.071304\u001b[0m\n",
      "iteration 136, train loss: \u001b[92m0.073949\u001b[0m, validation loss: \u001b[92m0.071285\u001b[0m\n",
      "iteration 137, train loss: 0.073981, validation loss: \u001b[92m0.071267\u001b[0m\n",
      "iteration 138, train loss: \u001b[92m0.073939\u001b[0m, validation loss: \u001b[92m0.071249\u001b[0m\n",
      "iteration 139, train loss: \u001b[92m0.073914\u001b[0m, validation loss: \u001b[92m0.071232\u001b[0m\n",
      "iteration 140, train loss: \u001b[92m0.07386\u001b[0m, validation loss: \u001b[92m0.071216\u001b[0m\n",
      "iteration 141, train loss: 0.073884, validation loss: \u001b[92m0.0712\u001b[0m\n",
      "iteration 142, train loss: \u001b[92m0.073853\u001b[0m, validation loss: \u001b[92m0.071184\u001b[0m\n",
      "iteration 143, train loss: \u001b[92m0.073806\u001b[0m, validation loss: \u001b[92m0.071173\u001b[0m\n",
      "iteration 144, train loss: 0.073898, validation loss: \u001b[92m0.071162\u001b[0m\n",
      "iteration 145, train loss: 0.073864, validation loss: \u001b[92m0.071151\u001b[0m\n",
      "iteration 146, train loss: 0.073842, validation loss: \u001b[92m0.07114\u001b[0m\n",
      "iteration 147, train loss: \u001b[92m0.073775\u001b[0m, validation loss: \u001b[92m0.071129\u001b[0m\n",
      "iteration 148, train loss: 0.073793, validation loss: \u001b[92m0.071118\u001b[0m\n",
      "iteration 149, train loss: 0.073823, validation loss: \u001b[92m0.071108\u001b[0m\n",
      "iteration 150, train loss: 0.073849, validation loss: \u001b[92m0.071097\u001b[0m\n",
      "iteration 151, train loss: \u001b[92m0.073766\u001b[0m, validation loss: \u001b[92m0.071087\u001b[0m\n",
      "iteration 152, train loss: 0.073768, validation loss: \u001b[92m0.071076\u001b[0m\n",
      "iteration 153, train loss: \u001b[92m0.073721\u001b[0m, validation loss: \u001b[92m0.071066\u001b[0m\n",
      "iteration 154, train loss: \u001b[92m0.073676\u001b[0m, validation loss: \u001b[92m0.071056\u001b[0m\n",
      "iteration 155, train loss: 0.073733, validation loss: \u001b[92m0.071045\u001b[0m\n",
      "iteration 156, train loss: 0.073683, validation loss: \u001b[92m0.071035\u001b[0m\n",
      "iteration 157, train loss: \u001b[92m0.073659\u001b[0m, validation loss: \u001b[92m0.071025\u001b[0m\n",
      "iteration 158, train loss: 0.0737, validation loss: \u001b[92m0.071015\u001b[0m\n",
      "iteration 159, train loss: 0.073748, validation loss: \u001b[92m0.071005\u001b[0m\n",
      "iteration 160, train loss: \u001b[92m0.073649\u001b[0m, validation loss: \u001b[92m0.070995\u001b[0m\n",
      "iteration 161, train loss: \u001b[92m0.073644\u001b[0m, validation loss: \u001b[92m0.070985\u001b[0m\n",
      "iteration 162, train loss: 0.073714, validation loss: \u001b[92m0.070975\u001b[0m\n",
      "iteration 163, train loss: \u001b[92m0.073603\u001b[0m, validation loss: \u001b[92m0.070966\u001b[0m\n",
      "iteration 164, train loss: 0.073734, validation loss: \u001b[92m0.070956\u001b[0m\n",
      "iteration 165, train loss: 0.073639, validation loss: \u001b[92m0.070947\u001b[0m\n",
      "iteration 166, train loss: 0.073658, validation loss: \u001b[92m0.070938\u001b[0m\n",
      "iteration 167, train loss: 0.073606, validation loss: \u001b[92m0.070929\u001b[0m\n",
      "iteration 168, train loss: 0.073712, validation loss: \u001b[92m0.07092\u001b[0m\n",
      "iteration 169, train loss: \u001b[92m0.073523\u001b[0m, validation loss: \u001b[92m0.07091\u001b[0m\n",
      "iteration 170, train loss: 0.073528, validation loss: \u001b[92m0.070901\u001b[0m\n",
      "iteration 171, train loss: 0.073589, validation loss: \u001b[92m0.070892\u001b[0m\n",
      "iteration 172, train loss: 0.073649, validation loss: \u001b[92m0.070883\u001b[0m\n",
      "iteration 173, train loss: 0.073564, validation loss: \u001b[92m0.070874\u001b[0m\n",
      "iteration 174, train loss: 0.073612, validation loss: \u001b[92m0.070865\u001b[0m\n",
      "iteration 175, train loss: 0.07361, validation loss: \u001b[92m0.070856\u001b[0m\n",
      "iteration 176, train loss: 0.073632, validation loss: \u001b[92m0.070847\u001b[0m\n",
      "iteration 177, train loss: 0.073617, validation loss: \u001b[92m0.070839\u001b[0m\n",
      "iteration 178, train loss: \u001b[92m0.07349\u001b[0m, validation loss: \u001b[92m0.07083\u001b[0m\n",
      "iteration 179, train loss: 0.073562, validation loss: \u001b[92m0.070821\u001b[0m\n",
      "iteration 180, train loss: 0.073585, validation loss: \u001b[92m0.070812\u001b[0m\n",
      "iteration 181, train loss: 0.073566, validation loss: \u001b[92m0.070804\u001b[0m\n",
      "iteration 182, train loss: 0.0735, validation loss: \u001b[92m0.070795\u001b[0m\n",
      "iteration 183, train loss: 0.073515, validation loss: \u001b[92m0.070786\u001b[0m\n",
      "iteration 184, train loss: 0.073505, validation loss: \u001b[92m0.070778\u001b[0m\n",
      "iteration 185, train loss: 0.07357, validation loss: \u001b[92m0.070769\u001b[0m\n",
      "iteration 186, train loss: \u001b[92m0.073457\u001b[0m, validation loss: \u001b[92m0.070761\u001b[0m\n",
      "iteration 187, train loss: 0.073611, validation loss: \u001b[92m0.070753\u001b[0m\n",
      "iteration 188, train loss: 0.073532, validation loss: \u001b[92m0.070745\u001b[0m\n",
      "iteration 189, train loss: 0.073498, validation loss: \u001b[92m0.070737\u001b[0m\n",
      "iteration 190, train loss: 0.073483, validation loss: \u001b[92m0.07073\u001b[0m\n",
      "iteration 191, train loss: 0.073503, validation loss: \u001b[92m0.070722\u001b[0m\n",
      "iteration 192, train loss: 0.073529, validation loss: \u001b[92m0.070715\u001b[0m\n",
      "iteration 193, train loss: \u001b[92m0.07339\u001b[0m, validation loss: \u001b[92m0.070707\u001b[0m\n",
      "iteration 194, train loss: 0.073426, validation loss: \u001b[92m0.070699\u001b[0m\n",
      "iteration 195, train loss: 0.073568, validation loss: \u001b[92m0.070691\u001b[0m\n",
      "iteration 196, train loss: \u001b[92m0.073334\u001b[0m, validation loss: \u001b[92m0.070683\u001b[0m\n",
      "iteration 197, train loss: 0.073447, validation loss: \u001b[92m0.070675\u001b[0m\n",
      "iteration 198, train loss: 0.073462, validation loss: \u001b[92m0.070667\u001b[0m\n",
      "iteration 199, train loss: 0.073497, validation loss: \u001b[92m0.070659\u001b[0m\n",
      "iteration 200, train loss: 0.073427, validation loss: \u001b[92m0.070651\u001b[0m\n",
      "iteration 201, train loss: \u001b[92m0.073327\u001b[0m, validation loss: \u001b[92m0.070643\u001b[0m\n",
      "iteration 202, train loss: 0.073505, validation loss: \u001b[92m0.070636\u001b[0m\n",
      "iteration 203, train loss: 0.073356, validation loss: \u001b[92m0.070628\u001b[0m\n",
      "iteration 204, train loss: 0.073356, validation loss: \u001b[92m0.07062\u001b[0m\n",
      "iteration 205, train loss: \u001b[92m0.07327\u001b[0m, validation loss: \u001b[92m0.070612\u001b[0m\n",
      "iteration 206, train loss: 0.073463, validation loss: \u001b[92m0.070604\u001b[0m\n",
      "iteration 207, train loss: 0.07334, validation loss: \u001b[92m0.070597\u001b[0m\n",
      "iteration 208, train loss: 0.073386, validation loss: \u001b[92m0.070589\u001b[0m\n",
      "iteration 209, train loss: 0.073338, validation loss: \u001b[92m0.070581\u001b[0m\n",
      "iteration 210, train loss: 0.073461, validation loss: \u001b[92m0.070573\u001b[0m\n",
      "iteration 211, train loss: 0.073313, validation loss: \u001b[92m0.070565\u001b[0m\n",
      "iteration 212, train loss: 0.073452, validation loss: \u001b[92m0.070558\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 213, train loss: 0.073369, validation loss: \u001b[92m0.070551\u001b[0m\n",
      "iteration 214, train loss: 0.07341, validation loss: \u001b[92m0.070544\u001b[0m\n",
      "iteration 215, train loss: 0.073377, validation loss: \u001b[92m0.070538\u001b[0m\n",
      "iteration 216, train loss: \u001b[92m0.073258\u001b[0m, validation loss: \u001b[92m0.070531\u001b[0m\n",
      "iteration 217, train loss: 0.073452, validation loss: \u001b[92m0.070524\u001b[0m\n",
      "iteration 218, train loss: 0.073352, validation loss: \u001b[92m0.070518\u001b[0m\n",
      "iteration 219, train loss: 0.073328, validation loss: \u001b[92m0.070512\u001b[0m\n",
      "iteration 220, train loss: 0.073433, validation loss: \u001b[92m0.070506\u001b[0m\n",
      "iteration 221, train loss: 0.073335, validation loss: \u001b[92m0.070501\u001b[0m\n",
      "iteration 222, train loss: 0.073348, validation loss: \u001b[92m0.070495\u001b[0m\n",
      "iteration 223, train loss: \u001b[92m0.073173\u001b[0m, validation loss: \u001b[92m0.07049\u001b[0m\n",
      "iteration 224, train loss: 0.073325, validation loss: \u001b[92m0.070484\u001b[0m\n",
      "iteration 225, train loss: 0.073327, validation loss: \u001b[92m0.070479\u001b[0m\n",
      "iteration 226, train loss: 0.073331, validation loss: \u001b[92m0.070473\u001b[0m\n",
      "iteration 227, train loss: 0.073299, validation loss: \u001b[92m0.070468\u001b[0m\n",
      "iteration 228, train loss: 0.073395, validation loss: \u001b[92m0.070463\u001b[0m\n",
      "iteration 229, train loss: 0.073236, validation loss: \u001b[92m0.070457\u001b[0m\n",
      "iteration 230, train loss: 0.073346, validation loss: \u001b[92m0.070452\u001b[0m\n",
      "iteration 231, train loss: 0.073272, validation loss: \u001b[92m0.070447\u001b[0m\n",
      "iteration 232, train loss: 0.073253, validation loss: \u001b[92m0.070442\u001b[0m\n",
      "iteration 233, train loss: 0.07334, validation loss: \u001b[92m0.070437\u001b[0m\n",
      "iteration 234, train loss: 0.07322, validation loss: \u001b[92m0.070431\u001b[0m\n",
      "iteration 235, train loss: 0.07324, validation loss: \u001b[92m0.070425\u001b[0m\n",
      "iteration 236, train loss: 0.073235, validation loss: \u001b[92m0.07042\u001b[0m\n",
      "iteration 237, train loss: 0.073175, validation loss: \u001b[92m0.070414\u001b[0m\n",
      "iteration 238, train loss: 0.07332, validation loss: \u001b[92m0.070408\u001b[0m\n",
      "iteration 239, train loss: 0.073308, validation loss: \u001b[92m0.070403\u001b[0m\n",
      "iteration 240, train loss: 0.073192, validation loss: \u001b[92m0.070397\u001b[0m\n",
      "iteration 241, train loss: 0.073273, validation loss: \u001b[92m0.070391\u001b[0m\n",
      "iteration 242, train loss: \u001b[92m0.073162\u001b[0m, validation loss: \u001b[92m0.070385\u001b[0m\n",
      "iteration 243, train loss: 0.073263, validation loss: \u001b[92m0.07038\u001b[0m\n",
      "iteration 244, train loss: 0.073228, validation loss: \u001b[92m0.070374\u001b[0m\n",
      "iteration 245, train loss: 0.073355, validation loss: \u001b[92m0.07037\u001b[0m\n",
      "iteration 246, train loss: 0.073333, validation loss: \u001b[92m0.070365\u001b[0m\n",
      "iteration 247, train loss: 0.073274, validation loss: \u001b[92m0.070362\u001b[0m\n",
      "iteration 248, train loss: 0.073315, validation loss: \u001b[92m0.070358\u001b[0m\n",
      "iteration 249, train loss: 0.073295, validation loss: \u001b[92m0.070354\u001b[0m\n",
      "iteration 250, train loss: \u001b[92m0.073126\u001b[0m, validation loss: \u001b[92m0.07035\u001b[0m\n",
      "iteration 251, train loss: 0.073342, validation loss: \u001b[92m0.070347\u001b[0m\n",
      "iteration 252, train loss: 0.073281, validation loss: \u001b[92m0.070344\u001b[0m\n",
      "iteration 253, train loss: 0.073302, validation loss: \u001b[92m0.070341\u001b[0m\n",
      "iteration 254, train loss: 0.073347, validation loss: \u001b[92m0.070338\u001b[0m\n",
      "iteration 255, train loss: 0.073194, validation loss: \u001b[92m0.070336\u001b[0m\n",
      "iteration 256, train loss: 0.073298, validation loss: \u001b[92m0.070333\u001b[0m\n",
      "iteration 257, train loss: \u001b[92m0.073078\u001b[0m, validation loss: \u001b[92m0.070329\u001b[0m\n",
      "iteration 258, train loss: 0.073129, validation loss: \u001b[92m0.070326\u001b[0m\n",
      "iteration 259, train loss: 0.073167, validation loss: \u001b[92m0.070321\u001b[0m\n",
      "iteration 260, train loss: 0.073252, validation loss: \u001b[92m0.070317\u001b[0m\n",
      "iteration 261, train loss: 0.073175, validation loss: \u001b[92m0.070313\u001b[0m\n",
      "iteration 262, train loss: 0.073206, validation loss: \u001b[92m0.070309\u001b[0m\n",
      "iteration 263, train loss: 0.07324, validation loss: \u001b[92m0.070305\u001b[0m\n",
      "iteration 264, train loss: 0.073198, validation loss: \u001b[92m0.070301\u001b[0m\n",
      "iteration 265, train loss: 0.07319, validation loss: \u001b[92m0.070297\u001b[0m\n",
      "iteration 266, train loss: 0.073189, validation loss: \u001b[92m0.070292\u001b[0m\n",
      "iteration 267, train loss: 0.073152, validation loss: \u001b[92m0.070287\u001b[0m\n",
      "iteration 268, train loss: 0.073202, validation loss: \u001b[92m0.070282\u001b[0m\n",
      "iteration 269, train loss: 0.073236, validation loss: \u001b[92m0.070277\u001b[0m\n",
      "iteration 270, train loss: 0.073155, validation loss: \u001b[92m0.070273\u001b[0m\n",
      "iteration 271, train loss: 0.073203, validation loss: \u001b[92m0.070268\u001b[0m\n",
      "iteration 272, train loss: 0.073197, validation loss: \u001b[92m0.070264\u001b[0m\n",
      "iteration 273, train loss: 0.073184, validation loss: \u001b[92m0.070261\u001b[0m\n",
      "iteration 274, train loss: 0.073157, validation loss: \u001b[92m0.070257\u001b[0m\n",
      "iteration 275, train loss: 0.073219, validation loss: \u001b[92m0.070254\u001b[0m\n",
      "iteration 276, train loss: \u001b[92m0.073056\u001b[0m, validation loss: \u001b[92m0.07025\u001b[0m\n",
      "iteration 277, train loss: 0.073178, validation loss: \u001b[92m0.070246\u001b[0m\n",
      "iteration 278, train loss: 0.07315, validation loss: \u001b[92m0.070243\u001b[0m\n",
      "iteration 279, train loss: 0.073111, validation loss: \u001b[92m0.070239\u001b[0m\n",
      "iteration 280, train loss: 0.073136, validation loss: \u001b[92m0.070236\u001b[0m\n",
      "iteration 281, train loss: 0.073101, validation loss: \u001b[92m0.070233\u001b[0m\n",
      "iteration 282, train loss: \u001b[92m0.073034\u001b[0m, validation loss: \u001b[92m0.070228\u001b[0m\n",
      "iteration 283, train loss: 0.073067, validation loss: \u001b[92m0.070224\u001b[0m\n",
      "iteration 284, train loss: 0.073184, validation loss: \u001b[92m0.07022\u001b[0m\n",
      "iteration 285, train loss: 0.073124, validation loss: \u001b[92m0.070215\u001b[0m\n",
      "iteration 286, train loss: 0.073166, validation loss: \u001b[92m0.070211\u001b[0m\n",
      "iteration 287, train loss: 0.073262, validation loss: \u001b[92m0.070208\u001b[0m\n",
      "iteration 288, train loss: 0.07322, validation loss: \u001b[92m0.070205\u001b[0m\n",
      "iteration 289, train loss: 0.073055, validation loss: \u001b[92m0.070201\u001b[0m\n",
      "iteration 290, train loss: 0.073102, validation loss: \u001b[92m0.070197\u001b[0m\n",
      "iteration 291, train loss: 0.073252, validation loss: \u001b[92m0.070194\u001b[0m\n",
      "iteration 292, train loss: 0.073184, validation loss: \u001b[92m0.07019\u001b[0m\n",
      "iteration 293, train loss: 0.073139, validation loss: \u001b[92m0.070187\u001b[0m\n",
      "iteration 294, train loss: 0.073089, validation loss: \u001b[92m0.070184\u001b[0m\n",
      "iteration 295, train loss: 0.073191, validation loss: \u001b[92m0.070181\u001b[0m\n",
      "iteration 296, train loss: 0.073179, validation loss: \u001b[92m0.070179\u001b[0m\n",
      "iteration 297, train loss: 0.073077, validation loss: \u001b[92m0.070176\u001b[0m\n",
      "iteration 298, train loss: 0.073103, validation loss: \u001b[92m0.070174\u001b[0m\n",
      "iteration 299, train loss: 0.073105, validation loss: \u001b[92m0.070171\u001b[0m\n",
      "iteration 300, train loss: 0.073089, validation loss: \u001b[92m0.070168\u001b[0m\n",
      "iteration 301, train loss: 0.073272, validation loss: \u001b[92m0.070166\u001b[0m\n",
      "iteration 302, train loss: 0.073097, validation loss: \u001b[92m0.070163\u001b[0m\n",
      "iteration 303, train loss: 0.073224, validation loss: \u001b[92m0.070161\u001b[0m\n",
      "iteration 304, train loss: 0.073053, validation loss: \u001b[92m0.070159\u001b[0m\n",
      "iteration 305, train loss: 0.073043, validation loss: \u001b[92m0.070157\u001b[0m\n",
      "iteration 306, train loss: 0.073101, validation loss: \u001b[92m0.070155\u001b[0m\n",
      "iteration 307, train loss: 0.073079, validation loss: \u001b[92m0.070153\u001b[0m\n",
      "iteration 308, train loss: 0.073089, validation loss: \u001b[92m0.070151\u001b[0m\n",
      "iteration 309, train loss: 0.073189, validation loss: \u001b[92m0.07015\u001b[0m\n",
      "iteration 310, train loss: 0.073038, validation loss: \u001b[92m0.070149\u001b[0m\n",
      "iteration 311, train loss: 0.073132, validation loss: \u001b[92m0.070147\u001b[0m\n",
      "iteration 312, train loss: \u001b[92m0.072892\u001b[0m, validation loss: \u001b[92m0.070145\u001b[0m\n",
      "iteration 313, train loss: 0.073117, validation loss: \u001b[92m0.070143\u001b[0m\n",
      "iteration 314, train loss: 0.073071, validation loss: \u001b[92m0.070141\u001b[0m\n",
      "iteration 315, train loss: 0.073045, validation loss: \u001b[92m0.070139\u001b[0m\n",
      "iteration 316, train loss: 0.07317, validation loss: \u001b[92m0.070137\u001b[0m\n",
      "iteration 317, train loss: 0.073193, validation loss: \u001b[92m0.070135\u001b[0m\n",
      "iteration 318, train loss: 0.073225, validation loss: \u001b[92m0.070134\u001b[0m\n",
      "iteration 319, train loss: 0.073072, validation loss: \u001b[92m0.070132\u001b[0m\n",
      "iteration 320, train loss: 0.073057, validation loss: \u001b[92m0.07013\u001b[0m\n",
      "iteration 321, train loss: 0.073096, validation loss: \u001b[92m0.070127\u001b[0m\n",
      "iteration 322, train loss: 0.073089, validation loss: \u001b[92m0.070125\u001b[0m\n",
      "iteration 323, train loss: 0.073069, validation loss: \u001b[92m0.070123\u001b[0m\n",
      "iteration 324, train loss: 0.07314, validation loss: \u001b[92m0.070121\u001b[0m\n",
      "iteration 325, train loss: 0.073148, validation loss: \u001b[92m0.070119\u001b[0m\n",
      "iteration 326, train loss: 0.07317, validation loss: \u001b[92m0.070117\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 327, train loss: 0.073088, validation loss: \u001b[92m0.070115\u001b[0m\n",
      "iteration 328, train loss: 0.073136, validation loss: \u001b[92m0.070114\u001b[0m\n",
      "iteration 329, train loss: 0.073036, validation loss: \u001b[92m0.070112\u001b[0m\n",
      "iteration 330, train loss: 0.073276, validation loss: \u001b[92m0.070111\u001b[0m\n",
      "iteration 331, train loss: 0.073034, validation loss: \u001b[92m0.07011\u001b[0m\n",
      "iteration 332, train loss: 0.073147, validation loss: \u001b[92m0.070108\u001b[0m\n",
      "iteration 333, train loss: 0.072955, validation loss: \u001b[92m0.070106\u001b[0m\n",
      "iteration 334, train loss: 0.073202, validation loss: \u001b[92m0.070104\u001b[0m\n",
      "iteration 335, train loss: 0.073087, validation loss: \u001b[92m0.070102\u001b[0m\n",
      "iteration 336, train loss: 0.073066, validation loss: \u001b[92m0.070101\u001b[0m\n",
      "iteration 337, train loss: 0.073062, validation loss: \u001b[92m0.070099\u001b[0m\n",
      "iteration 338, train loss: 0.073042, validation loss: \u001b[92m0.070098\u001b[0m\n",
      "iteration 339, train loss: 0.073108, validation loss: \u001b[92m0.070097\u001b[0m\n",
      "iteration 340, train loss: 0.073036, validation loss: \u001b[92m0.070096\u001b[0m\n",
      "iteration 341, train loss: 0.072962, validation loss: \u001b[92m0.070094\u001b[0m\n",
      "iteration 342, train loss: 0.073026, validation loss: \u001b[92m0.070092\u001b[0m\n",
      "iteration 343, train loss: 0.073104, validation loss: \u001b[92m0.070091\u001b[0m\n",
      "iteration 344, train loss: 0.073085, validation loss: \u001b[92m0.070089\u001b[0m\n",
      "iteration 345, train loss: 0.073041, validation loss: \u001b[92m0.070088\u001b[0m\n",
      "iteration 346, train loss: 0.073067, validation loss: \u001b[92m0.070087\u001b[0m\n",
      "iteration 347, train loss: 0.073112, validation loss: \u001b[92m0.070086\u001b[0m\n",
      "iteration 348, train loss: 0.073114, validation loss: \u001b[92m0.070085\u001b[0m\n",
      "iteration 349, train loss: 0.073082, validation loss: \u001b[92m0.070084\u001b[0m\n",
      "iteration 350, train loss: 0.073023, validation loss: \u001b[92m0.070083\u001b[0m\n",
      "iteration 351, train loss: 0.073087, validation loss: \u001b[92m0.070083\u001b[0m\n",
      "iteration 352, train loss: 0.073001, validation loss: \u001b[92m0.070082\u001b[0m\n",
      "iteration 353, train loss: 0.073043, validation loss: \u001b[92m0.070081\u001b[0m\n",
      "iteration 354, train loss: 0.073164, validation loss: \u001b[92m0.070081\u001b[0m\n",
      "iteration 355, train loss: 0.07304, validation loss: \u001b[92m0.070081\u001b[0m\n",
      "iteration 356, train loss: 0.073111, validation loss: \u001b[92m0.07008\u001b[0m\n",
      "iteration 357, train loss: 0.073023, validation loss: \u001b[92m0.07008\u001b[0m\n",
      "iteration 358, train loss: 0.07299, validation loss: \u001b[92m0.070079\u001b[0m\n",
      "iteration 359, train loss: 0.073004, validation loss: \u001b[92m0.070078\u001b[0m\n",
      "iteration 360, train loss: 0.07301, validation loss: \u001b[92m0.070075\u001b[0m\n",
      "iteration 361, train loss: 0.073026, validation loss: \u001b[92m0.070073\u001b[0m\n",
      "iteration 362, train loss: 0.073099, validation loss: \u001b[92m0.07007\u001b[0m\n",
      "iteration 363, train loss: 0.073, validation loss: \u001b[92m0.070067\u001b[0m\n",
      "iteration 364, train loss: 0.072958, validation loss: \u001b[92m0.070065\u001b[0m\n",
      "iteration 365, train loss: 0.073033, validation loss: \u001b[92m0.070062\u001b[0m\n",
      "iteration 366, train loss: 0.073016, validation loss: \u001b[92m0.07006\u001b[0m\n",
      "iteration 367, train loss: 0.072975, validation loss: \u001b[92m0.070056\u001b[0m\n",
      "iteration 368, train loss: 0.073051, validation loss: \u001b[92m0.070053\u001b[0m\n",
      "iteration 369, train loss: 0.073074, validation loss: \u001b[92m0.070051\u001b[0m\n",
      "iteration 370, train loss: 0.072934, validation loss: \u001b[92m0.070048\u001b[0m\n",
      "iteration 371, train loss: 0.073007, validation loss: \u001b[92m0.070044\u001b[0m\n",
      "iteration 372, train loss: 0.072999, validation loss: \u001b[92m0.070042\u001b[0m\n",
      "iteration 373, train loss: 0.073041, validation loss: \u001b[92m0.070039\u001b[0m\n",
      "iteration 374, train loss: 0.073094, validation loss: \u001b[92m0.070035\u001b[0m\n",
      "iteration 375, train loss: 0.072968, validation loss: \u001b[92m0.070032\u001b[0m\n",
      "iteration 376, train loss: 0.072985, validation loss: \u001b[92m0.070029\u001b[0m\n",
      "iteration 377, train loss: 0.072971, validation loss: \u001b[92m0.070026\u001b[0m\n",
      "iteration 378, train loss: 0.073097, validation loss: \u001b[92m0.070023\u001b[0m\n",
      "iteration 379, train loss: 0.073033, validation loss: \u001b[92m0.07002\u001b[0m\n",
      "iteration 380, train loss: 0.072953, validation loss: \u001b[92m0.070015\u001b[0m\n",
      "iteration 381, train loss: 0.073063, validation loss: \u001b[92m0.070012\u001b[0m\n",
      "iteration 382, train loss: 0.073138, validation loss: \u001b[92m0.070008\u001b[0m\n",
      "iteration 383, train loss: 0.072938, validation loss: \u001b[92m0.070004\u001b[0m\n",
      "iteration 384, train loss: 0.073001, validation loss: \u001b[92m0.07\u001b[0m\n",
      "iteration 385, train loss: 0.072943, validation loss: \u001b[92m0.069996\u001b[0m\n",
      "iteration 386, train loss: 0.07308, validation loss: \u001b[92m0.069992\u001b[0m\n",
      "iteration 387, train loss: 0.073002, validation loss: \u001b[92m0.069989\u001b[0m\n",
      "iteration 388, train loss: 0.07298, validation loss: \u001b[92m0.069986\u001b[0m\n",
      "iteration 389, train loss: 0.072943, validation loss: \u001b[92m0.069984\u001b[0m\n",
      "iteration 390, train loss: 0.07304, validation loss: \u001b[92m0.069982\u001b[0m\n",
      "iteration 391, train loss: 0.072983, validation loss: \u001b[92m0.069979\u001b[0m\n",
      "iteration 392, train loss: 0.073057, validation loss: \u001b[92m0.069977\u001b[0m\n",
      "iteration 393, train loss: 0.072944, validation loss: \u001b[92m0.069974\u001b[0m\n",
      "iteration 394, train loss: 0.073099, validation loss: \u001b[92m0.069971\u001b[0m\n",
      "iteration 395, train loss: \u001b[92m0.072827\u001b[0m, validation loss: \u001b[92m0.069968\u001b[0m\n",
      "iteration 396, train loss: 0.072946, validation loss: \u001b[92m0.069965\u001b[0m\n",
      "iteration 397, train loss: 0.073005, validation loss: \u001b[92m0.069961\u001b[0m\n",
      "iteration 398, train loss: 0.07292, validation loss: \u001b[92m0.069957\u001b[0m\n",
      "iteration 399, train loss: 0.072934, validation loss: \u001b[92m0.069953\u001b[0m\n",
      "iteration 400, train loss: 0.072867, validation loss: \u001b[92m0.069949\u001b[0m\n",
      "iteration 401, train loss: 0.073009, validation loss: \u001b[92m0.069945\u001b[0m\n",
      "iteration 402, train loss: \u001b[92m0.072756\u001b[0m, validation loss: \u001b[92m0.06994\u001b[0m\n",
      "iteration 403, train loss: 0.073057, validation loss: \u001b[92m0.069935\u001b[0m\n",
      "iteration 404, train loss: 0.072863, validation loss: \u001b[92m0.06993\u001b[0m\n",
      "iteration 405, train loss: 0.072827, validation loss: \u001b[92m0.069925\u001b[0m\n",
      "iteration 406, train loss: 0.072907, validation loss: \u001b[92m0.06992\u001b[0m\n",
      "iteration 407, train loss: 0.072967, validation loss: \u001b[92m0.069915\u001b[0m\n",
      "iteration 408, train loss: 0.072936, validation loss: \u001b[92m0.06991\u001b[0m\n",
      "iteration 409, train loss: 0.072961, validation loss: \u001b[92m0.069905\u001b[0m\n",
      "iteration 410, train loss: 0.072965, validation loss: \u001b[92m0.0699\u001b[0m\n",
      "iteration 411, train loss: 0.072966, validation loss: \u001b[92m0.069896\u001b[0m\n",
      "iteration 412, train loss: 0.072807, validation loss: \u001b[92m0.069891\u001b[0m\n",
      "iteration 413, train loss: 0.072863, validation loss: \u001b[92m0.069886\u001b[0m\n",
      "iteration 414, train loss: 0.072934, validation loss: \u001b[92m0.06988\u001b[0m\n",
      "iteration 415, train loss: 0.072857, validation loss: \u001b[92m0.069874\u001b[0m\n",
      "iteration 416, train loss: 0.07292, validation loss: \u001b[92m0.069868\u001b[0m\n",
      "iteration 417, train loss: 0.072837, validation loss: \u001b[92m0.069862\u001b[0m\n",
      "iteration 418, train loss: 0.072895, validation loss: \u001b[92m0.069855\u001b[0m\n",
      "iteration 419, train loss: 0.073007, validation loss: \u001b[92m0.069849\u001b[0m\n",
      "iteration 420, train loss: 0.072897, validation loss: \u001b[92m0.069843\u001b[0m\n",
      "iteration 421, train loss: 0.072808, validation loss: \u001b[92m0.069836\u001b[0m\n",
      "iteration 422, train loss: 0.07287, validation loss: \u001b[92m0.06983\u001b[0m\n",
      "iteration 423, train loss: 0.072878, validation loss: \u001b[92m0.069823\u001b[0m\n",
      "iteration 424, train loss: 0.072922, validation loss: \u001b[92m0.069817\u001b[0m\n",
      "iteration 425, train loss: 0.072842, validation loss: \u001b[92m0.06981\u001b[0m\n",
      "iteration 426, train loss: 0.072932, validation loss: \u001b[92m0.069804\u001b[0m\n",
      "iteration 427, train loss: \u001b[92m0.072738\u001b[0m, validation loss: \u001b[92m0.069797\u001b[0m\n",
      "iteration 428, train loss: 0.072908, validation loss: \u001b[92m0.06979\u001b[0m\n",
      "iteration 429, train loss: 0.072771, validation loss: \u001b[92m0.069783\u001b[0m\n",
      "iteration 430, train loss: 0.072854, validation loss: \u001b[92m0.069775\u001b[0m\n",
      "iteration 431, train loss: 0.07282, validation loss: \u001b[92m0.069768\u001b[0m\n",
      "iteration 432, train loss: 0.072837, validation loss: \u001b[92m0.069762\u001b[0m\n",
      "iteration 433, train loss: 0.072875, validation loss: \u001b[92m0.069754\u001b[0m\n",
      "iteration 434, train loss: \u001b[92m0.072696\u001b[0m, validation loss: \u001b[92m0.069746\u001b[0m\n",
      "iteration 435, train loss: \u001b[92m0.072649\u001b[0m, validation loss: \u001b[92m0.069737\u001b[0m\n",
      "iteration 436, train loss: 0.0728, validation loss: \u001b[92m0.069728\u001b[0m\n",
      "iteration 437, train loss: 0.072864, validation loss: \u001b[92m0.069719\u001b[0m\n",
      "iteration 438, train loss: 0.07267, validation loss: \u001b[92m0.069709\u001b[0m\n",
      "iteration 439, train loss: 0.072768, validation loss: \u001b[92m0.069699\u001b[0m\n",
      "iteration 440, train loss: 0.072711, validation loss: \u001b[92m0.069688\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 441, train loss: 0.07278, validation loss: \u001b[92m0.069677\u001b[0m\n",
      "iteration 442, train loss: 0.07281, validation loss: \u001b[92m0.069668\u001b[0m\n",
      "iteration 443, train loss: \u001b[92m0.072576\u001b[0m, validation loss: \u001b[92m0.069657\u001b[0m\n",
      "iteration 444, train loss: 0.072725, validation loss: \u001b[92m0.069647\u001b[0m\n",
      "iteration 445, train loss: \u001b[92m0.07256\u001b[0m, validation loss: \u001b[92m0.069636\u001b[0m\n",
      "iteration 446, train loss: 0.072597, validation loss: \u001b[92m0.069624\u001b[0m\n",
      "iteration 447, train loss: 0.072686, validation loss: \u001b[92m0.069613\u001b[0m\n",
      "iteration 448, train loss: 0.072605, validation loss: \u001b[92m0.0696\u001b[0m\n",
      "iteration 449, train loss: 0.072641, validation loss: \u001b[92m0.069588\u001b[0m\n",
      "iteration 450, train loss: 0.072694, validation loss: \u001b[92m0.069576\u001b[0m\n",
      "iteration 451, train loss: 0.072659, validation loss: \u001b[92m0.069563\u001b[0m\n",
      "iteration 452, train loss: \u001b[92m0.072549\u001b[0m, validation loss: \u001b[92m0.06955\u001b[0m\n",
      "iteration 453, train loss: 0.072634, validation loss: \u001b[92m0.069537\u001b[0m\n",
      "iteration 454, train loss: 0.07262, validation loss: \u001b[92m0.069524\u001b[0m\n",
      "iteration 455, train loss: 0.072736, validation loss: \u001b[92m0.069512\u001b[0m\n",
      "iteration 456, train loss: 0.072559, validation loss: \u001b[92m0.069499\u001b[0m\n",
      "iteration 457, train loss: 0.072639, validation loss: \u001b[92m0.069486\u001b[0m\n",
      "iteration 458, train loss: 0.072587, validation loss: \u001b[92m0.069472\u001b[0m\n",
      "iteration 459, train loss: \u001b[92m0.072536\u001b[0m, validation loss: \u001b[92m0.069459\u001b[0m\n",
      "iteration 460, train loss: 0.072686, validation loss: \u001b[92m0.069447\u001b[0m\n",
      "iteration 461, train loss: 0.072584, validation loss: \u001b[92m0.069434\u001b[0m\n",
      "iteration 462, train loss: 0.07258, validation loss: \u001b[92m0.069421\u001b[0m\n",
      "iteration 463, train loss: 0.072628, validation loss: \u001b[92m0.06941\u001b[0m\n",
      "iteration 464, train loss: \u001b[92m0.072501\u001b[0m, validation loss: \u001b[92m0.069397\u001b[0m\n",
      "iteration 465, train loss: 0.072569, validation loss: \u001b[92m0.069384\u001b[0m\n",
      "iteration 466, train loss: 0.072711, validation loss: \u001b[92m0.069371\u001b[0m\n",
      "iteration 467, train loss: 0.072537, validation loss: \u001b[92m0.069358\u001b[0m\n",
      "iteration 468, train loss: 0.072563, validation loss: \u001b[92m0.069343\u001b[0m\n",
      "iteration 469, train loss: 0.072575, validation loss: \u001b[92m0.06933\u001b[0m\n",
      "iteration 470, train loss: \u001b[92m0.072456\u001b[0m, validation loss: \u001b[92m0.069315\u001b[0m\n",
      "iteration 471, train loss: 0.072573, validation loss: \u001b[92m0.069301\u001b[0m\n",
      "iteration 472, train loss: \u001b[92m0.072414\u001b[0m, validation loss: \u001b[92m0.069287\u001b[0m\n",
      "iteration 473, train loss: 0.072452, validation loss: \u001b[92m0.069272\u001b[0m\n",
      "iteration 474, train loss: \u001b[92m0.072244\u001b[0m, validation loss: \u001b[92m0.069255\u001b[0m\n",
      "iteration 475, train loss: 0.072475, validation loss: \u001b[92m0.069238\u001b[0m\n",
      "iteration 476, train loss: 0.072423, validation loss: \u001b[92m0.06922\u001b[0m\n",
      "iteration 477, train loss: \u001b[92m0.072231\u001b[0m, validation loss: \u001b[92m0.069201\u001b[0m\n",
      "iteration 478, train loss: \u001b[92m0.072195\u001b[0m, validation loss: \u001b[92m0.069181\u001b[0m\n",
      "iteration 479, train loss: 0.072452, validation loss: \u001b[92m0.069161\u001b[0m\n",
      "iteration 480, train loss: 0.072391, validation loss: \u001b[92m0.06914\u001b[0m\n",
      "iteration 481, train loss: 0.072313, validation loss: \u001b[92m0.069119\u001b[0m\n",
      "iteration 482, train loss: 0.072467, validation loss: \u001b[92m0.069098\u001b[0m\n",
      "iteration 483, train loss: 0.072389, validation loss: \u001b[92m0.069077\u001b[0m\n",
      "iteration 484, train loss: 0.072265, validation loss: \u001b[92m0.069054\u001b[0m\n",
      "iteration 485, train loss: 0.072357, validation loss: \u001b[92m0.069032\u001b[0m\n",
      "iteration 486, train loss: 0.072266, validation loss: \u001b[92m0.06901\u001b[0m\n",
      "iteration 487, train loss: 0.072285, validation loss: \u001b[92m0.068986\u001b[0m\n",
      "iteration 488, train loss: \u001b[92m0.072118\u001b[0m, validation loss: \u001b[92m0.068961\u001b[0m\n",
      "iteration 489, train loss: \u001b[92m0.072115\u001b[0m, validation loss: \u001b[92m0.068935\u001b[0m\n",
      "iteration 490, train loss: \u001b[92m0.071939\u001b[0m, validation loss: \u001b[92m0.068907\u001b[0m\n",
      "iteration 491, train loss: 0.072216, validation loss: \u001b[92m0.068879\u001b[0m\n",
      "iteration 492, train loss: 0.072253, validation loss: \u001b[92m0.068852\u001b[0m\n",
      "iteration 493, train loss: 0.072344, validation loss: \u001b[92m0.068826\u001b[0m\n",
      "iteration 494, train loss: 0.072211, validation loss: \u001b[92m0.0688\u001b[0m\n",
      "iteration 495, train loss: 0.071967, validation loss: \u001b[92m0.068773\u001b[0m\n",
      "iteration 496, train loss: 0.07241, validation loss: \u001b[92m0.068747\u001b[0m\n",
      "iteration 497, train loss: 0.072062, validation loss: \u001b[92m0.068721\u001b[0m\n",
      "iteration 498, train loss: 0.072087, validation loss: \u001b[92m0.068696\u001b[0m\n",
      "iteration 499, train loss: 0.072049, validation loss: \u001b[92m0.068669\u001b[0m\n",
      "iteration 500, train loss: 0.072198, validation loss: \u001b[92m0.068642\u001b[0m\n",
      "iteration 501, train loss: 0.07205, validation loss: \u001b[92m0.068616\u001b[0m\n",
      "iteration 502, train loss: 0.072089, validation loss: \u001b[92m0.068589\u001b[0m\n",
      "iteration 503, train loss: 0.071976, validation loss: \u001b[92m0.068562\u001b[0m\n",
      "iteration 504, train loss: \u001b[92m0.071708\u001b[0m, validation loss: \u001b[92m0.068533\u001b[0m\n",
      "iteration 505, train loss: 0.072019, validation loss: \u001b[92m0.068503\u001b[0m\n",
      "iteration 506, train loss: 0.071979, validation loss: \u001b[92m0.068472\u001b[0m\n",
      "iteration 507, train loss: 0.071995, validation loss: \u001b[92m0.068442\u001b[0m\n",
      "iteration 508, train loss: 0.071924, validation loss: \u001b[92m0.068411\u001b[0m\n",
      "iteration 509, train loss: 0.071908, validation loss: \u001b[92m0.068379\u001b[0m\n",
      "iteration 510, train loss: 0.071852, validation loss: \u001b[92m0.068349\u001b[0m\n",
      "iteration 511, train loss: 0.071775, validation loss: \u001b[92m0.068316\u001b[0m\n",
      "iteration 512, train loss: 0.071771, validation loss: \u001b[92m0.068283\u001b[0m\n",
      "iteration 513, train loss: \u001b[92m0.071684\u001b[0m, validation loss: \u001b[92m0.068249\u001b[0m\n",
      "iteration 514, train loss: \u001b[92m0.071637\u001b[0m, validation loss: \u001b[92m0.068213\u001b[0m\n",
      "iteration 515, train loss: 0.071657, validation loss: \u001b[92m0.068175\u001b[0m\n",
      "iteration 516, train loss: 0.071859, validation loss: \u001b[92m0.068139\u001b[0m\n",
      "iteration 517, train loss: 0.071671, validation loss: \u001b[92m0.068101\u001b[0m\n",
      "iteration 518, train loss: 0.071729, validation loss: \u001b[92m0.068064\u001b[0m\n",
      "iteration 519, train loss: \u001b[92m0.071578\u001b[0m, validation loss: \u001b[92m0.068027\u001b[0m\n",
      "iteration 520, train loss: 0.071621, validation loss: \u001b[92m0.06799\u001b[0m\n",
      "iteration 521, train loss: 0.071634, validation loss: \u001b[92m0.067952\u001b[0m\n",
      "iteration 522, train loss: \u001b[92m0.071414\u001b[0m, validation loss: \u001b[92m0.067914\u001b[0m\n",
      "iteration 523, train loss: 0.071541, validation loss: \u001b[92m0.067877\u001b[0m\n",
      "iteration 524, train loss: \u001b[92m0.071351\u001b[0m, validation loss: \u001b[92m0.067839\u001b[0m\n",
      "iteration 525, train loss: 0.071576, validation loss: \u001b[92m0.067804\u001b[0m\n",
      "iteration 526, train loss: 0.071406, validation loss: \u001b[92m0.067769\u001b[0m\n",
      "iteration 527, train loss: 0.071368, validation loss: \u001b[92m0.067733\u001b[0m\n",
      "iteration 528, train loss: \u001b[92m0.071336\u001b[0m, validation loss: \u001b[92m0.067697\u001b[0m\n",
      "iteration 529, train loss: \u001b[92m0.07128\u001b[0m, validation loss: \u001b[92m0.067658\u001b[0m\n",
      "iteration 530, train loss: \u001b[92m0.071125\u001b[0m, validation loss: \u001b[92m0.06762\u001b[0m\n",
      "iteration 531, train loss: 0.071152, validation loss: \u001b[92m0.06758\u001b[0m\n",
      "iteration 532, train loss: 0.071186, validation loss: \u001b[92m0.067542\u001b[0m\n",
      "iteration 533, train loss: 0.071359, validation loss: \u001b[92m0.067506\u001b[0m\n",
      "iteration 534, train loss: \u001b[92m0.071114\u001b[0m, validation loss: \u001b[92m0.067468\u001b[0m\n",
      "iteration 535, train loss: 0.071158, validation loss: \u001b[92m0.06743\u001b[0m\n",
      "iteration 536, train loss: 0.071247, validation loss: \u001b[92m0.067392\u001b[0m\n",
      "iteration 537, train loss: 0.071157, validation loss: \u001b[92m0.067357\u001b[0m\n",
      "iteration 538, train loss: \u001b[92m0.071034\u001b[0m, validation loss: \u001b[92m0.067321\u001b[0m\n",
      "iteration 539, train loss: \u001b[92m0.070925\u001b[0m, validation loss: \u001b[92m0.067284\u001b[0m\n",
      "iteration 540, train loss: 0.071024, validation loss: \u001b[92m0.067248\u001b[0m\n",
      "iteration 541, train loss: 0.071021, validation loss: \u001b[92m0.067213\u001b[0m\n",
      "iteration 542, train loss: \u001b[92m0.0708\u001b[0m, validation loss: \u001b[92m0.067177\u001b[0m\n",
      "iteration 543, train loss: 0.070951, validation loss: \u001b[92m0.067143\u001b[0m\n",
      "iteration 544, train loss: \u001b[92m0.070735\u001b[0m, validation loss: \u001b[92m0.067108\u001b[0m\n",
      "iteration 545, train loss: 0.07081, validation loss: \u001b[92m0.067074\u001b[0m\n",
      "iteration 546, train loss: 0.070871, validation loss: \u001b[92m0.067041\u001b[0m\n",
      "iteration 547, train loss: 0.070786, validation loss: \u001b[92m0.06701\u001b[0m\n",
      "iteration 548, train loss: 0.070998, validation loss: \u001b[92m0.066982\u001b[0m\n",
      "iteration 549, train loss: 0.070757, validation loss: \u001b[92m0.066953\u001b[0m\n",
      "iteration 550, train loss: 0.070894, validation loss: \u001b[92m0.066927\u001b[0m\n",
      "iteration 551, train loss: \u001b[92m0.07072\u001b[0m, validation loss: \u001b[92m0.0669\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 552, train loss: \u001b[92m0.070635\u001b[0m, validation loss: \u001b[92m0.066873\u001b[0m\n",
      "iteration 553, train loss: 0.070804, validation loss: \u001b[92m0.066844\u001b[0m\n",
      "iteration 554, train loss: 0.070798, validation loss: \u001b[92m0.066814\u001b[0m\n",
      "iteration 555, train loss: 0.070766, validation loss: \u001b[92m0.06679\u001b[0m\n",
      "iteration 556, train loss: \u001b[92m0.070515\u001b[0m, validation loss: \u001b[92m0.066752\u001b[0m\n",
      "iteration 557, train loss: 0.070528, validation loss: \u001b[92m0.066727\u001b[0m\n",
      "iteration 558, train loss: 0.070573, validation loss: \u001b[92m0.066701\u001b[0m\n",
      "iteration 559, train loss: 0.070573, validation loss: \u001b[92m0.066675\u001b[0m\n",
      "iteration 560, train loss: \u001b[92m0.070376\u001b[0m, validation loss: \u001b[92m0.066647\u001b[0m\n",
      "iteration 561, train loss: 0.070615, validation loss: \u001b[92m0.066617\u001b[0m\n",
      "iteration 562, train loss: 0.070624, validation loss: \u001b[92m0.066586\u001b[0m\n",
      "iteration 563, train loss: 0.070481, validation loss: \u001b[92m0.06656\u001b[0m\n",
      "iteration 564, train loss: 0.070495, validation loss: \u001b[92m0.06654\u001b[0m\n",
      "iteration 565, train loss: 0.07039, validation loss: \u001b[92m0.06652\u001b[0m\n",
      "iteration 566, train loss: \u001b[92m0.070342\u001b[0m, validation loss: \u001b[92m0.0665\u001b[0m\n",
      "iteration 567, train loss: 0.070681, validation loss: \u001b[92m0.066477\u001b[0m\n",
      "iteration 568, train loss: 0.070642, validation loss: \u001b[92m0.066456\u001b[0m\n",
      "iteration 569, train loss: 0.070603, validation loss: \u001b[92m0.066435\u001b[0m\n",
      "iteration 570, train loss: 0.070763, validation loss: \u001b[92m0.066415\u001b[0m\n",
      "iteration 571, train loss: 0.070644, validation loss: \u001b[92m0.066396\u001b[0m\n",
      "iteration 572, train loss: 0.070592, validation loss: \u001b[92m0.066381\u001b[0m\n",
      "iteration 573, train loss: 0.070594, validation loss: \u001b[92m0.066364\u001b[0m\n",
      "iteration 574, train loss: 0.070549, validation loss: \u001b[92m0.066347\u001b[0m\n",
      "iteration 575, train loss: \u001b[92m0.070066\u001b[0m, validation loss: \u001b[92m0.066329\u001b[0m\n",
      "iteration 576, train loss: 0.070379, validation loss: \u001b[92m0.066313\u001b[0m\n",
      "iteration 577, train loss: 0.070244, validation loss: \u001b[92m0.066299\u001b[0m\n",
      "iteration 578, train loss: 0.070498, validation loss: \u001b[92m0.066284\u001b[0m\n",
      "iteration 579, train loss: 0.0704, validation loss: \u001b[92m0.06627\u001b[0m\n",
      "iteration 580, train loss: 0.070171, validation loss: \u001b[92m0.066253\u001b[0m\n",
      "iteration 581, train loss: \u001b[92m0.069921\u001b[0m, validation loss: \u001b[92m0.066235\u001b[0m\n",
      "iteration 582, train loss: 0.070407, validation loss: \u001b[92m0.066208\u001b[0m\n",
      "iteration 583, train loss: 0.070569, validation loss: \u001b[92m0.06618\u001b[0m\n",
      "iteration 584, train loss: 0.070272, validation loss: \u001b[92m0.066149\u001b[0m\n",
      "iteration 585, train loss: 0.070076, validation loss: \u001b[92m0.066118\u001b[0m\n",
      "iteration 586, train loss: 0.070136, validation loss: \u001b[92m0.066099\u001b[0m\n",
      "iteration 587, train loss: 0.070212, validation loss: \u001b[92m0.066088\u001b[0m\n",
      "iteration 588, train loss: 0.070396, validation loss: \u001b[92m0.066073\u001b[0m\n",
      "iteration 589, train loss: 0.070275, validation loss: \u001b[92m0.066057\u001b[0m\n",
      "iteration 590, train loss: 0.070113, validation loss: \u001b[92m0.066053\u001b[0m\n",
      "iteration 591, train loss: 0.070394, validation loss: 0.066059\n",
      "iteration 592, train loss: 0.070375, validation loss: 0.066074\n",
      "iteration 593, train loss: 0.070075, validation loss: 0.066092\n",
      "iteration 594, train loss: 0.070347, validation loss: 0.066089\n",
      "iteration 595, train loss: 0.070223, validation loss: 0.066073\n",
      "iteration 596, train loss: 0.070173, validation loss: \u001b[92m0.066027\u001b[0m\n",
      "iteration 597, train loss: 0.070199, validation loss: \u001b[92m0.065993\u001b[0m\n",
      "iteration 598, train loss: 0.070205, validation loss: \u001b[92m0.065968\u001b[0m\n",
      "iteration 599, train loss: 0.070194, validation loss: \u001b[92m0.065956\u001b[0m\n",
      "iteration 600, train loss: 0.069934, validation loss: \u001b[92m0.065944\u001b[0m\n",
      "iteration 601, train loss: 0.070312, validation loss: \u001b[92m0.065927\u001b[0m\n",
      "iteration 602, train loss: 0.070036, validation loss: \u001b[92m0.065906\u001b[0m\n",
      "iteration 603, train loss: 0.070156, validation loss: \u001b[92m0.065889\u001b[0m\n",
      "iteration 604, train loss: 0.070108, validation loss: \u001b[92m0.065888\u001b[0m\n",
      "iteration 605, train loss: 0.070212, validation loss: 0.065898\n",
      "iteration 606, train loss: 0.070006, validation loss: 0.06591\n",
      "iteration 607, train loss: 0.070212, validation loss: 0.065915\n",
      "iteration 608, train loss: \u001b[92m0.069873\u001b[0m, validation loss: 0.065898\n",
      "iteration 609, train loss: 0.069964, validation loss: \u001b[92m0.065875\u001b[0m\n",
      "iteration 610, train loss: 0.070068, validation loss: \u001b[92m0.065851\u001b[0m\n",
      "iteration 611, train loss: \u001b[92m0.069669\u001b[0m, validation loss: \u001b[92m0.06583\u001b[0m\n",
      "iteration 612, train loss: 0.06999, validation loss: \u001b[92m0.065815\u001b[0m\n",
      "iteration 613, train loss: 0.069879, validation loss: \u001b[92m0.065803\u001b[0m\n",
      "iteration 614, train loss: 0.070113, validation loss: \u001b[92m0.065794\u001b[0m\n",
      "iteration 615, train loss: \u001b[92m0.069615\u001b[0m, validation loss: \u001b[92m0.065783\u001b[0m\n",
      "iteration 616, train loss: 0.070112, validation loss: \u001b[92m0.065773\u001b[0m\n",
      "iteration 617, train loss: 0.070068, validation loss: \u001b[92m0.065769\u001b[0m\n",
      "iteration 618, train loss: 0.069825, validation loss: 0.065771\n",
      "iteration 619, train loss: 0.069798, validation loss: 0.065772\n",
      "iteration 620, train loss: 0.069828, validation loss: 0.065777\n",
      "iteration 621, train loss: 0.069823, validation loss: 0.065776\n",
      "iteration 622, train loss: 0.069829, validation loss: \u001b[92m0.065757\u001b[0m\n",
      "iteration 623, train loss: 0.070135, validation loss: \u001b[92m0.065729\u001b[0m\n",
      "iteration 624, train loss: 0.070005, validation loss: \u001b[92m0.065702\u001b[0m\n",
      "iteration 625, train loss: 0.069826, validation loss: \u001b[92m0.065681\u001b[0m\n",
      "iteration 626, train loss: 0.069966, validation loss: \u001b[92m0.065664\u001b[0m\n",
      "iteration 627, train loss: 0.069844, validation loss: \u001b[92m0.065652\u001b[0m\n",
      "iteration 628, train loss: 0.069707, validation loss: \u001b[92m0.065641\u001b[0m\n",
      "iteration 629, train loss: 0.069685, validation loss: \u001b[92m0.06563\u001b[0m\n",
      "iteration 630, train loss: 0.069777, validation loss: \u001b[92m0.065617\u001b[0m\n",
      "iteration 631, train loss: 0.069844, validation loss: \u001b[92m0.0656\u001b[0m\n",
      "iteration 632, train loss: 0.069764, validation loss: \u001b[92m0.065582\u001b[0m\n",
      "iteration 633, train loss: 0.069786, validation loss: \u001b[92m0.065566\u001b[0m\n",
      "iteration 634, train loss: 0.069991, validation loss: \u001b[92m0.065555\u001b[0m\n",
      "iteration 635, train loss: 0.06977, validation loss: \u001b[92m0.065544\u001b[0m\n",
      "iteration 636, train loss: 0.069953, validation loss: \u001b[92m0.065534\u001b[0m\n",
      "iteration 637, train loss: 0.069692, validation loss: \u001b[92m0.065527\u001b[0m\n",
      "iteration 638, train loss: \u001b[92m0.069418\u001b[0m, validation loss: \u001b[92m0.06552\u001b[0m\n",
      "iteration 639, train loss: \u001b[92m0.069286\u001b[0m, validation loss: \u001b[92m0.065513\u001b[0m\n",
      "iteration 640, train loss: 0.069413, validation loss: \u001b[92m0.065499\u001b[0m\n",
      "iteration 641, train loss: 0.0698, validation loss: \u001b[92m0.065491\u001b[0m\n",
      "iteration 642, train loss: 0.069729, validation loss: \u001b[92m0.065473\u001b[0m\n",
      "iteration 643, train loss: 0.069877, validation loss: \u001b[92m0.065471\u001b[0m\n",
      "iteration 644, train loss: 0.069904, validation loss: \u001b[92m0.065466\u001b[0m\n",
      "iteration 645, train loss: 0.069915, validation loss: \u001b[92m0.065459\u001b[0m\n",
      "iteration 646, train loss: \u001b[92m0.069197\u001b[0m, validation loss: \u001b[92m0.065453\u001b[0m\n",
      "iteration 647, train loss: 0.069693, validation loss: \u001b[92m0.065452\u001b[0m\n",
      "iteration 648, train loss: 0.069408, validation loss: 0.065457\n",
      "iteration 649, train loss: 0.06976, validation loss: 0.065456\n",
      "iteration 650, train loss: 0.069855, validation loss: 0.065455\n",
      "iteration 651, train loss: 0.069702, validation loss: \u001b[92m0.065442\u001b[0m\n",
      "iteration 652, train loss: 0.069553, validation loss: \u001b[92m0.065416\u001b[0m\n",
      "iteration 653, train loss: 0.069909, validation loss: \u001b[92m0.065383\u001b[0m\n",
      "iteration 654, train loss: 0.069928, validation loss: \u001b[92m0.065351\u001b[0m\n",
      "iteration 655, train loss: 0.069629, validation loss: \u001b[92m0.065333\u001b[0m\n",
      "iteration 656, train loss: 0.069698, validation loss: \u001b[92m0.06532\u001b[0m\n",
      "iteration 657, train loss: 0.069304, validation loss: \u001b[92m0.065307\u001b[0m\n",
      "iteration 658, train loss: 0.06956, validation loss: \u001b[92m0.065298\u001b[0m\n",
      "iteration 659, train loss: 0.069655, validation loss: \u001b[92m0.065294\u001b[0m\n",
      "iteration 660, train loss: 0.06963, validation loss: \u001b[92m0.065294\u001b[0m\n",
      "iteration 661, train loss: 0.069727, validation loss: \u001b[92m0.065291\u001b[0m\n",
      "iteration 662, train loss: 0.069629, validation loss: 0.065294\n",
      "iteration 663, train loss: 0.069419, validation loss: 0.065297\n",
      "iteration 664, train loss: 0.069744, validation loss: 0.065303\n",
      "iteration 665, train loss: 0.069436, validation loss: 0.065305\n",
      "iteration 666, train loss: 0.070006, validation loss: 0.065307\n",
      "iteration 667, train loss: 0.069462, validation loss: 0.065298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 668, train loss: 0.069289, validation loss: \u001b[92m0.065287\u001b[0m\n",
      "iteration 669, train loss: 0.069204, validation loss: \u001b[92m0.065277\u001b[0m\n",
      "iteration 670, train loss: 0.06986, validation loss: \u001b[92m0.065259\u001b[0m\n",
      "iteration 671, train loss: 0.069623, validation loss: \u001b[92m0.065224\u001b[0m\n",
      "iteration 672, train loss: 0.069705, validation loss: \u001b[92m0.065203\u001b[0m\n",
      "iteration 673, train loss: 0.069485, validation loss: \u001b[92m0.065187\u001b[0m\n",
      "iteration 674, train loss: 0.069366, validation loss: \u001b[92m0.065173\u001b[0m\n",
      "iteration 675, train loss: \u001b[92m0.069146\u001b[0m, validation loss: \u001b[92m0.06516\u001b[0m\n",
      "iteration 676, train loss: 0.069686, validation loss: \u001b[92m0.065149\u001b[0m\n",
      "iteration 677, train loss: 0.069828, validation loss: \u001b[92m0.065145\u001b[0m\n",
      "iteration 678, train loss: 0.069637, validation loss: 0.065152\n",
      "iteration 679, train loss: 0.069643, validation loss: 0.065163\n",
      "iteration 680, train loss: 0.069349, validation loss: 0.065177\n",
      "iteration 681, train loss: 0.069483, validation loss: 0.065183\n",
      "iteration 682, train loss: 0.069147, validation loss: 0.065168\n",
      "iteration 683, train loss: 0.069407, validation loss: 0.065148\n",
      "iteration 684, train loss: 0.069438, validation loss: \u001b[92m0.06513\u001b[0m\n",
      "iteration 685, train loss: 0.069309, validation loss: \u001b[92m0.065108\u001b[0m\n",
      "iteration 686, train loss: 0.069355, validation loss: \u001b[92m0.065093\u001b[0m\n",
      "iteration 687, train loss: 0.069497, validation loss: \u001b[92m0.065079\u001b[0m\n",
      "iteration 688, train loss: 0.069833, validation loss: \u001b[92m0.065066\u001b[0m\n",
      "iteration 689, train loss: 0.069194, validation loss: \u001b[92m0.06506\u001b[0m\n",
      "iteration 690, train loss: 0.069492, validation loss: \u001b[92m0.065048\u001b[0m\n",
      "iteration 691, train loss: 0.069591, validation loss: \u001b[92m0.06504\u001b[0m\n",
      "iteration 692, train loss: 0.069279, validation loss: \u001b[92m0.065034\u001b[0m\n",
      "iteration 693, train loss: 0.06939, validation loss: \u001b[92m0.06503\u001b[0m\n",
      "iteration 694, train loss: 0.069479, validation loss: \u001b[92m0.065026\u001b[0m\n",
      "iteration 695, train loss: 0.069176, validation loss: \u001b[92m0.065022\u001b[0m\n",
      "iteration 696, train loss: 0.069508, validation loss: \u001b[92m0.065017\u001b[0m\n",
      "iteration 697, train loss: 0.069589, validation loss: \u001b[92m0.065003\u001b[0m\n",
      "iteration 698, train loss: 0.069591, validation loss: \u001b[92m0.064988\u001b[0m\n",
      "iteration 699, train loss: \u001b[92m0.069124\u001b[0m, validation loss: \u001b[92m0.064968\u001b[0m\n",
      "iteration 700, train loss: 0.069362, validation loss: \u001b[92m0.064944\u001b[0m\n",
      "iteration 701, train loss: \u001b[92m0.069055\u001b[0m, validation loss: \u001b[92m0.064926\u001b[0m\n",
      "iteration 702, train loss: 0.069298, validation loss: \u001b[92m0.064909\u001b[0m\n",
      "iteration 703, train loss: \u001b[92m0.068903\u001b[0m, validation loss: \u001b[92m0.064885\u001b[0m\n",
      "iteration 704, train loss: 0.069188, validation loss: \u001b[92m0.064866\u001b[0m\n",
      "iteration 705, train loss: 0.069336, validation loss: \u001b[92m0.064854\u001b[0m\n",
      "iteration 706, train loss: 0.069181, validation loss: \u001b[92m0.064828\u001b[0m\n",
      "iteration 707, train loss: \u001b[92m0.068836\u001b[0m, validation loss: \u001b[92m0.064818\u001b[0m\n",
      "iteration 708, train loss: 0.06922, validation loss: 0.064825\n",
      "iteration 709, train loss: 0.069442, validation loss: 0.064826\n",
      "iteration 710, train loss: 0.069135, validation loss: 0.064822\n",
      "iteration 711, train loss: 0.069318, validation loss: \u001b[92m0.064814\u001b[0m\n",
      "iteration 712, train loss: 0.069294, validation loss: \u001b[92m0.064812\u001b[0m\n",
      "iteration 713, train loss: 0.06937, validation loss: \u001b[92m0.064804\u001b[0m\n",
      "iteration 714, train loss: 0.069258, validation loss: \u001b[92m0.064792\u001b[0m\n",
      "iteration 715, train loss: 0.069233, validation loss: \u001b[92m0.064772\u001b[0m\n",
      "iteration 716, train loss: 0.069286, validation loss: \u001b[92m0.06475\u001b[0m\n",
      "iteration 717, train loss: 0.069479, validation loss: \u001b[92m0.064734\u001b[0m\n",
      "iteration 718, train loss: 0.069084, validation loss: \u001b[92m0.064723\u001b[0m\n",
      "iteration 719, train loss: 0.069359, validation loss: \u001b[92m0.064722\u001b[0m\n",
      "iteration 720, train loss: 0.069423, validation loss: 0.06473\n",
      "iteration 721, train loss: 0.069072, validation loss: 0.064739\n",
      "iteration 722, train loss: 0.069233, validation loss: 0.06475\n",
      "iteration 723, train loss: \u001b[92m0.068772\u001b[0m, validation loss: 0.064746\n",
      "iteration 724, train loss: 0.069021, validation loss: \u001b[92m0.06472\u001b[0m\n",
      "iteration 725, train loss: 0.06908, validation loss: \u001b[92m0.064674\u001b[0m\n",
      "iteration 726, train loss: 0.069323, validation loss: \u001b[92m0.064623\u001b[0m\n",
      "iteration 727, train loss: 0.069064, validation loss: \u001b[92m0.064584\u001b[0m\n",
      "iteration 728, train loss: 0.069375, validation loss: \u001b[92m0.064562\u001b[0m\n",
      "iteration 729, train loss: 0.069165, validation loss: \u001b[92m0.064538\u001b[0m\n",
      "iteration 730, train loss: 0.069132, validation loss: \u001b[92m0.064512\u001b[0m\n",
      "iteration 731, train loss: 0.069371, validation loss: \u001b[92m0.064507\u001b[0m\n",
      "iteration 732, train loss: 0.069043, validation loss: 0.064515\n",
      "iteration 733, train loss: 0.069272, validation loss: 0.06453\n",
      "iteration 734, train loss: 0.068904, validation loss: 0.06455\n",
      "iteration 735, train loss: 0.069274, validation loss: 0.064567\n",
      "iteration 736, train loss: 0.069039, validation loss: 0.064567\n",
      "iteration 737, train loss: 0.069332, validation loss: 0.064546\n",
      "iteration 738, train loss: 0.069077, validation loss: 0.064524\n",
      "iteration 739, train loss: 0.069193, validation loss: 0.06451\n",
      "iteration 740, train loss: 0.069021, validation loss: 0.064509\n",
      "iteration 741, train loss: \u001b[92m0.068765\u001b[0m, validation loss: \u001b[92m0.064487\u001b[0m\n",
      "iteration 742, train loss: \u001b[92m0.068675\u001b[0m, validation loss: \u001b[92m0.064459\u001b[0m\n",
      "iteration 743, train loss: 0.068978, validation loss: \u001b[92m0.064438\u001b[0m\n",
      "iteration 744, train loss: \u001b[92m0.06864\u001b[0m, validation loss: \u001b[92m0.064426\u001b[0m\n",
      "iteration 745, train loss: 0.068921, validation loss: \u001b[92m0.064423\u001b[0m\n",
      "iteration 746, train loss: 0.068759, validation loss: 0.064424\n",
      "iteration 747, train loss: 0.068906, validation loss: \u001b[92m0.064412\u001b[0m\n",
      "iteration 748, train loss: 0.068741, validation loss: \u001b[92m0.064395\u001b[0m\n",
      "iteration 749, train loss: 0.068819, validation loss: \u001b[92m0.064386\u001b[0m\n",
      "iteration 750, train loss: 0.06909, validation loss: \u001b[92m0.064369\u001b[0m\n",
      "iteration 751, train loss: 0.068784, validation loss: \u001b[92m0.064329\u001b[0m\n",
      "iteration 752, train loss: 0.069244, validation loss: \u001b[92m0.06429\u001b[0m\n",
      "iteration 753, train loss: 0.069086, validation loss: \u001b[92m0.064255\u001b[0m\n",
      "iteration 754, train loss: 0.069003, validation loss: \u001b[92m0.06423\u001b[0m\n",
      "iteration 755, train loss: \u001b[92m0.068579\u001b[0m, validation loss: \u001b[92m0.064228\u001b[0m\n",
      "iteration 756, train loss: 0.068628, validation loss: 0.06423\n",
      "iteration 757, train loss: 0.068926, validation loss: 0.064232\n",
      "iteration 758, train loss: 0.068712, validation loss: \u001b[92m0.064203\u001b[0m\n",
      "iteration 759, train loss: 0.068769, validation loss: 0.064203\n",
      "iteration 760, train loss: 0.068814, validation loss: 0.064224\n",
      "iteration 761, train loss: 0.068827, validation loss: 0.064229\n",
      "iteration 762, train loss: 0.068954, validation loss: 0.064218\n",
      "iteration 763, train loss: 0.069075, validation loss: \u001b[92m0.064192\u001b[0m\n",
      "iteration 764, train loss: 0.068797, validation loss: \u001b[92m0.064165\u001b[0m\n",
      "iteration 765, train loss: 0.068767, validation loss: \u001b[92m0.06414\u001b[0m\n",
      "iteration 766, train loss: 0.069112, validation loss: \u001b[92m0.064121\u001b[0m\n",
      "iteration 767, train loss: 0.069046, validation loss: \u001b[92m0.064103\u001b[0m\n",
      "iteration 768, train loss: 0.068944, validation loss: \u001b[92m0.064087\u001b[0m\n",
      "iteration 769, train loss: 0.068721, validation loss: \u001b[92m0.064075\u001b[0m\n",
      "iteration 770, train loss: 0.068937, validation loss: \u001b[92m0.064074\u001b[0m\n",
      "iteration 771, train loss: 0.068834, validation loss: 0.064078\n",
      "iteration 772, train loss: 0.069104, validation loss: 0.064089\n",
      "iteration 773, train loss: 0.068765, validation loss: 0.064079\n",
      "iteration 774, train loss: 0.068831, validation loss: \u001b[92m0.064073\u001b[0m\n",
      "iteration 775, train loss: 0.068691, validation loss: \u001b[92m0.064062\u001b[0m\n",
      "iteration 776, train loss: 0.068743, validation loss: \u001b[92m0.064054\u001b[0m\n",
      "iteration 777, train loss: 0.068753, validation loss: \u001b[92m0.064037\u001b[0m\n",
      "iteration 778, train loss: 0.068593, validation loss: \u001b[92m0.064015\u001b[0m\n",
      "iteration 779, train loss: 0.068785, validation loss: \u001b[92m0.063992\u001b[0m\n",
      "iteration 780, train loss: 0.068703, validation loss: \u001b[92m0.063975\u001b[0m\n",
      "iteration 781, train loss: \u001b[92m0.068568\u001b[0m, validation loss: \u001b[92m0.063965\u001b[0m\n",
      "iteration 782, train loss: 0.068768, validation loss: \u001b[92m0.063952\u001b[0m\n",
      "iteration 783, train loss: 0.068604, validation loss: \u001b[92m0.063934\u001b[0m\n",
      "iteration 784, train loss: 0.068976, validation loss: \u001b[92m0.063929\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 785, train loss: 0.068722, validation loss: 0.063935\n",
      "iteration 786, train loss: 0.068584, validation loss: \u001b[92m0.063926\u001b[0m\n",
      "iteration 787, train loss: \u001b[92m0.0682\u001b[0m, validation loss: \u001b[92m0.063904\u001b[0m\n",
      "iteration 788, train loss: 0.068951, validation loss: \u001b[92m0.063893\u001b[0m\n",
      "iteration 789, train loss: 0.068476, validation loss: \u001b[92m0.063861\u001b[0m\n",
      "iteration 790, train loss: 0.068921, validation loss: \u001b[92m0.063828\u001b[0m\n",
      "iteration 791, train loss: 0.068913, validation loss: \u001b[92m0.063814\u001b[0m\n",
      "iteration 792, train loss: 0.068909, validation loss: \u001b[92m0.063807\u001b[0m\n",
      "iteration 793, train loss: 0.068716, validation loss: 0.063811\n",
      "iteration 794, train loss: 0.068779, validation loss: 0.06383\n",
      "iteration 795, train loss: 0.069103, validation loss: 0.063858\n",
      "iteration 796, train loss: 0.068601, validation loss: 0.063853\n",
      "iteration 797, train loss: 0.068423, validation loss: 0.063846\n",
      "iteration 798, train loss: 0.068793, validation loss: 0.063834\n",
      "iteration 799, train loss: 0.068633, validation loss: 0.063827\n",
      "iteration 800, train loss: 0.068487, validation loss: 0.063822\n",
      "iteration 801, train loss: 0.068858, validation loss: 0.063811\n",
      "iteration 802, train loss: 0.068576, validation loss: 0.063813\n",
      "iteration 803, train loss: 0.069003, validation loss: 0.063815\n",
      "iteration 804, train loss: 0.068395, validation loss: \u001b[92m0.063801\u001b[0m\n",
      "iteration 805, train loss: 0.06852, validation loss: \u001b[92m0.063784\u001b[0m\n",
      "iteration 806, train loss: 0.068353, validation loss: \u001b[92m0.063769\u001b[0m\n",
      "iteration 807, train loss: 0.068519, validation loss: \u001b[92m0.063745\u001b[0m\n",
      "iteration 808, train loss: 0.068687, validation loss: \u001b[92m0.063714\u001b[0m\n",
      "iteration 809, train loss: 0.068558, validation loss: \u001b[92m0.063678\u001b[0m\n",
      "iteration 810, train loss: \u001b[92m0.068088\u001b[0m, validation loss: \u001b[92m0.063646\u001b[0m\n",
      "iteration 811, train loss: 0.068289, validation loss: \u001b[92m0.063619\u001b[0m\n",
      "iteration 812, train loss: 0.068431, validation loss: \u001b[92m0.0636\u001b[0m\n",
      "iteration 813, train loss: 0.068702, validation loss: \u001b[92m0.063587\u001b[0m\n",
      "iteration 814, train loss: 0.069112, validation loss: \u001b[92m0.063586\u001b[0m\n",
      "iteration 815, train loss: 0.068218, validation loss: \u001b[92m0.063581\u001b[0m\n",
      "iteration 816, train loss: 0.068112, validation loss: \u001b[92m0.063565\u001b[0m\n",
      "iteration 817, train loss: 0.068228, validation loss: \u001b[92m0.063558\u001b[0m\n",
      "iteration 818, train loss: 0.068209, validation loss: \u001b[92m0.063546\u001b[0m\n",
      "iteration 819, train loss: 0.068348, validation loss: \u001b[92m0.063541\u001b[0m\n",
      "iteration 820, train loss: 0.068203, validation loss: \u001b[92m0.063528\u001b[0m\n",
      "iteration 821, train loss: 0.068591, validation loss: \u001b[92m0.063509\u001b[0m\n",
      "iteration 822, train loss: 0.068711, validation loss: \u001b[92m0.063486\u001b[0m\n",
      "iteration 823, train loss: 0.068636, validation loss: \u001b[92m0.063443\u001b[0m\n",
      "iteration 824, train loss: 0.06839, validation loss: \u001b[92m0.063416\u001b[0m\n",
      "iteration 825, train loss: 0.068503, validation loss: \u001b[92m0.0634\u001b[0m\n",
      "iteration 826, train loss: 0.068394, validation loss: \u001b[92m0.063391\u001b[0m\n",
      "iteration 827, train loss: 0.06851, validation loss: 0.063397\n",
      "iteration 828, train loss: 0.068401, validation loss: 0.063413\n",
      "iteration 829, train loss: 0.068585, validation loss: 0.063428\n",
      "iteration 830, train loss: 0.068115, validation loss: 0.063468\n",
      "iteration 831, train loss: 0.068278, validation loss: 0.063483\n",
      "iteration 832, train loss: 0.068289, validation loss: 0.063497\n",
      "iteration 833, train loss: 0.068602, validation loss: 0.063496\n",
      "iteration 834, train loss: 0.068272, validation loss: 0.063476\n",
      "iteration 835, train loss: 0.068223, validation loss: 0.063423\n",
      "iteration 836, train loss: 0.068232, validation loss: \u001b[92m0.063373\u001b[0m\n",
      "iteration 837, train loss: 0.068183, validation loss: \u001b[92m0.063325\u001b[0m\n",
      "iteration 838, train loss: 0.068579, validation loss: \u001b[92m0.063295\u001b[0m\n",
      "iteration 839, train loss: 0.068156, validation loss: \u001b[92m0.063287\u001b[0m\n",
      "iteration 840, train loss: 0.068395, validation loss: 0.063287\n",
      "iteration 841, train loss: \u001b[92m0.067541\u001b[0m, validation loss: \u001b[92m0.06328\u001b[0m\n",
      "iteration 842, train loss: 0.068753, validation loss: 0.063282\n",
      "iteration 843, train loss: 0.068254, validation loss: \u001b[92m0.063276\u001b[0m\n",
      "iteration 844, train loss: 0.068547, validation loss: 0.063282\n",
      "iteration 845, train loss: 0.068523, validation loss: 0.063322\n",
      "iteration 846, train loss: 0.068247, validation loss: 0.063338\n",
      "iteration 847, train loss: 0.06823, validation loss: 0.063325\n",
      "iteration 848, train loss: 0.067874, validation loss: \u001b[92m0.063267\u001b[0m\n",
      "iteration 849, train loss: 0.068646, validation loss: \u001b[92m0.063218\u001b[0m\n",
      "iteration 850, train loss: 0.068256, validation loss: \u001b[92m0.063191\u001b[0m\n",
      "iteration 851, train loss: 0.068355, validation loss: \u001b[92m0.063165\u001b[0m\n",
      "iteration 852, train loss: 0.068257, validation loss: \u001b[92m0.063165\u001b[0m\n",
      "iteration 853, train loss: 0.068523, validation loss: 0.063167\n",
      "iteration 854, train loss: 0.068334, validation loss: \u001b[92m0.063163\u001b[0m\n",
      "iteration 855, train loss: 0.068472, validation loss: 0.063182\n",
      "iteration 856, train loss: 0.068523, validation loss: 0.063205\n",
      "iteration 857, train loss: 0.067964, validation loss: 0.063197\n",
      "iteration 858, train loss: 0.068363, validation loss: 0.063193\n",
      "iteration 859, train loss: 0.06877, validation loss: 0.063193\n",
      "iteration 860, train loss: 0.068222, validation loss: 0.06318\n",
      "iteration 861, train loss: 0.068221, validation loss: 0.063171\n",
      "iteration 862, train loss: 0.068002, validation loss: \u001b[92m0.063162\u001b[0m\n",
      "iteration 863, train loss: 0.068521, validation loss: \u001b[92m0.063147\u001b[0m\n",
      "iteration 864, train loss: 0.068165, validation loss: 0.063154\n",
      "iteration 865, train loss: 0.068295, validation loss: \u001b[92m0.063141\u001b[0m\n",
      "iteration 866, train loss: 0.068278, validation loss: \u001b[92m0.063122\u001b[0m\n",
      "iteration 867, train loss: 0.068324, validation loss: \u001b[92m0.063116\u001b[0m\n",
      "iteration 868, train loss: 0.068548, validation loss: 0.063143\n",
      "iteration 869, train loss: 0.068101, validation loss: 0.063167\n",
      "iteration 870, train loss: 0.068074, validation loss: 0.063176\n",
      "iteration 871, train loss: 0.068126, validation loss: 0.063206\n",
      "iteration 872, train loss: 0.068312, validation loss: 0.06324\n",
      "iteration 873, train loss: 0.068164, validation loss: 0.063267\n",
      "iteration 874, train loss: 0.068276, validation loss: 0.063267\n",
      "iteration 875, train loss: 0.068059, validation loss: 0.063218\n",
      "iteration 876, train loss: 0.068143, validation loss: 0.063171\n",
      "iteration 877, train loss: 0.068351, validation loss: 0.063147\n",
      "iteration 878, train loss: 0.067825, validation loss: 0.063125\n",
      "iteration 879, train loss: 0.068022, validation loss: 0.063154\n",
      "iteration 880, train loss: 0.06838, validation loss: 0.063162\n",
      "iteration 881, train loss: 0.068016, validation loss: 0.06316\n",
      "iteration 882, train loss: 0.068152, validation loss: 0.063192\n",
      "iteration 883, train loss: 0.068349, validation loss: 0.063171\n",
      "iteration 884, train loss: 0.068197, validation loss: 0.063145\n",
      "iteration 885, train loss: 0.068316, validation loss: \u001b[92m0.063113\u001b[0m\n",
      "iteration 886, train loss: 0.068296, validation loss: \u001b[92m0.063071\u001b[0m\n",
      "iteration 887, train loss: 0.067896, validation loss: \u001b[92m0.063016\u001b[0m\n",
      "iteration 888, train loss: 0.068491, validation loss: \u001b[92m0.062963\u001b[0m\n",
      "iteration 889, train loss: 0.068097, validation loss: \u001b[92m0.062943\u001b[0m\n",
      "iteration 890, train loss: 0.067817, validation loss: 0.062947\n",
      "iteration 891, train loss: 0.067847, validation loss: 0.062952\n",
      "iteration 892, train loss: 0.068124, validation loss: 0.063014\n",
      "iteration 893, train loss: 0.067959, validation loss: 0.063023\n",
      "iteration 894, train loss: 0.067964, validation loss: 0.062965\n",
      "iteration 895, train loss: 0.067748, validation loss: \u001b[92m0.062892\u001b[0m\n",
      "iteration 896, train loss: 0.067981, validation loss: \u001b[92m0.062848\u001b[0m\n",
      "iteration 897, train loss: 0.068508, validation loss: \u001b[92m0.06283\u001b[0m\n",
      "iteration 898, train loss: 0.06814, validation loss: \u001b[92m0.062826\u001b[0m\n",
      "iteration 899, train loss: 0.068172, validation loss: 0.062857\n",
      "iteration 900, train loss: 0.067843, validation loss: 0.062954\n",
      "iteration 901, train loss: 0.067892, validation loss: 0.063071\n",
      "iteration 902, train loss: 0.068272, validation loss: 0.063128\n",
      "iteration 903, train loss: 0.068817, validation loss: 0.06308\n",
      "iteration 904, train loss: 0.068118, validation loss: 0.062984\n",
      "iteration 905, train loss: 0.068005, validation loss: 0.062919\n",
      "iteration 906, train loss: 0.068499, validation loss: 0.062861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 907, train loss: 0.068262, validation loss: 0.062827\n",
      "iteration 908, train loss: 0.068313, validation loss: 0.062829\n",
      "iteration 909, train loss: 0.068448, validation loss: 0.062842\n",
      "iteration 910, train loss: 0.06809, validation loss: 0.062874\n",
      "iteration 911, train loss: 0.067765, validation loss: 0.062882\n",
      "iteration 912, train loss: 0.067925, validation loss: 0.062928\n",
      "iteration 913, train loss: \u001b[92m0.067439\u001b[0m, validation loss: 0.06294\n",
      "iteration 914, train loss: 0.067924, validation loss: 0.062923\n",
      "iteration 915, train loss: 0.06764, validation loss: 0.062872\n",
      "iteration 916, train loss: 0.067654, validation loss: \u001b[92m0.062774\u001b[0m\n",
      "iteration 917, train loss: 0.068462, validation loss: \u001b[92m0.062726\u001b[0m\n",
      "iteration 918, train loss: 0.068009, validation loss: 0.062753\n",
      "iteration 919, train loss: 0.068082, validation loss: 0.062794\n",
      "iteration 920, train loss: 0.067715, validation loss: 0.062849\n",
      "iteration 921, train loss: 0.068216, validation loss: 0.062944\n",
      "iteration 922, train loss: 0.06795, validation loss: 0.062952\n",
      "iteration 923, train loss: 0.067954, validation loss: 0.06291\n",
      "iteration 924, train loss: 0.068021, validation loss: 0.06286\n",
      "iteration 925, train loss: 0.068031, validation loss: 0.062835\n",
      "iteration 926, train loss: 0.068005, validation loss: 0.062838\n",
      "iteration 927, train loss: 0.068051, validation loss: 0.0629\n",
      "iteration 928, train loss: 0.068249, validation loss: 0.062907\n",
      "iteration 929, train loss: 0.068091, validation loss: 0.062886\n",
      "iteration 930, train loss: 0.06788, validation loss: 0.062843\n",
      "iteration 931, train loss: 0.067637, validation loss: 0.062802\n",
      "iteration 932, train loss: 0.068164, validation loss: 0.062811\n",
      "iteration 933, train loss: 0.067698, validation loss: 0.062818\n",
      "iteration 934, train loss: 0.068126, validation loss: 0.062843\n",
      "iteration 935, train loss: 0.068045, validation loss: 0.062842\n",
      "iteration 936, train loss: 0.068145, validation loss: 0.062822\n",
      "iteration 937, train loss: 0.067841, validation loss: 0.062808\n",
      "iteration 938, train loss: 0.068075, validation loss: 0.062824\n",
      "iteration 939, train loss: 0.067999, validation loss: 0.062793\n",
      "iteration 940, train loss: 0.067946, validation loss: 0.062765\n",
      "iteration 941, train loss: 0.067566, validation loss: 0.062748\n",
      "iteration 942, train loss: 0.067987, validation loss: 0.062745\n",
      "iteration 943, train loss: 0.068218, validation loss: 0.062839\n",
      "iteration 944, train loss: 0.068358, validation loss: 0.062871\n",
      "iteration 945, train loss: 0.067999, validation loss: 0.062898\n",
      "iteration 946, train loss: 0.067598, validation loss: 0.062884\n",
      "iteration 947, train loss: 0.067882, validation loss: 0.062877\n",
      "iteration 948, train loss: 0.067972, validation loss: 0.062867\n",
      "iteration 949, train loss: 0.067671, validation loss: 0.06285\n",
      "iteration 950, train loss: 0.068211, validation loss: 0.062861\n",
      "iteration 951, train loss: 0.067724, validation loss: 0.062848\n",
      "iteration 952, train loss: 0.067924, validation loss: 0.062799\n",
      "iteration 953, train loss: 0.067704, validation loss: \u001b[92m0.062701\u001b[0m\n",
      "iteration 954, train loss: 0.068056, validation loss: \u001b[92m0.062672\u001b[0m\n",
      "iteration 955, train loss: 0.067717, validation loss: \u001b[92m0.062647\u001b[0m\n",
      "iteration 956, train loss: 0.068024, validation loss: 0.06267\n",
      "iteration 957, train loss: 0.067971, validation loss: 0.062664\n",
      "iteration 958, train loss: 0.06772, validation loss: 0.062734\n",
      "iteration 959, train loss: 0.067624, validation loss: 0.062877\n",
      "iteration 960, train loss: 0.067949, validation loss: 0.063006\n",
      "iteration 961, train loss: 0.068003, validation loss: 0.06307\n",
      "iteration 962, train loss: 0.067662, validation loss: 0.06304\n",
      "iteration 963, train loss: 0.067613, validation loss: 0.062886\n",
      "iteration 964, train loss: 0.067825, validation loss: 0.062723\n",
      "iteration 965, train loss: 0.068003, validation loss: \u001b[92m0.062574\u001b[0m\n",
      "iteration 966, train loss: 0.067861, validation loss: \u001b[92m0.062508\u001b[0m\n",
      "iteration 967, train loss: 0.068022, validation loss: 0.062517\n",
      "iteration 968, train loss: 0.068007, validation loss: 0.06261\n",
      "iteration 969, train loss: \u001b[92m0.067427\u001b[0m, validation loss: 0.06283\n",
      "iteration 970, train loss: 0.06766, validation loss: 0.063026\n",
      "iteration 971, train loss: 0.067941, validation loss: 0.06312\n",
      "iteration 972, train loss: 0.067653, validation loss: 0.063081\n",
      "iteration 973, train loss: 0.067584, validation loss: 0.062913\n",
      "iteration 974, train loss: 0.067853, validation loss: 0.062728\n",
      "iteration 975, train loss: 0.068002, validation loss: 0.0626\n",
      "iteration 976, train loss: 0.067685, validation loss: 0.062592\n",
      "iteration 977, train loss: 0.067993, validation loss: 0.062637\n",
      "iteration 978, train loss: 0.067914, validation loss: 0.062816\n",
      "iteration 979, train loss: 0.067931, validation loss: 0.062939\n",
      "iteration 980, train loss: 0.068062, validation loss: 0.062978\n",
      "iteration 981, train loss: 0.067533, validation loss: 0.062932\n",
      "iteration 982, train loss: 0.067993, validation loss: 0.062832\n",
      "iteration 983, train loss: 0.06758, validation loss: 0.062675\n",
      "iteration 984, train loss: 0.067686, validation loss: 0.062602\n",
      "iteration 985, train loss: 0.068129, validation loss: 0.062544\n",
      "iteration 986, train loss: 0.067736, validation loss: 0.062542\n",
      "iteration 987, train loss: 0.067789, validation loss: 0.06261\n",
      "iteration 988, train loss: 0.067758, validation loss: 0.062749\n",
      "iteration 989, train loss: 0.067857, validation loss: 0.062859\n",
      "iteration 990, train loss: 0.067748, validation loss: 0.062885\n",
      "iteration 991, train loss: 0.067985, validation loss: 0.06285\n",
      "iteration 992, train loss: 0.067803, validation loss: 0.062751\n",
      "iteration 993, train loss: 0.068025, validation loss: 0.062682\n",
      "iteration 994, train loss: 0.067871, validation loss: 0.062657\n",
      "iteration 995, train loss: 0.067971, validation loss: 0.062681\n",
      "iteration 996, train loss: 0.068261, validation loss: 0.062793\n",
      "iteration 997, train loss: 0.067617, validation loss: 0.062866\n",
      "iteration 998, train loss: 0.067854, validation loss: 0.062883\n",
      "iteration 999, train loss: 0.067532, validation loss: 0.062855\n",
      "iteration 1000, train loss: 0.067469, validation loss: 0.062747\n",
      "iteration 1001, train loss: 0.067765, validation loss: 0.062644\n",
      "iteration 1002, train loss: \u001b[92m0.067171\u001b[0m, validation loss: 0.062546\n",
      "iteration 1003, train loss: 0.067846, validation loss: 0.062583\n",
      "iteration 1004, train loss: 0.067773, validation loss: 0.062615\n",
      "iteration 1005, train loss: 0.068142, validation loss: 0.062653\n",
      "iteration 1006, train loss: 0.067852, validation loss: 0.062734\n",
      "iteration 1007, train loss: 0.067628, validation loss: 0.062799\n",
      "iteration 1008, train loss: 0.067777, validation loss: 0.062834\n",
      "iteration 1009, train loss: 0.067747, validation loss: 0.062863\n",
      "iteration 1010, train loss: 0.067784, validation loss: 0.062842\n",
      "iteration 1011, train loss: 0.068157, validation loss: 0.062853\n",
      "iteration 1012, train loss: 0.067764, validation loss: 0.06284\n",
      "iteration 1013, train loss: 0.067741, validation loss: 0.062837\n",
      "iteration 1014, train loss: 0.067666, validation loss: 0.062785\n",
      "iteration 1015, train loss: 0.068017, validation loss: 0.062732\n",
      "iteration 1016, train loss: 0.067761, validation loss: 0.062659\n",
      "iteration 1017, train loss: 0.0678, validation loss: 0.062616\n",
      "iteration 1018, train loss: 0.067526, validation loss: 0.062605\n",
      "iteration 1019, train loss: 0.067556, validation loss: 0.062614\n",
      "iteration 1020, train loss: 0.067739, validation loss: 0.062663\n",
      "iteration 1021, train loss: 0.067999, validation loss: 0.062727\n",
      "iteration 1022, train loss: 0.067814, validation loss: 0.062791\n",
      "iteration 1023, train loss: 0.06759, validation loss: 0.062846\n",
      "iteration 1024, train loss: 0.067528, validation loss: 0.062845\n",
      "iteration 1025, train loss: 0.067864, validation loss: 0.062812\n",
      "iteration 1026, train loss: 0.068081, validation loss: 0.062738\n",
      "iteration 1027, train loss: 0.067653, validation loss: 0.062644\n",
      "iteration 1028, train loss: 0.067621, validation loss: 0.062571\n",
      "iteration 1029, train loss: 0.067875, validation loss: 0.062548\n",
      "iteration 1030, train loss: 0.067468, validation loss: 0.062578\n",
      "iteration 1031, train loss: 0.067773, validation loss: 0.062615\n",
      "iteration 1032, train loss: 0.067919, validation loss: 0.062696\n",
      "iteration 1033, train loss: 0.06772, validation loss: 0.06275\n",
      "iteration 1034, train loss: 0.067805, validation loss: 0.062737\n",
      "iteration 1035, train loss: 0.067704, validation loss: 0.062711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1036, train loss: 0.067818, validation loss: 0.062683\n",
      "iteration 1037, train loss: 0.067587, validation loss: 0.062599\n",
      "iteration 1038, train loss: 0.068218, validation loss: 0.062643\n",
      "iteration 1039, train loss: 0.06781, validation loss: 0.062712\n",
      "iteration 1040, train loss: 0.068024, validation loss: 0.062848\n",
      "iteration 1041, train loss: 0.067387, validation loss: 0.062987\n",
      "iteration 1042, train loss: 0.067578, validation loss: 0.063015\n",
      "iteration 1043, train loss: 0.067731, validation loss: 0.062988\n",
      "iteration 1044, train loss: 0.067752, validation loss: 0.062894\n",
      "iteration 1045, train loss: 0.067744, validation loss: 0.062751\n",
      "iteration 1046, train loss: 0.067525, validation loss: 0.062603\n",
      "iteration 1047, train loss: 0.067804, validation loss: 0.062563\n",
      "iteration 1048, train loss: 0.06785, validation loss: 0.062554\n",
      "iteration 1049, train loss: 0.067725, validation loss: 0.062615\n",
      "iteration 1050, train loss: 0.067592, validation loss: 0.062768\n",
      "iteration 1051, train loss: 0.067424, validation loss: 0.062878\n",
      "iteration 1052, train loss: 0.067432, validation loss: 0.062917\n",
      "iteration 1053, train loss: 0.068072, validation loss: 0.06288\n",
      "iteration 1054, train loss: 0.067557, validation loss: 0.062742\n",
      "iteration 1055, train loss: 0.067718, validation loss: 0.062618\n",
      "iteration 1056, train loss: 0.068017, validation loss: 0.062532\n",
      "iteration 1057, train loss: 0.067819, validation loss: \u001b[92m0.0625\u001b[0m\n",
      "iteration 1058, train loss: 0.068069, validation loss: 0.062563\n",
      "iteration 1059, train loss: 0.067796, validation loss: 0.062652\n",
      "iteration 1060, train loss: 0.067658, validation loss: 0.062772\n",
      "iteration 1061, train loss: 0.068037, validation loss: 0.062911\n",
      "iteration 1062, train loss: 0.06752, validation loss: 0.062989\n",
      "iteration 1063, train loss: 0.067634, validation loss: 0.062938\n",
      "iteration 1064, train loss: 0.067543, validation loss: 0.062782\n",
      "iteration 1065, train loss: 0.067878, validation loss: 0.062655\n",
      "iteration 1066, train loss: 0.067556, validation loss: 0.062543\n",
      "iteration 1067, train loss: 0.067749, validation loss: 0.062517\n",
      "iteration 1068, train loss: 0.067529, validation loss: 0.062528\n",
      "iteration 1069, train loss: 0.0684, validation loss: 0.062635\n",
      "iteration 1070, train loss: 0.067872, validation loss: 0.06274\n",
      "iteration 1071, train loss: 0.067384, validation loss: 0.062814\n",
      "iteration 1072, train loss: 0.067658, validation loss: 0.062836\n",
      "iteration 1073, train loss: 0.067936, validation loss: 0.062774\n",
      "iteration 1074, train loss: 0.067978, validation loss: 0.062669\n",
      "iteration 1075, train loss: 0.067947, validation loss: 0.062588\n",
      "iteration 1076, train loss: \u001b[92m0.067119\u001b[0m, validation loss: 0.06255\n",
      "iteration 1077, train loss: 0.067862, validation loss: 0.062573\n",
      "iteration 1078, train loss: 0.067397, validation loss: 0.062615\n",
      "iteration 1079, train loss: 0.067608, validation loss: 0.06263\n",
      "iteration 1080, train loss: 0.067619, validation loss: 0.06264\n",
      "iteration 1081, train loss: 0.067647, validation loss: 0.062685\n",
      "iteration 1082, train loss: 0.067645, validation loss: 0.0627\n",
      "iteration 1083, train loss: 0.067948, validation loss: 0.062666\n",
      "iteration 1084, train loss: 0.067463, validation loss: 0.062658\n",
      "iteration 1085, train loss: 0.068017, validation loss: 0.062681\n",
      "iteration 1086, train loss: 0.067476, validation loss: 0.062668\n",
      "iteration 1087, train loss: \u001b[92m0.067017\u001b[0m, validation loss: 0.062644\n",
      "iteration 1088, train loss: 0.067736, validation loss: 0.062656\n",
      "iteration 1089, train loss: 0.067492, validation loss: 0.062591\n",
      "iteration 1090, train loss: 0.067984, validation loss: 0.062513\n",
      "iteration 1091, train loss: 0.067695, validation loss: \u001b[92m0.062492\u001b[0m\n",
      "iteration 1092, train loss: 0.068252, validation loss: 0.06251\n",
      "iteration 1093, train loss: 0.06768, validation loss: 0.062552\n",
      "iteration 1094, train loss: 0.067723, validation loss: 0.062611\n",
      "iteration 1095, train loss: 0.067768, validation loss: 0.062667\n",
      "iteration 1096, train loss: 0.067515, validation loss: 0.062701\n",
      "iteration 1097, train loss: 0.067936, validation loss: 0.062719\n",
      "iteration 1098, train loss: 0.067859, validation loss: 0.062751\n",
      "iteration 1099, train loss: 0.06762, validation loss: 0.062748\n",
      "iteration 1100, train loss: 0.06778, validation loss: 0.062703\n",
      "iteration 1101, train loss: 0.067673, validation loss: 0.062647\n",
      "iteration 1102, train loss: 0.0676, validation loss: 0.062601\n",
      "iteration 1103, train loss: 0.067879, validation loss: 0.062585\n",
      "iteration 1104, train loss: 0.067619, validation loss: 0.062652\n",
      "iteration 1105, train loss: 0.067754, validation loss: 0.062693\n",
      "iteration 1106, train loss: 0.067581, validation loss: 0.062688\n",
      "iteration 1107, train loss: 0.06762, validation loss: 0.0627\n",
      "iteration 1108, train loss: 0.068005, validation loss: 0.062772\n",
      "iteration 1109, train loss: 0.067271, validation loss: 0.06278\n",
      "iteration 1110, train loss: 0.067693, validation loss: 0.062746\n",
      "iteration 1111, train loss: 0.067916, validation loss: 0.062713\n",
      "iteration 1112, train loss: 0.06788, validation loss: 0.062679\n",
      "iteration 1113, train loss: 0.067311, validation loss: 0.062628\n",
      "iteration 1114, train loss: 0.067594, validation loss: 0.062596\n",
      "iteration 1115, train loss: 0.067682, validation loss: 0.062633\n",
      "iteration 1116, train loss: 0.06764, validation loss: 0.062627\n",
      "iteration 1117, train loss: 0.067682, validation loss: 0.062617\n",
      "iteration 1118, train loss: 0.068096, validation loss: 0.062653\n",
      "iteration 1119, train loss: 0.067507, validation loss: 0.062666\n",
      "iteration 1120, train loss: 0.067618, validation loss: 0.062653\n",
      "iteration 1121, train loss: 0.067864, validation loss: 0.062647\n",
      "iteration 1122, train loss: 0.067519, validation loss: 0.062624\n",
      "iteration 1123, train loss: 0.067689, validation loss: 0.062646\n",
      "iteration 1124, train loss: 0.067802, validation loss: 0.062689\n",
      "iteration 1125, train loss: 0.067654, validation loss: 0.062748\n",
      "iteration 1126, train loss: 0.067884, validation loss: 0.062762\n",
      "iteration 1127, train loss: 0.067514, validation loss: 0.062766\n",
      "iteration 1128, train loss: 0.067665, validation loss: 0.062712\n",
      "iteration 1129, train loss: 0.068221, validation loss: 0.062702\n",
      "iteration 1130, train loss: 0.067797, validation loss: 0.062688\n",
      "iteration 1131, train loss: 0.06793, validation loss: 0.062731\n",
      "iteration 1132, train loss: 0.067487, validation loss: 0.062742\n",
      "iteration 1133, train loss: 0.067837, validation loss: 0.062744\n",
      "iteration 1134, train loss: 0.067476, validation loss: 0.062738\n",
      "iteration 1135, train loss: 0.067805, validation loss: 0.062711\n",
      "iteration 1136, train loss: 0.068079, validation loss: 0.062738\n",
      "iteration 1137, train loss: 0.067649, validation loss: 0.062771\n",
      "iteration 1138, train loss: 0.067758, validation loss: 0.062766\n",
      "iteration 1139, train loss: 0.068028, validation loss: 0.062739\n",
      "iteration 1140, train loss: 0.067843, validation loss: 0.062747\n",
      "iteration 1141, train loss: 0.067948, validation loss: 0.062727\n",
      "iteration 1142, train loss: 0.067734, validation loss: 0.062735\n",
      "iteration 1143, train loss: 0.067916, validation loss: 0.062721\n",
      "iteration 1144, train loss: 0.067675, validation loss: 0.062682\n",
      "iteration 1145, train loss: 0.067644, validation loss: 0.062657\n",
      "iteration 1146, train loss: 0.067531, validation loss: 0.062674\n",
      "iteration 1147, train loss: 0.06777, validation loss: 0.062715\n",
      "iteration 1148, train loss: 0.067735, validation loss: 0.062778\n",
      "iteration 1149, train loss: 0.068109, validation loss: 0.062806\n",
      "iteration 1150, train loss: 0.067835, validation loss: 0.062753\n",
      "iteration 1151, train loss: 0.067257, validation loss: 0.062684\n",
      "iteration 1152, train loss: 0.068049, validation loss: 0.062657\n",
      "iteration 1153, train loss: 0.067568, validation loss: 0.062644\n",
      "iteration 1154, train loss: 0.067777, validation loss: 0.062665\n",
      "iteration 1155, train loss: 0.068206, validation loss: 0.06276\n",
      "iteration 1156, train loss: 0.067624, validation loss: 0.06282\n",
      "iteration 1157, train loss: 0.0678, validation loss: 0.062868\n",
      "iteration 1158, train loss: 0.06743, validation loss: 0.062817\n",
      "iteration 1159, train loss: 0.067806, validation loss: 0.062747\n",
      "iteration 1160, train loss: 0.067436, validation loss: 0.062674\n",
      "iteration 1161, train loss: 0.067716, validation loss: 0.0626\n",
      "iteration 1162, train loss: 0.067819, validation loss: 0.062503\n",
      "iteration 1163, train loss: 0.067957, validation loss: 0.062502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1164, train loss: 0.067652, validation loss: 0.062575\n",
      "iteration 1165, train loss: 0.067762, validation loss: 0.062629\n",
      "iteration 1166, train loss: 0.067967, validation loss: 0.062732\n",
      "iteration 1167, train loss: 0.067404, validation loss: 0.062814\n",
      "iteration 1168, train loss: 0.06726, validation loss: 0.062831\n",
      "iteration 1169, train loss: 0.067995, validation loss: 0.062779\n",
      "iteration 1170, train loss: 0.067664, validation loss: 0.062723\n",
      "iteration 1171, train loss: 0.067679, validation loss: 0.062599\n",
      "iteration 1172, train loss: 0.06792, validation loss: 0.062548\n",
      "iteration 1173, train loss: 0.067667, validation loss: 0.062543\n",
      "iteration 1174, train loss: 0.067784, validation loss: 0.06259\n",
      "iteration 1175, train loss: 0.067828, validation loss: 0.062677\n",
      "iteration 1176, train loss: 0.067665, validation loss: 0.062753\n",
      "iteration 1177, train loss: 0.067551, validation loss: 0.062719\n",
      "iteration 1178, train loss: 0.067824, validation loss: 0.062727\n",
      "iteration 1179, train loss: 0.067416, validation loss: 0.062665\n",
      "iteration 1180, train loss: 0.068106, validation loss: 0.062613\n",
      "iteration 1181, train loss: 0.067569, validation loss: 0.062539\n",
      "iteration 1182, train loss: 0.06769, validation loss: 0.062532\n",
      "iteration 1183, train loss: 0.067593, validation loss: 0.062549\n",
      "iteration 1184, train loss: 0.067756, validation loss: 0.062619\n",
      "iteration 1185, train loss: 0.067683, validation loss: 0.062683\n",
      "iteration 1186, train loss: 0.067933, validation loss: 0.062744\n",
      "iteration 1187, train loss: 0.067776, validation loss: 0.062821\n",
      "iteration 1188, train loss: 0.06768, validation loss: 0.062773\n",
      "iteration 1189, train loss: 0.067525, validation loss: 0.062675\n",
      "iteration 1190, train loss: 0.067655, validation loss: 0.062588\n",
      "iteration 1191, train loss: 0.067683, validation loss: 0.062571\n",
      "iteration 1192, train loss: 0.067924, validation loss: 0.062589\n",
      "iteration 1193, train loss: 0.067852, validation loss: 0.062633\n",
      "iteration 1194, train loss: 0.067676, validation loss: 0.062685\n",
      "iteration 1195, train loss: 0.067607, validation loss: 0.062742\n",
      "iteration 1196, train loss: 0.067619, validation loss: 0.062768\n",
      "iteration 1197, train loss: 0.067635, validation loss: 0.062752\n",
      "iteration 1198, train loss: 0.067426, validation loss: 0.062735\n",
      "iteration 1199, train loss: 0.067512, validation loss: 0.062663\n",
      "iteration 1200, train loss: 0.067709, validation loss: 0.062592\n",
      "iteration 1201, train loss: 0.067529, validation loss: 0.062514\n",
      "iteration 1202, train loss: 0.067731, validation loss: 0.062511\n",
      "iteration 1203, train loss: 0.067932, validation loss: 0.062622\n",
      "iteration 1204, train loss: 0.067254, validation loss: 0.062795\n",
      "iteration 1205, train loss: 0.067463, validation loss: 0.062923\n",
      "iteration 1206, train loss: 0.067931, validation loss: 0.062972\n",
      "iteration 1207, train loss: 0.067871, validation loss: 0.062861\n",
      "iteration 1208, train loss: 0.067753, validation loss: 0.062673\n",
      "iteration 1209, train loss: 0.067802, validation loss: 0.062525\n",
      "iteration 1210, train loss: 0.067827, validation loss: \u001b[92m0.062447\u001b[0m\n",
      "iteration 1211, train loss: 0.067871, validation loss: 0.062485\n",
      "iteration 1212, train loss: 0.067655, validation loss: 0.06252\n",
      "iteration 1213, train loss: 0.067684, validation loss: 0.062606\n",
      "iteration 1214, train loss: 0.067682, validation loss: 0.062647\n",
      "iteration 1215, train loss: 0.068023, validation loss: 0.062698\n",
      "iteration 1216, train loss: 0.067682, validation loss: 0.062651\n",
      "iteration 1217, train loss: 0.067744, validation loss: 0.062572\n",
      "iteration 1218, train loss: 0.067506, validation loss: 0.062583\n",
      "iteration 1219, train loss: 0.067561, validation loss: 0.062604\n",
      "iteration 1220, train loss: 0.067882, validation loss: 0.062642\n",
      "iteration 1221, train loss: 0.068153, validation loss: 0.062685\n",
      "iteration 1222, train loss: 0.067247, validation loss: 0.062673\n",
      "iteration 1223, train loss: 0.0675, validation loss: 0.06265\n",
      "iteration 1224, train loss: 0.068044, validation loss: 0.062616\n",
      "iteration 1225, train loss: 0.067707, validation loss: 0.062639\n",
      "iteration 1226, train loss: 0.067869, validation loss: 0.062702\n",
      "iteration 1227, train loss: 0.06758, validation loss: 0.06271\n",
      "iteration 1228, train loss: 0.068384, validation loss: 0.062697\n",
      "iteration 1229, train loss: 0.06777, validation loss: 0.062693\n",
      "iteration 1230, train loss: 0.068085, validation loss: 0.062726\n",
      "iteration 1231, train loss: 0.067823, validation loss: 0.062715\n",
      "iteration 1232, train loss: 0.067784, validation loss: 0.062725\n",
      "iteration 1233, train loss: 0.067657, validation loss: 0.062711\n",
      "iteration 1234, train loss: 0.067603, validation loss: 0.062688\n",
      "iteration 1235, train loss: 0.068034, validation loss: 0.062729\n",
      "iteration 1236, train loss: 0.067852, validation loss: 0.062788\n",
      "iteration 1237, train loss: 0.067643, validation loss: 0.062796\n",
      "iteration 1238, train loss: 0.067946, validation loss: 0.06278\n",
      "iteration 1239, train loss: 0.067541, validation loss: 0.06272\n",
      "iteration 1240, train loss: 0.067536, validation loss: 0.062643\n",
      "iteration 1241, train loss: 0.067767, validation loss: 0.062603\n",
      "iteration 1242, train loss: 0.067684, validation loss: 0.062639\n",
      "iteration 1243, train loss: 0.067693, validation loss: 0.062677\n",
      "iteration 1244, train loss: 0.067706, validation loss: 0.062681\n",
      "iteration 1245, train loss: 0.067857, validation loss: 0.062719\n",
      "iteration 1246, train loss: 0.067528, validation loss: 0.06275\n",
      "iteration 1247, train loss: 0.067778, validation loss: 0.062735\n",
      "iteration 1248, train loss: 0.067686, validation loss: 0.062667\n",
      "iteration 1249, train loss: 0.067708, validation loss: 0.06258\n",
      "iteration 1250, train loss: 0.067789, validation loss: 0.062504\n",
      "iteration 1251, train loss: 0.067747, validation loss: 0.062532\n",
      "iteration 1252, train loss: 0.068057, validation loss: 0.062707\n",
      "iteration 1253, train loss: 0.067572, validation loss: 0.062878\n",
      "iteration 1254, train loss: 0.067707, validation loss: 0.062951\n",
      "iteration 1255, train loss: 0.067827, validation loss: 0.06293\n",
      "iteration 1256, train loss: 0.067922, validation loss: 0.062825\n",
      "iteration 1257, train loss: 0.067526, validation loss: 0.062692\n",
      "iteration 1258, train loss: 0.067844, validation loss: 0.062621\n",
      "iteration 1259, train loss: 0.067877, validation loss: 0.062556\n",
      "iteration 1260, train loss: 0.067478, validation loss: 0.062549\n",
      "iteration 1261, train loss: 0.067635, validation loss: 0.062597\n",
      "iteration 1262, train loss: 0.067449, validation loss: 0.062658\n",
      "iteration 1263, train loss: 0.06782, validation loss: 0.062689\n",
      "iteration 1264, train loss: 0.067255, validation loss: 0.062653\n",
      "iteration 1265, train loss: 0.067483, validation loss: 0.062613\n",
      "iteration 1266, train loss: 0.068165, validation loss: 0.062599\n",
      "iteration 1267, train loss: 0.067689, validation loss: 0.062621\n",
      "iteration 1268, train loss: 0.067582, validation loss: 0.062646\n",
      "iteration 1269, train loss: 0.067779, validation loss: 0.062672\n",
      "iteration 1270, train loss: 0.067582, validation loss: 0.062669\n",
      "iteration 1271, train loss: 0.067714, validation loss: 0.062693\n",
      "iteration 1272, train loss: 0.067789, validation loss: 0.062686\n",
      "iteration 1273, train loss: 0.067603, validation loss: 0.062675\n",
      "iteration 1274, train loss: 0.067976, validation loss: 0.06271\n",
      "iteration 1275, train loss: 0.067698, validation loss: 0.062799\n",
      "iteration 1276, train loss: 0.068204, validation loss: 0.062834\n",
      "iteration 1277, train loss: 0.067603, validation loss: 0.062859\n",
      "iteration 1278, train loss: 0.06788, validation loss: 0.062877\n",
      "iteration 1279, train loss: 0.067476, validation loss: 0.062802\n",
      "iteration 1280, train loss: 0.067915, validation loss: 0.062687\n",
      "iteration 1281, train loss: 0.0673, validation loss: 0.062585\n",
      "iteration 1282, train loss: 0.067691, validation loss: 0.062558\n",
      "iteration 1283, train loss: 0.067613, validation loss: 0.06263\n",
      "iteration 1284, train loss: 0.067849, validation loss: 0.062654\n",
      "iteration 1285, train loss: 0.067502, validation loss: 0.062713\n",
      "iteration 1286, train loss: 0.067856, validation loss: 0.062778\n",
      "iteration 1287, train loss: 0.068069, validation loss: 0.062824\n",
      "iteration 1288, train loss: 0.067567, validation loss: 0.062838\n",
      "iteration 1289, train loss: 0.067376, validation loss: 0.062785\n",
      "iteration 1290, train loss: 0.067558, validation loss: 0.062701\n",
      "iteration 1291, train loss: 0.067556, validation loss: 0.062655\n",
      "iteration 1292, train loss: 0.067512, validation loss: 0.062716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1293, train loss: 0.067807, validation loss: 0.062844\n",
      "iteration 1294, train loss: 0.06798, validation loss: 0.062914\n",
      "iteration 1295, train loss: 0.067842, validation loss: 0.062885\n",
      "iteration 1296, train loss: 0.067996, validation loss: 0.062789\n",
      "iteration 1297, train loss: 0.067984, validation loss: 0.062686\n",
      "iteration 1298, train loss: 0.067283, validation loss: 0.062588\n",
      "iteration 1299, train loss: 0.067648, validation loss: 0.062559\n",
      "iteration 1300, train loss: 0.067175, validation loss: 0.062533\n",
      "iteration 1301, train loss: 0.067759, validation loss: 0.062559\n",
      "iteration 1302, train loss: 0.067559, validation loss: 0.062631\n",
      "iteration 1303, train loss: 0.067758, validation loss: 0.062691\n",
      "iteration 1304, train loss: 0.067689, validation loss: 0.062684\n",
      "iteration 1305, train loss: 0.067793, validation loss: 0.062618\n",
      "iteration 1306, train loss: 0.067826, validation loss: 0.062597\n",
      "iteration 1307, train loss: 0.067415, validation loss: 0.062558\n",
      "iteration 1308, train loss: 0.067878, validation loss: 0.062522\n",
      "iteration 1309, train loss: 0.067371, validation loss: 0.062466\n",
      "iteration 1310, train loss: 0.067587, validation loss: 0.062454\n",
      "iteration 1311, train loss: 0.067797, validation loss: 0.062504\n",
      "iteration 1312, train loss: 0.06776, validation loss: 0.06264\n",
      "iteration 1313, train loss: 0.067476, validation loss: 0.062828\n",
      "iteration 1314, train loss: 0.067335, validation loss: 0.062919\n",
      "iteration 1315, train loss: 0.06765, validation loss: 0.062898\n",
      "iteration 1316, train loss: 0.067338, validation loss: 0.062744\n",
      "iteration 1317, train loss: 0.06783, validation loss: 0.062588\n",
      "iteration 1318, train loss: 0.067628, validation loss: 0.062459\n",
      "iteration 1319, train loss: 0.067537, validation loss: \u001b[92m0.062441\u001b[0m\n",
      "iteration 1320, train loss: 0.068087, validation loss: 0.062445\n",
      "iteration 1321, train loss: 0.068092, validation loss: 0.062688\n",
      "iteration 1322, train loss: 0.067434, validation loss: 0.062884\n",
      "iteration 1323, train loss: 0.06831, validation loss: 0.062963\n",
      "iteration 1324, train loss: 0.067507, validation loss: 0.062925\n",
      "iteration 1325, train loss: 0.067836, validation loss: 0.062776\n",
      "iteration 1326, train loss: 0.067751, validation loss: 0.062568\n",
      "iteration 1327, train loss: 0.067446, validation loss: \u001b[92m0.062414\u001b[0m\n",
      "iteration 1328, train loss: 0.068117, validation loss: \u001b[92m0.062368\u001b[0m\n",
      "iteration 1329, train loss: 0.067709, validation loss: 0.062404\n",
      "iteration 1330, train loss: 0.068018, validation loss: 0.062581\n",
      "iteration 1331, train loss: 0.067627, validation loss: 0.062774\n",
      "iteration 1332, train loss: 0.068231, validation loss: 0.06287\n",
      "iteration 1333, train loss: 0.067823, validation loss: 0.062898\n",
      "iteration 1334, train loss: 0.067795, validation loss: 0.062832\n",
      "iteration 1335, train loss: 0.067824, validation loss: 0.062802\n",
      "iteration 1336, train loss: 0.067842, validation loss: 0.062721\n",
      "iteration 1337, train loss: 0.067653, validation loss: 0.062593\n",
      "iteration 1338, train loss: 0.067608, validation loss: 0.062483\n",
      "iteration 1339, train loss: 0.067593, validation loss: 0.062455\n",
      "iteration 1340, train loss: 0.067873, validation loss: 0.06249\n",
      "iteration 1341, train loss: 0.067741, validation loss: 0.062634\n",
      "iteration 1342, train loss: 0.067233, validation loss: 0.062786\n",
      "iteration 1343, train loss: 0.067613, validation loss: 0.062914\n",
      "iteration 1344, train loss: 0.067577, validation loss: 0.062888\n",
      "iteration 1345, train loss: 0.067998, validation loss: 0.062747\n",
      "iteration 1346, train loss: 0.067668, validation loss: 0.062627\n",
      "iteration 1347, train loss: 0.067685, validation loss: 0.0626\n",
      "iteration 1348, train loss: 0.067172, validation loss: 0.062583\n",
      "iteration 1349, train loss: 0.067881, validation loss: 0.062602\n",
      "iteration 1350, train loss: 0.067526, validation loss: 0.062628\n",
      "iteration 1351, train loss: 0.067653, validation loss: 0.062662\n",
      "iteration 1352, train loss: 0.067641, validation loss: 0.062658\n",
      "iteration 1353, train loss: 0.067546, validation loss: 0.062688\n",
      "iteration 1354, train loss: 0.067779, validation loss: 0.062705\n",
      "iteration 1355, train loss: 0.067947, validation loss: 0.062655\n",
      "iteration 1356, train loss: 0.067774, validation loss: 0.062545\n",
      "iteration 1357, train loss: 0.067811, validation loss: 0.062478\n",
      "iteration 1358, train loss: 0.067838, validation loss: 0.062429\n",
      "iteration 1359, train loss: 0.067249, validation loss: 0.062437\n",
      "iteration 1360, train loss: 0.06769, validation loss: 0.062488\n",
      "iteration 1361, train loss: 0.068121, validation loss: 0.06267\n",
      "iteration 1362, train loss: 0.06717, validation loss: 0.062792\n",
      "iteration 1363, train loss: 0.067968, validation loss: 0.062867\n",
      "iteration 1364, train loss: 0.067966, validation loss: 0.062834\n",
      "iteration 1365, train loss: 0.067981, validation loss: 0.062852\n",
      "iteration 1366, train loss: 0.06781, validation loss: 0.062773\n",
      "iteration 1367, train loss: 0.067911, validation loss: 0.06264\n",
      "iteration 1368, train loss: 0.067149, validation loss: 0.06256\n",
      "iteration 1369, train loss: 0.067499, validation loss: 0.06252\n",
      "iteration 1370, train loss: 0.06767, validation loss: 0.062516\n",
      "iteration 1371, train loss: 0.067597, validation loss: 0.062568\n",
      "iteration 1372, train loss: 0.067669, validation loss: 0.062628\n",
      "iteration 1373, train loss: 0.067992, validation loss: 0.062774\n",
      "iteration 1374, train loss: 0.067794, validation loss: 0.062855\n",
      "iteration 1375, train loss: 0.067656, validation loss: 0.062906\n",
      "iteration 1376, train loss: 0.067681, validation loss: 0.062841\n",
      "iteration 1377, train loss: 0.068015, validation loss: 0.062687\n",
      "iteration 1378, train loss: 0.067989, validation loss: 0.062569\n",
      "iteration 1379, train loss: 0.067536, validation loss: 0.062459\n",
      "iteration 1380, train loss: 0.06769, validation loss: 0.062451\n",
      "iteration 1381, train loss: 0.067836, validation loss: 0.062471\n",
      "iteration 1382, train loss: 0.067706, validation loss: 0.062592\n",
      "iteration 1383, train loss: 0.067499, validation loss: 0.062716\n",
      "iteration 1384, train loss: 0.068084, validation loss: 0.062848\n",
      "iteration 1385, train loss: 0.067781, validation loss: 0.062866\n",
      "iteration 1386, train loss: 0.067887, validation loss: 0.062814\n",
      "iteration 1387, train loss: 0.067769, validation loss: 0.062712\n",
      "iteration 1388, train loss: 0.067759, validation loss: 0.062636\n",
      "iteration 1389, train loss: 0.067593, validation loss: 0.062599\n",
      "iteration 1390, train loss: 0.067924, validation loss: 0.062615\n",
      "iteration 1391, train loss: 0.067786, validation loss: 0.062688\n",
      "iteration 1392, train loss: 0.067887, validation loss: 0.062725\n",
      "iteration 1393, train loss: 0.067509, validation loss: 0.062718\n",
      "iteration 1394, train loss: 0.068005, validation loss: 0.06274\n",
      "iteration 1395, train loss: 0.067729, validation loss: 0.062726\n",
      "iteration 1396, train loss: 0.067567, validation loss: 0.062658\n",
      "iteration 1397, train loss: 0.067591, validation loss: 0.062547\n",
      "iteration 1398, train loss: 0.067822, validation loss: 0.062518\n",
      "iteration 1399, train loss: 0.067465, validation loss: 0.062525\n",
      "iteration 1400, train loss: 0.067527, validation loss: 0.062556\n",
      "iteration 1401, train loss: 0.0677, validation loss: 0.062613\n",
      "iteration 1402, train loss: 0.067742, validation loss: 0.062678\n",
      "iteration 1403, train loss: 0.068144, validation loss: 0.062697\n",
      "iteration 1404, train loss: 0.067745, validation loss: 0.062702\n",
      "iteration 1405, train loss: 0.067672, validation loss: 0.062708\n",
      "iteration 1406, train loss: 0.067768, validation loss: 0.062698\n",
      "iteration 1407, train loss: 0.067834, validation loss: 0.062686\n",
      "iteration 1408, train loss: 0.067704, validation loss: 0.06267\n",
      "iteration 1409, train loss: 0.067765, validation loss: 0.062694\n",
      "iteration 1410, train loss: 0.067956, validation loss: 0.062758\n",
      "iteration 1411, train loss: 0.067656, validation loss: 0.062784\n",
      "iteration 1412, train loss: 0.067568, validation loss: 0.062747\n",
      "iteration 1413, train loss: 0.067535, validation loss: 0.062703\n",
      "iteration 1414, train loss: 0.067848, validation loss: 0.06262\n",
      "iteration 1415, train loss: 0.067674, validation loss: 0.062597\n",
      "iteration 1416, train loss: 0.067403, validation loss: 0.062593\n",
      "iteration 1417, train loss: 0.067779, validation loss: 0.062637\n",
      "iteration 1418, train loss: 0.067649, validation loss: 0.0627\n",
      "iteration 1419, train loss: 0.067682, validation loss: 0.06277\n",
      "iteration 1420, train loss: 0.067496, validation loss: 0.062752\n",
      "iteration 1421, train loss: 0.067417, validation loss: 0.06263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1422, train loss: 0.067838, validation loss: 0.062519\n",
      "iteration 1423, train loss: 0.067727, validation loss: 0.0625\n",
      "iteration 1424, train loss: 0.068123, validation loss: 0.062479\n",
      "iteration 1425, train loss: 0.067788, validation loss: 0.062549\n",
      "iteration 1426, train loss: 0.067421, validation loss: 0.062681\n",
      "iteration 1427, train loss: 0.067563, validation loss: 0.062729\n",
      "iteration 1428, train loss: 0.06742, validation loss: 0.06265\n",
      "iteration 1429, train loss: 0.067899, validation loss: 0.062591\n",
      "iteration 1430, train loss: 0.067872, validation loss: 0.062564\n",
      "iteration 1431, train loss: 0.067576, validation loss: 0.06251\n",
      "iteration 1432, train loss: 0.067734, validation loss: 0.062494\n",
      "iteration 1433, train loss: 0.0676, validation loss: 0.062542\n",
      "iteration 1434, train loss: 0.068103, validation loss: 0.062652\n",
      "iteration 1435, train loss: 0.067139, validation loss: 0.062762\n",
      "iteration 1436, train loss: 0.06781, validation loss: 0.062788\n",
      "iteration 1437, train loss: 0.067675, validation loss: 0.062733\n",
      "iteration 1438, train loss: 0.068078, validation loss: 0.062599\n",
      "iteration 1439, train loss: 0.067872, validation loss: 0.062478\n",
      "iteration 1440, train loss: 0.067453, validation loss: 0.062412\n",
      "iteration 1441, train loss: 0.067396, validation loss: 0.062411\n",
      "iteration 1442, train loss: 0.067677, validation loss: 0.062439\n",
      "iteration 1443, train loss: 0.067812, validation loss: 0.062511\n",
      "iteration 1444, train loss: 0.067771, validation loss: 0.062665\n",
      "iteration 1445, train loss: 0.06768, validation loss: 0.062748\n",
      "iteration 1446, train loss: 0.067789, validation loss: 0.062762\n",
      "iteration 1447, train loss: 0.068037, validation loss: 0.062705\n",
      "iteration 1448, train loss: 0.067767, validation loss: 0.062567\n",
      "iteration 1449, train loss: 0.067633, validation loss: 0.06252\n",
      "iteration 1450, train loss: 0.068009, validation loss: 0.062558\n",
      "iteration 1451, train loss: 0.067465, validation loss: 0.062619\n",
      "iteration 1452, train loss: 0.067411, validation loss: 0.062645\n",
      "iteration 1453, train loss: 0.067755, validation loss: 0.062667\n",
      "iteration 1454, train loss: 0.0678, validation loss: 0.062732\n",
      "iteration 1455, train loss: 0.067514, validation loss: 0.06278\n",
      "iteration 1456, train loss: 0.067801, validation loss: 0.062788\n",
      "iteration 1457, train loss: 0.067683, validation loss: 0.062784\n",
      "iteration 1458, train loss: 0.06752, validation loss: 0.062735\n",
      "iteration 1459, train loss: 0.067662, validation loss: 0.062613\n",
      "iteration 1460, train loss: 0.067841, validation loss: 0.062552\n",
      "iteration 1461, train loss: 0.067946, validation loss: 0.062583\n",
      "iteration 1462, train loss: 0.067745, validation loss: 0.062622\n",
      "iteration 1463, train loss: 0.06758, validation loss: 0.062709\n",
      "iteration 1464, train loss: 0.067526, validation loss: 0.062817\n",
      "iteration 1465, train loss: 0.067786, validation loss: 0.062856\n",
      "iteration 1466, train loss: 0.06782, validation loss: 0.062776\n",
      "iteration 1467, train loss: 0.067996, validation loss: 0.062645\n",
      "iteration 1468, train loss: 0.067724, validation loss: 0.062533\n",
      "iteration 1469, train loss: 0.067769, validation loss: 0.062499\n",
      "iteration 1470, train loss: 0.067908, validation loss: 0.062527\n",
      "iteration 1471, train loss: 0.06784, validation loss: 0.062567\n",
      "iteration 1472, train loss: 0.068125, validation loss: 0.062619\n",
      "iteration 1473, train loss: 0.067527, validation loss: 0.062728\n",
      "iteration 1474, train loss: 0.067948, validation loss: 0.062854\n",
      "iteration 1475, train loss: 0.068071, validation loss: 0.062906\n",
      "iteration 1476, train loss: 0.068004, validation loss: 0.062818\n",
      "iteration 1477, train loss: 0.068245, validation loss: 0.062666\n",
      "iteration 1478, train loss: 0.067843, validation loss: 0.062545\n",
      "iteration 1479, train loss: 0.067489, validation loss: 0.062506\n",
      "iteration 1480, train loss: 0.067793, validation loss: 0.062518\n",
      "iteration 1481, train loss: 0.067552, validation loss: 0.062597\n",
      "iteration 1482, train loss: 0.067515, validation loss: 0.062677\n",
      "iteration 1483, train loss: 0.067729, validation loss: 0.06274\n",
      "iteration 1484, train loss: 0.067592, validation loss: 0.062741\n",
      "iteration 1485, train loss: 0.06783, validation loss: 0.062729\n",
      "iteration 1486, train loss: 0.067927, validation loss: 0.062682\n",
      "iteration 1487, train loss: 0.06776, validation loss: 0.06259\n",
      "iteration 1488, train loss: 0.067696, validation loss: 0.062505\n",
      "iteration 1489, train loss: 0.067858, validation loss: 0.062449\n",
      "iteration 1490, train loss: 0.067628, validation loss: 0.062485\n",
      "iteration 1491, train loss: 0.067296, validation loss: 0.062552\n",
      "iteration 1492, train loss: 0.067813, validation loss: 0.062706\n",
      "iteration 1493, train loss: 0.067943, validation loss: 0.062752\n",
      "iteration 1494, train loss: 0.06773, validation loss: 0.062751\n",
      "iteration 1495, train loss: 0.067851, validation loss: 0.062714\n",
      "iteration 1496, train loss: 0.067889, validation loss: 0.062624\n",
      "iteration 1497, train loss: 0.067474, validation loss: 0.062493\n",
      "iteration 1498, train loss: 0.067914, validation loss: 0.062417\n",
      "iteration 1499, train loss: 0.067965, validation loss: 0.062452\n",
      "iteration 1500, train loss: 0.067285, validation loss: 0.062572\n",
      "iteration 1501, train loss: 0.067494, validation loss: 0.062657\n",
      "iteration 1502, train loss: 0.067732, validation loss: 0.062682\n",
      "iteration 1503, train loss: 0.067677, validation loss: 0.062631\n",
      "iteration 1504, train loss: 0.067945, validation loss: 0.062562\n",
      "iteration 1505, train loss: 0.067678, validation loss: 0.062465\n",
      "iteration 1506, train loss: 0.067344, validation loss: 0.062443\n",
      "iteration 1507, train loss: 0.067607, validation loss: 0.062499\n",
      "iteration 1508, train loss: 0.067257, validation loss: 0.062505\n",
      "iteration 1509, train loss: 0.06773, validation loss: 0.062499\n",
      "iteration 1510, train loss: 0.067626, validation loss: 0.062502\n",
      "iteration 1511, train loss: 0.067398, validation loss: 0.062472\n",
      "iteration 1512, train loss: 0.067891, validation loss: 0.06243\n",
      "iteration 1513, train loss: 0.067744, validation loss: 0.062411\n",
      "iteration 1514, train loss: 0.067837, validation loss: 0.062436\n",
      "iteration 1515, train loss: 0.067885, validation loss: 0.062481\n",
      "iteration 1516, train loss: 0.067782, validation loss: 0.062606\n",
      "iteration 1517, train loss: 0.068066, validation loss: 0.062654\n",
      "iteration 1518, train loss: 0.067772, validation loss: 0.062604\n",
      "iteration 1519, train loss: 0.067654, validation loss: 0.062542\n",
      "iteration 1520, train loss: 0.067388, validation loss: 0.06249\n",
      "iteration 1521, train loss: 0.067673, validation loss: 0.062519\n",
      "iteration 1522, train loss: 0.067554, validation loss: 0.062553\n",
      "iteration 1523, train loss: 0.067763, validation loss: 0.062569\n",
      "iteration 1524, train loss: 0.067763, validation loss: 0.062605\n",
      "iteration 1525, train loss: 0.067628, validation loss: 0.062552\n",
      "iteration 1526, train loss: 0.067664, validation loss: 0.062496\n",
      "iteration 1527, train loss: 0.067843, validation loss: 0.062587\n",
      "iteration 1528, train loss: 0.067979, validation loss: 0.0627\n",
      "iteration 1529, train loss: 0.067967, validation loss: 0.062814\n",
      "iteration 1530, train loss: 0.068075, validation loss: 0.062889\n",
      "iteration 1531, train loss: 0.067871, validation loss: 0.062787\n",
      "iteration 1532, train loss: 0.068, validation loss: 0.062643\n",
      "iteration 1533, train loss: 0.067672, validation loss: 0.062524\n",
      "iteration 1534, train loss: 0.067691, validation loss: 0.062493\n",
      "iteration 1535, train loss: 0.06778, validation loss: 0.062541\n",
      "iteration 1536, train loss: 0.06776, validation loss: 0.062648\n",
      "iteration 1537, train loss: 0.067575, validation loss: 0.062725\n",
      "iteration 1538, train loss: 0.067794, validation loss: 0.062763\n",
      "iteration 1539, train loss: 0.067707, validation loss: 0.062752\n",
      "iteration 1540, train loss: 0.067455, validation loss: 0.062691\n",
      "iteration 1541, train loss: 0.067881, validation loss: 0.062577\n",
      "iteration 1542, train loss: 0.067385, validation loss: 0.062456\n",
      "iteration 1543, train loss: 0.067504, validation loss: 0.06239\n",
      "iteration 1544, train loss: 0.068012, validation loss: 0.062546\n",
      "iteration 1545, train loss: 0.067378, validation loss: 0.062813\n",
      "iteration 1546, train loss: 0.067264, validation loss: 0.062939\n",
      "iteration 1547, train loss: 0.067967, validation loss: 0.062899\n",
      "iteration 1548, train loss: 0.067737, validation loss: 0.06281\n",
      "iteration 1549, train loss: 0.067239, validation loss: 0.062626\n",
      "iteration 1550, train loss: 0.067434, validation loss: 0.062455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1551, train loss: 0.067953, validation loss: 0.0624\n",
      "iteration 1552, train loss: 0.067502, validation loss: 0.062411\n",
      "iteration 1553, train loss: 0.067746, validation loss: 0.062426\n",
      "iteration 1554, train loss: 0.067859, validation loss: 0.062471\n",
      "iteration 1555, train loss: 0.067567, validation loss: 0.062561\n",
      "iteration 1556, train loss: 0.067731, validation loss: 0.062702\n",
      "iteration 1557, train loss: 0.067682, validation loss: 0.06277\n",
      "iteration 1558, train loss: 0.067549, validation loss: 0.062759\n",
      "iteration 1559, train loss: 0.067816, validation loss: 0.062654\n",
      "iteration 1560, train loss: 0.067849, validation loss: 0.062585\n",
      "iteration 1561, train loss: 0.067352, validation loss: 0.062519\n",
      "iteration 1562, train loss: 0.067664, validation loss: 0.062549\n",
      "iteration 1563, train loss: 0.067665, validation loss: 0.062569\n",
      "iteration 1564, train loss: 0.06763, validation loss: 0.062593\n",
      "iteration 1565, train loss: 0.067578, validation loss: 0.062611\n",
      "iteration 1566, train loss: 0.067589, validation loss: 0.062591\n",
      "iteration 1567, train loss: 0.067391, validation loss: 0.062501\n",
      "iteration 1568, train loss: 0.067873, validation loss: 0.062447\n",
      "iteration 1569, train loss: 0.067953, validation loss: 0.062393\n",
      "iteration 1570, train loss: 0.06783, validation loss: 0.062386\n",
      "iteration 1571, train loss: 0.067511, validation loss: 0.062466\n",
      "iteration 1572, train loss: 0.067979, validation loss: 0.062646\n",
      "iteration 1573, train loss: 0.067849, validation loss: 0.062782\n",
      "iteration 1574, train loss: 0.067675, validation loss: 0.062833\n",
      "iteration 1575, train loss: 0.067904, validation loss: 0.062739\n",
      "iteration 1576, train loss: 0.067547, validation loss: 0.062624\n",
      "iteration 1577, train loss: 0.067386, validation loss: 0.062501\n",
      "iteration 1578, train loss: 0.06767, validation loss: 0.062452\n",
      "iteration 1579, train loss: 0.06786, validation loss: 0.062472\n",
      "iteration 1580, train loss: 0.067743, validation loss: 0.062562\n",
      "iteration 1581, train loss: 0.067793, validation loss: 0.062659\n",
      "iteration 1582, train loss: 0.067597, validation loss: 0.062663\n",
      "iteration 1583, train loss: 0.067771, validation loss: 0.0626\n",
      "iteration 1584, train loss: 0.067982, validation loss: 0.062616\n",
      "iteration 1585, train loss: 0.068243, validation loss: 0.062566\n",
      "iteration 1586, train loss: 0.067697, validation loss: 0.06252\n",
      "iteration 1587, train loss: 0.067726, validation loss: 0.062474\n",
      "iteration 1588, train loss: 0.067678, validation loss: 0.06242\n",
      "iteration 1589, train loss: 0.067688, validation loss: 0.062397\n",
      "iteration 1590, train loss: 0.067551, validation loss: 0.062505\n",
      "iteration 1591, train loss: 0.067842, validation loss: 0.062677\n",
      "iteration 1592, train loss: 0.067389, validation loss: 0.062841\n",
      "iteration 1593, train loss: 0.067914, validation loss: 0.062889\n",
      "iteration 1594, train loss: 0.067427, validation loss: 0.062839\n",
      "iteration 1595, train loss: 0.067655, validation loss: 0.06264\n",
      "iteration 1596, train loss: 0.067626, validation loss: 0.062413\n",
      "iteration 1597, train loss: 0.067617, validation loss: \u001b[92m0.062328\u001b[0m\n",
      "iteration 1598, train loss: 0.067764, validation loss: 0.062343\n",
      "iteration 1599, train loss: 0.06766, validation loss: 0.06241\n",
      "iteration 1600, train loss: 0.067586, validation loss: 0.062519\n",
      "iteration 1601, train loss: 0.067877, validation loss: 0.062643\n",
      "iteration 1602, train loss: 0.067384, validation loss: 0.062701\n",
      "iteration 1603, train loss: 0.067615, validation loss: 0.06268\n",
      "iteration 1604, train loss: 0.067389, validation loss: 0.062579\n",
      "iteration 1605, train loss: 0.067575, validation loss: 0.062454\n",
      "iteration 1606, train loss: 0.06774, validation loss: 0.062417\n",
      "iteration 1607, train loss: 0.067322, validation loss: 0.062411\n",
      "iteration 1608, train loss: 0.067755, validation loss: 0.062448\n",
      "iteration 1609, train loss: 0.067384, validation loss: 0.062568\n",
      "iteration 1610, train loss: 0.067814, validation loss: 0.062685\n",
      "iteration 1611, train loss: 0.068018, validation loss: 0.062721\n",
      "iteration 1612, train loss: 0.068, validation loss: 0.06267\n",
      "iteration 1613, train loss: 0.067658, validation loss: 0.062497\n",
      "iteration 1614, train loss: 0.067491, validation loss: 0.062354\n",
      "iteration 1615, train loss: 0.067789, validation loss: 0.06235\n",
      "iteration 1616, train loss: 0.068156, validation loss: 0.062369\n",
      "iteration 1617, train loss: 0.067978, validation loss: 0.062428\n",
      "iteration 1618, train loss: 0.067978, validation loss: 0.062556\n",
      "iteration 1619, train loss: 0.068093, validation loss: 0.062678\n",
      "iteration 1620, train loss: 0.067432, validation loss: 0.062687\n",
      "iteration 1621, train loss: 0.067644, validation loss: 0.062661\n",
      "iteration 1622, train loss: 0.067283, validation loss: 0.062582\n",
      "iteration 1623, train loss: 0.067839, validation loss: 0.06252\n",
      "iteration 1624, train loss: 0.067869, validation loss: 0.062461\n",
      "iteration 1625, train loss: 0.067658, validation loss: 0.062506\n",
      "iteration 1626, train loss: 0.067813, validation loss: 0.062554\n",
      "iteration 1627, train loss: 0.067686, validation loss: 0.062583\n",
      "iteration 1628, train loss: 0.067488, validation loss: 0.062579\n",
      "iteration 1629, train loss: 0.067652, validation loss: 0.062555\n",
      "iteration 1630, train loss: 0.067539, validation loss: 0.062518\n",
      "iteration 1631, train loss: 0.067578, validation loss: 0.062549\n",
      "iteration 1632, train loss: 0.067887, validation loss: 0.06259\n",
      "iteration 1633, train loss: 0.067513, validation loss: 0.062592\n",
      "iteration 1634, train loss: 0.067824, validation loss: 0.062633\n",
      "iteration 1635, train loss: 0.067814, validation loss: 0.062632\n",
      "iteration 1636, train loss: 0.067857, validation loss: 0.062603\n",
      "iteration 1637, train loss: 0.067846, validation loss: 0.062542\n",
      "iteration 1638, train loss: 0.067458, validation loss: 0.062474\n",
      "iteration 1639, train loss: 0.067428, validation loss: 0.062425\n",
      "iteration 1640, train loss: 0.067795, validation loss: 0.062409\n",
      "iteration 1641, train loss: 0.067786, validation loss: 0.062487\n",
      "iteration 1642, train loss: 0.067694, validation loss: 0.062576\n",
      "iteration 1643, train loss: 0.06787, validation loss: 0.062614\n",
      "iteration 1644, train loss: 0.067422, validation loss: 0.062604\n",
      "iteration 1645, train loss: 0.067531, validation loss: 0.062581\n",
      "iteration 1646, train loss: 0.067696, validation loss: 0.06252\n",
      "iteration 1647, train loss: 0.067552, validation loss: 0.062505\n",
      "iteration 1648, train loss: 0.067847, validation loss: 0.062478\n",
      "iteration 1649, train loss: 0.067876, validation loss: 0.062513\n",
      "iteration 1650, train loss: 0.067509, validation loss: 0.062555\n",
      "iteration 1651, train loss: 0.067639, validation loss: 0.062591\n",
      "iteration 1652, train loss: 0.067163, validation loss: 0.062583\n",
      "iteration 1653, train loss: 0.067828, validation loss: 0.062553\n",
      "iteration 1654, train loss: 0.067766, validation loss: 0.062501\n",
      "iteration 1655, train loss: 0.067446, validation loss: 0.062474\n",
      "iteration 1656, train loss: 0.067706, validation loss: 0.062456\n",
      "iteration 1657, train loss: 0.067656, validation loss: 0.06247\n",
      "iteration 1658, train loss: 0.067488, validation loss: 0.062496\n",
      "iteration 1659, train loss: 0.067652, validation loss: 0.062499\n",
      "iteration 1660, train loss: 0.067338, validation loss: 0.062483\n",
      "iteration 1661, train loss: 0.067513, validation loss: 0.062459\n",
      "iteration 1662, train loss: 0.067525, validation loss: 0.062396\n",
      "iteration 1663, train loss: 0.067789, validation loss: 0.062401\n",
      "iteration 1664, train loss: 0.067486, validation loss: 0.062456\n",
      "iteration 1665, train loss: 0.067525, validation loss: 0.062492\n",
      "iteration 1666, train loss: 0.067679, validation loss: 0.062559\n",
      "iteration 1667, train loss: 0.067898, validation loss: 0.062591\n",
      "iteration 1668, train loss: 0.067542, validation loss: 0.062591\n",
      "iteration 1669, train loss: 0.067549, validation loss: 0.062565\n",
      "iteration 1670, train loss: 0.067828, validation loss: 0.062459\n",
      "iteration 1671, train loss: 0.067593, validation loss: 0.062381\n",
      "iteration 1672, train loss: 0.06797, validation loss: 0.06238\n",
      "iteration 1673, train loss: 0.067906, validation loss: 0.06246\n",
      "iteration 1674, train loss: 0.067562, validation loss: 0.062616\n",
      "iteration 1675, train loss: 0.067436, validation loss: 0.062792\n",
      "iteration 1676, train loss: 0.067671, validation loss: 0.062835\n",
      "iteration 1677, train loss: 0.068057, validation loss: 0.062783\n",
      "iteration 1678, train loss: 0.067585, validation loss: 0.062636\n",
      "iteration 1679, train loss: 0.067628, validation loss: 0.062517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1680, train loss: 0.067371, validation loss: 0.062426\n",
      "iteration 1681, train loss: 0.067735, validation loss: 0.062374\n",
      "iteration 1682, train loss: 0.067457, validation loss: 0.062379\n",
      "iteration 1683, train loss: 0.067271, validation loss: 0.062409\n",
      "iteration 1684, train loss: 0.067743, validation loss: 0.062516\n",
      "iteration 1685, train loss: 0.067841, validation loss: 0.062659\n",
      "iteration 1686, train loss: 0.067359, validation loss: 0.062738\n",
      "iteration 1687, train loss: 0.067765, validation loss: 0.062674\n",
      "iteration 1688, train loss: 0.067463, validation loss: 0.062492\n",
      "iteration 1689, train loss: 0.067442, validation loss: 0.062386\n",
      "iteration 1690, train loss: 0.067744, validation loss: 0.062379\n",
      "iteration 1691, train loss: 0.067862, validation loss: 0.062407\n",
      "iteration 1692, train loss: 0.067701, validation loss: 0.062436\n",
      "iteration 1693, train loss: 0.067511, validation loss: 0.062538\n",
      "iteration 1694, train loss: 0.067585, validation loss: 0.062621\n",
      "iteration 1695, train loss: 0.067586, validation loss: 0.062691\n",
      "iteration 1696, train loss: 0.067607, validation loss: 0.062639\n",
      "iteration 1697, train loss: 0.067964, validation loss: 0.062548\n",
      "iteration 1698, train loss: 0.06772, validation loss: 0.062405\n",
      "iteration 1699, train loss: 0.067407, validation loss: 0.062333\n",
      "iteration 1700, train loss: 0.067834, validation loss: \u001b[92m0.062322\u001b[0m\n",
      "iteration 1701, train loss: 0.067578, validation loss: 0.062395\n",
      "iteration 1702, train loss: 0.067542, validation loss: 0.06261\n",
      "iteration 1703, train loss: 0.067901, validation loss: 0.062804\n",
      "iteration 1704, train loss: 0.067983, validation loss: 0.062844\n",
      "iteration 1705, train loss: 0.067515, validation loss: 0.062715\n",
      "iteration 1706, train loss: 0.06785, validation loss: 0.062549\n",
      "iteration 1707, train loss: 0.06757, validation loss: 0.062413\n",
      "iteration 1708, train loss: 0.067787, validation loss: \u001b[92m0.062318\u001b[0m\n",
      "iteration 1709, train loss: 0.067788, validation loss: \u001b[92m0.062314\u001b[0m\n",
      "iteration 1710, train loss: 0.067676, validation loss: 0.062468\n",
      "iteration 1711, train loss: 0.067725, validation loss: 0.062634\n",
      "iteration 1712, train loss: 0.067814, validation loss: 0.062747\n",
      "iteration 1713, train loss: 0.067704, validation loss: 0.062696\n",
      "iteration 1714, train loss: 0.067547, validation loss: 0.06255\n",
      "iteration 1715, train loss: 0.067395, validation loss: 0.062368\n",
      "iteration 1716, train loss: 0.067605, validation loss: \u001b[92m0.062268\u001b[0m\n",
      "iteration 1717, train loss: 0.067745, validation loss: 0.062276\n",
      "iteration 1718, train loss: 0.067667, validation loss: 0.062491\n",
      "iteration 1719, train loss: 0.067646, validation loss: 0.062696\n",
      "iteration 1720, train loss: 0.067444, validation loss: 0.06278\n",
      "iteration 1721, train loss: 0.068174, validation loss: 0.06275\n",
      "iteration 1722, train loss: 0.067511, validation loss: 0.062664\n",
      "iteration 1723, train loss: 0.06739, validation loss: 0.062519\n",
      "iteration 1724, train loss: 0.067395, validation loss: 0.062406\n",
      "iteration 1725, train loss: 0.067511, validation loss: 0.062421\n",
      "iteration 1726, train loss: 0.067216, validation loss: 0.062441\n",
      "iteration 1727, train loss: 0.067749, validation loss: 0.062558\n",
      "iteration 1728, train loss: 0.067959, validation loss: 0.062645\n",
      "iteration 1729, train loss: 0.06793, validation loss: 0.062704\n",
      "iteration 1730, train loss: 0.06765, validation loss: 0.062656\n",
      "iteration 1731, train loss: 0.067454, validation loss: 0.062582\n",
      "iteration 1732, train loss: 0.067663, validation loss: 0.062566\n",
      "iteration 1733, train loss: 0.06742, validation loss: 0.062464\n",
      "iteration 1734, train loss: 0.06759, validation loss: 0.062428\n",
      "iteration 1735, train loss: 0.067984, validation loss: 0.06241\n",
      "iteration 1736, train loss: 0.067616, validation loss: 0.062416\n",
      "iteration 1737, train loss: 0.067582, validation loss: 0.062443\n",
      "iteration 1738, train loss: 0.06752, validation loss: 0.062503\n",
      "iteration 1739, train loss: 0.067607, validation loss: 0.062555\n",
      "iteration 1740, train loss: 0.067735, validation loss: 0.062563\n",
      "iteration 1741, train loss: 0.06796, validation loss: 0.062556\n",
      "iteration 1742, train loss: 0.067399, validation loss: 0.062499\n",
      "iteration 1743, train loss: 0.067263, validation loss: 0.062403\n",
      "iteration 1744, train loss: 0.067756, validation loss: 0.062374\n",
      "iteration 1745, train loss: 0.067454, validation loss: 0.062359\n",
      "iteration 1746, train loss: 0.067568, validation loss: 0.062371\n",
      "iteration 1747, train loss: 0.067609, validation loss: 0.062405\n",
      "iteration 1748, train loss: 0.06727, validation loss: 0.062461\n",
      "iteration 1749, train loss: 0.067807, validation loss: 0.062557\n",
      "iteration 1750, train loss: 0.067358, validation loss: 0.062578\n",
      "iteration 1751, train loss: 0.067589, validation loss: 0.062581\n",
      "iteration 1752, train loss: 0.067515, validation loss: 0.062539\n",
      "iteration 1753, train loss: 0.067214, validation loss: 0.062412\n",
      "iteration 1754, train loss: 0.067769, validation loss: 0.062395\n",
      "iteration 1755, train loss: 0.06746, validation loss: 0.062382\n",
      "iteration 1756, train loss: 0.067711, validation loss: 0.062397\n",
      "iteration 1757, train loss: 0.067367, validation loss: 0.062398\n",
      "iteration 1758, train loss: 0.067696, validation loss: 0.062432\n",
      "iteration 1759, train loss: 0.067832, validation loss: 0.06248\n",
      "iteration 1760, train loss: 0.067935, validation loss: 0.062527\n",
      "iteration 1761, train loss: 0.067641, validation loss: 0.062517\n",
      "iteration 1762, train loss: 0.068114, validation loss: 0.06254\n",
      "iteration 1763, train loss: 0.067571, validation loss: 0.062536\n",
      "iteration 1764, train loss: 0.067586, validation loss: 0.062498\n",
      "iteration 1765, train loss: 0.067759, validation loss: 0.06247\n",
      "iteration 1766, train loss: 0.067583, validation loss: 0.062484\n",
      "iteration 1767, train loss: 0.067941, validation loss: 0.062492\n",
      "iteration 1768, train loss: 0.068007, validation loss: 0.062444\n",
      "iteration 1769, train loss: 0.067421, validation loss: 0.062468\n",
      "iteration 1770, train loss: 0.067454, validation loss: 0.062521\n",
      "iteration 1771, train loss: 0.067436, validation loss: 0.062507\n",
      "iteration 1772, train loss: 0.067651, validation loss: 0.062412\n",
      "iteration 1773, train loss: 0.067304, validation loss: 0.062375\n",
      "iteration 1774, train loss: 0.06713, validation loss: 0.062481\n",
      "iteration 1775, train loss: 0.067354, validation loss: 0.062556\n",
      "iteration 1776, train loss: 0.067636, validation loss: 0.062583\n",
      "iteration 1777, train loss: 0.067421, validation loss: 0.062669\n",
      "iteration 1778, train loss: 0.067641, validation loss: 0.062664\n",
      "iteration 1779, train loss: 0.067841, validation loss: 0.062573\n",
      "iteration 1780, train loss: 0.067625, validation loss: 0.062477\n",
      "iteration 1781, train loss: 0.067371, validation loss: 0.06237\n",
      "iteration 1782, train loss: 0.067771, validation loss: 0.062342\n",
      "iteration 1783, train loss: 0.067458, validation loss: 0.062314\n",
      "iteration 1784, train loss: 0.067709, validation loss: 0.062328\n",
      "iteration 1785, train loss: 0.067173, validation loss: 0.062422\n",
      "iteration 1786, train loss: 0.06739, validation loss: 0.062575\n",
      "iteration 1787, train loss: 0.067654, validation loss: 0.062692\n",
      "iteration 1788, train loss: 0.067716, validation loss: 0.06276\n",
      "iteration 1789, train loss: 0.06777, validation loss: 0.062689\n",
      "iteration 1790, train loss: 0.067676, validation loss: 0.06252\n",
      "iteration 1791, train loss: 0.067354, validation loss: 0.062341\n",
      "iteration 1792, train loss: 0.067451, validation loss: \u001b[92m0.062246\u001b[0m\n",
      "iteration 1793, train loss: 0.067357, validation loss: \u001b[92m0.062239\u001b[0m\n",
      "iteration 1794, train loss: 0.067764, validation loss: 0.0623\n",
      "iteration 1795, train loss: 0.067632, validation loss: 0.06252\n",
      "iteration 1796, train loss: 0.067606, validation loss: 0.062758\n",
      "iteration 1797, train loss: 0.067759, validation loss: 0.062799\n",
      "iteration 1798, train loss: 0.067831, validation loss: 0.062685\n",
      "iteration 1799, train loss: 0.067394, validation loss: 0.06247\n",
      "iteration 1800, train loss: 0.067067, validation loss: 0.062277\n",
      "iteration 1801, train loss: 0.067771, validation loss: \u001b[92m0.062236\u001b[0m\n",
      "iteration 1802, train loss: 0.068065, validation loss: 0.062277\n",
      "iteration 1803, train loss: 0.067488, validation loss: 0.062409\n",
      "iteration 1804, train loss: 0.06746, validation loss: 0.062499\n",
      "iteration 1805, train loss: 0.067197, validation loss: 0.062542\n",
      "iteration 1806, train loss: 0.067374, validation loss: 0.062495\n",
      "iteration 1807, train loss: 0.067836, validation loss: 0.062386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1808, train loss: 0.067082, validation loss: 0.062299\n",
      "iteration 1809, train loss: 0.067622, validation loss: 0.062316\n",
      "iteration 1810, train loss: 0.067637, validation loss: 0.062333\n",
      "iteration 1811, train loss: 0.067385, validation loss: 0.06233\n",
      "iteration 1812, train loss: 0.067743, validation loss: 0.062322\n",
      "iteration 1813, train loss: 0.067561, validation loss: 0.062302\n",
      "iteration 1814, train loss: 0.067505, validation loss: 0.062301\n",
      "iteration 1815, train loss: 0.067707, validation loss: 0.062324\n",
      "iteration 1816, train loss: 0.067534, validation loss: 0.062346\n",
      "iteration 1817, train loss: 0.067824, validation loss: 0.06239\n",
      "iteration 1818, train loss: 0.067411, validation loss: 0.062422\n",
      "iteration 1819, train loss: 0.067773, validation loss: 0.062394\n",
      "iteration 1820, train loss: 0.067539, validation loss: 0.062333\n",
      "iteration 1821, train loss: 0.067428, validation loss: 0.062253\n",
      "iteration 1822, train loss: 0.067683, validation loss: \u001b[92m0.062222\u001b[0m\n",
      "iteration 1823, train loss: 0.067754, validation loss: 0.062277\n",
      "iteration 1824, train loss: 0.067525, validation loss: 0.062369\n",
      "iteration 1825, train loss: 0.067452, validation loss: 0.062514\n",
      "iteration 1826, train loss: 0.067493, validation loss: 0.062628\n",
      "iteration 1827, train loss: 0.067348, validation loss: 0.062639\n",
      "iteration 1828, train loss: 0.067545, validation loss: 0.062551\n",
      "iteration 1829, train loss: 0.067676, validation loss: 0.062423\n",
      "iteration 1830, train loss: 0.067422, validation loss: 0.062283\n",
      "iteration 1831, train loss: 0.067585, validation loss: 0.062235\n",
      "iteration 1832, train loss: 0.067414, validation loss: 0.062244\n",
      "iteration 1833, train loss: 0.067128, validation loss: 0.062314\n",
      "iteration 1834, train loss: 0.067532, validation loss: 0.062399\n",
      "iteration 1835, train loss: 0.067583, validation loss: 0.062456\n",
      "iteration 1836, train loss: 0.067588, validation loss: 0.062467\n",
      "iteration 1837, train loss: 0.067552, validation loss: 0.062393\n",
      "iteration 1838, train loss: 0.067503, validation loss: 0.062277\n",
      "iteration 1839, train loss: 0.067539, validation loss: \u001b[92m0.062205\u001b[0m\n",
      "iteration 1840, train loss: 0.067869, validation loss: 0.062215\n",
      "iteration 1841, train loss: 0.067342, validation loss: 0.062325\n",
      "iteration 1842, train loss: 0.067591, validation loss: 0.062511\n",
      "iteration 1843, train loss: 0.067419, validation loss: 0.062691\n",
      "iteration 1844, train loss: 0.067562, validation loss: 0.062713\n",
      "iteration 1845, train loss: 0.067675, validation loss: 0.062602\n",
      "iteration 1846, train loss: 0.067525, validation loss: 0.062474\n",
      "iteration 1847, train loss: 0.06735, validation loss: 0.062347\n",
      "iteration 1848, train loss: 0.067811, validation loss: 0.062289\n",
      "iteration 1849, train loss: 0.067928, validation loss: 0.062333\n",
      "iteration 1850, train loss: 0.067258, validation loss: 0.062416\n",
      "iteration 1851, train loss: 0.06761, validation loss: 0.062474\n",
      "iteration 1852, train loss: 0.06781, validation loss: 0.062499\n",
      "iteration 1853, train loss: 0.067661, validation loss: 0.062483\n",
      "iteration 1854, train loss: 0.067674, validation loss: 0.062445\n",
      "iteration 1855, train loss: 0.067561, validation loss: 0.062341\n",
      "iteration 1856, train loss: 0.067737, validation loss: 0.062286\n",
      "iteration 1857, train loss: 0.067396, validation loss: 0.062352\n",
      "iteration 1858, train loss: 0.067369, validation loss: 0.062459\n",
      "iteration 1859, train loss: 0.067739, validation loss: 0.062497\n",
      "iteration 1860, train loss: 0.067586, validation loss: 0.062491\n",
      "iteration 1861, train loss: 0.067484, validation loss: 0.062442\n",
      "iteration 1862, train loss: 0.067677, validation loss: 0.062355\n",
      "iteration 1863, train loss: 0.067491, validation loss: 0.062275\n",
      "iteration 1864, train loss: 0.067261, validation loss: \u001b[92m0.062201\u001b[0m\n",
      "iteration 1865, train loss: 0.067319, validation loss: \u001b[92m0.062162\u001b[0m\n",
      "iteration 1866, train loss: 0.067562, validation loss: 0.062253\n",
      "iteration 1867, train loss: 0.067464, validation loss: 0.062336\n",
      "iteration 1868, train loss: 0.067501, validation loss: 0.062455\n",
      "iteration 1869, train loss: 0.067266, validation loss: 0.062536\n",
      "iteration 1870, train loss: 0.067775, validation loss: 0.062526\n",
      "iteration 1871, train loss: 0.067737, validation loss: 0.062466\n",
      "iteration 1872, train loss: 0.067352, validation loss: 0.06232\n",
      "iteration 1873, train loss: 0.067391, validation loss: 0.062283\n",
      "iteration 1874, train loss: 0.067621, validation loss: 0.062271\n",
      "iteration 1875, train loss: 0.067837, validation loss: 0.062323\n",
      "iteration 1876, train loss: 0.067984, validation loss: 0.062435\n",
      "iteration 1877, train loss: 0.067763, validation loss: 0.062512\n",
      "iteration 1878, train loss: 0.067714, validation loss: 0.062552\n",
      "iteration 1879, train loss: 0.067679, validation loss: 0.062576\n",
      "iteration 1880, train loss: 0.067617, validation loss: 0.062545\n",
      "iteration 1881, train loss: 0.067422, validation loss: 0.062461\n",
      "iteration 1882, train loss: 0.067368, validation loss: 0.062344\n",
      "iteration 1883, train loss: 0.067588, validation loss: 0.062262\n",
      "iteration 1884, train loss: 0.067246, validation loss: 0.06223\n",
      "iteration 1885, train loss: 0.067711, validation loss: 0.062319\n",
      "iteration 1886, train loss: 0.067295, validation loss: 0.062386\n",
      "iteration 1887, train loss: 0.067519, validation loss: 0.062476\n",
      "iteration 1888, train loss: 0.067509, validation loss: 0.062584\n",
      "iteration 1889, train loss: 0.067557, validation loss: 0.062574\n",
      "iteration 1890, train loss: 0.067645, validation loss: 0.062455\n",
      "iteration 1891, train loss: 0.067613, validation loss: 0.062271\n",
      "iteration 1892, train loss: 0.067609, validation loss: \u001b[92m0.062156\u001b[0m\n",
      "iteration 1893, train loss: 0.067465, validation loss: \u001b[92m0.062134\u001b[0m\n",
      "iteration 1894, train loss: 0.06739, validation loss: 0.062201\n",
      "iteration 1895, train loss: 0.067515, validation loss: 0.062315\n",
      "iteration 1896, train loss: 0.067404, validation loss: 0.06244\n",
      "iteration 1897, train loss: 0.067557, validation loss: 0.062545\n",
      "iteration 1898, train loss: 0.067386, validation loss: 0.062528\n",
      "iteration 1899, train loss: 0.067509, validation loss: 0.062341\n",
      "iteration 1900, train loss: 0.068038, validation loss: 0.062191\n",
      "iteration 1901, train loss: 0.067349, validation loss: \u001b[92m0.062102\u001b[0m\n",
      "iteration 1902, train loss: 0.067554, validation loss: 0.062112\n",
      "iteration 1903, train loss: 0.067682, validation loss: 0.062269\n",
      "iteration 1904, train loss: 0.067781, validation loss: 0.062469\n",
      "iteration 1905, train loss: 0.067711, validation loss: 0.062599\n",
      "iteration 1906, train loss: 0.067551, validation loss: 0.062545\n",
      "iteration 1907, train loss: 0.067668, validation loss: 0.062421\n",
      "iteration 1908, train loss: 0.067923, validation loss: 0.062275\n",
      "iteration 1909, train loss: 0.068448, validation loss: 0.062244\n",
      "iteration 1910, train loss: 0.067378, validation loss: 0.062242\n",
      "iteration 1911, train loss: 0.067484, validation loss: 0.06235\n",
      "iteration 1912, train loss: 0.067826, validation loss: 0.062412\n",
      "iteration 1913, train loss: 0.067321, validation loss: 0.06237\n",
      "iteration 1914, train loss: 0.067352, validation loss: 0.062328\n",
      "iteration 1915, train loss: 0.067851, validation loss: 0.062302\n",
      "iteration 1916, train loss: 0.067509, validation loss: 0.062246\n",
      "iteration 1917, train loss: 0.067577, validation loss: 0.06227\n",
      "iteration 1918, train loss: 0.067584, validation loss: 0.062268\n",
      "iteration 1919, train loss: 0.067497, validation loss: 0.062292\n",
      "iteration 1920, train loss: 0.067639, validation loss: 0.062378\n",
      "iteration 1921, train loss: 0.067294, validation loss: 0.062461\n",
      "iteration 1922, train loss: 0.067639, validation loss: 0.062465\n",
      "iteration 1923, train loss: 0.067691, validation loss: 0.062429\n",
      "iteration 1924, train loss: 0.067433, validation loss: 0.062366\n",
      "iteration 1925, train loss: 0.067195, validation loss: 0.062296\n",
      "iteration 1926, train loss: 0.067781, validation loss: 0.062271\n",
      "iteration 1927, train loss: 0.067214, validation loss: 0.062251\n",
      "iteration 1928, train loss: 0.067774, validation loss: 0.062303\n",
      "iteration 1929, train loss: 0.067555, validation loss: 0.062336\n",
      "iteration 1930, train loss: 0.067651, validation loss: 0.062338\n",
      "iteration 1931, train loss: 0.067536, validation loss: 0.062344\n",
      "iteration 1932, train loss: 0.067515, validation loss: 0.062306\n",
      "iteration 1933, train loss: 0.067184, validation loss: 0.062298\n",
      "iteration 1934, train loss: 0.067507, validation loss: 0.062365\n",
      "iteration 1935, train loss: 0.067231, validation loss: 0.06246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1936, train loss: 0.06724, validation loss: 0.062512\n",
      "iteration 1937, train loss: 0.067366, validation loss: 0.062462\n",
      "iteration 1938, train loss: 0.067296, validation loss: 0.062327\n",
      "iteration 1939, train loss: 0.067811, validation loss: 0.062222\n",
      "iteration 1940, train loss: 0.067594, validation loss: 0.062151\n",
      "iteration 1941, train loss: 0.067141, validation loss: 0.062107\n",
      "iteration 1942, train loss: 0.067055, validation loss: 0.062157\n",
      "iteration 1943, train loss: 0.067525, validation loss: 0.062291\n",
      "iteration 1944, train loss: 0.067447, validation loss: 0.062425\n",
      "iteration 1945, train loss: 0.067666, validation loss: 0.062451\n",
      "iteration 1946, train loss: 0.067206, validation loss: 0.062314\n",
      "iteration 1947, train loss: 0.067481, validation loss: 0.062152\n",
      "iteration 1948, train loss: 0.067275, validation loss: \u001b[92m0.062066\u001b[0m\n",
      "iteration 1949, train loss: 0.067532, validation loss: \u001b[92m0.062039\u001b[0m\n",
      "iteration 1950, train loss: 0.067099, validation loss: 0.062106\n",
      "iteration 1951, train loss: 0.067597, validation loss: 0.062317\n",
      "iteration 1952, train loss: 0.067945, validation loss: 0.062469\n",
      "iteration 1953, train loss: 0.067418, validation loss: 0.062487\n",
      "iteration 1954, train loss: 0.067693, validation loss: 0.062369\n",
      "iteration 1955, train loss: 0.067361, validation loss: 0.0622\n",
      "iteration 1956, train loss: 0.067403, validation loss: 0.062073\n",
      "iteration 1957, train loss: 0.067376, validation loss: \u001b[92m0.062034\u001b[0m\n",
      "iteration 1958, train loss: 0.067203, validation loss: \u001b[92m0.062024\u001b[0m\n",
      "iteration 1959, train loss: 0.067246, validation loss: 0.062068\n",
      "iteration 1960, train loss: 0.067615, validation loss: 0.062208\n",
      "iteration 1961, train loss: 0.06738, validation loss: 0.062278\n",
      "iteration 1962, train loss: 0.067221, validation loss: 0.062215\n",
      "iteration 1963, train loss: 0.067527, validation loss: 0.062156\n",
      "iteration 1964, train loss: 0.067232, validation loss: 0.062078\n",
      "iteration 1965, train loss: 0.067404, validation loss: \u001b[92m0.062002\u001b[0m\n",
      "iteration 1966, train loss: \u001b[92m0.067007\u001b[0m, validation loss: \u001b[92m0.061995\u001b[0m\n",
      "iteration 1967, train loss: 0.067262, validation loss: 0.062082\n",
      "iteration 1968, train loss: 0.067622, validation loss: 0.062254\n",
      "iteration 1969, train loss: 0.067306, validation loss: 0.06246\n",
      "iteration 1970, train loss: 0.067529, validation loss: 0.062519\n",
      "iteration 1971, train loss: 0.067432, validation loss: 0.062411\n",
      "iteration 1972, train loss: 0.067629, validation loss: 0.062205\n",
      "iteration 1973, train loss: 0.067474, validation loss: 0.06207\n",
      "iteration 1974, train loss: 0.067332, validation loss: 0.06204\n",
      "iteration 1975, train loss: 0.067349, validation loss: 0.062085\n",
      "iteration 1976, train loss: 0.067247, validation loss: 0.062232\n",
      "iteration 1977, train loss: 0.067305, validation loss: 0.062377\n",
      "iteration 1978, train loss: 0.067057, validation loss: 0.062451\n",
      "iteration 1979, train loss: 0.067094, validation loss: 0.062376\n",
      "iteration 1980, train loss: 0.067299, validation loss: 0.06225\n",
      "iteration 1981, train loss: 0.067369, validation loss: 0.06212\n",
      "iteration 1982, train loss: 0.067254, validation loss: 0.062073\n",
      "iteration 1983, train loss: 0.067402, validation loss: 0.062119\n",
      "iteration 1984, train loss: 0.067681, validation loss: 0.06215\n",
      "iteration 1985, train loss: 0.067513, validation loss: 0.062154\n",
      "iteration 1986, train loss: 0.067335, validation loss: 0.062196\n",
      "iteration 1987, train loss: 0.067411, validation loss: 0.062191\n",
      "iteration 1988, train loss: 0.067423, validation loss: 0.062161\n",
      "iteration 1989, train loss: 0.067351, validation loss: 0.062118\n",
      "iteration 1990, train loss: 0.06702, validation loss: 0.062088\n",
      "iteration 1991, train loss: 0.067288, validation loss: 0.062074\n",
      "iteration 1992, train loss: 0.067379, validation loss: 0.062108\n",
      "iteration 1993, train loss: 0.067241, validation loss: 0.062071\n",
      "iteration 1994, train loss: 0.067554, validation loss: 0.06205\n",
      "iteration 1995, train loss: 0.067212, validation loss: 0.062038\n",
      "iteration 1996, train loss: 0.067802, validation loss: 0.062076\n",
      "iteration 1997, train loss: 0.067303, validation loss: 0.062151\n",
      "iteration 1998, train loss: 0.067358, validation loss: 0.062155\n",
      "iteration 1999, train loss: \u001b[92m0.06682\u001b[0m, validation loss: 0.062121\n",
      "iteration 2000, train loss: 0.067303, validation loss: 0.062109\n",
      "iteration 2001, train loss: 0.067275, validation loss: 0.062133\n",
      "iteration 2002, train loss: 0.06707, validation loss: 0.06214\n",
      "iteration 2003, train loss: 0.067111, validation loss: 0.062066\n",
      "iteration 2004, train loss: 0.067836, validation loss: 0.06202\n",
      "iteration 2005, train loss: 0.067318, validation loss: \u001b[92m0.06197\u001b[0m\n",
      "iteration 2006, train loss: 0.06729, validation loss: \u001b[92m0.061965\u001b[0m\n",
      "iteration 2007, train loss: 0.067156, validation loss: \u001b[92m0.061944\u001b[0m\n",
      "iteration 2008, train loss: 0.06761, validation loss: 0.061954\n",
      "iteration 2009, train loss: 0.067434, validation loss: 0.062046\n",
      "iteration 2010, train loss: 0.067534, validation loss: 0.062142\n",
      "iteration 2011, train loss: 0.06746, validation loss: 0.062168\n",
      "iteration 2012, train loss: 0.067828, validation loss: 0.062091\n",
      "iteration 2013, train loss: 0.06755, validation loss: 0.062034\n",
      "iteration 2014, train loss: 0.067238, validation loss: 0.061997\n",
      "iteration 2015, train loss: 0.067234, validation loss: 0.061968\n",
      "iteration 2016, train loss: 0.067006, validation loss: 0.061978\n",
      "iteration 2017, train loss: 0.067577, validation loss: 0.06204\n",
      "iteration 2018, train loss: 0.067369, validation loss: 0.062097\n",
      "iteration 2019, train loss: 0.067347, validation loss: 0.062096\n",
      "iteration 2020, train loss: 0.067209, validation loss: 0.062041\n",
      "iteration 2021, train loss: 0.067605, validation loss: 0.061997\n",
      "iteration 2022, train loss: 0.06743, validation loss: 0.062019\n",
      "iteration 2023, train loss: 0.067143, validation loss: 0.062038\n",
      "iteration 2024, train loss: 0.067102, validation loss: 0.062075\n",
      "iteration 2025, train loss: 0.06712, validation loss: 0.062209\n",
      "iteration 2026, train loss: 0.067337, validation loss: 0.062277\n",
      "iteration 2027, train loss: 0.067122, validation loss: 0.062213\n",
      "iteration 2028, train loss: 0.067574, validation loss: 0.062103\n",
      "iteration 2029, train loss: 0.067067, validation loss: 0.061971\n",
      "iteration 2030, train loss: 0.067217, validation loss: \u001b[92m0.061898\u001b[0m\n",
      "iteration 2031, train loss: 0.067575, validation loss: \u001b[92m0.061874\u001b[0m\n",
      "iteration 2032, train loss: 0.067068, validation loss: 0.061894\n",
      "iteration 2033, train loss: 0.067348, validation loss: 0.061972\n",
      "iteration 2034, train loss: 0.067075, validation loss: 0.062075\n",
      "iteration 2035, train loss: 0.067732, validation loss: 0.062168\n",
      "iteration 2036, train loss: 0.067051, validation loss: 0.062213\n",
      "iteration 2037, train loss: 0.067367, validation loss: 0.062111\n",
      "iteration 2038, train loss: 0.067194, validation loss: 0.061961\n",
      "iteration 2039, train loss: 0.067207, validation loss: \u001b[92m0.061871\u001b[0m\n",
      "iteration 2040, train loss: 0.067375, validation loss: \u001b[92m0.061824\u001b[0m\n",
      "iteration 2041, train loss: 0.067475, validation loss: \u001b[92m0.061819\u001b[0m\n",
      "iteration 2042, train loss: 0.067222, validation loss: 0.06189\n",
      "iteration 2043, train loss: 0.067079, validation loss: 0.061943\n",
      "iteration 2044, train loss: 0.067436, validation loss: 0.061988\n",
      "iteration 2045, train loss: 0.067256, validation loss: 0.061977\n",
      "iteration 2046, train loss: 0.067578, validation loss: 0.06196\n",
      "iteration 2047, train loss: 0.066835, validation loss: 0.061926\n",
      "iteration 2048, train loss: 0.067251, validation loss: 0.0619\n",
      "iteration 2049, train loss: 0.067371, validation loss: 0.061853\n",
      "iteration 2050, train loss: 0.067182, validation loss: 0.061848\n",
      "iteration 2051, train loss: 0.06707, validation loss: 0.06184\n",
      "iteration 2052, train loss: 0.067053, validation loss: 0.061871\n",
      "iteration 2053, train loss: 0.067282, validation loss: 0.061929\n",
      "iteration 2054, train loss: 0.067371, validation loss: 0.061928\n",
      "iteration 2055, train loss: 0.067491, validation loss: 0.061924\n",
      "iteration 2056, train loss: 0.06718, validation loss: 0.061874\n",
      "iteration 2057, train loss: 0.067326, validation loss: 0.061826\n",
      "iteration 2058, train loss: 0.066907, validation loss: \u001b[92m0.061788\u001b[0m\n",
      "iteration 2059, train loss: 0.067322, validation loss: \u001b[92m0.061757\u001b[0m\n",
      "iteration 2060, train loss: 0.066973, validation loss: 0.061766\n",
      "iteration 2061, train loss: 0.067562, validation loss: 0.061814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2062, train loss: 0.067325, validation loss: 0.061842\n",
      "iteration 2063, train loss: 0.066921, validation loss: 0.061888\n",
      "iteration 2064, train loss: 0.067113, validation loss: 0.061862\n",
      "iteration 2065, train loss: 0.066858, validation loss: 0.061817\n",
      "iteration 2066, train loss: \u001b[92m0.066806\u001b[0m, validation loss: \u001b[92m0.061722\u001b[0m\n",
      "iteration 2067, train loss: 0.066903, validation loss: \u001b[92m0.06166\u001b[0m\n",
      "iteration 2068, train loss: 0.067001, validation loss: \u001b[92m0.061635\u001b[0m\n",
      "iteration 2069, train loss: 0.067002, validation loss: \u001b[92m0.061623\u001b[0m\n",
      "iteration 2070, train loss: 0.067246, validation loss: 0.061626\n",
      "iteration 2071, train loss: 0.067139, validation loss: 0.061636\n",
      "iteration 2072, train loss: 0.066849, validation loss: 0.061624\n",
      "iteration 2073, train loss: \u001b[92m0.0667\u001b[0m, validation loss: 0.061641\n",
      "iteration 2074, train loss: 0.066754, validation loss: \u001b[92m0.061622\u001b[0m\n",
      "iteration 2075, train loss: 0.066965, validation loss: 0.061646\n",
      "iteration 2076, train loss: 0.066917, validation loss: \u001b[92m0.06161\u001b[0m\n",
      "iteration 2077, train loss: 0.067013, validation loss: \u001b[92m0.061558\u001b[0m\n",
      "iteration 2078, train loss: 0.067207, validation loss: \u001b[92m0.061477\u001b[0m\n",
      "iteration 2079, train loss: 0.067224, validation loss: \u001b[92m0.06145\u001b[0m\n",
      "iteration 2080, train loss: 0.067497, validation loss: 0.06152\n",
      "iteration 2081, train loss: 0.067139, validation loss: 0.061611\n",
      "iteration 2082, train loss: 0.066705, validation loss: 0.061641\n",
      "iteration 2083, train loss: 0.067167, validation loss: 0.06156\n",
      "iteration 2084, train loss: 0.067249, validation loss: 0.06148\n",
      "iteration 2085, train loss: 0.067004, validation loss: \u001b[92m0.061428\u001b[0m\n",
      "iteration 2086, train loss: 0.067047, validation loss: \u001b[92m0.061392\u001b[0m\n",
      "iteration 2087, train loss: 0.066726, validation loss: \u001b[92m0.061383\u001b[0m\n",
      "iteration 2088, train loss: 0.067247, validation loss: 0.061411\n",
      "iteration 2089, train loss: \u001b[92m0.066681\u001b[0m, validation loss: 0.06147\n",
      "iteration 2090, train loss: 0.06683, validation loss: 0.061472\n",
      "iteration 2091, train loss: 0.067294, validation loss: 0.061432\n",
      "iteration 2092, train loss: \u001b[92m0.06666\u001b[0m, validation loss: 0.061393\n",
      "iteration 2093, train loss: 0.066887, validation loss: 0.061385\n",
      "iteration 2094, train loss: 0.067205, validation loss: 0.061409\n",
      "iteration 2095, train loss: 0.066822, validation loss: 0.061416\n",
      "iteration 2096, train loss: \u001b[92m0.066549\u001b[0m, validation loss: 0.061507\n",
      "iteration 2097, train loss: 0.06684, validation loss: 0.061552\n",
      "iteration 2098, train loss: 0.067168, validation loss: 0.06149\n",
      "iteration 2099, train loss: 0.066791, validation loss: \u001b[92m0.061363\u001b[0m\n",
      "iteration 2100, train loss: 0.067072, validation loss: \u001b[92m0.061253\u001b[0m\n",
      "iteration 2101, train loss: 0.066837, validation loss: \u001b[92m0.061226\u001b[0m\n",
      "iteration 2102, train loss: 0.06686, validation loss: 0.061231\n",
      "iteration 2103, train loss: 0.067282, validation loss: 0.061266\n",
      "iteration 2104, train loss: 0.066713, validation loss: 0.061279\n",
      "iteration 2105, train loss: 0.066756, validation loss: 0.061333\n",
      "iteration 2106, train loss: 0.067, validation loss: 0.061306\n",
      "iteration 2107, train loss: \u001b[92m0.066437\u001b[0m, validation loss: \u001b[92m0.061212\u001b[0m\n",
      "iteration 2108, train loss: \u001b[92m0.066316\u001b[0m, validation loss: \u001b[92m0.061127\u001b[0m\n",
      "iteration 2109, train loss: 0.066747, validation loss: \u001b[92m0.061097\u001b[0m\n",
      "iteration 2110, train loss: 0.067047, validation loss: 0.061098\n",
      "iteration 2111, train loss: 0.066637, validation loss: \u001b[92m0.061078\u001b[0m\n",
      "iteration 2112, train loss: \u001b[92m0.066192\u001b[0m, validation loss: \u001b[92m0.061053\u001b[0m\n",
      "iteration 2113, train loss: 0.066497, validation loss: \u001b[92m0.060974\u001b[0m\n",
      "iteration 2114, train loss: 0.067377, validation loss: \u001b[92m0.060938\u001b[0m\n",
      "iteration 2115, train loss: 0.066629, validation loss: 0.060952\n",
      "iteration 2116, train loss: 0.066909, validation loss: 0.060997\n",
      "iteration 2117, train loss: 0.067047, validation loss: 0.061037\n",
      "iteration 2118, train loss: 0.066575, validation loss: 0.061058\n",
      "iteration 2119, train loss: 0.066634, validation loss: 0.06108\n",
      "iteration 2120, train loss: 0.066649, validation loss: 0.061027\n",
      "iteration 2121, train loss: 0.067181, validation loss: 0.060999\n",
      "iteration 2122, train loss: 0.06687, validation loss: 0.060957\n",
      "iteration 2123, train loss: 0.06713, validation loss: \u001b[92m0.060935\u001b[0m\n",
      "iteration 2124, train loss: 0.066844, validation loss: \u001b[92m0.060894\u001b[0m\n",
      "iteration 2125, train loss: 0.066786, validation loss: 0.060901\n",
      "iteration 2126, train loss: 0.067116, validation loss: 0.061063\n",
      "iteration 2127, train loss: 0.066896, validation loss: 0.061147\n",
      "iteration 2128, train loss: 0.066893, validation loss: 0.061077\n",
      "iteration 2129, train loss: 0.066998, validation loss: 0.06095\n",
      "iteration 2130, train loss: 0.067126, validation loss: 0.060917\n",
      "iteration 2131, train loss: 0.066937, validation loss: 0.060956\n",
      "iteration 2132, train loss: 0.066483, validation loss: 0.060947\n",
      "iteration 2133, train loss: 0.066814, validation loss: 0.060938\n",
      "iteration 2134, train loss: 0.066468, validation loss: 0.060991\n",
      "iteration 2135, train loss: 0.067191, validation loss: 0.061068\n",
      "iteration 2136, train loss: 0.066323, validation loss: 0.061033\n",
      "iteration 2137, train loss: 0.066596, validation loss: 0.060957\n",
      "iteration 2138, train loss: 0.066211, validation loss: 0.060903\n",
      "iteration 2139, train loss: 0.066664, validation loss: \u001b[92m0.060853\u001b[0m\n",
      "iteration 2140, train loss: 0.066552, validation loss: \u001b[92m0.060801\u001b[0m\n",
      "iteration 2141, train loss: 0.066764, validation loss: \u001b[92m0.060772\u001b[0m\n",
      "iteration 2142, train loss: 0.066828, validation loss: 0.060789\n",
      "iteration 2143, train loss: \u001b[92m0.066159\u001b[0m, validation loss: 0.060799\n",
      "iteration 2144, train loss: 0.066624, validation loss: \u001b[92m0.060764\u001b[0m\n",
      "iteration 2145, train loss: 0.066946, validation loss: \u001b[92m0.060742\u001b[0m\n",
      "iteration 2146, train loss: 0.066289, validation loss: 0.060758\n",
      "iteration 2147, train loss: 0.066348, validation loss: 0.060802\n",
      "iteration 2148, train loss: \u001b[92m0.066116\u001b[0m, validation loss: 0.060789\n",
      "iteration 2149, train loss: 0.066651, validation loss: 0.06078\n",
      "iteration 2150, train loss: 0.066818, validation loss: 0.06076\n",
      "iteration 2151, train loss: 0.066894, validation loss: \u001b[92m0.060738\u001b[0m\n",
      "iteration 2152, train loss: 0.066763, validation loss: \u001b[92m0.060717\u001b[0m\n",
      "iteration 2153, train loss: 0.066569, validation loss: \u001b[92m0.060714\u001b[0m\n",
      "iteration 2154, train loss: 0.066403, validation loss: 0.060719\n",
      "iteration 2155, train loss: 0.067191, validation loss: 0.060726\n",
      "iteration 2156, train loss: 0.066255, validation loss: 0.060737\n",
      "iteration 2157, train loss: 0.0662, validation loss: 0.060735\n",
      "iteration 2158, train loss: 0.066207, validation loss: \u001b[92m0.060711\u001b[0m\n",
      "iteration 2159, train loss: 0.066232, validation loss: \u001b[92m0.060694\u001b[0m\n",
      "iteration 2160, train loss: 0.066775, validation loss: \u001b[92m0.060659\u001b[0m\n",
      "iteration 2161, train loss: 0.06657, validation loss: \u001b[92m0.06065\u001b[0m\n",
      "iteration 2162, train loss: 0.066256, validation loss: 0.060656\n",
      "iteration 2163, train loss: 0.066398, validation loss: 0.06065\n",
      "iteration 2164, train loss: 0.066488, validation loss: \u001b[92m0.060623\u001b[0m\n",
      "iteration 2165, train loss: 0.06643, validation loss: \u001b[92m0.060621\u001b[0m\n",
      "iteration 2166, train loss: 0.06673, validation loss: 0.060638\n",
      "iteration 2167, train loss: 0.066442, validation loss: 0.060695\n",
      "iteration 2168, train loss: 0.066349, validation loss: 0.060699\n",
      "iteration 2169, train loss: 0.067089, validation loss: 0.060646\n",
      "iteration 2170, train loss: 0.066639, validation loss: \u001b[92m0.060592\u001b[0m\n",
      "iteration 2171, train loss: 0.066651, validation loss: \u001b[92m0.060568\u001b[0m\n",
      "iteration 2172, train loss: 0.066286, validation loss: \u001b[92m0.060536\u001b[0m\n",
      "iteration 2173, train loss: 0.066392, validation loss: \u001b[92m0.060532\u001b[0m\n",
      "iteration 2174, train loss: 0.066569, validation loss: 0.060567\n",
      "iteration 2175, train loss: 0.066989, validation loss: 0.060621\n",
      "iteration 2176, train loss: 0.066529, validation loss: 0.060631\n",
      "iteration 2177, train loss: \u001b[92m0.066036\u001b[0m, validation loss: 0.060577\n",
      "iteration 2178, train loss: 0.06632, validation loss: 0.06059\n",
      "iteration 2179, train loss: 0.06631, validation loss: 0.060603\n",
      "iteration 2180, train loss: 0.066258, validation loss: 0.06061\n",
      "iteration 2181, train loss: 0.066453, validation loss: 0.060636\n",
      "iteration 2182, train loss: 0.066354, validation loss: 0.060649\n",
      "iteration 2183, train loss: 0.066451, validation loss: 0.060645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2184, train loss: 0.066914, validation loss: 0.060539\n",
      "iteration 2185, train loss: 0.06637, validation loss: \u001b[92m0.060508\u001b[0m\n",
      "iteration 2186, train loss: \u001b[92m0.065916\u001b[0m, validation loss: 0.060527\n",
      "iteration 2187, train loss: 0.066164, validation loss: 0.060567\n",
      "iteration 2188, train loss: 0.066509, validation loss: 0.060514\n",
      "iteration 2189, train loss: 0.065957, validation loss: \u001b[92m0.060499\u001b[0m\n",
      "iteration 2190, train loss: 0.066154, validation loss: 0.060503\n",
      "iteration 2191, train loss: 0.06609, validation loss: 0.060508\n",
      "iteration 2192, train loss: 0.066121, validation loss: \u001b[92m0.060459\u001b[0m\n",
      "iteration 2193, train loss: 0.066224, validation loss: \u001b[92m0.060396\u001b[0m\n",
      "iteration 2194, train loss: 0.066025, validation loss: \u001b[92m0.060393\u001b[0m\n",
      "iteration 2195, train loss: 0.066594, validation loss: \u001b[92m0.060386\u001b[0m\n",
      "iteration 2196, train loss: 0.06665, validation loss: \u001b[92m0.060361\u001b[0m\n",
      "iteration 2197, train loss: 0.066938, validation loss: 0.060432\n",
      "iteration 2198, train loss: 0.066329, validation loss: 0.060523\n",
      "iteration 2199, train loss: 0.066297, validation loss: 0.060504\n",
      "iteration 2200, train loss: 0.066733, validation loss: 0.060424\n",
      "iteration 2201, train loss: 0.066167, validation loss: 0.060432\n",
      "iteration 2202, train loss: 0.066623, validation loss: 0.060495\n",
      "iteration 2203, train loss: 0.066923, validation loss: 0.06054\n",
      "iteration 2204, train loss: \u001b[92m0.065911\u001b[0m, validation loss: 0.060614\n",
      "iteration 2205, train loss: 0.066369, validation loss: 0.060703\n",
      "iteration 2206, train loss: 0.066244, validation loss: 0.060634\n",
      "iteration 2207, train loss: 0.066177, validation loss: 0.060492\n",
      "iteration 2208, train loss: 0.066167, validation loss: 0.060414\n",
      "iteration 2209, train loss: 0.066137, validation loss: 0.060443\n",
      "iteration 2210, train loss: 0.066703, validation loss: 0.060438\n",
      "iteration 2211, train loss: 0.066055, validation loss: 0.060395\n",
      "iteration 2212, train loss: 0.066172, validation loss: 0.060489\n",
      "iteration 2213, train loss: 0.065961, validation loss: 0.060612\n",
      "iteration 2214, train loss: 0.066227, validation loss: 0.060628\n",
      "iteration 2215, train loss: 0.06676, validation loss: 0.060557\n",
      "iteration 2216, train loss: 0.06613, validation loss: 0.060478\n",
      "iteration 2217, train loss: 0.066173, validation loss: 0.060458\n",
      "iteration 2218, train loss: 0.066286, validation loss: 0.06041\n",
      "iteration 2219, train loss: \u001b[92m0.065901\u001b[0m, validation loss: 0.060382\n",
      "iteration 2220, train loss: 0.066312, validation loss: 0.060367\n",
      "iteration 2221, train loss: 0.066739, validation loss: 0.060363\n",
      "iteration 2222, train loss: 0.0662, validation loss: \u001b[92m0.060348\u001b[0m\n",
      "iteration 2223, train loss: 0.066498, validation loss: \u001b[92m0.060334\u001b[0m\n",
      "iteration 2224, train loss: 0.06665, validation loss: 0.060369\n",
      "iteration 2225, train loss: 0.066469, validation loss: 0.060432\n",
      "iteration 2226, train loss: 0.066312, validation loss: 0.060486\n",
      "iteration 2227, train loss: 0.066065, validation loss: 0.060476\n",
      "iteration 2228, train loss: 0.066249, validation loss: 0.060514\n",
      "iteration 2229, train loss: 0.066283, validation loss: 0.06052\n",
      "iteration 2230, train loss: 0.06634, validation loss: 0.060484\n",
      "iteration 2231, train loss: 0.066277, validation loss: 0.060415\n",
      "iteration 2232, train loss: 0.066569, validation loss: 0.060415\n",
      "iteration 2233, train loss: 0.06662, validation loss: 0.060466\n",
      "iteration 2234, train loss: 0.066044, validation loss: 0.060477\n",
      "iteration 2235, train loss: 0.066316, validation loss: 0.060521\n",
      "iteration 2236, train loss: 0.06665, validation loss: 0.060618\n",
      "iteration 2237, train loss: \u001b[92m0.065801\u001b[0m, validation loss: 0.060563\n",
      "iteration 2238, train loss: 0.066297, validation loss: 0.060413\n",
      "iteration 2239, train loss: 0.066455, validation loss: 0.060425\n",
      "iteration 2240, train loss: 0.066425, validation loss: 0.060469\n",
      "iteration 2241, train loss: 0.066929, validation loss: 0.060457\n",
      "iteration 2242, train loss: 0.06611, validation loss: 0.060484\n",
      "iteration 2243, train loss: 0.066687, validation loss: 0.060551\n",
      "iteration 2244, train loss: 0.066337, validation loss: 0.060547\n",
      "iteration 2245, train loss: 0.06615, validation loss: 0.060501\n",
      "iteration 2246, train loss: \u001b[92m0.065689\u001b[0m, validation loss: 0.060429\n",
      "iteration 2247, train loss: 0.066066, validation loss: 0.060348\n",
      "iteration 2248, train loss: 0.066323, validation loss: \u001b[92m0.060295\u001b[0m\n",
      "iteration 2249, train loss: 0.066274, validation loss: \u001b[92m0.060286\u001b[0m\n",
      "iteration 2250, train loss: 0.066462, validation loss: 0.0603\n",
      "iteration 2251, train loss: 0.065922, validation loss: 0.060325\n",
      "iteration 2252, train loss: 0.066117, validation loss: 0.060315\n",
      "iteration 2253, train loss: 0.065961, validation loss: 0.060292\n",
      "iteration 2254, train loss: 0.066454, validation loss: 0.060298\n",
      "iteration 2255, train loss: 0.065841, validation loss: 0.060313\n",
      "iteration 2256, train loss: 0.066339, validation loss: \u001b[92m0.060272\u001b[0m\n",
      "iteration 2257, train loss: 0.06583, validation loss: \u001b[92m0.060248\u001b[0m\n",
      "iteration 2258, train loss: 0.066043, validation loss: \u001b[92m0.060204\u001b[0m\n",
      "iteration 2259, train loss: 0.066428, validation loss: \u001b[92m0.060176\u001b[0m\n",
      "iteration 2260, train loss: 0.066169, validation loss: 0.060219\n",
      "iteration 2261, train loss: 0.066621, validation loss: 0.060375\n",
      "iteration 2262, train loss: 0.066379, validation loss: 0.060381\n",
      "iteration 2263, train loss: 0.06627, validation loss: 0.060276\n",
      "iteration 2264, train loss: 0.066468, validation loss: 0.060242\n",
      "iteration 2265, train loss: 0.065971, validation loss: 0.060293\n",
      "iteration 2266, train loss: \u001b[92m0.065333\u001b[0m, validation loss: 0.060249\n",
      "iteration 2267, train loss: 0.065943, validation loss: 0.060212\n",
      "iteration 2268, train loss: 0.06575, validation loss: 0.060272\n",
      "iteration 2269, train loss: 0.066484, validation loss: 0.060325\n",
      "iteration 2270, train loss: 0.065978, validation loss: 0.060258\n",
      "iteration 2271, train loss: 0.06603, validation loss: 0.060203\n",
      "iteration 2272, train loss: 0.065537, validation loss: 0.060209\n",
      "iteration 2273, train loss: 0.066604, validation loss: 0.060259\n",
      "iteration 2274, train loss: 0.066407, validation loss: 0.060265\n",
      "iteration 2275, train loss: 0.066382, validation loss: 0.060273\n",
      "iteration 2276, train loss: 0.066185, validation loss: 0.060334\n",
      "iteration 2277, train loss: 0.066046, validation loss: 0.060312\n",
      "iteration 2278, train loss: 0.066212, validation loss: 0.060277\n",
      "iteration 2279, train loss: 0.065934, validation loss: 0.060274\n",
      "iteration 2280, train loss: 0.066186, validation loss: 0.060301\n",
      "iteration 2281, train loss: 0.065798, validation loss: 0.060304\n",
      "iteration 2282, train loss: 0.066239, validation loss: 0.060313\n",
      "iteration 2283, train loss: 0.065982, validation loss: 0.060338\n",
      "iteration 2284, train loss: 0.065676, validation loss: 0.060372\n",
      "iteration 2285, train loss: 0.065674, validation loss: 0.060374\n",
      "iteration 2286, train loss: 0.066042, validation loss: 0.060311\n",
      "iteration 2287, train loss: 0.066275, validation loss: 0.060231\n",
      "iteration 2288, train loss: 0.06623, validation loss: 0.060225\n",
      "iteration 2289, train loss: 0.065978, validation loss: 0.060225\n",
      "iteration 2290, train loss: 0.066572, validation loss: 0.060222\n",
      "iteration 2291, train loss: 0.066016, validation loss: 0.060276\n",
      "iteration 2292, train loss: 0.06651, validation loss: 0.060341\n",
      "iteration 2293, train loss: 0.066552, validation loss: 0.060274\n",
      "iteration 2294, train loss: 0.066186, validation loss: 0.060267\n",
      "iteration 2295, train loss: 0.066178, validation loss: 0.060288\n",
      "iteration 2296, train loss: 0.066253, validation loss: 0.060272\n",
      "iteration 2297, train loss: 0.06622, validation loss: 0.060256\n",
      "iteration 2298, train loss: 0.06583, validation loss: 0.060268\n",
      "iteration 2299, train loss: 0.066322, validation loss: 0.060329\n",
      "iteration 2300, train loss: 0.066412, validation loss: 0.060324\n",
      "iteration 2301, train loss: 0.065903, validation loss: 0.060228\n",
      "iteration 2302, train loss: 0.065642, validation loss: 0.060182\n",
      "iteration 2303, train loss: 0.065832, validation loss: 0.060231\n",
      "iteration 2304, train loss: 0.066491, validation loss: 0.06029\n",
      "iteration 2305, train loss: 0.066763, validation loss: 0.060298\n",
      "iteration 2306, train loss: 0.065852, validation loss: 0.060328\n",
      "iteration 2307, train loss: 0.065882, validation loss: 0.06032\n",
      "iteration 2308, train loss: 0.066017, validation loss: 0.060228\n",
      "iteration 2309, train loss: 0.066633, validation loss: \u001b[92m0.060171\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2310, train loss: 0.065856, validation loss: \u001b[92m0.060121\u001b[0m\n",
      "iteration 2311, train loss: 0.06598, validation loss: \u001b[92m0.060099\u001b[0m\n",
      "iteration 2312, train loss: 0.066278, validation loss: \u001b[92m0.060098\u001b[0m\n",
      "iteration 2313, train loss: 0.066177, validation loss: 0.06012\n",
      "iteration 2314, train loss: 0.065977, validation loss: 0.060191\n",
      "iteration 2315, train loss: 0.066121, validation loss: 0.060185\n",
      "iteration 2316, train loss: 0.06581, validation loss: 0.060122\n",
      "iteration 2317, train loss: 0.065914, validation loss: \u001b[92m0.060068\u001b[0m\n",
      "iteration 2318, train loss: 0.065956, validation loss: \u001b[92m0.06004\u001b[0m\n",
      "iteration 2319, train loss: 0.065749, validation loss: \u001b[92m0.06\u001b[0m\n",
      "iteration 2320, train loss: 0.065634, validation loss: 0.060053\n",
      "iteration 2321, train loss: 0.06592, validation loss: 0.060124\n",
      "iteration 2322, train loss: 0.065879, validation loss: 0.060092\n",
      "iteration 2323, train loss: 0.065835, validation loss: \u001b[92m0.059951\u001b[0m\n",
      "iteration 2324, train loss: 0.066237, validation loss: \u001b[92m0.05989\u001b[0m\n",
      "iteration 2325, train loss: 0.066032, validation loss: 0.059946\n",
      "iteration 2326, train loss: 0.065833, validation loss: 0.05995\n",
      "iteration 2327, train loss: 0.065974, validation loss: 0.059966\n",
      "iteration 2328, train loss: 0.065735, validation loss: 0.06009\n",
      "iteration 2329, train loss: 0.065645, validation loss: 0.060224\n",
      "iteration 2330, train loss: 0.066029, validation loss: 0.060136\n",
      "iteration 2331, train loss: 0.066697, validation loss: 0.059965\n",
      "iteration 2332, train loss: 0.065992, validation loss: 0.059981\n",
      "iteration 2333, train loss: 0.066003, validation loss: 0.060013\n",
      "iteration 2334, train loss: 0.066116, validation loss: 0.059985\n",
      "iteration 2335, train loss: 0.066048, validation loss: 0.059989\n",
      "iteration 2336, train loss: 0.065869, validation loss: 0.060122\n",
      "iteration 2337, train loss: 0.065714, validation loss: 0.060302\n",
      "iteration 2338, train loss: 0.065834, validation loss: 0.060319\n",
      "iteration 2339, train loss: 0.066337, validation loss: 0.060074\n",
      "iteration 2340, train loss: 0.066053, validation loss: 0.06\n",
      "iteration 2341, train loss: 0.065974, validation loss: 0.060041\n",
      "iteration 2342, train loss: 0.065727, validation loss: 0.06001\n",
      "iteration 2343, train loss: 0.066368, validation loss: 0.059972\n",
      "iteration 2344, train loss: 0.066138, validation loss: 0.060066\n",
      "iteration 2345, train loss: 0.065845, validation loss: 0.060208\n",
      "iteration 2346, train loss: 0.065888, validation loss: 0.060222\n",
      "iteration 2347, train loss: 0.066611, validation loss: 0.060067\n",
      "iteration 2348, train loss: 0.066328, validation loss: 0.059987\n",
      "iteration 2349, train loss: 0.066377, validation loss: 0.06\n",
      "iteration 2350, train loss: 0.065907, validation loss: 0.060013\n",
      "iteration 2351, train loss: 0.066433, validation loss: 0.060011\n",
      "iteration 2352, train loss: 0.066089, validation loss: 0.06013\n",
      "iteration 2353, train loss: 0.066052, validation loss: 0.060319\n",
      "iteration 2354, train loss: 0.066045, validation loss: 0.060289\n",
      "iteration 2355, train loss: 0.066244, validation loss: 0.060169\n",
      "iteration 2356, train loss: 0.066031, validation loss: 0.060117\n",
      "iteration 2357, train loss: 0.065833, validation loss: 0.060096\n",
      "iteration 2358, train loss: 0.065609, validation loss: 0.060071\n",
      "iteration 2359, train loss: 0.065724, validation loss: 0.060063\n",
      "iteration 2360, train loss: 0.065975, validation loss: 0.060046\n",
      "iteration 2361, train loss: 0.066501, validation loss: 0.060034\n",
      "iteration 2362, train loss: 0.065781, validation loss: 0.060064\n",
      "iteration 2363, train loss: 0.066038, validation loss: 0.060056\n",
      "iteration 2364, train loss: 0.065892, validation loss: 0.06001\n",
      "iteration 2365, train loss: 0.066134, validation loss: 0.059972\n",
      "iteration 2366, train loss: 0.065814, validation loss: 0.059944\n",
      "iteration 2367, train loss: 0.065572, validation loss: 0.059916\n",
      "iteration 2368, train loss: 0.066235, validation loss: 0.059919\n",
      "iteration 2369, train loss: 0.066373, validation loss: 0.059931\n",
      "iteration 2370, train loss: 0.065993, validation loss: 0.059928\n",
      "iteration 2371, train loss: 0.066205, validation loss: 0.059914\n",
      "iteration 2372, train loss: 0.065693, validation loss: 0.059913\n",
      "iteration 2373, train loss: 0.065807, validation loss: 0.059968\n",
      "iteration 2374, train loss: 0.066082, validation loss: 0.059951\n",
      "iteration 2375, train loss: 0.065431, validation loss: 0.059927\n",
      "iteration 2376, train loss: 0.066028, validation loss: 0.059961\n",
      "iteration 2377, train loss: 0.06562, validation loss: 0.059965\n",
      "iteration 2378, train loss: 0.065897, validation loss: 0.059913\n",
      "iteration 2379, train loss: 0.065574, validation loss: 0.059897\n",
      "iteration 2380, train loss: 0.065858, validation loss: 0.059915\n",
      "iteration 2381, train loss: 0.066075, validation loss: 0.059915\n",
      "iteration 2382, train loss: 0.065725, validation loss: 0.05992\n",
      "iteration 2383, train loss: 0.066258, validation loss: 0.059914\n",
      "iteration 2384, train loss: 0.065784, validation loss: \u001b[92m0.059887\u001b[0m\n",
      "iteration 2385, train loss: 0.065359, validation loss: \u001b[92m0.059874\u001b[0m\n",
      "iteration 2386, train loss: 0.0659, validation loss: \u001b[92m0.059864\u001b[0m\n",
      "iteration 2387, train loss: 0.065984, validation loss: 0.059886\n",
      "iteration 2388, train loss: 0.065573, validation loss: 0.05991\n",
      "iteration 2389, train loss: 0.065897, validation loss: 0.059933\n",
      "iteration 2390, train loss: 0.066405, validation loss: 0.059974\n",
      "iteration 2391, train loss: 0.065596, validation loss: 0.059943\n",
      "iteration 2392, train loss: 0.066197, validation loss: 0.059867\n",
      "iteration 2393, train loss: 0.066062, validation loss: \u001b[92m0.059829\u001b[0m\n",
      "iteration 2394, train loss: 0.066068, validation loss: 0.059838\n",
      "iteration 2395, train loss: 0.0657, validation loss: 0.059873\n",
      "iteration 2396, train loss: 0.066098, validation loss: 0.0599\n",
      "iteration 2397, train loss: 0.065333, validation loss: 0.059926\n",
      "iteration 2398, train loss: 0.065946, validation loss: 0.059922\n",
      "iteration 2399, train loss: 0.065836, validation loss: 0.059902\n",
      "iteration 2400, train loss: 0.065604, validation loss: 0.059877\n",
      "iteration 2401, train loss: 0.065759, validation loss: 0.059876\n",
      "iteration 2402, train loss: 0.065955, validation loss: 0.059909\n",
      "iteration 2403, train loss: 0.06601, validation loss: 0.059912\n",
      "iteration 2404, train loss: 0.06591, validation loss: 0.059873\n",
      "iteration 2405, train loss: 0.06608, validation loss: 0.059884\n",
      "iteration 2406, train loss: 0.066011, validation loss: 0.059907\n",
      "iteration 2407, train loss: 0.066386, validation loss: 0.059922\n",
      "iteration 2408, train loss: 0.065822, validation loss: 0.05998\n",
      "iteration 2409, train loss: 0.066464, validation loss: 0.060037\n",
      "iteration 2410, train loss: 0.066007, validation loss: 0.06006\n",
      "iteration 2411, train loss: 0.065793, validation loss: 0.060091\n",
      "iteration 2412, train loss: 0.065941, validation loss: 0.060038\n",
      "iteration 2413, train loss: 0.065828, validation loss: 0.060004\n",
      "iteration 2414, train loss: 0.066259, validation loss: 0.060014\n",
      "iteration 2415, train loss: 0.065857, validation loss: 0.060035\n",
      "iteration 2416, train loss: 0.066233, validation loss: 0.060045\n",
      "iteration 2417, train loss: 0.06563, validation loss: 0.060061\n",
      "iteration 2418, train loss: 0.065912, validation loss: 0.060055\n",
      "iteration 2419, train loss: \u001b[92m0.065302\u001b[0m, validation loss: 0.059994\n",
      "iteration 2420, train loss: 0.065909, validation loss: 0.05996\n",
      "iteration 2421, train loss: 0.06612, validation loss: 0.059917\n",
      "iteration 2422, train loss: 0.065934, validation loss: 0.05992\n",
      "iteration 2423, train loss: 0.065804, validation loss: 0.059957\n",
      "iteration 2424, train loss: 0.065893, validation loss: 0.05996\n",
      "iteration 2425, train loss: 0.066231, validation loss: 0.059943\n",
      "iteration 2426, train loss: 0.06641, validation loss: 0.059945\n",
      "iteration 2427, train loss: 0.066598, validation loss: 0.059919\n",
      "iteration 2428, train loss: 0.066065, validation loss: 0.059917\n",
      "iteration 2429, train loss: 0.065917, validation loss: 0.059948\n",
      "iteration 2430, train loss: 0.066006, validation loss: 0.059959\n",
      "iteration 2431, train loss: 0.066152, validation loss: 0.059949\n",
      "iteration 2432, train loss: 0.066104, validation loss: 0.059929\n",
      "iteration 2433, train loss: 0.065797, validation loss: 0.059891\n",
      "iteration 2434, train loss: 0.066224, validation loss: 0.059874\n",
      "iteration 2435, train loss: 0.066059, validation loss: 0.059898\n",
      "iteration 2436, train loss: 0.066041, validation loss: 0.059931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2437, train loss: 0.065728, validation loss: 0.059947\n",
      "iteration 2438, train loss: 0.065307, validation loss: 0.059966\n",
      "iteration 2439, train loss: 0.065936, validation loss: 0.059964\n",
      "iteration 2440, train loss: 0.065589, validation loss: 0.059978\n",
      "iteration 2441, train loss: 0.065651, validation loss: 0.059943\n",
      "iteration 2442, train loss: \u001b[92m0.065114\u001b[0m, validation loss: 0.059908\n",
      "iteration 2443, train loss: 0.065783, validation loss: 0.05987\n",
      "iteration 2444, train loss: 0.065974, validation loss: 0.059841\n",
      "iteration 2445, train loss: 0.066108, validation loss: \u001b[92m0.059819\u001b[0m\n",
      "iteration 2446, train loss: 0.066201, validation loss: \u001b[92m0.059802\u001b[0m\n",
      "iteration 2447, train loss: 0.065622, validation loss: 0.059822\n",
      "iteration 2448, train loss: 0.066213, validation loss: 0.059873\n",
      "iteration 2449, train loss: 0.066151, validation loss: 0.059969\n",
      "iteration 2450, train loss: 0.065894, validation loss: 0.060004\n",
      "iteration 2451, train loss: 0.065962, validation loss: 0.059996\n",
      "iteration 2452, train loss: 0.066341, validation loss: 0.059998\n",
      "iteration 2453, train loss: 0.066288, validation loss: 0.060002\n",
      "iteration 2454, train loss: 0.06572, validation loss: 0.060004\n",
      "iteration 2455, train loss: 0.065987, validation loss: 0.059998\n",
      "iteration 2456, train loss: 0.065823, validation loss: 0.059959\n",
      "iteration 2457, train loss: 0.06578, validation loss: 0.059944\n",
      "iteration 2458, train loss: 0.065825, validation loss: 0.059971\n",
      "iteration 2459, train loss: 0.066137, validation loss: 0.060013\n",
      "iteration 2460, train loss: 0.065896, validation loss: 0.060064\n",
      "iteration 2461, train loss: 0.065991, validation loss: 0.060066\n",
      "iteration 2462, train loss: 0.065374, validation loss: 0.060036\n",
      "iteration 2463, train loss: 0.06571, validation loss: 0.059991\n",
      "iteration 2464, train loss: 0.065519, validation loss: 0.059961\n",
      "iteration 2465, train loss: 0.06567, validation loss: 0.059929\n",
      "iteration 2466, train loss: 0.066029, validation loss: 0.059938\n",
      "iteration 2467, train loss: 0.066431, validation loss: 0.059946\n",
      "iteration 2468, train loss: 0.065775, validation loss: 0.059911\n",
      "iteration 2469, train loss: 0.065703, validation loss: 0.059888\n",
      "iteration 2470, train loss: 0.06653, validation loss: 0.059955\n",
      "iteration 2471, train loss: 0.065615, validation loss: 0.059974\n",
      "iteration 2472, train loss: 0.065834, validation loss: 0.059927\n",
      "iteration 2473, train loss: 0.06565, validation loss: 0.059923\n",
      "iteration 2474, train loss: 0.066104, validation loss: 0.059949\n",
      "iteration 2475, train loss: 0.066086, validation loss: 0.059922\n",
      "iteration 2476, train loss: 0.065867, validation loss: 0.059874\n",
      "iteration 2477, train loss: 0.065931, validation loss: 0.059835\n",
      "iteration 2478, train loss: 0.065774, validation loss: 0.059814\n",
      "iteration 2479, train loss: 0.065614, validation loss: 0.059816\n",
      "iteration 2480, train loss: 0.065466, validation loss: 0.059817\n",
      "iteration 2481, train loss: 0.066345, validation loss: 0.059867\n",
      "iteration 2482, train loss: 0.065931, validation loss: 0.059879\n",
      "iteration 2483, train loss: 0.066415, validation loss: 0.059871\n",
      "iteration 2484, train loss: 0.065566, validation loss: 0.059852\n",
      "iteration 2485, train loss: 0.065985, validation loss: 0.059834\n",
      "iteration 2486, train loss: 0.065235, validation loss: \u001b[92m0.059797\u001b[0m\n",
      "iteration 2487, train loss: 0.065729, validation loss: 0.059798\n",
      "iteration 2488, train loss: 0.066351, validation loss: 0.059815\n",
      "iteration 2489, train loss: 0.065778, validation loss: 0.059801\n",
      "iteration 2490, train loss: 0.066055, validation loss: \u001b[92m0.059771\u001b[0m\n",
      "iteration 2491, train loss: 0.065771, validation loss: 0.059771\n",
      "iteration 2492, train loss: 0.066437, validation loss: 0.059779\n",
      "iteration 2493, train loss: 0.065741, validation loss: 0.059779\n",
      "iteration 2494, train loss: 0.065834, validation loss: 0.059795\n",
      "iteration 2495, train loss: 0.065977, validation loss: 0.059861\n",
      "iteration 2496, train loss: 0.066124, validation loss: 0.059926\n",
      "iteration 2497, train loss: 0.065757, validation loss: 0.059888\n",
      "iteration 2498, train loss: 0.065825, validation loss: 0.059889\n",
      "iteration 2499, train loss: 0.06578, validation loss: 0.059994\n",
      "iteration 2500, train loss: 0.066374, validation loss: 0.059978\n",
      "iteration 2501, train loss: 0.065777, validation loss: 0.059895\n",
      "iteration 2502, train loss: 0.066105, validation loss: 0.059878\n",
      "iteration 2503, train loss: 0.065235, validation loss: 0.059865\n",
      "iteration 2504, train loss: 0.065306, validation loss: 0.059859\n",
      "iteration 2505, train loss: 0.065909, validation loss: 0.059851\n",
      "iteration 2506, train loss: 0.066001, validation loss: 0.059905\n",
      "iteration 2507, train loss: 0.066242, validation loss: 0.060007\n",
      "iteration 2508, train loss: 0.065976, validation loss: 0.059954\n",
      "iteration 2509, train loss: 0.065878, validation loss: 0.059854\n",
      "iteration 2510, train loss: 0.065464, validation loss: 0.059803\n",
      "iteration 2511, train loss: 0.065934, validation loss: 0.059818\n",
      "iteration 2512, train loss: 0.065863, validation loss: 0.059858\n",
      "iteration 2513, train loss: 0.066052, validation loss: 0.059871\n",
      "iteration 2514, train loss: 0.065633, validation loss: 0.059854\n",
      "iteration 2515, train loss: 0.065602, validation loss: 0.059916\n",
      "iteration 2516, train loss: 0.065763, validation loss: 0.059915\n",
      "iteration 2517, train loss: 0.065468, validation loss: 0.059891\n",
      "iteration 2518, train loss: 0.065718, validation loss: 0.059815\n",
      "iteration 2519, train loss: 0.065555, validation loss: 0.059791\n",
      "iteration 2520, train loss: 0.066019, validation loss: 0.059804\n",
      "iteration 2521, train loss: 0.066183, validation loss: \u001b[92m0.059748\u001b[0m\n",
      "iteration 2522, train loss: 0.065863, validation loss: \u001b[92m0.059709\u001b[0m\n",
      "iteration 2523, train loss: 0.06603, validation loss: 0.059772\n",
      "iteration 2524, train loss: 0.065827, validation loss: 0.059814\n",
      "iteration 2525, train loss: 0.066301, validation loss: 0.059756\n",
      "iteration 2526, train loss: 0.065734, validation loss: 0.059755\n",
      "iteration 2527, train loss: 0.066051, validation loss: 0.05988\n",
      "iteration 2528, train loss: 0.066002, validation loss: 0.059931\n",
      "iteration 2529, train loss: 0.065577, validation loss: 0.059887\n",
      "iteration 2530, train loss: 0.065839, validation loss: 0.059778\n",
      "iteration 2531, train loss: 0.06537, validation loss: 0.059804\n",
      "iteration 2532, train loss: 0.066693, validation loss: 0.059785\n",
      "iteration 2533, train loss: 0.065762, validation loss: 0.059758\n",
      "iteration 2534, train loss: 0.065611, validation loss: 0.059751\n",
      "iteration 2535, train loss: 0.065781, validation loss: 0.059768\n",
      "iteration 2536, train loss: 0.065805, validation loss: 0.059751\n",
      "iteration 2537, train loss: 0.066225, validation loss: 0.059747\n",
      "iteration 2538, train loss: 0.065763, validation loss: 0.059794\n",
      "iteration 2539, train loss: 0.065753, validation loss: 0.059861\n",
      "iteration 2540, train loss: 0.065172, validation loss: 0.059851\n",
      "iteration 2541, train loss: 0.06572, validation loss: 0.059783\n",
      "iteration 2542, train loss: 0.065699, validation loss: 0.059738\n",
      "iteration 2543, train loss: 0.065737, validation loss: 0.059793\n",
      "iteration 2544, train loss: 0.065194, validation loss: 0.059764\n",
      "iteration 2545, train loss: 0.065581, validation loss: 0.059722\n",
      "iteration 2546, train loss: 0.066009, validation loss: \u001b[92m0.059705\u001b[0m\n",
      "iteration 2547, train loss: 0.065808, validation loss: 0.059729\n",
      "iteration 2548, train loss: 0.065524, validation loss: 0.059825\n",
      "iteration 2549, train loss: 0.065887, validation loss: 0.059818\n",
      "iteration 2550, train loss: 0.06607, validation loss: 0.059745\n",
      "iteration 2551, train loss: 0.065547, validation loss: 0.059735\n",
      "iteration 2552, train loss: 0.066016, validation loss: 0.059731\n",
      "iteration 2553, train loss: 0.065631, validation loss: 0.059743\n",
      "iteration 2554, train loss: 0.065695, validation loss: 0.059763\n",
      "iteration 2555, train loss: 0.065522, validation loss: 0.059853\n",
      "iteration 2556, train loss: 0.066441, validation loss: 0.059874\n",
      "iteration 2557, train loss: 0.065583, validation loss: 0.059826\n",
      "iteration 2558, train loss: 0.06538, validation loss: 0.059755\n",
      "iteration 2559, train loss: 0.065847, validation loss: 0.059736\n",
      "iteration 2560, train loss: 0.065722, validation loss: 0.059725\n",
      "iteration 2561, train loss: 0.065777, validation loss: 0.059717\n",
      "iteration 2562, train loss: 0.065914, validation loss: 0.059738\n",
      "iteration 2563, train loss: 0.066097, validation loss: 0.059795\n",
      "iteration 2564, train loss: 0.065876, validation loss: 0.05986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2565, train loss: 0.066035, validation loss: 0.059861\n",
      "iteration 2566, train loss: 0.066401, validation loss: 0.059823\n",
      "iteration 2567, train loss: 0.065813, validation loss: 0.059773\n",
      "iteration 2568, train loss: 0.066091, validation loss: 0.05975\n",
      "iteration 2569, train loss: 0.066004, validation loss: 0.059735\n",
      "iteration 2570, train loss: 0.065749, validation loss: 0.059709\n",
      "iteration 2571, train loss: 0.065716, validation loss: 0.059792\n",
      "iteration 2572, train loss: 0.065608, validation loss: 0.059904\n",
      "iteration 2573, train loss: 0.065414, validation loss: 0.059842\n",
      "iteration 2574, train loss: 0.065259, validation loss: 0.059731\n",
      "iteration 2575, train loss: 0.065216, validation loss: \u001b[92m0.05969\u001b[0m\n",
      "iteration 2576, train loss: 0.065425, validation loss: \u001b[92m0.059684\u001b[0m\n",
      "iteration 2577, train loss: 0.065799, validation loss: \u001b[92m0.059673\u001b[0m\n",
      "iteration 2578, train loss: 0.065324, validation loss: 0.059684\n",
      "iteration 2579, train loss: 0.065708, validation loss: 0.059707\n",
      "iteration 2580, train loss: 0.065423, validation loss: 0.059716\n",
      "iteration 2581, train loss: 0.065643, validation loss: 0.059697\n",
      "iteration 2582, train loss: 0.065798, validation loss: \u001b[92m0.059662\u001b[0m\n",
      "iteration 2583, train loss: 0.065803, validation loss: \u001b[92m0.059632\u001b[0m\n",
      "iteration 2584, train loss: 0.065711, validation loss: 0.059648\n",
      "iteration 2585, train loss: 0.065899, validation loss: 0.059671\n",
      "iteration 2586, train loss: 0.065971, validation loss: 0.059653\n",
      "iteration 2587, train loss: 0.065561, validation loss: 0.059657\n",
      "iteration 2588, train loss: 0.065976, validation loss: 0.059675\n",
      "iteration 2589, train loss: 0.065677, validation loss: 0.059643\n",
      "iteration 2590, train loss: 0.066269, validation loss: \u001b[92m0.059599\u001b[0m\n",
      "iteration 2591, train loss: 0.065723, validation loss: 0.059601\n",
      "iteration 2592, train loss: 0.065815, validation loss: 0.059624\n",
      "iteration 2593, train loss: 0.065403, validation loss: 0.059673\n",
      "iteration 2594, train loss: 0.065167, validation loss: 0.059718\n",
      "iteration 2595, train loss: 0.065948, validation loss: 0.059738\n",
      "iteration 2596, train loss: 0.065472, validation loss: 0.059756\n",
      "iteration 2597, train loss: 0.065644, validation loss: 0.059678\n",
      "iteration 2598, train loss: 0.06582, validation loss: 0.0596\n",
      "iteration 2599, train loss: 0.06588, validation loss: 0.059636\n",
      "iteration 2600, train loss: 0.065864, validation loss: 0.059642\n",
      "iteration 2601, train loss: 0.0662, validation loss: 0.059627\n",
      "iteration 2602, train loss: 0.065979, validation loss: 0.059696\n",
      "iteration 2603, train loss: 0.065527, validation loss: 0.059807\n",
      "iteration 2604, train loss: 0.066078, validation loss: 0.059818\n",
      "iteration 2605, train loss: 0.065573, validation loss: 0.059791\n",
      "iteration 2606, train loss: 0.066182, validation loss: 0.059756\n",
      "iteration 2607, train loss: 0.066406, validation loss: 0.059718\n",
      "iteration 2608, train loss: 0.065367, validation loss: 0.059768\n",
      "iteration 2609, train loss: 0.065569, validation loss: 0.059778\n",
      "iteration 2610, train loss: 0.065733, validation loss: 0.05974\n",
      "iteration 2611, train loss: 0.065844, validation loss: 0.059743\n",
      "iteration 2612, train loss: 0.065782, validation loss: 0.059781\n",
      "iteration 2613, train loss: 0.065666, validation loss: 0.059791\n",
      "iteration 2614, train loss: 0.066214, validation loss: 0.05977\n",
      "iteration 2615, train loss: 0.065589, validation loss: 0.059696\n",
      "iteration 2616, train loss: 0.065756, validation loss: 0.059643\n",
      "iteration 2617, train loss: 0.065972, validation loss: 0.059632\n",
      "iteration 2618, train loss: 0.065437, validation loss: 0.059633\n",
      "iteration 2619, train loss: 0.065479, validation loss: 0.059753\n",
      "iteration 2620, train loss: 0.065973, validation loss: 0.059751\n",
      "iteration 2621, train loss: 0.065471, validation loss: 0.059692\n",
      "iteration 2622, train loss: 0.065353, validation loss: 0.059661\n",
      "iteration 2623, train loss: 0.06602, validation loss: 0.059641\n",
      "iteration 2624, train loss: 0.0657, validation loss: 0.059641\n",
      "iteration 2625, train loss: 0.065779, validation loss: 0.05964\n",
      "iteration 2626, train loss: 0.065983, validation loss: 0.05962\n",
      "iteration 2627, train loss: 0.065468, validation loss: \u001b[92m0.059594\u001b[0m\n",
      "iteration 2628, train loss: 0.065287, validation loss: 0.059622\n",
      "iteration 2629, train loss: 0.066111, validation loss: 0.059654\n",
      "iteration 2630, train loss: 0.066056, validation loss: 0.059601\n",
      "iteration 2631, train loss: 0.065417, validation loss: \u001b[92m0.05957\u001b[0m\n",
      "iteration 2632, train loss: 0.06556, validation loss: 0.059579\n",
      "iteration 2633, train loss: 0.065743, validation loss: 0.059599\n",
      "iteration 2634, train loss: 0.065531, validation loss: 0.059577\n",
      "iteration 2635, train loss: 0.065507, validation loss: \u001b[92m0.059537\u001b[0m\n",
      "iteration 2636, train loss: 0.066023, validation loss: \u001b[92m0.059533\u001b[0m\n",
      "iteration 2637, train loss: 0.065484, validation loss: 0.059563\n",
      "iteration 2638, train loss: 0.065778, validation loss: 0.059535\n",
      "iteration 2639, train loss: 0.065659, validation loss: \u001b[92m0.059496\u001b[0m\n",
      "iteration 2640, train loss: 0.066132, validation loss: 0.059546\n",
      "iteration 2641, train loss: 0.065857, validation loss: 0.059581\n",
      "iteration 2642, train loss: 0.066099, validation loss: 0.059643\n",
      "iteration 2643, train loss: 0.065944, validation loss: 0.059686\n",
      "iteration 2644, train loss: 0.065411, validation loss: 0.059728\n",
      "iteration 2645, train loss: 0.065597, validation loss: 0.05969\n",
      "iteration 2646, train loss: 0.065512, validation loss: 0.059639\n",
      "iteration 2647, train loss: 0.065173, validation loss: 0.059598\n",
      "iteration 2648, train loss: \u001b[92m0.06503\u001b[0m, validation loss: 0.059583\n",
      "iteration 2649, train loss: 0.065487, validation loss: 0.059579\n",
      "iteration 2650, train loss: \u001b[92m0.064982\u001b[0m, validation loss: 0.059578\n",
      "iteration 2651, train loss: 0.066156, validation loss: 0.059578\n",
      "iteration 2652, train loss: 0.065387, validation loss: 0.059585\n",
      "iteration 2653, train loss: 0.06561, validation loss: 0.059602\n",
      "iteration 2654, train loss: 0.065557, validation loss: 0.059581\n",
      "iteration 2655, train loss: 0.065195, validation loss: 0.059549\n",
      "iteration 2656, train loss: 0.065517, validation loss: 0.059522\n",
      "iteration 2657, train loss: 0.065807, validation loss: \u001b[92m0.059485\u001b[0m\n",
      "iteration 2658, train loss: 0.065572, validation loss: 0.059492\n",
      "iteration 2659, train loss: 0.06557, validation loss: 0.059509\n",
      "iteration 2660, train loss: 0.065803, validation loss: 0.059506\n",
      "iteration 2661, train loss: 0.065656, validation loss: 0.059486\n",
      "iteration 2662, train loss: 0.065889, validation loss: \u001b[92m0.059431\u001b[0m\n",
      "iteration 2663, train loss: 0.06623, validation loss: \u001b[92m0.059421\u001b[0m\n",
      "iteration 2664, train loss: 0.06547, validation loss: 0.05944\n",
      "iteration 2665, train loss: 0.065886, validation loss: 0.059476\n",
      "iteration 2666, train loss: 0.065977, validation loss: 0.059525\n",
      "iteration 2667, train loss: 0.065829, validation loss: 0.059576\n",
      "iteration 2668, train loss: 0.065701, validation loss: 0.059583\n",
      "iteration 2669, train loss: 0.065917, validation loss: 0.059597\n",
      "iteration 2670, train loss: 0.065612, validation loss: 0.059565\n",
      "iteration 2671, train loss: 0.065896, validation loss: 0.059604\n",
      "iteration 2672, train loss: 0.065788, validation loss: 0.059638\n",
      "iteration 2673, train loss: 0.065425, validation loss: 0.059603\n",
      "iteration 2674, train loss: 0.065505, validation loss: 0.059613\n",
      "iteration 2675, train loss: 0.065572, validation loss: 0.059677\n",
      "iteration 2676, train loss: 0.0651, validation loss: 0.05971\n",
      "iteration 2677, train loss: 0.065646, validation loss: 0.059672\n",
      "iteration 2678, train loss: 0.066232, validation loss: 0.059641\n",
      "iteration 2679, train loss: 0.0654, validation loss: 0.059601\n",
      "iteration 2680, train loss: 0.066235, validation loss: 0.059562\n",
      "iteration 2681, train loss: 0.065504, validation loss: 0.059558\n",
      "iteration 2682, train loss: 0.065831, validation loss: 0.059646\n",
      "iteration 2683, train loss: 0.066036, validation loss: 0.059698\n",
      "iteration 2684, train loss: 0.065465, validation loss: 0.059549\n",
      "iteration 2685, train loss: 0.065365, validation loss: 0.059492\n",
      "iteration 2686, train loss: 0.06541, validation loss: 0.059503\n",
      "iteration 2687, train loss: 0.065727, validation loss: 0.059539\n",
      "iteration 2688, train loss: 0.065157, validation loss: 0.05969\n",
      "iteration 2689, train loss: 0.066202, validation loss: 0.059734\n",
      "iteration 2690, train loss: 0.065502, validation loss: 0.059598\n",
      "iteration 2691, train loss: 0.065129, validation loss: 0.059468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2692, train loss: 0.064993, validation loss: 0.059501\n",
      "iteration 2693, train loss: 0.065683, validation loss: 0.059496\n",
      "iteration 2694, train loss: 0.065618, validation loss: 0.059509\n",
      "iteration 2695, train loss: 0.066031, validation loss: 0.05959\n",
      "iteration 2696, train loss: 0.06569, validation loss: 0.05958\n",
      "iteration 2697, train loss: 0.06592, validation loss: 0.059567\n",
      "iteration 2698, train loss: 0.065913, validation loss: 0.059567\n",
      "iteration 2699, train loss: 0.065396, validation loss: 0.059534\n",
      "iteration 2700, train loss: 0.065923, validation loss: 0.059551\n",
      "iteration 2701, train loss: 0.065613, validation loss: 0.059607\n",
      "iteration 2702, train loss: 0.065342, validation loss: 0.059729\n",
      "iteration 2703, train loss: 0.065255, validation loss: 0.059717\n",
      "iteration 2704, train loss: 0.066038, validation loss: 0.059556\n",
      "iteration 2705, train loss: 0.066112, validation loss: 0.059549\n",
      "iteration 2706, train loss: 0.065399, validation loss: 0.059558\n",
      "iteration 2707, train loss: 0.06582, validation loss: 0.059558\n",
      "iteration 2708, train loss: 0.065733, validation loss: 0.059578\n",
      "iteration 2709, train loss: 0.065798, validation loss: 0.059581\n",
      "iteration 2710, train loss: 0.065781, validation loss: 0.059637\n",
      "iteration 2711, train loss: 0.065512, validation loss: 0.059634\n",
      "iteration 2712, train loss: 0.065776, validation loss: 0.059547\n",
      "iteration 2713, train loss: 0.06544, validation loss: 0.059507\n",
      "iteration 2714, train loss: 0.065825, validation loss: 0.0595\n",
      "iteration 2715, train loss: 0.065896, validation loss: 0.059483\n",
      "iteration 2716, train loss: 0.065497, validation loss: 0.059521\n",
      "iteration 2717, train loss: 0.065557, validation loss: 0.059556\n",
      "iteration 2718, train loss: 0.065718, validation loss: 0.059594\n",
      "iteration 2719, train loss: 0.065447, validation loss: 0.059546\n",
      "iteration 2720, train loss: 0.065286, validation loss: 0.059498\n",
      "iteration 2721, train loss: 0.066083, validation loss: 0.059449\n",
      "iteration 2722, train loss: \u001b[92m0.064945\u001b[0m, validation loss: \u001b[92m0.059412\u001b[0m\n",
      "iteration 2723, train loss: 0.065754, validation loss: \u001b[92m0.059397\u001b[0m\n",
      "iteration 2724, train loss: 0.065824, validation loss: 0.059415\n",
      "iteration 2725, train loss: 0.065621, validation loss: 0.059398\n",
      "iteration 2726, train loss: 0.065284, validation loss: 0.059438\n",
      "iteration 2727, train loss: 0.065429, validation loss: 0.059435\n",
      "iteration 2728, train loss: 0.065992, validation loss: 0.0594\n",
      "iteration 2729, train loss: 0.06571, validation loss: \u001b[92m0.059393\u001b[0m\n",
      "iteration 2730, train loss: 0.065563, validation loss: \u001b[92m0.059379\u001b[0m\n",
      "iteration 2731, train loss: 0.065574, validation loss: \u001b[92m0.059347\u001b[0m\n",
      "iteration 2732, train loss: 0.065204, validation loss: \u001b[92m0.059333\u001b[0m\n",
      "iteration 2733, train loss: 0.065041, validation loss: 0.059372\n",
      "iteration 2734, train loss: 0.06583, validation loss: 0.059449\n",
      "iteration 2735, train loss: 0.065559, validation loss: 0.059497\n",
      "iteration 2736, train loss: 0.065671, validation loss: 0.059525\n",
      "iteration 2737, train loss: 0.065736, validation loss: 0.059485\n",
      "iteration 2738, train loss: 0.065838, validation loss: 0.05939\n",
      "iteration 2739, train loss: 0.065817, validation loss: 0.059406\n",
      "iteration 2740, train loss: 0.065048, validation loss: 0.059411\n",
      "iteration 2741, train loss: 0.065715, validation loss: 0.059428\n",
      "iteration 2742, train loss: 0.065747, validation loss: 0.05954\n",
      "iteration 2743, train loss: 0.065659, validation loss: 0.059607\n",
      "iteration 2744, train loss: 0.065737, validation loss: 0.05955\n",
      "iteration 2745, train loss: 0.065889, validation loss: 0.059488\n",
      "iteration 2746, train loss: 0.065856, validation loss: 0.0595\n",
      "iteration 2747, train loss: 0.065767, validation loss: 0.059489\n",
      "iteration 2748, train loss: 0.065758, validation loss: 0.059487\n",
      "iteration 2749, train loss: 0.066175, validation loss: 0.059507\n",
      "iteration 2750, train loss: 0.065267, validation loss: 0.059512\n",
      "iteration 2751, train loss: 0.065484, validation loss: 0.059498\n",
      "iteration 2752, train loss: 0.065684, validation loss: 0.059511\n",
      "iteration 2753, train loss: 0.065336, validation loss: 0.059564\n",
      "iteration 2754, train loss: 0.065904, validation loss: 0.059585\n",
      "iteration 2755, train loss: 0.065807, validation loss: 0.059603\n",
      "iteration 2756, train loss: 0.065366, validation loss: 0.059696\n",
      "iteration 2757, train loss: 0.065785, validation loss: 0.05967\n",
      "iteration 2758, train loss: 0.065984, validation loss: 0.059554\n",
      "iteration 2759, train loss: 0.06575, validation loss: 0.059546\n",
      "iteration 2760, train loss: 0.066062, validation loss: 0.059546\n",
      "iteration 2761, train loss: 0.065943, validation loss: 0.05955\n",
      "iteration 2762, train loss: 0.06569, validation loss: 0.059563\n",
      "iteration 2763, train loss: 0.065427, validation loss: 0.059563\n",
      "iteration 2764, train loss: 0.065818, validation loss: 0.059573\n",
      "iteration 2765, train loss: 0.065674, validation loss: 0.059534\n",
      "iteration 2766, train loss: 0.066144, validation loss: 0.059565\n",
      "iteration 2767, train loss: 0.06579, validation loss: 0.059544\n",
      "iteration 2768, train loss: 0.066054, validation loss: 0.059501\n",
      "iteration 2769, train loss: 0.06543, validation loss: 0.059658\n",
      "iteration 2770, train loss: 0.065327, validation loss: 0.059693\n",
      "iteration 2771, train loss: 0.065972, validation loss: 0.059545\n",
      "iteration 2772, train loss: 0.065721, validation loss: 0.059514\n",
      "iteration 2773, train loss: 0.065535, validation loss: 0.059579\n",
      "iteration 2774, train loss: 0.066299, validation loss: 0.059543\n",
      "iteration 2775, train loss: 0.065821, validation loss: 0.059585\n",
      "iteration 2776, train loss: 0.065893, validation loss: 0.059666\n",
      "iteration 2777, train loss: 0.065559, validation loss: 0.059655\n",
      "iteration 2778, train loss: 0.066263, validation loss: 0.059613\n",
      "iteration 2779, train loss: 0.065268, validation loss: 0.059538\n",
      "iteration 2780, train loss: 0.065632, validation loss: 0.059528\n",
      "iteration 2781, train loss: 0.065734, validation loss: 0.059505\n",
      "iteration 2782, train loss: 0.065386, validation loss: 0.059488\n",
      "iteration 2783, train loss: 0.065531, validation loss: 0.059488\n",
      "iteration 2784, train loss: 0.065807, validation loss: 0.05955\n",
      "iteration 2785, train loss: 0.065187, validation loss: 0.059634\n",
      "iteration 2786, train loss: 0.065977, validation loss: 0.059535\n",
      "iteration 2787, train loss: 0.065461, validation loss: 0.05939\n",
      "iteration 2788, train loss: 0.065007, validation loss: 0.059421\n",
      "iteration 2789, train loss: 0.065693, validation loss: 0.059376\n",
      "iteration 2790, train loss: 0.065664, validation loss: 0.059373\n",
      "iteration 2791, train loss: 0.065503, validation loss: 0.059496\n",
      "iteration 2792, train loss: 0.065671, validation loss: 0.059673\n",
      "iteration 2793, train loss: 0.06625, validation loss: 0.05956\n",
      "iteration 2794, train loss: 0.065846, validation loss: 0.059416\n",
      "iteration 2795, train loss: 0.065901, validation loss: 0.059411\n",
      "iteration 2796, train loss: 0.065402, validation loss: 0.059371\n",
      "iteration 2797, train loss: 0.066132, validation loss: 0.059457\n",
      "iteration 2798, train loss: 0.065631, validation loss: 0.059565\n",
      "iteration 2799, train loss: 0.06528, validation loss: 0.05958\n",
      "iteration 2800, train loss: 0.065424, validation loss: 0.059565\n",
      "iteration 2801, train loss: 0.065769, validation loss: 0.059512\n",
      "iteration 2802, train loss: 0.065347, validation loss: 0.059482\n",
      "iteration 2803, train loss: 0.065558, validation loss: 0.059446\n",
      "iteration 2804, train loss: 0.065381, validation loss: 0.059455\n",
      "iteration 2805, train loss: 0.065576, validation loss: 0.059477\n",
      "iteration 2806, train loss: 0.065714, validation loss: 0.059489\n",
      "iteration 2807, train loss: 0.06566, validation loss: 0.05948\n",
      "iteration 2808, train loss: 0.06546, validation loss: 0.059438\n",
      "iteration 2809, train loss: 0.066029, validation loss: 0.059457\n",
      "iteration 2810, train loss: 0.066044, validation loss: 0.059453\n",
      "iteration 2811, train loss: 0.065543, validation loss: 0.059465\n",
      "iteration 2812, train loss: 0.065782, validation loss: 0.059534\n",
      "iteration 2813, train loss: 0.066012, validation loss: 0.059591\n",
      "iteration 2814, train loss: 0.065908, validation loss: 0.05963\n",
      "iteration 2815, train loss: 0.065502, validation loss: 0.059642\n",
      "iteration 2816, train loss: 0.065378, validation loss: 0.059667\n",
      "iteration 2817, train loss: 0.0657, validation loss: 0.059653\n",
      "iteration 2818, train loss: 0.065859, validation loss: 0.059632\n",
      "iteration 2819, train loss: 0.065761, validation loss: 0.05961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2820, train loss: 0.065725, validation loss: 0.059645\n",
      "iteration 2821, train loss: 0.065601, validation loss: 0.059669\n",
      "iteration 2822, train loss: 0.065277, validation loss: 0.059701\n",
      "iteration 2823, train loss: 0.065603, validation loss: 0.059706\n",
      "iteration 2824, train loss: 0.065463, validation loss: 0.059682\n",
      "iteration 2825, train loss: 0.065314, validation loss: 0.059621\n",
      "iteration 2826, train loss: 0.065605, validation loss: 0.05957\n",
      "iteration 2827, train loss: 0.065768, validation loss: 0.05954\n",
      "iteration 2828, train loss: 0.065702, validation loss: 0.059513\n",
      "iteration 2829, train loss: 0.065705, validation loss: 0.059516\n",
      "iteration 2830, train loss: 0.065773, validation loss: 0.059553\n",
      "iteration 2831, train loss: 0.065338, validation loss: 0.059555\n",
      "iteration 2832, train loss: 0.065697, validation loss: 0.059498\n",
      "iteration 2833, train loss: 0.06545, validation loss: 0.059508\n",
      "iteration 2834, train loss: 0.06577, validation loss: 0.059521\n",
      "iteration 2835, train loss: 0.06585, validation loss: 0.05951\n",
      "iteration 2836, train loss: 0.065744, validation loss: 0.059478\n",
      "iteration 2837, train loss: 0.065368, validation loss: 0.059455\n",
      "iteration 2838, train loss: 0.065777, validation loss: 0.059481\n",
      "iteration 2839, train loss: 0.065641, validation loss: 0.059468\n",
      "iteration 2840, train loss: 0.065415, validation loss: 0.059489\n",
      "iteration 2841, train loss: 0.065287, validation loss: 0.059522\n",
      "iteration 2842, train loss: 0.064981, validation loss: 0.059538\n",
      "iteration 2843, train loss: 0.065713, validation loss: 0.05951\n",
      "iteration 2844, train loss: 0.065291, validation loss: 0.059447\n",
      "iteration 2845, train loss: 0.065611, validation loss: 0.05942\n",
      "iteration 2846, train loss: 0.065686, validation loss: 0.059349\n",
      "iteration 2847, train loss: 0.065365, validation loss: 0.059393\n",
      "iteration 2848, train loss: 0.065202, validation loss: 0.059436\n",
      "iteration 2849, train loss: 0.065258, validation loss: 0.059457\n",
      "iteration 2850, train loss: 0.065439, validation loss: 0.059426\n",
      "iteration 2851, train loss: 0.065854, validation loss: 0.059424\n",
      "iteration 2852, train loss: 0.066057, validation loss: 0.059424\n",
      "iteration 2853, train loss: 0.065683, validation loss: 0.059401\n",
      "iteration 2854, train loss: 0.065555, validation loss: 0.059365\n",
      "iteration 2855, train loss: 0.065787, validation loss: 0.05937\n",
      "iteration 2856, train loss: 0.065683, validation loss: 0.059418\n",
      "iteration 2857, train loss: 0.066078, validation loss: 0.059479\n",
      "iteration 2858, train loss: 0.065792, validation loss: 0.059527\n",
      "iteration 2859, train loss: 0.065682, validation loss: 0.059565\n",
      "iteration 2860, train loss: 0.06559, validation loss: 0.059655\n",
      "iteration 2861, train loss: 0.065798, validation loss: 0.0596\n",
      "iteration 2862, train loss: 0.065722, validation loss: 0.059533\n",
      "iteration 2863, train loss: 0.066057, validation loss: 0.059463\n",
      "iteration 2864, train loss: 0.06536, validation loss: 0.059469\n",
      "iteration 2865, train loss: 0.066246, validation loss: 0.059498\n",
      "iteration 2866, train loss: 0.065658, validation loss: 0.059521\n",
      "iteration 2867, train loss: 0.065759, validation loss: 0.059614\n",
      "iteration 2868, train loss: 0.065901, validation loss: 0.059733\n",
      "iteration 2869, train loss: 0.065867, validation loss: 0.059672\n",
      "iteration 2870, train loss: 0.065653, validation loss: 0.05957\n",
      "iteration 2871, train loss: 0.065308, validation loss: 0.059547\n",
      "iteration 2872, train loss: 0.065806, validation loss: 0.05952\n",
      "iteration 2873, train loss: 0.065695, validation loss: 0.059527\n",
      "iteration 2874, train loss: 0.065291, validation loss: 0.059555\n",
      "iteration 2875, train loss: 0.065584, validation loss: 0.059549\n",
      "iteration 2876, train loss: 0.065964, validation loss: 0.059543\n",
      "iteration 2877, train loss: 0.065547, validation loss: 0.059563\n",
      "iteration 2878, train loss: 0.065591, validation loss: 0.059575\n",
      "iteration 2879, train loss: 0.065679, validation loss: 0.059543\n",
      "iteration 2880, train loss: 0.065708, validation loss: 0.059412\n",
      "iteration 2881, train loss: 0.065294, validation loss: 0.05939\n",
      "iteration 2882, train loss: 0.065837, validation loss: 0.059421\n",
      "iteration 2883, train loss: 0.065315, validation loss: 0.059352\n",
      "iteration 2884, train loss: 0.065458, validation loss: \u001b[92m0.059322\u001b[0m\n",
      "iteration 2885, train loss: 0.065967, validation loss: 0.059344\n",
      "iteration 2886, train loss: 0.065715, validation loss: \u001b[92m0.059322\u001b[0m\n",
      "iteration 2887, train loss: 0.065691, validation loss: 0.059346\n",
      "iteration 2888, train loss: 0.065312, validation loss: 0.0594\n",
      "iteration 2889, train loss: 0.065403, validation loss: 0.059462\n",
      "iteration 2890, train loss: 0.065351, validation loss: 0.059391\n",
      "iteration 2891, train loss: 0.065951, validation loss: 0.059363\n",
      "iteration 2892, train loss: 0.065305, validation loss: 0.059378\n",
      "iteration 2893, train loss: 0.065328, validation loss: 0.059417\n",
      "iteration 2894, train loss: 0.065904, validation loss: 0.059434\n",
      "iteration 2895, train loss: 0.065295, validation loss: 0.059378\n",
      "iteration 2896, train loss: 0.065643, validation loss: 0.059451\n",
      "iteration 2897, train loss: 0.065072, validation loss: 0.059428\n",
      "iteration 2898, train loss: 0.065383, validation loss: 0.059372\n",
      "iteration 2899, train loss: 0.0656, validation loss: 0.059382\n",
      "iteration 2900, train loss: 0.06565, validation loss: 0.059376\n",
      "iteration 2901, train loss: 0.06547, validation loss: 0.059352\n",
      "iteration 2902, train loss: 0.066022, validation loss: 0.059353\n",
      "iteration 2903, train loss: 0.066116, validation loss: 0.05942\n",
      "iteration 2904, train loss: 0.065615, validation loss: 0.059406\n",
      "iteration 2905, train loss: 0.065381, validation loss: 0.059394\n",
      "iteration 2906, train loss: 0.066008, validation loss: 0.059406\n",
      "iteration 2907, train loss: 0.065374, validation loss: 0.05941\n",
      "iteration 2908, train loss: 0.065512, validation loss: 0.059387\n",
      "iteration 2909, train loss: 0.065393, validation loss: 0.059397\n",
      "iteration 2910, train loss: 0.065548, validation loss: 0.059419\n",
      "iteration 2911, train loss: 0.065917, validation loss: 0.059422\n",
      "iteration 2912, train loss: 0.065195, validation loss: 0.059433\n",
      "iteration 2913, train loss: 0.066041, validation loss: 0.059455\n",
      "iteration 2914, train loss: 0.065968, validation loss: 0.059475\n",
      "iteration 2915, train loss: 0.065502, validation loss: 0.059468\n",
      "iteration 2916, train loss: 0.065633, validation loss: 0.059441\n",
      "iteration 2917, train loss: 0.065714, validation loss: 0.059446\n",
      "iteration 2918, train loss: 0.065363, validation loss: 0.059478\n",
      "iteration 2919, train loss: 0.065365, validation loss: 0.059513\n",
      "iteration 2920, train loss: 0.065858, validation loss: 0.059538\n",
      "iteration 2921, train loss: 0.065281, validation loss: 0.059509\n",
      "iteration 2922, train loss: 0.065793, validation loss: 0.059489\n",
      "iteration 2923, train loss: 0.065628, validation loss: 0.059467\n",
      "iteration 2924, train loss: 0.065613, validation loss: 0.059394\n",
      "iteration 2925, train loss: 0.065504, validation loss: 0.059381\n",
      "iteration 2926, train loss: 0.06545, validation loss: 0.059355\n",
      "iteration 2927, train loss: 0.065957, validation loss: 0.059361\n",
      "iteration 2928, train loss: 0.065473, validation loss: 0.059385\n",
      "iteration 2929, train loss: 0.065472, validation loss: 0.059403\n",
      "iteration 2930, train loss: 0.065407, validation loss: 0.059355\n",
      "iteration 2931, train loss: 0.065456, validation loss: 0.059362\n",
      "iteration 2932, train loss: 0.06507, validation loss: 0.059362\n",
      "iteration 2933, train loss: 0.06538, validation loss: 0.059368\n",
      "iteration 2934, train loss: 0.065064, validation loss: 0.059324\n",
      "iteration 2935, train loss: 0.065411, validation loss: 0.059334\n",
      "iteration 2936, train loss: 0.065605, validation loss: 0.05934\n",
      "iteration 2937, train loss: 0.065423, validation loss: 0.059386\n",
      "iteration 2938, train loss: 0.065843, validation loss: 0.05937\n",
      "iteration 2939, train loss: 0.06593, validation loss: 0.059355\n",
      "iteration 2940, train loss: 0.065577, validation loss: 0.059426\n",
      "iteration 2941, train loss: 0.065628, validation loss: 0.059459\n",
      "iteration 2942, train loss: 0.065199, validation loss: 0.059422\n",
      "iteration 2943, train loss: 0.065799, validation loss: 0.059377\n",
      "iteration 2944, train loss: 0.065115, validation loss: 0.059399\n",
      "iteration 2945, train loss: 0.065928, validation loss: 0.059431\n",
      "iteration 2946, train loss: 0.066058, validation loss: 0.059462\n",
      "iteration 2947, train loss: 0.065321, validation loss: 0.059447\n",
      "iteration 2948, train loss: 0.065015, validation loss: 0.059419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2949, train loss: 0.065385, validation loss: 0.059368\n",
      "iteration 2950, train loss: 0.065302, validation loss: 0.059336\n",
      "iteration 2951, train loss: 0.065634, validation loss: \u001b[92m0.059299\u001b[0m\n",
      "iteration 2952, train loss: 0.066098, validation loss: \u001b[92m0.059265\u001b[0m\n",
      "iteration 2953, train loss: 0.065279, validation loss: 0.059282\n",
      "iteration 2954, train loss: 0.066162, validation loss: 0.059279\n",
      "iteration 2955, train loss: 0.065193, validation loss: 0.059298\n",
      "iteration 2956, train loss: 0.065272, validation loss: 0.059297\n",
      "iteration 2957, train loss: 0.065351, validation loss: 0.059288\n",
      "iteration 2958, train loss: 0.065405, validation loss: 0.059282\n",
      "iteration 2959, train loss: 0.066084, validation loss: 0.059306\n",
      "iteration 2960, train loss: 0.065361, validation loss: 0.059338\n",
      "iteration 2961, train loss: 0.065578, validation loss: 0.059366\n",
      "iteration 2962, train loss: 0.065521, validation loss: 0.059357\n",
      "iteration 2963, train loss: 0.06589, validation loss: 0.059345\n",
      "iteration 2964, train loss: 0.065317, validation loss: 0.059351\n",
      "iteration 2965, train loss: 0.066048, validation loss: 0.059358\n",
      "iteration 2966, train loss: 0.065548, validation loss: 0.059401\n",
      "iteration 2967, train loss: 0.065688, validation loss: 0.059412\n",
      "iteration 2968, train loss: 0.065789, validation loss: 0.059379\n",
      "iteration 2969, train loss: 0.065593, validation loss: 0.059397\n",
      "iteration 2970, train loss: 0.065397, validation loss: 0.059392\n",
      "iteration 2971, train loss: \u001b[92m0.064852\u001b[0m, validation loss: 0.059394\n",
      "iteration 2972, train loss: 0.065696, validation loss: 0.059398\n",
      "iteration 2973, train loss: 0.065126, validation loss: 0.059391\n",
      "iteration 2974, train loss: 0.065788, validation loss: 0.059377\n",
      "iteration 2975, train loss: 0.06565, validation loss: 0.059379\n",
      "iteration 2976, train loss: 0.065776, validation loss: 0.059462\n",
      "iteration 2977, train loss: 0.066033, validation loss: 0.05943\n",
      "iteration 2978, train loss: 0.064953, validation loss: 0.059402\n",
      "iteration 2979, train loss: 0.065563, validation loss: 0.059417\n",
      "iteration 2980, train loss: 0.06612, validation loss: 0.059478\n",
      "iteration 2981, train loss: 0.065793, validation loss: 0.059495\n",
      "iteration 2982, train loss: 0.06563, validation loss: 0.059348\n",
      "iteration 2983, train loss: 0.065663, validation loss: 0.059355\n",
      "iteration 2984, train loss: 0.066146, validation loss: 0.059337\n",
      "iteration 2985, train loss: 0.065889, validation loss: 0.059339\n",
      "iteration 2986, train loss: 0.065147, validation loss: 0.059372\n",
      "iteration 2987, train loss: 0.065571, validation loss: 0.059362\n",
      "iteration 2988, train loss: 0.065396, validation loss: 0.059331\n",
      "iteration 2989, train loss: 0.065347, validation loss: 0.059322\n",
      "iteration 2990, train loss: 0.065661, validation loss: 0.059314\n",
      "iteration 2991, train loss: 0.065159, validation loss: 0.059314\n",
      "iteration 2992, train loss: 0.065773, validation loss: 0.059324\n",
      "iteration 2993, train loss: 0.066702, validation loss: 0.059315\n",
      "iteration 2994, train loss: 0.065501, validation loss: 0.059354\n",
      "iteration 2995, train loss: 0.065107, validation loss: 0.059423\n",
      "iteration 2996, train loss: 0.065692, validation loss: 0.059449\n",
      "iteration 2997, train loss: 0.065894, validation loss: 0.059422\n",
      "iteration 2998, train loss: 0.06523, validation loss: 0.059412\n",
      "iteration 2999, train loss: 0.065367, validation loss: 0.05937\n"
     ]
    }
   ],
   "source": [
    "rnn_trained, train_losses, val_losses, net_params = trainer.run_training(train_mask=mask, same_batch=same_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1769b0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2IAAAEvCAYAAAAn9nIJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxkUlEQVR4nO3dd3hUVf4/8PckmUnPpDcSQuiEhFASmiKgAoIogh0EFNRVsC0qIuqCZb+oq64i4Lq6q7IqNor8pKsUaYIQlBBCDaSS3uskc35/HO9MhpmENDLJzPv1PPNMcuvn3jPlfuace45KCCFARERERERE7cbB2gEQERERERHZGyZiRERERERE7YyJGBERERERUTtjIkZERERERNTOmIgRERERERG1MyZiRERERERE7YyJGBERERERUTtjIkZERERERNTOnKwdQGen1+uRmZkJT09PqFQqa4dDRERERERWIoRAaWkpQkND4eDQeJ0XE7FWyszMRHh4uLXDICIiIiKiDiItLQ1hYWGNLsNErJU8PT0ByJPt5eVl5WgAnU6H7du3Y/z48VCr1dYOh9oAy9Q2sVxtD8vUNrFcbQ/L1PZ0pDItKSlBeHi4IUdoDBOxVlKaI3p5eXWYRMzNzQ1eXl5WfyFS22CZ2iaWq+1hmdomlqvtYZnano5Ypk25ZYmddRAREREREbUzJmIAfvjhB/Tp0we9evXCxx9/bO1wiIiIiIjIxtl908Ta2losWLAAO3fuhJeXFwYPHoxp06bB19fX2qEREREREZGNsvsasUOHDqF///7o0qULPD09MWnSJGzbts3aYRERERERkQ3r9InYnj17cMsttyA0NBQqlQobNmwwW2bVqlWIjIyEi4sLhgwZgl9++cUwLzMzE126dDH8HxYWhoyMjPYInYiIiIiI7FSnT8TKy8sRGxuLFStWWJz/9ddf46mnnsILL7yAhIQEjBo1ChMnTkRqaioAOeja5TgwMxERERERXU2d/h6xiRMnYuLEiQ3Of+eddzB37lw8+OCDAIB3330X27ZtwwcffIBly5ahS5cuJjVg6enpGDZsWIPbq66uRnV1teH/kpISALLbTJ1O19rDaRXV1q1wXLwYsSEh0I0bZ9VYqO0orytrv76obbFcbQ/L1DaxXG0Py9T2dKQybU4MnT4Ra0xNTQ2OHDmCRYsWmUwfP3489u/fDwAYOnQoEhMTkZGRAS8vL2zevBl/+9vfGtzmsmXL8PLLL5tN3759O9zc3Nr2AJopdO9exCcmwl2lwo4dO6waC7U9lqltYrnaHpZp4xwcHODg0Lka5Dg5OWHnzp3WDoPaEMvU9lyNMtXr9dDr9c1ap6KiosnL2nQilpeXh7q6OgQFBZlMDwoKwqVLlwDIQnv77bcxduxY6PV6LFy4EH5+fg1u8/nnn8eCBQsM/yujZ48fP97qAzqrqqoAAA61tRg3blyHGdCOWken02HHjh0sUxvDcrU9LNPGlZaWoqCgADU1NdYOpVmEEKiqqoKLiwtvXbARLFPbczXLVKPRwNfXF56enk1aXmkt1xQ2nYgpLi8QIYTJtFtvvRW33nprk7bl7OwMZ2dns+lqtdr6X7wuLgAAlV7fMeKhNsUytU0sV9vDMjVXUlKC7OxseHh4IDAwEGq1utNcAOv1epSVlcHDw6PT1eSRZSxT23M1ylQIAZ1Oh+LiYmRnZ8PJyalJlS7N+fy36UTM398fjo6OhtovRU5OjlktmU34s+BVdXVWDoSIiMgoLy8PHh4eCAsL6zQJmEKv16OmpgYuLi68aLcRLFPbc7XK1NXVFZ6enkhPT0deXl6bt36z6VefRqPBkCFDzNrr79ixAyNHjrRSVFeRk8yrmYgREVFHodPpUF1dDa1W2+mSMCIilUoFrVaL6urqNu8MpNPXiJWVleHs2bOG/1NSUnDs2DH4+vqia9euWLBgAWbOnIm4uDiMGDEC//73v5GamopHHnnEilFfJX8mYg5MxIiIqIOo+/M7ic01iaizUj6/6urq2vSzrNMnYr/99hvGjh1r+F/pSGP27Nn49NNPcffddyM/Px+vvPIKsrKyEB0djc2bNyMiIsJaIV89rBEjIqIOirVhRNRZXa3Pr06fiI0ZM8bioMz1zZs3D/PmzWuniKyI94gREREREXUKNn2P2NW0cuVKREVFIT4+3tqhGLFpIhERERFRp8BErIXmz5+PpKQkHD582NqhGLFpIhERERFRp8BEzJYwESMiIqJm2rVrF1QqFcaMGdMu++vWrRtUKhUuXLjQLvu72qqqqhAREYGoqCjo9XqTeSqVivdHtsCnn34KlUqF+++/v022N2fOHDg5OSE5OblNttdWmIjZkj/vEXO47EOAiIiIOrZ3330XS5cuRVFRkbVDoWZ6//33kZqaihdffJHjknVQixcvBgA8//zzVo7EFF8ttkSpEauttXIgRERE1BzvvvsuXn75ZaskYm5ubujTpw+6du3a7vvu7EpKSrBs2TJ0794dd999t7XDoQb07NkTd955JzZs2ICDBw9aOxwDJmK2hE0TiYiIqJmGDh2K5ORkrF692tqhdDpffPEFCgsLMXPmTDg6Olo7HGrE7NmzAcgO9zoKJmK2xNMTAOBUUwNUVlo5GCIiIiLb9tFHHwEA7r33XitHQldy4403wt/fH2vXru0wTYCZiNmQbJ0vwlXpmIa1SPhPgrXDISIioitQOiW4ePEiACAyMtLQwYNKpcKuXbsAmHaoUVtbizfffBMxMTFwc3NDt27dDNtLTEzEkiVLMGLECISEhECj0SAkJATTpk3D/v37LcbQUGcdFy5cgEqlMmz/888/R1xcHNzc3ODr64s777wT58+fb9PzodPp8P7772Po0KHw8vKCu7s7YmNj8fe//x0VFRUW10lMTMSMGTMQHh4OjUYDb29v9OrVC9OnT8fWrVtNlhVCYPXq1Zg0aRJ8fX2h0WgQHByMIUOGYOHChUhPT29yrKdPn0ZCQgJ69OiBPn36NPtYy8vL8dprr2HAgAFwd3eHl5cXhg0bhpUrV6K2kdtMfvrpJ1x//fXw8vKCt7c3brjhBvz8889m5dVUFy9exF/+8hd0794dzs7O8PT0RPfu3TF16lR89dVXFtfJyMjAggULEBUVBXd3d2i1WsTExOCZZ57BmTNnTJY9ePAgFi5ciLi4OAQGBsLZ2Rnh4eGYOXMmTpw40axYFQUFBXjhhRcQHR1t2P+4cePw0UcfmXWYonBycsKECRNQWVmJjRs3tmi/bU5QqxQXFwsAori42NqhiL//XQhAPrxQJE59fsjaIVEbqKmpERs2bBA1NTXWDoXaEMvV9rBMLausrBRJSUmisrLS2qG0SF1dnSgsLBR1dXVXZfubN28W11xzjXB2dhYARFxcnLjmmmsMj6NHjwohhNi5c6cAIK677jpx8803CwCiR48eYsiQIaJ///6G7d1www0CgPD29hb9+vUTgwcPFv7+/gKAcHR0FF988YVZDMq2R48ebTI9JSVFABARERFi0aJFhr9jY2MN8YaEhIjc3NxmHXNERIQAIFJSUkymV1RUiOuvv14AEABEv379xIABA4SDg4MAIAYOHCjy8vJM1vn111+Fq6urACC0Wq2IjY0V0dHRQqvVCgBiypQpJss//fTThu137dpVxMfHi8jISKHRaAQAsX79+iYfx0cffSQAiHvvvbfBZZR9XS4nJ0fExMQIAMLBwUEMGDBA9OvXz7D8uHHjLL5nPvvsM6FSqQQA4e/vL+Lj44Wfn59wcHAQ//jHPwxl1FQpKSmG14ebm5uIiYkRAwcOFL6+vgKAiI2NNVvnxx9/FF5eXgKAUKvVYsCAASI6Olq4ubkJAGLJkiUmy/fo0UMAEH5+fiI6OlrExsYaysfV1VXs3LnTbB+ffPKJACBmz55tNi8xMVF06dJFABAajUZERUWJHj16GM7LHXfcIfR6vcXjfe+99wQAMXfu3CafIyGa9znWnNyAiVgLrVixQvTr10/07t27wyRiRUVCvP9WmeimuSgAISZofhKivNzaYVEr8eLONrFcbQ/L1DImYk3TUHKiUJIlR0dHERgYKPbv32+YV//cfvvtt+KPP/4wWVev14sNGzYIDw8P4eXlJUpKSixuu6FEzMnJSXh5eYnNmzcb5mVlZYkBAwYIAOK5555rk2NVkqTQ0FBx5MgRw/QzZ86Ivn37CgDirrvuMlln8uTJAoBYvHixqK6uNpl3+PBhk8QzJydHODg4CK1WK7Zs2WJSppWVlWLNmjXi999/b/JxPPDAAwKAeOuttxpcpqFE7PbbbxcARP/+/cXZs2dNYg4KChIAxMKFC03WuXjxoiHZefHFF0Vtba0QQgidTicWLVok1Gp1sxOxxx57zJDwlJaWmsw7efKk+PDDD81iUJKoWbNmifz8fMO8uro68cMPP4iNGzearPPZZ5+Jc+fOmUzT6XTi448/Fk5OTqJ79+5m76+GErGysjJDYvfEE08Yrr/r6urEgQMHRP/+/QUAsWLFCovHu3//fgFA9OnT58onpx4mYh1UR6oRE0JeCPz7/U3CEToBCHH02S+tHRK1Ei/ubBPL1fawTC274gWMXi9EWVmHfdSVlIjC9HRRV1JiPr+BX91boqmJGACxdu3aFu3jxRdfFADMasWulIgBEG+//bbZ9jZu3CgAiAEDBjQrDkvHWlxcbEgyLNVKHTp0SAAQKpXKJHHp06dPk6/DDhw4IACI2267rU2Sa6X2zlIto8JSInb69GlD7Y1S41nfN998IwAId3d3k6RZqZW88cYbLe5r9OjRzU7EJkyYIAA0OQGdN2+eACBuuOGGBmudmuO+++4TAMS+fftMpjeUiC1fvlwAEFOnTjWZrvxgkpCQIFQqlejevbvF/SmvaRcXl2bFebUSMaemNmGkziMwXIepsefx3e+98fWXtRj0prUjIiIiakBFBeDhYe0oGuQAwLuhmWVlgLt7+wUDQKvVYsqUKY0uk5qaii+//BJHjx5FXl4eampqAAA5OTkAgN9//x3Tp09v1n7nzp1rNi0+Ph4A2uQ+sb1796KiogJdu3a1eHzx8fEYMWIEDhw4gB07dqBHjx4AgPDwcJw6dQrffPMNHnzwwUb3ER4eDgA4dOgQ0tLS0L9//1bFnJeXBwDw9fVt1no7duyAEALXXnstBg0aZDb/9ttvR1hYGNLT07Fv3z7cdNNNhvUA4IEHHrC43QceeAC7d+9uVizKOfnuu+8QExNzxcGnv//+ewDAs88+26yBqpOTk7FmzRocP34cBQUFhnvgUlNTAcjX5MiRI6+4nXXr1gFAg2U9YMAAdOvWDefPn0d6ejrCwsJM5itlVVVVhbKyMnhY+bOHiZiNmjLHG989CXyfEYfXL10CgoOtHRIRERG1Uq9evRrtJv2zzz7DI488gqqqqgaXKSgoaNY+/f39odVqzaYHBgYCAMrKypq1PUtOnz4NAOjbt2+DF/j9+/fHgQMHDMsCwFNPPYUff/wRDz30EN5++21MmDAB1157LcaOHQs/Pz+T9bt06YI777wT3377LYYMGYIxY8Zg7NixGDVqFIYPHw4np+ZdFivn2NnZuVnrKfFHRUVZnO/g4IC+ffsiPT0dp0+fNiRiSicYAwYMsLheQ9MbM3/+fHz22Wd49dVXsXr1atx0000YNWoUxo4di9DQUJNlS0tLkZGRAQAYPnx4k/exbNkyvPjiiw12ogE0/TV5/PhxAMDf/vY3/N///Z/JvNraWjg5ORkS5IyMDLNEzNXV1fB3ZWWl1RMx9ppoo26a7gMH1CEZ/ZC5ruMMXEdERGTCzU3WLHXQh76kBEXp6dCXlJjPd3Nr99Pl3kgN3Llz5/DQQw+hqqoKTz/9NBISElBSUgK9Xg8hhKGrdZ1O1yb7dHBou8tIJZlTkjtLgoKCAMiEQHHzzTdj06ZNGDlyJE6fPo333nsPd955J4KDg3HXXXcZEgfF6tWr8be//Q0BAQHYsWMHFi9ejFGjRiE0NBRvvfVWo8nC5ZTaleZ2hd7SYy0vLwcAeP45XNHlGpremIEDB2LPnj0YP348MjIy8OGHH+K+++5DWFgYJkyYgJMnTxqWLSkpMfxtKTG3ZM+ePVi8eDFUKhWWLVuGEydOoKyszPCafOGFFwA0/TVZXFwMADhy5Aj27dtn8vj111+xb98+wzmrtDCUk5LwqVSqZtdkXg2sEbNRWi0Q5ZeNxPxQHN6ciynzrB0RERGRBSpVuzfvaxa9HqirkzG2YeJxNXzzzTfQ6XS455578NZbb5nNT0tLs0JUTaPUTCjNJy3Jzs4GYJ5wTJo0CZMmTUJBQQF++eUX/PTTT1izZg2+/fZbnD17Fr/++ivUajUAwMXFBUuWLMFf//pXZGZmYu/evfjhhx+wadMmPPvsswCAZ555pkkxK4lUc2sYW3qs7u7uKCkpabAGsn7S1hzDhw/Htm3bUFZWhn379mHnzp348ssvsX37dowbNw6JiYnw9vY2iaW4uLhJydgXX3wBQDZlXLRokdn85r4mPTw8UFRUhDNnzqBnz56G6Xq9HiUlJfDy8mr0BwKlrHx9fTvEANwd+xOFWiU+WlaZHz6mtnIkRERE1Jjm3G/TkAsXLgBAg/fa/P77763ex9XSu3dvAMDJkychhLC4jDLmlLLs5Xx9fTFlyhQsX74ciYmJ0Gq1SEhIwG+//WZx+b59++Lhhx/Gxo0bsWrVKgDGAZqbYuDAgYaYm0OJPykpyeJ8vV6P5ORkk2Xr//3HH39YXE9pttdSHh4emDBhAl5//XUkJyejR48eyMjIwJYtWwAAXl5ehqZ+Bw82rbVVW78mleaciYmJzVpPoZzzwYMHt2j9tsZEzIbFX6sBABzJDpPDixEREVGHpNy7Yqk5VXO3odSm1JecnIz/9//+X4u3fbVde+21cHNzQ1pamqFDiPp+++03HDhwACqVCuPGjbvi9oKCghAZGQkAyMzMvOLyyj1PTVm2fsxKbM0xfvx4qFQq7N27FwkJCWbz161bh/T0dLi7u+Oaa64xTFeO+9NPP7W43Yamt4SbmxtiYmIAmJ6T2267DQDw9ttvN2k7jb0mt2/f3uxEbNq0aQCA5cuXN5iwN+bQoUMAgFGjRjV73auBiZgN6z8mAABwujYSuHTJytEQERFRQ7p37w4Aze71rj4lMVi1ahWOHTtmmH769Gnceeed0Gg0rYrxavLy8sKjjz4KAHjsscdMEpRz585h9uzZAIC77rrL0GMiANxzzz3YtGmToWdIxXfffYfjx49DpVIZeib86aef8Oyzz5rVRJWVleEf//gHgObVlIwcORLu7u747bffGu0c5XI9e/Y0JBSzZs0y6XXy6NGjeOKJJwDI81C/OeAjjzwCNzc3bN++HUuXLkVdXR0A2UnFiy++iL179zY5BsWjjz6Kr7/+GhUVFSbT9+zZg59++gmA6Tl59tlnodVqsWPHDsydOxeFhYWGeXq9Hps3b8YPP/xgmKa8Jl9//XWkpKQYph8+fBhz5syBi4tLs+L9y1/+gu7du2Pnzp2YMWMGsrKyTOaXlZXhm2++wYIFCyyuv2/fPgAyGe4QmtWJPpnpiOOIKePYZGQIAQjhgFpRvfVna4dGLcSxiWwTy9X2sEwt44DOTbN69WrDmFPR0dFi9OjRYvTo0SIhIUEI0fBYX/XpdDoxfPhww8DP/fr1E9HR0UKlUomQkBDx2muvWRyb6UrjiDU2LhUsjJN1JQ2NmVZRUSHGjh1r2GZUVJSIjY0Vjo6OAoCIjY0VeXl5Jusogws7OzuL6OhoER8fL0JCQgzbeOmllwzLrl+/3jDd399fxMXFidjYWMP4ZVqt1mQg6aaYO3euACC+++47i/MbOj85OTkiJibGUFaxsbEiKirKsPyNN95o8T3z6aefGsYgCwgIEPHx8cLf3184ODiIN998UwBocAwtS2JjYw2Ddvfr108MHTrUUD4AxH333We2zo4dO4Snp6cAINRqtYiNjRUxMTHC3d1dABBLliwxLFtcXCy6d+8uAAiNRiNiYmIMY79FRUWJBQsWmK0jRMPjiAkhB5qOjIwUAISDg4Po16+fGDZsmOjZs6fhtTJs2DCz9dLS0oRKpRL9+/dv8vlRXK1xxFgj1kIrV65EVFSUYQyNjigkBHB3rIQejkjZ1/SqdiIiImpfM2fOxHvvvYcBAwbg3Llz2L17N3bv3t2sHvmcnJywbds2PP744wgKCsLZs2dRVFSEuXPn4siRI+jSpcvVO4A24Orqim3btuG9995DXFwcLl68iNOnTyMqKgqvvfYa9u/fb9Yl/WeffYaHH34YvXr1QmZmJv744w+4ublh6tSp2L17N1555RXDsqNGjcLy5csxefJkuLu7IykpCRcuXEDPnj2xcOFCJCcnN/veoYcffhiAsVOKpgoICMCBAwfwyiuvoF+/fjh9+jQuXryI+Ph4vP/++9i8ebPF2qLZs2dj+/btGDNmDCorK5GcnIz+/ftj69atmDRpEoDm9Z74z3/+E08++SQGDBiAvLw8Q03qhAkTsHHjRqxevdpsnRtvvBGJiYl47LHHEBERgeTkZKSlpaFHjx549tlnMXPmTMOyXl5e2Lt3L2bNmgUvLy+cOnUKNTU1WLBgAQ4cONCinh779u2L33//Ha+//jri4+ORkZGBY8eOoaamBqNHj8Zbb72Fr776ymy9r776CkKIK443155UQvDmodYoKSmBVqtFcXExvLy8rB0OdDodNm/ejEmTJkGtVmNgQAZ+z+uC/zftE0xea3kAQOrYLi9Tsg0sV9vDMrWsqqoKKSkpiIyMbHYzpI6gqb2xUefR1mU6fvx47Ny5E2fOnEG3bt1aH2ALrV27FnfccQemTJmCDRs2WC0Oa7hSmdbW1qJPnz4oKyvDuXPnmj1+WHM+x5qTG/ATxcb1CpVjTpxLYVETERERtbU33ngDdXV1ZgMMt7dPPvkEAEw6+CDpiy++wPnz57FkyRKrD+JcH6/ObVx4uHzOyOaQcURERERtbdCgQfjoo48QGRnZrAGhW2Lt2rXYvHmzoaMOAKioqMDChQuxadMmuLu7mzQNJEmlUuHVV181NCXtKHh1buNCuzsDADKK3KwcCREREZFtmjt3brvs5/jx43j55Zfh4uKCHj16wNnZGSdPnkRlZSUcHR3x4YcfIjg4uF1i6UxmzZpl7RAsYiJm47r0lW1TMyp8gNpawIlFTkRERNQZTZkyBenp6dizZw/S0tJQWVmJgIAA3HrrrXj66ac7dCdyZI5X5TauS5QWAJCBLkBWlrGtIhERERF1KoMGDcLHH39s7TCojfAeMRsXGiaLOANdIC6Zj2pORERERETtj4mYjQsNlc+VcEPR+QLrBkNERERERACYiNk8NzfA26kUAJB1ttzK0RAREREREcBEzC4EuMkELDe10sqREBERERERwESsxVauXImoqKhO0TtNgEcVACA3U2flSIiIiIiICGAi1mLz589HUlISDh8+bO1QrijARyZguTnCypEQERERERHARMwuBPrLBCw3n8VNRERERNQR8MrcDgQEOwIAcos1Vo6EiIiIiIgAJmJ2ISBMJmC5Za5WjoSIiIiIiAAmYnYhIMINAJBb7QkI3idGRERERGRtTMTsQECkJwAgV/gDhYVWjoaIiIis5cKFC1CpVOjWrZvZvG7dukGlUuHChQvN2ub9998PlUqFTz/9tMnrfPrpp1CpVLj//vubta+ObNWqVVCpVPjqq69Mpi9duhQqlQpLly61TmCdWEtfk5acP38earUa9913X+sDayNMxOxAQJc/myYiACgosHI0RERERLalrKwMr7zyCvr27Yu77rrL2uGQBd27d8f06dPx5ZdfIiEhwdrhAGAiZhcCA+VzHvwh8vKtGwwRERF1SD169ECfPn2gVqutHUqn889//hPZ2dlYtGgRHBx4ed1RPf/88xBCYPHixdYOBQDgZO0A6OoLCJDPtVCjKLUEPsOtGw8RERF1PD/99JO1Q+iU6urq8K9//Qtubm644447rB0ONaJv374YNmwYtm3bhrNnz6Jnz55WjYcpux3QaAAPxwoAQEFauZWjISIiIrIdP/zwAzIzM3HrrbfC3d3d2uHQFdxzzz0QQuA///mPtUNhImYvfJxlIlaYVWXlSIiIiEhx4sQJqFQq+Pr6oqampsHlhgwZApVKhY0bNxqmnT9/Hm+88QbGjBmD8PBwODs7IyAgADfddBM2bdrU7Fga6xihvLwczz//PCIjI+Hi4oJu3brh6aefRllZWbP30xQnTpzAzJkzERYWBo1Gg6CgINx+++04ePCgxeVra2vx3nvvYejQofD09ISzszNCQ0MxcuRILFmyBEVFRSbLp6am4pFHHkH37t3h7OwMT09PdO/eHVOnTjXrbONKvv76awDAzTff3KJj3b9/P6ZNm4agoCBoNBqEhYVh1qxZOHnyZIPrlJaWYuHChejWrRtcXFwQGRmJ5557DuXl5S3qPEUIgdWrV+O6666Dt7c3NBoNgoODMWTIECxcuBDp6ekW1/n2228xadIkBAYGwtnZGV27dsXEiRPN9l1UVIT//Oc/mDJlCnr27AlXV1dotVoMGzYMy5cvR21tbZNjrW/btm249dZbERISgqCgIHTt2hUPPPAAzp071+A6kydPBmAsN6sS1CrFxcUCgCguLrZ2KEIIIWpqasSGDRtETU2NyfRYv1QBCLFtxmdWioxaqqEypc6N5Wp7WKaWVVZWiqSkJFFZWWntUFqkrq5OFBYWirq6uqu2j5iYGAFAbNy40eL8U6dOCQDCx8dHVFdXG6bPnTtXABAeHh6id+/eIi4uToSEhAgAAoB4/fXXzbaVkpIiAIiIiAizeREREQKASElJMZleVlYmhg4dKgAIlUoloqOjRVRUlFCpVGLw4MHinnvuEQDEJ5980uRj/uSTTwQAMXv2bLN533//vXB2dhYAhLe3t4iLixMBAQECgHBwcBD//ve/zda5/fbbDcfdo0cPER8fL8LDw4Wjo6MAIBISEgzLnjt3Tvj5+QkAws3NTcTExIiBAwcKX19fAUDExsY2+TiEECIsLEwAEKdOnbI4f8mSJQKAWLJkidm8VatWCZVKJQCIwMBAERcXJ7y9vQUA4eLiIn744QezdYqLi8WgQYMM5yMmJkb0799fqFQqER8fL+69995ml8fTTz9tOH9du3YV8fHxIjIyUmg0GgFArF+/3mT56upqMXXqVMM6ISEhIj4+XnTp0sVwPPX973//EwCERqMRERERIj4+XnTv3l04ODgIAOLmm2+2+B5r6DUphBBPPvmkYf+BgYFiwIABwsvLSwAQXl5eYt++fQ0er1LWaWlpTTo/zfkca05uwBoxO+HjoQMAFObVWTkSIiIiqm/69OkAgDVr1licr0y//fbbodFoDNOVGqKSkhKcOnUKhw8fRmZmJvbs2YOQkBC88MILjdYMNNVLL72EQ4cOISIiAsePH8fx48dx4sQJJCQkIDs7G2vXrm31PhSZmZmYOXMmqqur8eSTTyI7OxuHDx/GpUuX8Pe//x16vR7z58/HH3/8YVjnyJEjWLt2LcLDw5GUlISzZ8/i0KFDSE1NRUFBAT766CP4+fkZln/nnXeQn5+PWbNmITs7G3/88QcSEhKQn5+PkydPYt68eU2ONzU1Fenp6fD09ESvXr2adazHjh3DE088ASEE3nzzTWRlZRmOdd68eaiqqsKMGTOQlZVlst7ixYuRkJCA7t27IzExEX/88QcSExNx/Phx5OTk4LvvvmtWHLm5ufjnP/8JrVaLvXv34uLFizh06BDOnz+P4uJirFmzBt27dzdZ57nnnsP69evh7++PLVu2IDMzE4cOHUJ6ejrS09OxZMkSk+UHDBiAH374ASUlJbhw4QIOHTqEc+fO4cyZM7juuuuwadMm/O9//2tyzB9++CHee+89REZGYufOncjKysLu3buRl5eH1157DSUlJbj77rtRVWW5JVhcXBwAYO/evc06V22uSWkgmVmxYoXo16+f6N27d6eoEZsWc1oAQqyK/4+VIqOW4q/stonlantYppZd6ZdkvV6IsrKO+ygpqRPp6YWipKTObJ5e3zbn6MKFC0KlUgl3d3dRXl5uNr9v374CgPjpp5+avM2PP/5YABB///vfTaY3t0aspKREuLm5CQBi06ZNZuusW7fOUCvRFjViL7zwggAgBg4caHG9SZMmCQBi5syZhmlr1qwRAMRf//rXJu17/PjxZrVkLbVnzx4BQPTq1avBZRqqEZsxY4YAIKZMmWK2jl6vF/379xcAxEsvvWSYXlRUJFxcXAQAsXfvXrP1du7c2ezyOHDggAAgpk6d2qTlMzIyhFqtFgDEnj17mrROY86ePSsAiHHjxpnNs/SarK6uFsHBwcLR0VEcPXpUCGFec63UkK5evdriPmfPnt1grbElV6tGjL0mttD8+fMxf/58lJSUQKvVWjucK/LxEQCAgkJWghIRUcdRUQF4eFg7isY4APC2OKesDGiLvhkiIiIwcuRI7Nu3Dxs3bsQ999xjmJeQkIDk5GSEhIRgzJgxZuvm5ubiyy+/xK+//oqcnBxDDUBxcTEA4Pfff29VbL/88gsqKioQERGBiRMnms2fMmUKunTpgoyMjFbtR7F9+3YAwGOPPWZx/pNPPonNmzcblgOA8PBwALLXx4KCAvj6+ja6D2X5tWvXIjY2FiqVqsXx5uXlAcAV92mJcgyPP/642TyVSoUnnngCf/nLX7B9+3a88sorAGR5VFVVoVevXrjmmmvM1hszZgwiIyORkpLS5DiU8/Hrr78iNTUVXbt2bXT5zZs3Q6fTYfjw4Rg1alST91NdXY21a9di586dSE1NRUVFBYQQhvlNfa0eOHAAly5dQnx8PAYNGmRxmVtvvRVr167F7t27MXPmTLP5Snnl5uY2Of6rgYmYnfD1kwlYYSmLnIiIqKOZPn069u3bhzVr1pgkYkqzxLvvvttsfKrt27fjrrvuMiRdlhQUFLQqrtOnTwOQ3X5bSlgcHBzQu3fvNkvElP1FRUVZnN+/f38AQHZ2NkpKSuDl5YURI0Zg2LBh+PXXXxEeHo5x48bhuuuuw+jRozF48GCzuOfNm4fVq1fjtddew//+9z/cdNNNGDVqFMaOHYvQ0NBmxaskvs7Ozs1ar6ioyJAEXOlYlXMCAGfOnAEgm/o1JCYmplmJWJcuXXDnnXfi22+/Rc+ePTF27FiMGTMGo0aNwvDhw+HkZHrtqHQiMnx408dDSk1Nxfjx43Hq1KkGl2nqa/X48eMAgAsXLuDaa681TK+trTXEqnTO0tDr0tXVFQBQWVnZpH1eLbwqtxM+gXJwxoLy5n1QEBERXU1ubrJmqaPS6/WGC/7LEyE3t7bbz1133YUnn3wSW7duRWFhIXx8fCCEMPTsptxHpigqKsI999yD4uJizJo1C/PmzUOfPn0Mcf74448YN24cdDpdq+JSekUMUAYltSAoKKhV+7C0v8DAwCvuq7S01HC8W7Zswcsvv4zPP/8c33//Pb7//nsAsrZx6dKluP/++w3rDRw4EJs2bcI//vEP7Ny5Ex9++CE+/PBDqFQqjBs3Du+++y769evXpHiVmpXLe2Vs6nE25VhLS0sN08rL5TBEnp6eDW67sXkNWb16NaKiovDxxx9j+/bthtq6gIAALFy4EAsWLDC8/ktKSgAA3t7eTd7+/fffj1OnTmHYsGF4+eWXMXDgQPj6+kKtVqO2ttbw3BTKDw+5ublXrNFqKNFSkj5/f/8mH8PVwHZqdsI3RCZghdWuVo6EiIjISKWSzfs646MVLdrM+Pv748Ybb0RNTQ3WrVsHANi3bx9SU1PRs2dPxMfHmyy/ZcsWFBYWYsSIEfj0008xbNgweHt7Gy6W09LS2iQujz/bjTZ2wZuTk9Mm+6q/v4a2mZ2dbfi7fsLh4+ODd999F7m5uUhISMB7772HsWPH4uLFi3jggQfMOrCIj483JL1bt27Fc889h7CwMGzfvh3jxo1rcmKlJFHNrXn0qNce90rHWv84lXHKGhs2oH7i1lQuLi5YunQp0tPTcfLkSXz44Ye45ZZbkJ+fj2effRbvvPOOYVklnqaeo8zMTOzcuRNubm7YvHkzJkyYgKCgIKjVspKgua9V5dzNmDEDQggIIVBXV4fCwkLU1dUZpgkhsGvXLovbUMqrsR8Y2gMTMTvh00X+bFeg8wRaOFYDERERXT1KrdeXX35p8nzvvfeaLauM9TVixAiLTQZbe2+Yonfv3gCAU6dOmdzPo9Dr9Y02N2vp/pKSkizOP3HiBABZW+Tl5WU2X6VSYeDAgXjiiSfw888/Y9GiRQCAjz76yOL2PDw8MGHCBLz++utITk5Gjx49kJGRgS1btjQp3n79+kGj0SAjI8NQU9QU3t7ehiTgSseqnJP6f9fvNfJyStO9lurbty8efvhhbNy4EatWrQJgev6UJpMNjel2uYsXLxq2a+leuua+VpWmnImJic1arz7lnA8ePLjF22gLTMTshG/Yn4kYfIFG2pITERGRdUydOhWurq7YtWsX0tLSDLU4lhIx5R6X+jVEivz8fPznP/9pk5iuvfZauLm54cKFC9i2bZvZ/I0bN7bZ/WEAMGHCBADAihUrLM5fvny5yXJXotzHlJmZecVl3dzcEBMT0+TlAVmTFBcXByEEjh492qR1FMoxvP/++2bzhBCG6fWP9dprr4WLiwtOnz6NAwcOmK23Z8+eZt0fdiWWzt+kSZOgVqtx8OBB7Nu374rbUF6rOTk5FpP5N998s1kxjRo1Cv7+/vj9998brPFqTHl5OU6ePGkoO2tiImYnfAPl7YCF8AGa2Y6ZiIiIrj4PDw/ccsst0Ov1ePjhh5Gbm4uBAwdavF9J6a3um2++wY8//miYnpWVhdtvv73J99tciZeXFx566CEAspMLpaMGQNbKPPHEE4YmZm3h0UcfhZeXF44dO4a//vWvqKmpASBr3t58801s2rQJarUaTz/9tGGdL774Aq+++qqhllCRn59vSNzq13zMmzcP69atQ0VFhcnye/bswU8//WS2/JWMHz8eQPPHpHr66afh5OSE77//Hm+//Tb0ej0AoKamBk8++SQSExOh1Wrx6KOPGtbRarWYO3cuAGDmzJkmtZFJSUmYPXt2s8vjp59+wrPPPmtWM1dWVoZ//OMfAEzPR0hIiKFXy2nTppn0YAnIpE3p5RGQNWg+Pj5IT0/H3//+d0MyVlVVhSeffBIJCQnNitfFxcWw/TvvvBPr1683S/ASExPx3HPPWUwUDxw4gLq6OowZM8ZkXD6raFLn+dSg5owV0B4aGsfm/HkhACFcUS7E4cNWio5agmMT2SaWq+1hmVrWnPF3OqLLxye62jZs2GAYBwqAeOONNxpc9o477jAs17NnTzFw4EDh5OQkPD09xbvvvisAiNGjR5us09xxxIQQorS0VAwZMkQAECqVSsTExIjo6GihUqnE4MGDxT333NNm44gJIcT3338vNBqNACB8fHxEfHy8CAwMFACEg4OD+PDDD02W/+c//2k4D126dBHx8fEiOjrasI0uXbqIixcvGpaPjY0VAISTk5Po16+fGDp0qOHYAYj77ruvycchhBAXL14UDg4OIjo62uL8hsYRE0KIVatWCZVKJQCIoKAgER8fL7y9vQUA4ezsLH744QezdYqLi8XAgQMN52PAgAEiJiZGqFQqERcXZyiPhsbQutz69esNxx4QECDi4uJEbGysYfw4rVYrjhw5YrJOVVWVmDJlimG90NBQER8fL8LCwgzHU9+KFSsMywYHB4u4uDjh5eUlVCqV+OijjwzzLtfQa1IIIRYtWmRYz9fXVwwePFgMHjxY+Pr6GqZv2bLFbL0HH3xQABDffPNNk86PEFdvHDHWiNkJpUluJdxQlc2miURERB3RxIkT4ePjA0De71S/K/vLffHFF3jppZfQrVs3XLx4EZcuXcIdd9yBw4cPIzY2ts1i8vDwwK5du/Dcc8+ha9euOHXqFEpLS/HXv/4Vu3fvbnbX7Vdy66234siRI5gxYwZcXFxw7NgxCCEwdepU7N27Fw8//LDJ8rfffjveeOMNjBs3Do6Ojjh+/DiysrIQHR2N1157DYmJiSZjY7399tt45JFHMGDAAOTl5eHYsWMAZBPAjRs3YvXq1c2Kt2vXrpgwYQISExMbvXfLkkcffRS//PILbrvtNuj1ehw7dgxubm647777cPToUdx8881m63h5eWHPnj145plnEBYWhuTkZJSUlOCvf/0rdu7caagNbWrviaNGjcLy5ctxyy23wMPDA0lJSbhw4QJ69uyJhQsXIjk52ayG0NnZGevXr8cXX3yBG264AVVVVfj999/h4OCASZMmmZ3D+fPn4/PPP8fAgQNRUFCAs2fPIi4uDps3b8aDDz7YrHOmWLZsGfbt24fp06fD3d0diYmJuHDhAsLCwjBnzhxs2rQJN9xwg8k6Op0Oa9euRUBAAKZMmdKi/bYllRAWGmtSkykDOhcXF1u8abS96XQ6bN682dB+V6HXA2rHOujhiMwP/x9CHr7FilFSczRUptS5sVxtD8vUsqqqKqSkpCAyMhIuLi7WDqfZGuu+njqnq1Gm+/btw7XXXou5c+fi448/bpNttlRMTAwSExORkJCAgQMHWjWW9tLUMv3kk08wZ84cvPnmm3j22WebvP3mfI41JzfgJ4qdcHAAfDRy7InCTOsOXkdERERkS6655hrcdtttWL16taGXQGs4fPgwEhMT4e3tbejdkKS6ujr83//9H8LDw/H4449bOxwATMTsio+zvCm1ILt1gzsSERERkam33noLixcvbrMx3BqzePFis94qDx06hLvuugsAMGfOHNbMXyYjIwMzZszAZ5991mFq552sHQC1Hx+3aqAUKMrjOGJEREREbalHjx5YunRpu+xr2bJlWLZsGYKDgxEeHo6cnBxDTVxcXBxefvnldomjM+natWu7lU9TsUbMjni7ywSsqEBv5UiIiIiIqKXeeOMNjB49GoAcEDk/Px9DhgzBG2+8gd27d8PDw8PKEVJTsEbMjnh71QHgMGJEREREndnChQuxcOFCa4dBrcQasRZauXIloqKiEB8fb+1QmsxbK5+LSlTWDYSIiIiIyM4xEWuh+fPnIykpCYcPH7Z2KE2m9ZHFXVTGmzeJiIiIiKyJiZgd8faXLVGLKjRWjoSIiIiIyL4xEbMj3oEyASuq6hhddhIRkf0QQlg7BCKiFrlan19MxOyId7AzAKBI5wbwC5GIiNqBg4O81Kirq7NyJERELaN8fimfZ22FiZgd8Q51BwAUCy+gvNzK0RARkT1Qq9VwdHREZWWltUMhImqRyspKODo6tvkg2UzE7Ih30J81YvBmH/ZERNQuVCoV3NzcUFxczFoxIup06urqUFxcDDc3N6hUbdvzOMcRsyPePvLFUwRvoPASEBZm3YCIiMguBAYG4sKFC7h48SJ8fX3h7Ozc5hc0V4ter0dNTQ2qqqravFkSWQfL1PZcjTIVQqC6uhoFBQXQ6/UIDAxsk+3Wx0TMjnh7y+cieEMUJqNzfAUSEVFnp9FoEBYWhry8PGRlZVk7nGYRQqCyshKurq6dJnmkxrFMbc/VLFN3d3cEBwdDo2n7XseZiNkRJRGrhRoVl0rgbtVoiIjInri5uaFr166ora1FbW2ttcNpMp1Ohz179uC6665r8/tDyDpYprbnapWpk5MTnJyuXrrERMyOuLkBjqo61AlHFGVWMBEjIqJ2d7UvbNqao6Mjamtr4eLiwot2G8EytT2dtUzZMNaOqFSAt1r2llh0qcrK0RARERER2S8mYnbG20UmYEU5NVaOhIiIiIjIfjERszPebjIBK8pnF8JERERERNbCRMzOeHvIG6SLCvRWjoSIiIiIyH4xEbMz3l4yASsuFlaOhIiIiIjIfjERszOGscRKHK0aBxERERGRPWMiZme8fWUCVlTeeboOJiIiIiKyNVf1ajw1NRVr1qxBZmYmBg8ejJkzZ8LBgbmfNWn9ZJEXVTpbORIiIiIiIvvV6qzogw8+gK+vL5YvX24y/eDBg4iJicHixYvx/vvvY86cOZgwYQL0enYSYU3eQRoAQFGVq5UjISIiIiKyX61OxDZu3IiSkhJMmzbNZPqCBQtQWlqKkSNH4qmnnkJISAh+/vlnfPXVV63dJbWCd7BMwIrqPACdzsrREBERERHZp1YnYsnJyQgICEBYWJhhWkpKCg4ePIh+/fphz549eOedd7B161YIIfDxxx+3dpfUCt4hfyZi8AaKiqwaCxERERGRvWp1Ipabm2uShAHAzp07AQD33HMPVCoVACA6Oho9e/bE2bNnW7vLDmHlypWIiopCfHy8tUNpFm+/PzvrgDdQWGjdYIiIiIiI7FSrE7G6ujpUVVWZTPvll1+gUqkwevRok+m+vr7Izc1t7S47hPnz5yMpKQmHDx+2dijNYui+njViRERERERW0+pErFu3bjh79iyK/ryor6urw9atW+Hi4oIRI0aYLFtQUABfX9/W7pJaoX4iJgpYI0ZEREREZA2tTsRuvvlmVFdXY/r06fjhhx/w8MMPIzs7GzfffDPUarVhueLiYpw/fx4RERGt3SW1gpKI6aBBVW6pVWMhIiIiIrJXrR5HbPHixdiwYQO2bt2Kbdu2QQgBrVaLV1991WS5tWvXQq/XY+zYsa3dJbWChwfggDro4YiizAqwE3siIiIiovbX6kTM19cXR48exccff4wzZ84gPDwcDzzwAEJCQkyWO3/+PKZMmYLbb7+9tbukVlCpAK2mEoU1Hii6VIWQK69CRERERERtrNWJGAB4eXlhwYIFjS7z2muvtcWuqA14u1TLRCyX44gREREREVlDq+8Ro87H270GAFCUX2flSIiIiIiI7FOrE7HMzExs3LgRiYmJJtOFEHjnnXfQr18/aLVaXH/99Th27Fhrd0dtwNtDJmBFhcLKkRARERER2adWJ2Lvvfcepk6diqSkJJPp77zzDp599lmcOnUKpaWl2LVrF2644Qbk5OS0dpfUSt5aPQCgqNjKgRARERER2alWJ2I//fQTNBoNbrvtNsO0uro6vPnmm3BwcMC//vUvHDt2DNOnT0dhYSHefffd1u6SWsnbWwUAKCptk1sEiYiIiIiomVqdiGVkZKBLly7QaDSGaQcPHkRubi5uvvlmPPzwwxgwYAA+/PBDuLm5YcuWLa3dJbWSt58s9qJyJmJERERERNbQ6kSsoKAA/v7+JtN++eUXqFQqTJ482TDN3d0dvXr1wsWLF1u7S2olb3850HZRpbOVIyEiIiIisk+tTsTc3NyQnZ1tMm3Xrl0AgOuuu85kulqthk7HLtOtzTtIJmDF1a6AYIcdRERERETtrdWJWExMDFJTU3Hw4EEAQFpaGnbu3IkuXbqgd+/eJstevHgRQUFBrd0ltZI22BUAUAQtUFpq5WiIiIiIiOxPqxOxBx98EEIITJo0CXfccQdGjhyJ2tpaPPjggybLnTx5Erm5uYiOjm7tLqmVvAPl/XxF8AaKiqwaCxERERGRPWp1IjZr1iwsWLAAJSUlWLduHTIyMnDHHXdg0aJFJst98sknAIBx48a1dpfUSt7e8rkI3kBhoTVDISIiIiKyS23Sbd5bb72FRYsW4dy5cwgPD0doaKjZMjfddBOuueYajBo1qi12Sa1gkogVnbJmKEREREREdqnN+i/39/c36z2xvuuvv76tdkWtxBoxIiIiIiLravOBpCorK3Hu3DmUlpbC09MTPXr0gKura1vvhlpBScSq4YKq3FK4WDUaIiIiIiL70+p7xBTbtm3DmDFjoNVqERsbi2uvvRaxsbHQarW4/vrrsX379rbaFbWSpyeggh4AUJRZYeVoiIiIiIjsT5skYkuXLsWkSZOwZ88e1NbWQq1WIzQ0FGq1GrW1tdi1axcmTpyIpUuXtsXuqJUcHACtcxUAoCi72srREBERERHZn1YnYlu3bsUrr7wCBwcHzJs3D6dOnUJVVRXS0tJQVVWFU6dOYd68eXB0dMSrr76Kbdu2tUXc1EpaF5mAFeXVWjkSIiIiIiL70+pEbPny5VCpVPjvf/+LFStWoFevXibze/XqhRUrVuC///0vhBB47733WrtLagPebjoAQHE+EzEiIiIiovbW6kTs8OHDCAsLw8yZMxtd7r777kN4eDgOHTrU2l1SG/D2rAMAFBUJK0dCRERERGR/Wp2IlZaWIigoqEnLBgUFoby8vLW7pDbgrZUJWFFxm/XXQkRERERETdTqq/DQ0FAkJydfMcEqLy/HyZMnERIS0tpdUhvw9lEBAIpKHa0cCRERERGR/Wl1IjZhwgSUlZXhoYceQk1NjcVlampq8OCDD6KiogI33XRTa3dJbcDbTyZgRRVqK0dCRERERGR/Wj2g8+LFi/H111/j66+/xq5du/DQQw8hKioKgYGByMnJQVJSEj766CNkZ2dDq9Xi+eefb4u4qZW8A2QCVlTF4ZyJiIiIiNpbqxOx8PBwbNmyBXfddRfS0tLw2muvmS0jhEDXrl3xzTffIDw8vLW7pDbgHSwTsKJaD6CmBtBorBwREREREZH9aHUiBgDDhg1DcnIyvvzyS2zfvh2nT59GWVkZPDw80Lt3b0yYMAH33nsvUlJS8Mcff2DAgAFtsVurWrlyJVauXIm6ujprh9Ii3kHOAIAieAOFhUATO1whIiIiIqLWa5NEDABcXV0xd+5czJ07t8FlRo8ejcLCQtTWdv6xq+bPn4/58+ejpKQEWq3W2uE0m9ZH3h5YCB+gqIiJGBERERFRO2r3vsuF4LhVHYGPj3wuhlbWiBERERERUbvhIFJ2yttbPhfBW9aIERERERFRu2EiZqeURKwQPqwRIyIiIiJqZ0zE7JTSNLEaLqjKLbVuMEREREREdoaJmJ3y9ARU0AMAirIqrRwNEREREZF9YSJmpxwcAK1zFQCgMLvGytEQEREREdmXZndfv3r16hbvrLq6usXrUtvzcatGUbUbivI6/3ACRERERESdSbMTsfvvvx8qlapFOxNCtHhdanve7rVAIVBUoLd2KEREREREdqXZiVjXrl2ZTNkIb686AOy9noiIiIiovTU7Ebtw4cJVCIOswcdbPheWOFo1DiIiIiIie8POOuyYt68s/qKyZufjRERERETUCkzE7Ji3v0zAiio0Vo6EiIiIiMi+MBGzYz6BagBAYbUboGeHHURERERE7YWJmB3zDnYBABRBC5SWWjkaIiIiIiL7wUTMjnkHyBqxIngDhYXWDYaIiIiIyI4wEbNjPj7yuRA+7MOeiIiIiKgdMRGzY97e8pk1YkRERERE7YuJmB1TEjHWiBERERERtS8mYnbM11c+F8Eb+nzWiBERERERtRcmYnbM318+6+GIgswq6wZDRERERGRHmIjZMbUa8HauAADkZemsHA0RERERkf1gImbn/N0qAQC5OcLKkRARERER2Q8mYnYuwKsaAJCba+VAiIiIiIjsCBMxOxfgWwcAyMu3ciBERERERHaEiZid8w9QAQByC5ysHAkRERERkf1gImbnAkJkApZb6mLlSIiIiIiI7AcTMTsXEC4TsLwKN0Cwww4iIiIiovbARMzO+Ue4AwByhR9QVGTdYIiIiIiI7AQTMTsXEKoGAOQiAMjJsXI0RERERET2gYmYnQsIkM958GciRkRERETUTpiI2Tl/f/mciwCIbCZiRERERETtgYmYnVNqxKrgivK0AusGQ0RERERkJ5iI2Tl3d8DFsQYAkHex3MrREBERERHZByZidk6lAvzdKwEAuenVVo6GiIiIiMg+MBEjBHjJBCw3q9bKkRARERER2QcmYoQAXz0AIC/PyoEQEREREdkJJmIE/wAVACC30MnKkRARERER2QcmYoSAEJmA5ZY4WzkSIiIiIiL7wESM4B/mAgDIq/YAamqsHA0RERERke1jIkYI6OoKQA7qjEuXrBwNEREREZHtYyJGCAiSL4M8+ANpaVaOhoiIiIjI9jERIwQGyudLCGYiRkRERETUDpiIEbp0kc8Z6AKRykSMiIiIiOhqYyL2p6lTp8LHxwd33HGHtUNpd6Gh8rkaLsg/U2DdYIiIiIiI7AATsT898cQTWL16tbXDsApnZyDQswIAkH62ysrREBERERHZPiZifxo7diw8PT2tHYbVhAXKbuvT04SVIyEiIiIisn2dIhHbs2cPbrnlFoSGhkKlUmHDhg1my6xatQqRkZFwcXHBkCFD8Msvv7R/oJ1YWJgKAJCeo7ZyJEREREREtq9TJGLl5eWIjY3FihUrLM7/+uuv8dRTT+GFF15AQkICRo0ahYkTJyI1NdWwzJAhQxAdHW32yMzMbK/D6NDCejgDANJLtUAVmycSEREREV1NTtYOoCkmTpyIiRMnNjj/nXfewdy5c/Hggw8CAN59911s27YNH3zwAZYtWwYAOHLkSJvEUl1djerqasP/JSUlAACdTgedTtcm+2gNJYbmxhLaXb4U0hAOXUoK0LNnm8dGLdPSMqWOjeVqe1imtonlantYpranI5Vpc2LoFIlYY2pqanDkyBEsWrTIZPr48eOxf//+Nt/fsmXL8PLLL5tN3759O9zc3Np8fy21Y8eOZi2flxcGYAjSEYZD336LvNjYqxMYtVhzy5Q6B5ar7WGZ2iaWq+1hmdqejlCmFRUVTV620ydieXl5qKurQ1BQkMn0oKAgXLp0qcnbmTBhAo4ePYry8nKEhYVh/fr1iI+PN1vu+eefx4IFCwz/l5SUIDw8HOPHj4eXl1fLD6SN6HQ67NixA+PGjYNa3fT7vdzdVXj3XSAdYRju7w/9pElXL0hqlpaWKXVsLFfbwzK1TSxX28MytT0dqUyV1nJN0ekTMYVKpTL5XwhhNq0x27Zta9Jyzs7OcHZ2NpuuVqutXvD1NTeebt3kcxrC4XA+BY4d6FhI6mivMWobLFfbwzK1TSxX28MytT0doUybs/9O0VlHY/z9/eHo6GhW+5WTk2NWS0YNCwsDVCqBSrgh90SOtcMhIiIiIrJpnT4R02g0GDJkiFmb0B07dmDkyJFWiqrzcXEBwgNkb4lnk2utHA0RERERkW3rFE0Ty8rKcPbsWcP/KSkpOHbsGHx9fdG1a1csWLAAM2fORFxcHEaMGIF///vfSE1NxSOPPGLFqDufnj2B1BzgbJozRtbUABqNtUMiIiIiIrJJnSIR++233zB27FjD/0pnGbNnz8ann36Ku+++G/n5+XjllVeQlZWF6OhobN68GREREdYKuVPq2d8FP+8HztRFAidOAIMGWTskIiIiIiKb1CkSsTFjxkAI0egy8+bNw7x589opItvUq7fs3OQMegFHjjARIyIiIiK6Sjr9PWLWsnLlSkRFRVns4r6z6tdPPh9HDHD0qHWDISIiIiKyYUzEWmj+/PlISkrC4cOHrR1Kmxk8WD4noy/KD52wbjBERERERDaMiRgZhIQAIYG10MMRvx8TQGmptUMiIiIiIrJJTMTIxOB4edvg0boBwK5d1g2GiIiIiMhGMREjE0OGyOfDiAe2b7duMERERERENoqJGJlQxsDehTEQ25iIERERERFdDUzEyMS11wJqtUAqInD+TC1w4YK1QyIiIiIisjlMxMiEuzswfLgcT+xnXA98+aWVIyIiIiIisj1MxMjMjTfK5024GfjgA6C21roBERERERHZGCZiLWSLAzorpk6Vz1txE0rSi4Fvv7VuQERERERENoaJWAvZ4oDOiuhooG9foBou2IDbgMcfBzIzrR0WEREREZHNYCJGZlQqYPp0+fd7rs9D5OcD110H7N0LCGHd4IiIiIiIbAATMbLo0UcBNzfgaGU/rPZfAJw7B4waBfTvDzzzDLBhA1BYaO0wiYiIiIg6JSdrB0Adk78/8MIL8vF41T8QNVmL+B+XASdPysfbb8sFBwyQ7Rh79wZCQuQjOBhwcJCZXHAw4OEBODrKqjZHRzmPiIiIiMiOMRGjBj33HLBjB7BrlwOu+/FvmDPjOdwXtgvDLnwNh10/AxcvAn/8IR/N4eEhe2IMCJDPzs7yHjQnJ6BfPyA8XC6nUslat6oqOc3RUS5bUCDnJybKJLBbN8DPT26rrg5ITwdiYoCKCrmun5/8OzkZiIoCfv8d6NJF7t/PD/jtN+Dzz4E5c4Bhw+R+HRzkvlUquW+9HqislE0zdTrg0iXAx0fGkZoKdO8up3frJp+dnQFXV7lfAOjVC9BoABcXIChITtPr5TEBcrsqVWuKi4iIiIg6ESZi1CBHR2DjRuDOO4Ft24BV/3HGKkxAaOgE3HILMKp3Nq71PYGuOUegupAik5OsLPmoqJCJRn6++YbLyuRzWprp9Joa4MgR+bjcwYOWg0xJsTx9zZqmH6jio4/kw9p69pSJW00N8NNPcNJq0e/GG6HKz5cjbgcFySRPo7F2pERERETUQkzEqFGensCWLcDPPwP//S/w//6frLz68EPgQwQBCIKX1/Xo1QuIiAD8YgD/sbKiydsbcHOug7tjFdxQIf+uyoebhwPcnGrg7lQNt8p8aBzrgMOHZQ1VSgrQp4+xU5C8PJmQBATIpK62FigpkbVRly4BSUmyOWRAAKBWy/UyM401TbW1snZLrwcuXJDNJffskTVvPXrIWriUFFlL1r27rDHT6+V2ioqMtVQ6naxdU6sBrRbIzZV/JyUB1dXGExYRIZetqZHTS0ubf9LPnpWPP6mKi9F77Vpg7VrzZQcNAoYMkTENGSKTOJ0OiI2Vx8paNiIiIqIOiYkYXZFKBdxwg3xUVwM//igTs19+AY4elXlRQxVZgCMA9z8fABBstoSTE+DmNhlubrKix9lZVvZoNMa/zaaV/vk8SK7v6Fjv0c/0fweHP/++5s/nuy9bvokPZTtqtXwo8Sh/q50ENM4qwzS1GnBQ/ZlQ6vWyljA/XyZ4gYEyYaytBYqL5YndtUs2c9TpZHPIY8eAffsAABUBAXBTkj+dznjyEhLkozk0Grn/9HRgwgTZnFOnk/f7RUfLEzp8OJCdDYwcKZNOrVbuW6+X65eUAL6+QHm5PC4/P/lCYeJHRERE1CRMxKhZnJ2Bm2+WD0BWEp07B5w+LfOK/HxZiZWXJyuDlOv0igrzv+vq5DaUSq6SEusdV9swT0IcHVV/Jo+O8PDwhKenJ7y8ZE2jp2coPD1ljuPnB/j3uBV+Q+XfQUFA166yVlGn02HHpk2YNGEC1M7OQE6OvC/v88/lTvbskQVz+nTTwqypkUkYINucKnbvbt3hN+aGG4ATJ4DJk4GPP5YvoIwMec9cfj7wl78Ajzwij+233+T8oiKZmAYGyuVCQ2XC6ugoXzRK7WllJeDuLpvEOjvLBJGIiIiog2Mi1kIrV67EypUrUadkE3bKxUX2aN+/f/PWU/q8uDxBq6iQeYLSsu/yvy09K3101H/o9ebTGno0d1mdztj6sP5z/RaKiro6mSdUVsq8orm8vICuXZ3g4jIc27Zr0K+fA6KighEVE4zgz8abVkAJIe+/c3CQ2fEff8hERukQZfVq2ZRSKbiqquYH1FI//SSfP/5YPm/aZDr/mWfkoy3UP7aoKOMxW3LrrbJZq6Oj8deDiAhg1izg3/8Gvv5aFu5LLwHXXy+PY9YsWUuoJIRdugCHDgEDB8qOaHx95fmurTV2/CKE8dcHT0+ZbPbsCeeCAtn5TF2dzL5VKrmcm5tc77ffZPNTJye5PZ1OLuvuLtsJd+8uazEVBQXy3svYWPm/0gmMXi9/6fD2lk10XVzkPtzc5LZzcow1rV26mK5bXi6P1cXF8jkUQj7S0+WL/PRpYOpUY/NgQL451Gp5TKWl8g3h7y+T8+ho476VWtf66wLyfO/dK2tqU1PlObn8HkkhZBn6+RnPuXLOlCpqvV5Oy8uTib2ipkaeH39/WUOt0cjqeZ1Onp/6bzQhZLPoU6eAwYPlm7Qhyv40GuOHhbt7w8tfLjVVvpZ79zafV10tf3wIDm64bBpSVSVfo1eqwS4okMfr59f4cnl5srn45eVWX02NLBenVl52WOrYSGnKfvSoLBdlIEwiog5OJQRH6G2NkpISaLVaFBcXw6uxL+R2otPpsHnzZkyaNAlqtdra4dgVIYyt/JQE8vLbxeo/Skrkc1GRrBRSHnl58vrKUj8n9Xl7yzyjTx/ZeaTy3L27vOZstooKeRHj4iLvu3NxkReCLi7ypsCMDODLL2XvkgUF8uJs3z554VpQIC/qc3LkfWr17nEjG+LuLnsGPXHC2pFQW1Luo73StJby8DB20nS5IUNku/awMGNN/eXq/6Di5CST34IC2XlRcDDw3XeN71/5YeTyH05DQ2VSWlQkj/XFF2XS+913QGSkbId/4gREeDhq8/OhrqiQ74Hu3eW6wcHyJuqcHGDSJPmBn50tuxw+elQe1zPPAPHxcn8VFcD69cCNN8ovBRcXedwFBfKHGJ1O9gYcFyf/dnMz/fFFUVUly4cdNrUYr5VsT0cq0+bkBkzEWomJGF0t5eUyDzp3rhabNyfC3T0GZ844IilJVng1dI3k5CSvE5TkrFs3+UN/QIDx2c+vhclaSyj3x7m4yOCEkAfWtavxl+2UFOCrr2Stkqur7DzljjuMWWtGhqwd2rFD1j7deqs8yDVr5IVPfd7exqEPiIhsVWys/KxU3HSTrD0ODQWefhp4+GFZQ63RAM8+C5w5I5PS338HbrlF1uiuWmVssfD55/KLwdFRNlX38wOGDpVJ5fjx8vO5uhp4+WVZsz9tmlx3/Xrgvvvkj3QODjLpPXlS1to/9pj8jD5yRP5w99hjsrn5oUPAqFEyie3WTe6/qkrWlhcUyM60APlD38aNsrm6h4eMb80a+YNfXJxxXFKlVt7HRy5XUiIT6vh4Wev+66+ytUJVFeDiAp2nJ37csQM33n471PWTXOWein//W56/4Hr3tefmyl9I+/SR5zAmxrQWuK7O2AJFr5ex5+XJXpCbo7bWWHN8tYa2UeJTzqmyX2W819ZsF2jf8WKFAFauRG3fvthUXt4hrn+ZiLUjJmJ0tVkq06oq+f168qSsxDp1Sg6TduqU/A5pCm9v+YNrWJj8W963Jj+Xlb8tPdzc5PdZQID8fu/w/XMkJ8ueZe66S35BOjsbv9yU4HNy5JePu7u8WCkvlwepnKDXXwfefRf4v/+TzeMCA4Fly2QTvMRE+Yv60aPyhAwdaux05ddfZccrUVHAunXA7NnyxJ0+LbcFoO6dd+Co08kLmu7dZdPBffuAMWPksA2xsfJx5oy8QOnbV16Y5ORYPt5+/eTFQ0WF3D8gax2ysuSXZFCQsZfQGTPkhcWqVcb1leaKJSXynH3zjdx/VJRsEujjAzz0kFx23jyZUH/+uTwPADB3rtzHxo1yvykpspYhMxPYvNm4H6UmpP7/AQHA1q2Nl+ett8oahSeeME4bNUqWsUKjAWbOlBcaGRnygu2HH+RFXkSEvGAVQu7r0CF50VdSIi+0FPffL8/RihXy4uymm+Sx+vjIJN/Dw/h6OHdOvmYuf/M9/LDx5tnHH5eviSNHgOXLTZfr1k2+/iIi5GtHqQGZPFk+v/SScb+KESNkPF9/Lf/v0kX+0tK9u7ww9vIyv/FWqzV2tlO/LfU998gfQgD5un/oIfl6++UX03PSr598fVZWmm43KEh+KBUXm5fX5eV8uR495Plrqrlzgf/8R/49cqQsh+3bZdzTp8vyImovzzwDvPWWfG+cPNmybfTqJd9vfn5XbgpzJR4e8nOhvFx+X4SENP3+cUsxAfJzedIk+WPqli3yMyouTn63BQbK92Ntrfk2lFp1R0djbXRDx+jtLe9xWbRIft/+2VGZRX37yu91C85PmoTwDRusfv3LRKwdMRGjq605ZSqEvO6sn5hlZMhrqbw84w96bfWud3CQFVhKj5fKs7u7vA609PD0lNf5oaHyR01lmr29XNv8varTycSytffgUIvx87cRHWHQeuWD7/I4iotlkgoYakxQVyeTVZ0OtceOYXN+PibecgvUTk6ymaWnp7zIVKnkssoPOwcOyJoSlUpeyGZkyAQ5Kcn4QXf0qPxRwsFB/u/vL2uPFBMmyAvhkBDghRfa59wQ2YjaPXvgNGqUVWNoTm7Ab2wiG6JSyQqcsDDZUaEldXXyh/W8PHk9kZ5uvF+trMz8XrbLHxUV8oe22lp5HWKpIqAlnJ3lj3leXvLHf3d3+b9Wa+xXwtvb+HB1lQ9//z97nfSXP4rbbR7CC3/qyKydhDUWg5KEAcaOTxwd5YcOADFyJIRSm6tSyYQKMDa/qv+hc801xr8HD5YPQNZgKqZOBV591XIslyesixcb/1Z6f/LwkH/Xb0KmfDirVLI33G7d5H1u/v7A/v0ysTt/Xn6wurrKX8Lc3GQTxJISWaO6YYP88N2xQ9bC/vWvchiV7t2BpUtlc72QEFkDPXUq8OmnwPHjslYkIkLWCJWWyi8VZ2d57I6Ocr+urjLBVKlkEqrU2hYUmA7Jcrk5c+SXVUKC7IgIAP73P3msr78ua9wV06bJZpCX16ZERxtr7AEgIgKiqgqqy5u1N8f48TKeltaC0dXT2lrFdmavlyxEdsvRUX43+/vL/+Pimr8NpRM+pUfIigrTv5WEThmW4PKH0vv8xYuyMxNA/vhcXS0/Q+t/tzaXt7exB/vycnk9EhJirKlTOsNTOtLLzZWttTw95XVBv37y3Gg0stVacLCsuauultcu2dnGiieluWZOjpwWEiKPZ9s22bJw5045LThYbi8yUl6jeHvLZFgIeZ7y8mQrDycnGU9IiNxeQYHsLHDIENn60dtbnruzZ423V4SEmDfpr6iQx2mpc7naWmPHMrW18rpTqUxLTZUtxaqqZBkGBMjl1GrZsjEwUP7v5CRbuwghWyzW377SAqWoyHhLRm2trJ0dMEBenwFyuZMnZas2jUaeB51Onk/ltVldLVtR9u4tx3yPj5fnTal5PXdO3rYCyPizsmRsfn5yf+Hhcj/FxbLsHB2NnUgqlSP1W88o8V9+m0RZmbEDSeX2j9xceW2r08n4Kysbv7WiuFguo9xyUlAgp0VGyn06OBhjy86Wyynld+mSjC0kpOHtN5Uy5n1oqNx+/ev+/Hz5enB2lseidHbZ0ltGlHNcXi7fe03JxTpCxZlVNXbwjo4yCVP+rk/5MAJks9z6rr9ePoeHm29z/Hjj30qyuGiRfL58jMrLt/viiw3H2pgZM1q23uUefli+Ac+cMf0gakhNjfyAcHBArU6HLd9/j4mTJ0PdUK+jer18s7i4GD+ULnfpkrHHW0B+WNTUNDyMihCmPcMKIRPgykpZTvXvSauulh+0MTHyQz0hQb454+LME/GkJPnlMm6c/NLo2lXWxo4YYfzCs/TaUpL7nTtlU19fXxnT5fd41dYaP0AB+UG8c6fsMMfTU34g1z9GRf19FhTIps3KjxOlpcZBYpVYVq+WX25eXrJTKCGAYcPks4eHPLfLlskvBVdXebxFRUBlJXTFxdicloZJyvhKnQSbJrYSmybS1WbrZarTmdbGlZTIz/iKCmOvkkqCV1QkH8XF8v+qKuO4dUpi05loNLWoqZG/hynJnU4nv/OVHtUbolxE+/jI9YqL5Xoqlfz+9fCQ56euTiZwVVXycaVz5Okpl1N+pHZwkN+JRUXye6+62rSjmMhIWU6AfK5/65ESo0LpR0VJ5C3F4ugoEykhZBzKthvi4SFfH87O8jUByGuJggI5r/7QEhqNnHfpknF9V1d5jeHubuxnQK83dtSn0cht1T8mQMbn7Cy3HRQkkycXFwEHh1r06uUEd3cViovl69LJSSa5gFw2L8+Y8A0cKEeaCAyUMdTUyGV9fOTyWq28dgHkedFo5HFlZsr/CwpkGSjJs7u7PAalzwDloVbLRKu2Vv4A0r+/fJ24u8v4qquNPzKo1cZz5u1tvIaLjZXn09tb/ghQXi6v93JzjeVaXCzPTbducojDujp5TMoPGl27yoRSWeeXX2TC362bLJejR2WS2Lu3cd3ISHmcJSXyOKurZa1/drY8F8qtadXVMkYXF/k6SkuTy+l08nxFRhp72k9NBcaOlcd26ZK8nXLwYHlORo+W5ywoSB5TYWEt0tIOYNiwEaiqcoK/vzz2s2dlMq5c38fGyveJi4ssNyVnoo7H1r9X7VFHKlPeI9aOmIjR1cYybRqlyWVurrFPA1dXYx8XlZXy4qmyUl58KUML+Psbk7rcXFnLogzvpVxkpafLC7uMDNmCR/nVvqREXnhptXJfly7JCzdPzysnEPW5upr3f6DQaIy1hu3p8iTqalDKob32R9SevLzkZ05YmPws8fOTr/fgYPm3i4t83w8cKN/nw4bJpFR5Lyi14JaG1qPW4feq7elIZcp7xIjI7lze5FIxaNDV3W/9+/+VX+z9/WUrki5dZC2Lg4NMFLOyZIdPsoZPh6++2odZs65BQIAa2dnyF31fX3nbhZeXrK2IiJAdr/j5yYTQ1VVerB0/Ln+BP31aNtXz8pKJol4v95GbK6c7O8taAA8Pua6zs7EZnFotk1AnJ/ns5yc7uNNqZQ1FWpqMISNDbjMrS+4nL09uu6BA1oyEhsrtKp20KMlodrYsl9OnZfPKkydlHMpQUAEBch9qtbEWTBkXGpDPPXvKZonDhskalr595brnz8tt5uTIi1WlBVFgoDxfsbHy1hitVtZ05OUZk+0ePeSFb3W1MZmurpaxOjjImqWsLHkO1Gq5T2Xc6sJCWZYhIbLlTHCwrNGSHQrW4siRg4iIGA53dycIYbyQ9vaWsaSlyYvxoiK5jTNnZKzFxcakPy5OPgcHyx8GwsJkGRQUyOOrP/B9377GZqAeHsYmnj4+xua+ysPRUR7X4MGy2a1SA+fmJvfv6ipjzc+X50q591KtlmV16pQ8b3l58n8vL3neAwPl60kZO9HLy3hrUXKycexzFxcZe22t3HZWlqwhq6uTx6aMd62cM6XF2LFjxlqzmhoZz9mz8nV06pRcp6RELq/EpdPJ9052tiw3pQVbUZHcl6+vPE6Nxth81t1dxqpWy7+Vjo28vPRISqqCSuUKd3cVqquNQ3xlZcl1lOG+lJro2lpjjfbFi/LRFF5ecp8VFXJ7Li6y7MLDZTKnvIddXOTfLi7Gh/IDkdIJkqurfH/k5cn/q6qMndMVFsqyq6mRn0deXnJeTIz8u6ZGvia8veX+8vONY5srtdDK+PRVVaZNaesrLjbWytp1k1OiBrBGrJVYI0ZXG8vUNrFcbQ/L1DY1Vq61tTJ51+nkQ2mOqPRZ4eYmm0GmpMjkp6xMJkBKjXx+vmx6WlEhf1zorPz8ZPIcFiaT27o6mXyWlMiEUK02Ju7+/jKpDgoyJpouLnI9JWn39JTnKz9fJs6OjsbtK8N+KR1VqtVy3eRkud+qKpn4hYTI/3195T6V+2Krq4GUFD1yc7MRGBiE0FAH1NbK5TUaWW7KmJvK0FqFhXI9pQOpnBxj8+ToaBmDcj+t8iNYZqaM2cFBxqBWG4c5c3eXy6SlyWWCg007pSoqkgm00pxYiUFpspuaahyNQumkqkcPuQ/l1jYHB7kNrVYek3I7W12dfH0q59HT07hM9+5yn0pnWWlp8niqq+VyPXsabx9zdJTnTAg5/7ffZBmnpspmwErzXuV2Ob3eeI70etPbElxdjedIYamT08buIe1In7+sEWsHK1euxMqVK1Gn3JlOREREdkXpMFGp9VF4espaUkDWzDWlN+3ycmMHg2Vl8iK2pERuNzNT1gIqNaHK2MfKvZ9VVfICubhYLltcLC9yg4KMyYGzs7yoT0+XF9kREcY+H/R6mVycOWO5E0OltlS5gAaMHd0Axo7q6g91pygrM52XlSUfLXHoUMvWM+cAoA16v+lglBYYlob1utr7rX/vcH1arfHeTkAm3VVVlpd1c5PbqX+fa0SE/KGivFy+loQwtl5wcJDvQTc3oLraCWVlk7Fxo8CECVfnOK8GJmItNH/+fMyfP9+Q9RIRERG1lLu7bBpYn9JTZrduslO79lBTI5OmgADDUGrw9DR2yFJZaeyVtaBAXgQnJ8t5KSmy5sXZWa4TGCiTNKVpbFiYXC8nB4bm2M7O8iK7rs7YQYtOJ2vZ/PxkLZLS0+nFi3IZpakwIC/Si4tlzZSHh7EX0/R0mYQWFMgaJqUHUCcnwN29DqmpJ9ClS3+UlTnC01PWTJaWyuNWOp5xcJDxaLWypqqgQC6jNG0F5HEoNaNVVcYawe7d5T5ra2UTY6XDRKVjqqoqmSg7OBg7qCork8srw8v5+spzpYzBrvTum59v7NVWOXdlZcZkWaHUMirNzZVattJS2XRerZa1XvU7WfLxMfboq9XKmjZnZ3mcmZnG5aqrjQmfXg9DJzZK01blHF4+zvvlSZirq7EjKaXDpfr7sDQe9eWJptyHCoAjKiraOQttJSZiRERERARAXrwHBcm/lfEaFQ4OxiZtgEyUAOO9uEOGtE+MraXT6bF5cwomTeoHtbrj9IQiROPDPTTUNE8ImbApNZbBwTK5aeodM6WlMikrLTX2vtvQ+OeKmhqlt1iZGFm6TzAnRz48PIzDYpSXy//VaplYOjrKRFLpEVlJltVqOS0rSy6vNN10cpLLK00ddTrlXmcdDh7ciRtuGNu0g+4gmIgREREREVmZ0hlKY/Mbmq4kzwpleK6mUIagqz+k2pU6V9FoLA9NV19goHxciTJsiZLY11+/d+8rrw/IhCwlpdLkh4POwOHKixAREREREVFbYiJGRERERETUzpiIERERERERtTMmYkRERERERO2MiRgREREREVE7YyJGRERERETUzpiIERERERERtTOOI9ZK4s8R70pKSqwciaTT6VBRUYGSkhKo1Wprh0NtgGVqm1iutodlaptYrraHZWp7OlKZKjmBkiM0holYK5WWlgIAwq80qh0REREREdmF0tJSaLXaRpdRiaaka9QgvV6PzMxMeHp6QnWlYcjbQUlJCcLDw5GWlgYvLy9rh0NtgGVqm1iutodlaptYrraHZWp7OlKZCiFQWlqK0NBQODg0fhcYa8RaycHBAWFhYdYOw4yXl5fVX4jUtlimtonlantYpraJ5Wp7WKa2p6OU6ZVqwhTsrIOIiIiIiKidMREjIiIiIiJqZ0zEbIyzszOWLFkCZ2dna4dCbYRlaptYrraHZWqbWK62h2VqezprmbKzDiIiIiIionbGGjEiIiIiIqJ2xkSMiIiIiIionTERIyIiIiIiamdMxIiIiIiIiNoZEzEbsmrVKkRGRsLFxQVDhgzBL7/8Yu2QqAFLly6FSqUyeQQHBxvmCyGwdOlShIaGwtXVFWPGjMGJEydMtlFdXY3HH38c/v7+cHd3x6233or09PT2PhS7tWfPHtxyyy0IDQ2FSqXChg0bTOa3VRkWFhZi5syZ0Gq10Gq1mDlzJoqKiq7y0dmvK5Xr/fffb/beHT58uMkyLNeOZdmyZYiPj4enpycCAwNx22234dSpUybL8P3auTSlTPle7Xw++OADDBgwwDAo84gRI7BlyxbDfFt8nzIRsxFff/01nnrqKbzwwgtISEjAqFGjMHHiRKSmplo7NGpA//79kZWVZXgcP37cMO/NN9/EO++8gxUrVuDw4cMIDg7GuHHjUFpaaljmqaeewvr16/HVV19h7969KCsrw+TJk1FXV2eNw7E75eXliI2NxYoVKyzOb6synD59Oo4dO4atW7di69atOHbsGGbOnHnVj89eXalcAeCmm24yee9u3rzZZD7LtWPZvXs35s+fj4MHD2LHjh2ora3F+PHjUV5ebliG79fOpSllCvC92tmEhYXh9ddfx2+//YbffvsN119/PaZMmWJItmzyfSrIJgwdOlQ88sgjJtP69u0rFi1aZKWIqDFLliwRsbGxFufp9XoRHBwsXn/9dcO0qqoqodVqxb/+9S8hhBBFRUVCrVaLr776yrBMRkaGcHBwEFu3br2qsZM5AGL9+vWG/9uqDJOSkgQAcfDgQcMyBw4cEABEcnLyVT4qurxchRBi9uzZYsqUKQ2uw3Lt+HJycgQAsXv3biEE36+24PIyFYLvVVvh4+MjPv74Y5t9n7JGzAbU1NTgyJEjGD9+vMn08ePHY//+/VaKiq7kzJkzCA0NRWRkJO655x6cP38eAJCSkoJLly6ZlKezszNGjx5tKM8jR45Ap9OZLBMaGoro6GiWeQfQVmV44MABaLVaDBs2zLDM8OHDodVqWc5WtGvXLgQGBqJ379546KGHkJOTY5jHcu34iouLAQC+vr4A+H61BZeXqYLv1c6rrq4OX331FcrLyzFixAibfZ8yEbMBeXl5qKurQ1BQkMn0oKAgXLp0yUpRUWOGDRuG1atXY9u2bfjoo49w6dIljBw5Evn5+YYya6w8L126BI1GAx8fnwaXIetpqzK8dOkSAgMDzbYfGBjIcraSiRMn4osvvsDPP/+Mt99+G4cPH8b111+P6upqACzXjk4IgQULFuDaa69FdHQ0AL5fOztLZQrwvdpZHT9+HB4eHnB2dsYjjzyC9evXIyoqymbfp07tvke6alQqlcn/QgizadQxTJw40fB3TEwMRowYgR49euCzzz4z3EzckvJkmXcsbVGGlpZnOVvP3Xffbfg7OjoacXFxiIiIwKZNmzBt2rQG12O5dgyPPfYY/vjjD+zdu9dsHt+vnVNDZcr3aufUp08fHDt2DEVFRVi7di1mz56N3bt3G+bb2vuUNWI2wN/fH46OjmaZfE5OjtkvB9Qxubu7IyYmBmfOnDH0nthYeQYHB6OmpgaFhYUNLkPW01ZlGBwcjOzsbLPt5+bmspw7iJCQEERERODMmTMAWK4d2eOPP46NGzdi586dCAsLM0zn+7XzaqhMLeF7tXPQaDTo2bMn4uLisGzZMsTGxuK9996z2fcpEzEboNFoMGTIEOzYscNk+o4dOzBy5EgrRUXNUV1djZMnTyIkJASRkZEIDg42Kc+amhrs3r3bUJ5DhgyBWq02WSYrKwuJiYks8w6grcpwxIgRKC4uxqFDhwzL/PrrryguLmY5dxD5+flIS0tDSEgIAJZrRySEwGOPPYZ169bh559/RmRkpMl8vl87nyuVqSV8r3ZOQghUV1fb7vu0XbsGoavmq6++Emq1WvznP/8RSUlJ4qmnnhLu7u7iwoUL1g6NLHj66afFrl27xPnz58XBgwfF5MmThaenp6G8Xn/9daHVasW6devE8ePHxb333itCQkJESUmJYRuPPPKICAsLEz/++KM4evSouP7660VsbKyora211mHZldLSUpGQkCASEhIEAPHOO++IhIQEcfHiRSFE25XhTTfdJAYMGCAOHDggDhw4IGJiYsTkyZPb/XjtRWPlWlpaKp5++mmxf/9+kZKSInbu3ClGjBghunTpwnLtwB599FGh1WrFrl27RFZWluFRUVFhWIbv187lSmXK92rn9Pzzz4s9e/aIlJQU8ccff4jFixcLBwcHsX37diGEbb5PmYjZkJUrV4qIiAih0WjE4MGDTbpxpY7l7rvvFiEhIUKtVovQ0FAxbdo0ceLECcN8vV4vlixZIoKDg4Wzs7O47rrrxPHjx022UVlZKR577DHh6+srXF1dxeTJk0Vqamp7H4rd2rlzpwBg9pg9e7YQou3KMD8/X8yYMUN4enoKT09PMWPGDFFYWNhOR2l/GivXiooKMX78eBEQECDUarXo2rWrmD17tlmZsVw7FkvlCUB88sknhmX4fu1crlSmfK92TnPmzDFcxwYEBIgbbrjBkIQJYZvvU5UQQrRf/RsRERERERHxHjEiIiIiIqJ2xkSMiIiIiIionTERIyIiIiIiamdMxIiIiIiIiNoZEzEiIiIiIqJ2xkSMiIiIiIionTERIyIiIiIiamdMxIiIiGzM0qVLoVKpsHTpUmuHQkREDWAiRkREnUZeXh5UKhWmTZtmmHb+/HmoVCrMmTPHipERERE1DxMxIiLqNA4cOAAAGDFihGHa/v37zaYRERF1dEzEiIio01ASsZEjRxqmKYlY/WlEREQdHRMxIiLqNA4cOACNRoMhQ4YYpu3btw9arRZRUVFWjIyIiKh5mIgREVGnUFdXh8OHD2PQoEFwcXEBAJSWliIxMRHDhw+HSqVq8bYrKirwxhtvIC4uDl5eXnBzc8PAgQPxj3/8A9XV1WbL1+8M49KlS5g7dy5CQ0Ph4uKCfv364a233kJtbW2D+9u/fz+mTZuGoKAgaDQahIWFYdasWTh58mSjce7YsQPTpk1DaGgonJ2dERoairFjx2LlypUW4wSA4uJiPPXUU+jatSucnZ3Rs2dPvPrqq43GR0REVx8TMSIi6rBUKpXh4eTkhPLycvz666+GaV5eXtDr9di2bZvJss2RkZGB+Ph4LFq0CL///juCgoLQrVs3nDhxAgsXLsSNN96IyspKi+vm5+dj6NCh+OyzzxAUFISIiAgkJyfj2WefxZ133gm9Xm+2zgcffIBrr70W69evBwDExsaivLwc//vf/zB48GBs2rTJ4r4ee+wxjB8/HuvXr0dNTQ0GDBgAjUaDPXv24LHHHkNWVpbZOsXFxRgxYgRWrlwJPz8/hIaG4ty5c/jb3/6GRx99tFnniYiI2hYTMSIi6rBGjx5teISHhwMABg0aZJgWEhICABg6dKjJsk2l1+tx1113ISkpCffccw/S09Nx5swZJCUlISUlBaNGjcLevXvxt7/9zeL6//rXv+Dt7Y2zZ88iISEBp06dwu7du6HVarFhwwZ88MEHJssfO3YMTzzxBIQQePPNN5GVlYXDhw/j0qVLmDdvHqqqqjBjxgyzpOq9997DypUr4ebmhv/973/IycnB4cOHceHCBeTm5uLtt9+Gu7u7WXwrV65EQEAALl68iISEBKSkpGDjxo1wdHTExx9/jOTk5CafKyIiamOCiIioE7jllluESqUSeXl5hmnXXXedcHFxEVVVVS3a5saNGwUAER8fL3Q6ndn8zMxM4eHhITw8PERFRYVh+pIlSwQAAUAcOXLEbL3ly5cLAKJbt25Cr9cbps+YMUMAEFOmTDFbR6/Xi/79+wsA4qWXXjJMr6ioEH5+fgKAWL16dZOOS4nP1dVVpKWlmc2fNm2aACDeeeedJm2PiIjaHmvEiIiow6urq8Pu3bsRGxsLPz8/AEBlZSUOHjyIkSNHwtnZuUXbXbduHQDg/vvvh5OTk9n8kJAQxMfHo6ysDEeOHDGbP2LECAwePNhs+pw5c+Di4oILFy7g1KlThunbt28HADz++ONm66hUKjzxxBMmywGyM5L8/HyEhoZixowZzTq+m266CWFhYWbT4+PjAcgx2IiIyDrMv3WIiIg6mN9++w0lJSW44YYbDNP27duHmpoaXH/99S3e7vHjxwHI+7a+/PJLi8ucPn0agLyX7HL9+vWzuI67uzvCw8Nx5swZnD59Gn379kVRURFyc3MBoMEeHvv372+yTwCGDjyGDh0KB4fm/X7ao0cPi9MDAwMBAGVlZc3aHhERtR0mYkRE1OEkJCSY1Brl5OQAAL7//nscPHgQAAz3UX3++efYsmULAHn/2Pvvv9/k/RQXFwMAEhMTr7ispQ47lITGkqCgIJw5cwalpaUATJOehtYLCgoCAMM6AFBSUgIA8Pb2vmKMl7N03xgAQ0InhGj2NomIqG0wESMiog6nuLgY+/btM5t+9uxZnD171mRa/Q4nLDUvbIyHhwcA2S38jTfe2Ow4lRouS5Tk0dPT02Rfyjylo5H6srOzTdap/3dRUVGz4yMioo6L94gREVGHM2bMGAghDA8/Pz8MHDjQ8H9VVRU0Gg1GjRplstyuXbuatR+liWBTasQsaWjcr4qKCqSmpgIAevfuDUDWaAUEBAAAkpKSLK534sQJk3UAY3PFw4cPW+wOn4iIOicmYkRE1KGdOnUK+fn5GDFihGHa0aNHUVNTg5EjR7Zq29OmTQMAfPjhh6iqqmr2+vv378exY8fMpv/3v/9FVVUVIiIi0KdPH8P0CRMmAIDF5pNCCMN0ZTkAuOaaa+Dv74+MjAysWbOm2TESEVHHxESMiIg6tAMHDgCASSKmTLvmmmtate2pU6di+PDhSE5Oxi233GLW7LG6uhqbNm3CnDlzLK7v5OSE+++/HxcvXjRMqz/u2DPPPGMywPTTTz8NJycnfP/993j77bcNNVw1NTV48sknkZiYCK1WazLYsouLC1566SUAwF/+8hesWbPG5N6uwsJC/POf/2y0mSQREXU8TMSIiKhD279/PwDLiVj9aS3h4OCAdevWYdCgQfjxxx/Rq1cv9OrVC8OHD0f//v3h5eWFyZMnY/PmzRbX/8tf/oKCggL07NkTgwYNQt++fTFq1CgUFhbilltuwbx580yWHzhwIJYvXw6VSoVnnnkGoaGhGDp0KIKCgvD+++/D2dkZX3zxBYKDg03We/zxx/Hoo4+ivLwc06dPR2BgIIYOHYrIyEgEBARgwYIFKC8vb9W5ICKi9sVEjIiIOrQDBw7A398fPXv2NJnWu3dv+Pv7t3r7ISEhOHDgAFatWoXrrrsO+fn5SEhIQGlpKYYOHYqXX34ZO3futLiuv78/Dh06hFmzZiE7OxspKSno06cP3njjDaxbt85id/OPPvoofvnlF9x2223Q6/U4duwY3NzccN999+Ho0aO4+eabzdZRqVRYtWoVNm3ahMmTJ0OlUuH333+HTqfD6NGjsWrVKoSGhrb6XBARUftRCfZdS0RE1CxLly7Fyy+/jCVLlmDp0qXWDoeIiDoh1ogRERERERG1MyZiRERERERE7YyJGBERERERUTtjIkZERERERNTO2FkHERERERFRO2ONGBERERERUTtjIkZERERERNTOmIgRERERERG1MyZiRERERERE7YyJGBERERERUTtjIkZERERERNTOmIgRERERERG1MyZiRERERERE7YyJGBERERERUTv7/0VOXNRS8stTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_trainloss = plt.figure(figsize=(10, 3))\n",
    "plt.plot(train_losses, color='r', label='train loss (log scale)')\n",
    "plt.plot(val_losses, color='b', label='valid loss (log scale)')\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"# epoch\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "if disp:\n",
    "    plt.show()\n",
    "if not (datasaver is None): datasaver.save_figure(fig_trainloss, \"train&valid_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a28645ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "RNN_valid = RNN_numpy(N=net_params[\"N\"],\n",
    "                      dt=net_params[\"dt\"],\n",
    "                      tau=net_params[\"tau\"],\n",
    "                      activation=numpify(activation),\n",
    "                      W_inp=net_params[\"W_inp\"],\n",
    "                      W_rec=net_params[\"W_rec\"],\n",
    "                      W_out=net_params[\"W_out\"],\n",
    "                      bias_rec=net_params[\"bias_rec\"],\n",
    "                      y_init=net_params[\"y_init\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a57a2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = PerformanceAnalyzer(RNN_valid)\n",
    "score_function = lambda x, y: np.mean((x - y) ** 2)\n",
    "input_batch_valid, target_batch_valid, conditions_valid = task.get_batch()\n",
    "score = analyzer.get_validation_score(score_function, input_batch_valid, target_batch_valid,\n",
    "                                      mask, sigma_rec=sigma_rec, sigma_inp=sigma_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daf932fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE validation: 0.07589\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE validation: {np.round(score, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcea4a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting random trials\n"
     ]
    }
   ],
   "source": [
    "print(f\"Plotting random trials\")\n",
    "inds = np.random.choice(np.arange(input_batch_valid.shape[-1]), 5)\n",
    "inputs = input_batch_valid[..., inds]\n",
    "targets = target_batch_valid[..., inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92639ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 120, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecb5e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_valid.clear_history()\n",
    "RNN_valid.run(input_timeseries=input_batch_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d6e9e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 120, 360)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53761701",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = RNN_valid.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b8c6366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 120, 360)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba1e8021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2909ab0a0>,\n",
       " <matplotlib.lines.Line2D at 0x2909ab100>,\n",
       " <matplotlib.lines.Line2D at 0x2909ab130>,\n",
       " <matplotlib.lines.Line2D at 0x2909ab220>,\n",
       " <matplotlib.lines.Line2D at 0x2909ab310>,\n",
       " <matplotlib.lines.Line2D at 0x2909ab400>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.round(output[:6, -1, :].T, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
