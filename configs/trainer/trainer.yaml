# @package trainer
_target_: trainRNNbrain.trainer.Trainer.Trainer
trainer_tag: "BaseTrainer"
max_iter: 3000
anneal_noise: False
lr: 0.001
weight_decay: 1e-6
max_grad_norm: 50.0
lr_scale_exp: 0.3333333333333333
same_batch: True
monitor: True
lambda_orth: 0.0
orth_args:
  orth_input_only: True
lambda_iwm: 0
iwm_args:
  cap100: 0.5
  gamma: 5.0
  eps: 1e-12
lambda_rwm: 0.0
rwm_args:
  cap100: 0.02
  account4dale: True
  N_ref: 100
  k_ref: 20
  gamma: 5.0
  eps: 1e-12
lambda_owm: 0
owm_args:
  cap100: 0.07
  gamma: 5.0
  eps: 1e-12
lambda_met: 0
met_args: {}
lambda_rws: 0.05 # sparsity
rws_args:
  tg_deg: 20
  eps: 1e-12
lambda_frm: 0.2 # magnitude of firing rates
frm_args:
  cap_fr: 0.3
  tau: 0.1
  g_top: 3.0
  g_bot: 3.0
  alpha: 1.0
  beta: 1.0
  eps: 1e-12
lambda_hm: 0.0 # magnitude of hidden activities
hm_args:
  h_thr: -0.1
  tau: 0.1
  g_top: 3.0
  g_bot: 3.0
  alpha: 1.0
  beta: 1.0
lambda_tv: 0 # trial output variability (across trials)
tv_args: {}
lambda_fri: 0
fri_args:
  method: 'hhi'
lambda_hi: 0
hi_args:
  method: 'hhi'
lambda_htvar: 0
htvar_args: {}
lambda_hlvar: 0
hlvar_args: {}
lambda_cl: 0
cl_args: {}
lambda_effdim: 0.0 # (0.05 - 0.1 good for dimensionality regularization)
effdim_args:
  k: 8
  eps: 1e-12
dropout: False
dropout_args:
  dropout_kind: "mute"
  sampling_method: "participation"
  eta: 0.5
  drop_rate: 0.05
  dropout_beta: 1.0
  activity_q: 0.9

