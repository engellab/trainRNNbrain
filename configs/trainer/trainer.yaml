# @package trainer
_target_: trainRNNbrain.trainer.Trainer.Trainer
trainer_tag: "BaseTrainer"
max_iter: 1500
anneal_noise: True
lr: 0.001
weight_decay: 1e-6
max_grad_norm: 50.0
lr_scale_exp: 0.3333333333333333
same_batch: False
monitor: True
lambda_orth: 0.3
orth_args:
  orth_input_only: True
lambda_iwm: 0
iwm_args:
  cap100: 0.5
  gamma: 5.0
  eps: 1e-12
lambda_rwm: 0.0
rwm_args:
  cap100: 0.07
  N_ref: 100
  k_ref: 20
  gamma: 5.0
  eps: 1e-12
lambda_ow: 0
ow_args:
  c: 2.0
  cap100: 0.3
  alpha: 1.0
  gamma: 5.0
  eps: 1e-12
lambda_met: 0
met_args: {}
lambda_rws: 0.05 # sparsity
rws_args:
  tg_deg: 20
  eps: 1e-12
lambda_sm: 0.0 # magnitude of firing rates
sm_args:
  cap_s: 0.3
  quantile_kind: 'logsumexp'
  p: 15
  q: 0.9
  tau: 0.1
  penalty_type: 'additive'
  g_top: 3.0
  g_bot: 3.0
  alpha: 1.0
  beta: 1.0
  eps: 1e-12
lambda_hm: 0.0 # magnitude of hidden activities
hm_args:
  h_thr: -0.3
  quantile_kind: 'logsumexp'
  p: 15
  q: 0.9
  tau: 0.1
  penalty_type: 'additive'
lambda_tv: 0 # trial output variability (across trials)
tv_args: {}
lambda_si: 0 
si_args:
  method: 'hhi'
lambda_hi: 0
hi_args:
  method: 'hhi'
lambda_htvar: 0
htvar_args: {}
lambda_hlvar: 0
hlvar_args: {}
lambda_cl: 0
cl_args: {}
lambda_effdim: 0.0 # (0.05 - 0.1 good for dimensionality regularization)
effdim_args:
  k: 8
  eps: 1e-12
dropout: False
dropout_args:
  dropout_kind: "mute"
  sampling_method: "participation"
  eta: 0.5
  drop_rate: 0.05
  dropout_beta: 1.0
  activity_q: 0.9

